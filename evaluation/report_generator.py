"""
è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
"""
from typing import Dict
from pathlib import Path
import json
from utils.evaluation_utils import format_percentage


class EvaluationReport:
    """è¯„ä¼°æŠ¥å‘Šç±»"""
    
    def __init__(self, metrics: Dict):
        """
        åˆå§‹åŒ–æŠ¥å‘Š
        
        Args:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        self.metrics = metrics
    
    def print_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "=" * 70)
        print("è¯„ä¼°ç»“æœæ‘˜è¦")
        print("=" * 70)
        
        total = self.metrics['total_samples']
        overall_acc = self.metrics['overall_accuracy']
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ ·æœ¬æ€»æ•°: {total}")
        print(f"  æ€»ä½“å‡†ç¡®ç‡ (Overall Accuracy): {format_percentage(overall_acc)}")
        
        # Flakyæ£€æµ‹æŒ‡æ ‡
        flaky_metrics = self.metrics['flaky_detection']
        print(f"\nğŸ” Flakyæ£€æµ‹æŒ‡æ ‡:")
        print(f"  å‡†ç¡®ç‡ (Accuracy): {format_percentage(flaky_metrics['accuracy'])}")
        print(f"  ç²¾ç¡®ç‡ (Precision): {format_percentage(flaky_metrics['precision'])}")
        print(f"  å¬å›ç‡ (Recall): {format_percentage(flaky_metrics['recall'])}")
        print(f"  F1åˆ†æ•°: {format_percentage(flaky_metrics['f1'])}")
        
        # æ··æ·†çŸ©é˜µ
        cm = flaky_metrics['confusion_matrix']
        print(f"\n  æ··æ·†çŸ©é˜µ:")
        print(f"                é¢„æµ‹Flaky  é¢„æµ‹Non-Flaky")
        print(f"  å®é™…Flaky      {cm['tp']:>6}      {cm['fn']:>6}")
        print(f"  å®é™…Non-Flaky  {cm['fp']:>6}      {cm['tn']:>6}")
        
        # ç±»åˆ«åˆ†ç±»æŒ‡æ ‡
        category_metrics = self.metrics['category_classification']
        print(f"\nğŸ“‹ ç±»åˆ«åˆ†ç±»æŒ‡æ ‡:")
        print(f"  åˆ†ç±»å‡†ç¡®ç‡: {format_percentage(category_metrics['accuracy'])}")
        
        print(f"\n  å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
        print(f"  {'ç±»åˆ«':<15} {'æ ·æœ¬æ•°':>8} {'å‡†ç¡®ç‡':>10} {'ç²¾ç¡®ç‡':>10} {'å¬å›ç‡':>10} {'F1':>10}")
        print(f"  {'-'*15} {'-'*8} {'-'*10} {'-'*10} {'-'*10} {'-'*10}")
        
        for category, stats in category_metrics['per_category'].items():
            print(f"  {category:<15} {stats['total']:>8} "
                  f"{format_percentage(stats['accuracy']):>10} "
                  f"{format_percentage(stats['precision']):>10} "
                  f"{format_percentage(stats['recall']):>10} "
                  f"{format_percentage(stats['f1']):>10}")
        
        print("=" * 70)
    
    def print_detailed(self):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        self.print_summary()
        
        print("\n" + "=" * 70)
        print("è¯¦ç»†åˆ†æ")
        print("=" * 70)
        
        # å„ç±»åˆ«çš„æ”¯æŒåº¦ï¼ˆæ ·æœ¬æ•°ï¼‰
        category_metrics = self.metrics['category_classification']['per_category']
        print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
        total = self.metrics['total_samples']
        
        for category, stats in sorted(category_metrics.items(), 
                                      key=lambda x: x[1]['total'], 
                                      reverse=True):
            count = stats['total']
            percentage = count / total * 100 if total > 0 else 0
            bar_length = int(percentage / 2)
            bar = 'â–ˆ' * bar_length
            print(f"  {category:<15} {count:>4} ({percentage:>5.2f}%) {bar}")
        
        # æ€§èƒ½åˆ†æ
        print(f"\nğŸ“ˆ æ€§èƒ½åˆ†æ:")
        
        flaky_f1 = self.metrics['flaky_detection']['f1']
        category_acc = self.metrics['category_classification']['accuracy']
        
        if flaky_f1 >= 0.9:
            print(f"  âœ… Flakyæ£€æµ‹æ€§èƒ½ä¼˜ç§€ (F1={format_percentage(flaky_f1)})")
        elif flaky_f1 >= 0.7:
            print(f"  âœ“ Flakyæ£€æµ‹æ€§èƒ½è‰¯å¥½ (F1={format_percentage(flaky_f1)})")
        else:
            print(f"  âš  Flakyæ£€æµ‹æ€§èƒ½éœ€è¦æ”¹è¿› (F1={format_percentage(flaky_f1)})")
        
        if category_acc >= 0.8:
            print(f"  âœ… ç±»åˆ«åˆ†ç±»æ€§èƒ½ä¼˜ç§€ (Acc={format_percentage(category_acc)})")
        elif category_acc >= 0.6:
            print(f"  âœ“ ç±»åˆ«åˆ†ç±»æ€§èƒ½è‰¯å¥½ (Acc={format_percentage(category_acc)})")
        else:
            print(f"  âš  ç±»åˆ«åˆ†ç±»æ€§èƒ½éœ€è¦æ”¹è¿› (Acc={format_percentage(category_acc)})")
        
        # é”™è¯¯æ¡ˆä¾‹åˆ†æ
        if 'error_cases' in self.metrics and self.metrics['error_cases']:
            self._print_error_cases()
        
        print("=" * 70)
    
    def _print_error_cases(self):
        """æ‰“å°é”™è¯¯æ¡ˆä¾‹è¯¦æƒ…"""
        error_cases = self.metrics['error_cases']
        
        print(f"\nâŒ é”™è¯¯æ¡ˆä¾‹è¯¦æƒ… (å…± {len(error_cases)} ä¸ª):")
        print("=" * 70)
        
        # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„
        flaky_errors = [e for e in error_cases if e['error_type'] in ['flaky', 'both']]
        category_errors = [e for e in error_cases if e['error_type'] in ['category', 'both']]
        
        if flaky_errors:
            print(f"\nğŸ”´ Flakyåˆ¤æ–­é”™è¯¯ ({len(flaky_errors)} ä¸ª):")
            print(f"{'ID':<10} {'é¢„æµ‹ç»“æœ':<25} {'å®é™…ç»“æœ':<25}")
            print("-" * 70)
            for case in flaky_errors[:20]:  # æœ€å¤šæ˜¾ç¤º20ä¸ª
                print(f"{case['id']:<10} {case['predicted']:<25} {case['actual']:<25}")
            if len(flaky_errors) > 20:
                print(f"... è¿˜æœ‰ {len(flaky_errors) - 20} ä¸ªé”™è¯¯æ¡ˆä¾‹")
        
        if category_errors and category_errors != flaky_errors:
            print(f"\nğŸŸ¡ ç±»åˆ«åˆ¤æ–­é”™è¯¯ ({len(category_errors)} ä¸ª):")
            print(f"{'ID':<10} {'é¢„æµ‹ç»“æœ':<25} {'å®é™…ç»“æœ':<25}")
            print("-" * 70)
            for case in category_errors[:20]:  # æœ€å¤šæ˜¾ç¤º20ä¸ª
                print(f"{case['id']:<10} {case['predicted']:<25} {case['actual']:<25}")
            if len(category_errors) > 20:
                print(f"... è¿˜æœ‰ {len(category_errors) - 20} ä¸ªé”™è¯¯æ¡ˆä¾‹")
        
        # ç»Ÿè®¡é”™è¯¯ç±»å‹åˆ†å¸ƒ
        error_type_counts = {}
        for case in error_cases:
            pred_type = case['predicted'].split(' - ')[1] if ' - ' in case['predicted'] else case['predicted']
            actual_type = case['actual'].split(' - ')[1] if ' - ' in case['actual'] else case['actual']
            key = f"{actual_type} â†’ {pred_type}"
            error_type_counts[key] = error_type_counts.get(key, 0) + 1
        
        if error_type_counts:
            print(f"\nğŸ“Š é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
            for error_type, count in sorted(error_type_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  {error_type:<30} {count:>4} ä¸ª")
    
    def save_to_json(self, output_file: Path):
        """
        ä¿å­˜æŠ¥å‘Šä¸ºJSONæ ¼å¼
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        from utils import save_json
        save_json(self.metrics, output_file)
    
    def save_to_text(self, output_file: Path):
        """
        ä¿å­˜æŠ¥å‘Šä¸ºæ–‡æœ¬æ ¼å¼
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import sys
        from io import StringIO
        
        # æ•è·printè¾“å‡º
        old_stdout = sys.stdout
        sys.stdout = StringIO()
        
        self.print_detailed()
        
        output = sys.stdout.getvalue()
        sys.stdout = old_stdout
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(output)
        
        print(f"âœ“ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
