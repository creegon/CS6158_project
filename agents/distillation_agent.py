"""
DistillationAgent - æ•°æ®è’¸é¦Agent
ç”¨äºç”ŸæˆåŒ…å«æ¨ç†è¿‡ç¨‹çš„è®­ç»ƒæ•°æ®é›†
"""
import time
from pathlib import Path
from typing import Optional, Union, List, Dict
from tqdm import tqdm
import pandas as pd

from agents.base_agent import BaseAgent
from utils import (
    load_csv,
    sample_data,
    convert_to_alpaca_format,
    save_json,
    load_prompt,
    format_prompt
)
from config import (
    DATASET_PATH,
    OUTPUT_DIR,
    API_BATCH_SIZE,
    API_BATCH_DELAY,
    CHECKPOINT_INTERVAL
)


class DistillationAgent(BaseAgent):
    """
    æ•°æ®è’¸é¦Agent
    è¯»å–åŸå§‹æµ‹è¯•æ•°æ®ï¼Œç”ŸæˆåŒ…å«æ¨ç†è¿‡ç¨‹çš„Alpacaæ ¼å¼æ•°æ®é›†
    """
    
    def __init__(self,
                 dataset_path: Optional[Union[str, Path]] = None,
                 output_dir: Optional[Union[str, Path]] = None,
                 test_mode: str = 'all',
                 test_size: int = 10,
                 random_seed: Optional[int] = None,
                 code_column: str = 'code',
                 batch_size: Optional[int] = None,
                 batch_delay: Optional[float] = None,
                 checkpoint_interval: Optional[int] = None,
                 **kwargs):
        """
        åˆå§‹åŒ–DistillationAgent
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            test_mode: æµ‹è¯•æ¨¡å¼ ['all', 'first', 'last', 'random']
            test_size: æµ‹è¯•æ—¶ä½¿ç”¨çš„æ•°æ®é‡
            random_seed: éšæœºç§å­
            code_column: ä»£ç åˆ—å
            batch_size: æ‰¹æ¬¡å¤§å°
            batch_delay: æ‰¹æ¬¡å»¶è¿Ÿï¼ˆç§’ï¼‰
            checkpoint_interval: æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”
            **kwargs: ä¼ é€’ç»™BaseAgentçš„å…¶ä»–å‚æ•°
        """
        super().__init__(**kwargs)
        
        self.dataset_path = Path(dataset_path) if dataset_path else DATASET_PATH
        self.output_dir = Path(output_dir) if output_dir else OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.test_mode = test_mode
        self.test_size = test_size
        self.random_seed = random_seed
        self.code_column = code_column
        
        self.batch_size = batch_size or API_BATCH_SIZE
        self.batch_delay = batch_delay if batch_delay is not None else API_BATCH_DELAY
        self.checkpoint_interval = checkpoint_interval or CHECKPOINT_INTERVAL
        
        # åŠ è½½promptæ¨¡æ¿
        self.system_prompt = load_prompt('distillation_system')
        self.user_template = load_prompt('distillation_user')
        
        # å­˜å‚¨ç»“æœ
        self.distilled_dataset: List[Dict] = []
        self.failed_indices: List[int] = []
    
    def get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        return "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æµ‹è¯•ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææµ‹è¯•ä»£ç å¹¶è¯†åˆ«Flaky Testsã€‚"
    
    def generate_user_prompt(self, row: pd.Series) -> str:
        """
        ç”Ÿæˆç”¨æˆ·æç¤ºè¯
        
        Args:
            row: æ•°æ®è¡Œ
            
        Returns:
            æ ¼å¼åŒ–çš„ç”¨æˆ·æç¤ºè¯
        """
        test_code = row.get(self.code_column, row.get('full_code', ''))
        return format_prompt(self.user_template, test_code=test_code)
    
    def process_single_row(self, idx: int, row: pd.Series) -> Optional[Dict]:
        """
        å¤„ç†å•æ¡æ•°æ®
        
        Args:
            idx: æ•°æ®ç´¢å¼•
            row: æ•°æ®è¡Œ
            
        Returns:
            Alpacaæ ¼å¼çš„æ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        # ç”Ÿæˆprompt
        user_prompt = self.generate_user_prompt(row)
        
        # è°ƒç”¨APIè·å–æ¨ç†è¿‡ç¨‹
        reasoning = self.call_api(user_prompt)
        
        if reasoning is None:
            print(f"\nâš  ç¬¬ {idx} æ¡æ•°æ®å¤„ç†å¤±è´¥")
            self.failed_indices.append(idx)
            return None
        
        # è½¬æ¢ä¸ºAlpacaæ ¼å¼
        alpaca_item = convert_to_alpaca_format(row, reasoning, self.code_column)
        return alpaca_item
    
    def save_checkpoint(self, checkpoint_name: str = 'checkpoint') -> None:
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        
        Args:
            checkpoint_name: æ£€æŸ¥ç‚¹æ–‡ä»¶å
        """
        checkpoint_file = self.output_dir / f"{checkpoint_name}.json"
        save_json(self.distilled_dataset, checkpoint_file)
    
    def run(self,
            dataset_path: Optional[Union[str, Path]] = None,
            output_name: str = 'distillation_dataset') -> Dict:
        """
        æ‰§è¡Œè’¸é¦ä»»åŠ¡
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–å‚æ•°ï¼‰
            output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            åŒ…å«ç»“æœç»Ÿè®¡çš„å­—å…¸
        """
        # åŠ è½½æ•°æ®
        if dataset_path:
            self.dataset_path = Path(dataset_path)
        
        print("\n" + "=" * 60)
        print("å¼€å§‹æ•°æ®è’¸é¦ä»»åŠ¡")
        print("=" * 60)
        
        df = load_csv(self.dataset_path)
        
        # æ ¹æ®æµ‹è¯•æ¨¡å¼é‡‡æ ·æ•°æ®
        if self.test_mode != 'all':
            print(f"\nğŸ“Š æµ‹è¯•æ¨¡å¼: {self.test_mode}, é‡‡æ · {self.test_size} æ¡æ•°æ®")
            df = sample_data(df, mode=self.test_mode, n=self.test_size, random_seed=self.random_seed)
        
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(df)} æ¡æ•°æ®...")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"   æ‰¹æ¬¡å»¶è¿Ÿ: {self.batch_delay}ç§’")
        print(f"   æ£€æŸ¥ç‚¹é—´éš”: {self.checkpoint_interval}æ¡")
        print("=" * 60 + "\n")
        
        # é‡ç½®ç»“æœ
        self.distilled_dataset = []
        self.failed_indices = []
        self.reset_stats()
        
        # å¤„ç†æ•°æ®
        start_time = time.time()
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è¿›åº¦"):
            alpaca_item = self.process_single_row(idx, row)
            
            if alpaca_item:
                self.distilled_dataset.append(alpaca_item)
            
            # æ‰¹æ¬¡å»¶è¿Ÿ
            if (idx + 1) % self.batch_size == 0:
                time.sleep(self.batch_delay)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (idx + 1) % self.checkpoint_interval == 0:
                self.save_checkpoint('temp_checkpoint')
                print(f"\nâœ“ å·²å¤„ç† {idx + 1} æ¡ï¼Œæ£€æŸ¥ç‚¹å·²ä¿å­˜")
        
        elapsed_time = time.time() - start_time
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_file = self.output_dir / f"{output_name}.json"
        save_json(self.distilled_dataset, output_file)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("è’¸é¦ä»»åŠ¡å®Œæˆ")
        print("=" * 60)
        print(f"âœ“ æˆåŠŸ: {len(self.distilled_dataset)} æ¡")
        print(f"âœ— å¤±è´¥: {len(self.failed_indices)} æ¡")
        print(f"â± è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print("=" * 60)
        
        # æ‰“å°APIç»Ÿè®¡
        self.print_stats()
        
        # æ˜¾ç¤ºç¤ºä¾‹
        if self.distilled_dataset:
            print("\nç¤ºä¾‹æ•°æ®:")
            print("=" * 60)
            import json
            print(json.dumps(self.distilled_dataset[0], ensure_ascii=False, indent=2))
            print("=" * 60)
        
        return {
            "success_count": len(self.distilled_dataset),
            "failed_count": len(self.failed_indices),
            "failed_indices": self.failed_indices,
            "elapsed_time": elapsed_time,
            "output_file": str(output_file),
            "api_stats": self.get_stats()
        }
