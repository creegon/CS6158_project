"""
DistillationAgent - æ•°æ®è’¸é¦Agent
ç”¨äºç”ŸæˆåŒ…å«æ¨ç†è¿‡ç¨‹çš„è®­ç»ƒæ•°æ®é›†
"""
import time
from pathlib import Path
from typing import Optional, Union, List, Dict
from tqdm import tqdm
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from agents.base_agent import BaseAgent
from utils import (
    load_csv,
    sample_data,
    convert_to_alpaca_format,
    save_json,
    load_prompt,
    format_prompt
)
from config import (
    DATASET_PATH,
    OUTPUT_DIR,
    API_BATCH_SIZE,
    API_BATCH_DELAY,
    CHECKPOINT_INTERVAL
)


class DistillationAgent(BaseAgent):
    """
    æ•°æ®è’¸é¦Agent
    è¯»å–åŸå§‹æµ‹è¯•æ•°æ®ï¼Œç”ŸæˆåŒ…å«æ¨ç†è¿‡ç¨‹çš„Alpacaæ ¼å¼æ•°æ®é›†
    """
    
    def __init__(self,
                 dataset_path: Optional[Union[str, Path]] = None,
                 output_dir: Optional[Union[str, Path]] = None,
                 test_mode: str = 'all',
                 test_size: int = 10,
                 random_seed: Optional[int] = None,
                 code_column: str = 'code',
                 batch_size: Optional[int] = None,
                 batch_delay: Optional[float] = None,
                 checkpoint_interval: Optional[int] = None,
                 parallel_workers: int = 1,
                 api_matcher=None,
                 top_k_shots: int = 3,
                 **kwargs):
        """
        åˆå§‹åŒ–DistillationAgent
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            test_mode: æµ‹è¯•æ¨¡å¼ ['all', 'first', 'last', 'random']
            test_size: æµ‹è¯•æ—¶ä½¿ç”¨çš„æ•°æ®é‡
            random_seed: éšæœºç§å­
            code_column: ä»£ç åˆ—å
            batch_size: æ‰¹æ¬¡å¤§å°
            batch_delay: æ‰¹æ¬¡å»¶è¿Ÿï¼ˆç§’ï¼‰
            checkpoint_interval: æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”
            parallel_workers: å¹¶è¡Œæ¨ç†çš„å·¥ä½œçº¿ç¨‹æ•°ï¼Œ1è¡¨ç¤ºä¸²è¡Œå¤„ç†
            api_matcher: APIç­¾ååŒ¹é…å™¨ï¼Œç”¨äºæ£€ç´¢few-shot examples
            top_k_shots: ä½¿ç”¨çš„few-shotæ ·æœ¬æ•°é‡
            **kwargs: ä¼ é€’ç»™BaseAgentçš„å…¶ä»–å‚æ•°
        """
        super().__init__(**kwargs)
        
        self.dataset_path = Path(dataset_path) if dataset_path else DATASET_PATH
        self.output_dir = Path(output_dir) if output_dir else OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.test_mode = test_mode
        self.test_size = test_size
        self.random_seed = random_seed
        self.code_column = code_column
        
        self.batch_size = batch_size or API_BATCH_SIZE
        self.batch_delay = batch_delay if batch_delay is not None else API_BATCH_DELAY
        self.checkpoint_interval = checkpoint_interval or CHECKPOINT_INTERVAL
        self.parallel_workers = max(1, parallel_workers)  # ç¡®ä¿è‡³å°‘ä¸º1
        
        # APIåŒ¹é…ç›¸å…³
        self.api_matcher = api_matcher
        self.top_k_shots = top_k_shots if api_matcher else 0
        
        # åŠ è½½promptæ¨¡æ¿
        self.system_prompt = load_prompt('distillation_system')
        self.user_template = load_prompt('distillation_user')
        
        # å­˜å‚¨ç»“æœ
        self.distilled_dataset: List[Dict] = []
        self.failed_indices: List[int] = []
        
        # çº¿ç¨‹å®‰å…¨é”
        self._lock = threading.Lock()
    
    def get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        return "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æµ‹è¯•ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææµ‹è¯•ä»£ç å¹¶è¯†åˆ«Flaky Testsã€‚"
    
    def generate_user_prompt(self, row: pd.Series) -> str:
        """
        ç”Ÿæˆç”¨æˆ·æç¤ºè¯ï¼ˆæ”¯æŒAPIåŒ¹é…çš„few-shot examplesï¼‰
        
        Args:
            row: æ•°æ®è¡Œ
            
        Returns:
            æ ¼å¼åŒ–çš„ç”¨æˆ·æç¤ºè¯
        """
        prompt, _ = self.generate_user_prompt_with_examples(row)
        return prompt
    
    def generate_user_prompt_with_examples(self, row: pd.Series) -> tuple:
        """
        ç”Ÿæˆç”¨æˆ·æç¤ºè¯å¹¶è¿”å›few-shot examplesï¼ˆæ”¯æŒAPIåŒ¹é…ï¼‰
        
        Args:
            row: æ•°æ®è¡Œ
            
        Returns:
            (æ ¼å¼åŒ–çš„ç”¨æˆ·æç¤ºè¯, few-shot examplesåˆ—è¡¨)
        """
        full_code = row.get(self.code_column, row.get('full_code', ''))
        project = row.get('project', 'Unknown')
        test_name = row.get('test_name', 'Unknown')
        
        # åŸºç¡€prompt
        base_prompt = format_prompt(
            self.user_template, 
            project=project,
            test_name=test_name,
            full_code=full_code
        )
        
        few_shot_examples = None
        
        # å¦‚æœå¯ç”¨APIåŒ¹é…ï¼Œæ·»åŠ few-shot examples
        if self.api_matcher and self.top_k_shots > 0:
            try:
                # æ£€ç´¢æœ€ç›¸ä¼¼çš„æ¡ˆä¾‹
                similar_cases = self.api_matcher.retrieve_top_k(
                    full_code, 
                    top_k=self.top_k_shots,
                    min_similarity=0.1  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
                )
                
                if similar_cases:
                    # æ„å»ºfew-shot examplesè®°å½•ï¼ˆç”¨äºdebugï¼‰
                    few_shot_examples = []
                    for i, (idx, similarity, case_row) in enumerate(similar_cases, 1):
                        example_info = {
                            'index': int(idx),
                            'similarity': float(similarity),
                            'project': str(case_row.get('project', 'Unknown')),
                            'test_name': str(case_row.get('test_name', 'Unknown')),
                            'category': int(case_row.get('category', -1)),
                            'code_preview': str(case_row.get(self.code_column, case_row.get('full_code', ''))[:200])
                        }
                        if 'id' in case_row:
                            example_info['id'] = int(case_row['id'])
                        few_shot_examples.append(example_info)
                    
                    # æ„å»ºfew-shot examplesæ–‡æœ¬ï¼ˆæ’å…¥promptï¼‰
                    examples_text = "\n\nå‚è€ƒæ¡ˆä¾‹ï¼ˆæ ¹æ®APIç­¾åç›¸ä¼¼åº¦æ£€ç´¢ï¼‰ï¼š\n"
                    examples_text += "=" * 60 + "\n"
                    
                    for i, (idx, similarity, case_row) in enumerate(similar_cases, 1):
                        case_code = case_row.get(self.code_column, case_row.get('full_code', ''))
                        case_category = case_row.get('category', 'Unknown')
                        case_project = case_row.get('project', 'Unknown')
                        
                        examples_text += f"\nã€æ¡ˆä¾‹ {i}ã€‘(ç›¸ä¼¼åº¦: {similarity:.2f})\n"
                        examples_text += f"é¡¹ç›®: {case_project}\n"
                        examples_text += f"åˆ†ç±»: {case_category}\n"
                        examples_text += f"ä»£ç :\n{case_code}\n"
                        examples_text += "-" * 60 + "\n"
                    
                    # å°†few-shot examplesæ’å…¥åˆ°promptä¸­
                    # åœ¨åŸå§‹ä»£ç ä¹‹å‰æ’å…¥å‚è€ƒæ¡ˆä¾‹
                    base_prompt = base_prompt.replace(
                        full_code,
                        examples_text + "\nå¾…åˆ†æçš„æµ‹è¯•ä»£ç :\n" + full_code
                    )
            
            except Exception as e:
                print(f"âš  APIåŒ¹é…å¤±è´¥: {e}")
                # å¦‚æœåŒ¹é…å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸºç¡€prompt
        
        return base_prompt, few_shot_examples
    
    def process_single_row(self, idx: int, row: pd.Series, include_id: bool = False) -> Optional[Dict]:
        """
        å¤„ç†å•æ¡æ•°æ®
        
        Args:
            idx: æ•°æ®ç´¢å¼•
            row: æ•°æ®è¡Œ
            include_id: æ˜¯å¦åŒ…å«IDå­—æ®µ
            
        Returns:
            Alpacaæ ¼å¼çš„æ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        # ç”Ÿæˆpromptï¼ˆåŒæ—¶è·å–few-shot examplesï¼‰
        user_prompt, few_shot_examples = self.generate_user_prompt_with_examples(row)
        
        # è°ƒç”¨APIè·å–æ¨ç†è¿‡ç¨‹
        reasoning = self.call_api(user_prompt)
        
        if reasoning is None:
            with self._lock:
                print(f"\nâš  ç¬¬ {idx} æ¡æ•°æ®å¤„ç†å¤±è´¥")
                self.failed_indices.append(idx)
            return None
        
        # è½¬æ¢ä¸ºAlpacaæ ¼å¼ï¼Œä¼ å…¥ system_promptã€user_template å’Œ few_shot_examples
        alpaca_item = convert_to_alpaca_format(
            row, 
            reasoning, 
            self.code_column, 
            include_id=include_id,
            system_prompt=self.system_prompt,
            user_template=self.user_template,
            few_shot_examples=few_shot_examples
        )
        return alpaca_item
    
    def process_single_row_with_index(self, task: tuple) -> tuple:
        """
        å¤„ç†å•æ¡æ•°æ®ï¼ˆå¸¦ç´¢å¼•ï¼Œç”¨äºå¹¶è¡Œå¤„ç†ï¼‰
        
        Args:
            task: (idx, row, include_id) å…ƒç»„
            
        Returns:
            (idx, alpaca_item) å…ƒç»„
        """
        idx, row, include_id = task
        alpaca_item = self.process_single_row(idx, row, include_id=include_id)
        return (idx, alpaca_item)
    
    def save_checkpoint(self, checkpoint_name: str = 'checkpoint') -> None:
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        
        Args:
            checkpoint_name: æ£€æŸ¥ç‚¹æ–‡ä»¶å
        """
        checkpoint_file = self.output_dir / f"{checkpoint_name}.json"
        save_json(self.distilled_dataset, checkpoint_file)
    
    def run(self,
            dataset_path: Optional[Union[str, Path]] = None,
            output_name: str = 'distillation_dataset') -> Dict:
        """
        æ‰§è¡Œè’¸é¦ä»»åŠ¡
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–å‚æ•°ï¼‰
            output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            åŒ…å«ç»“æœç»Ÿè®¡çš„å­—å…¸
        """
        # åŠ è½½æ•°æ®
        if dataset_path:
            self.dataset_path = Path(dataset_path)
        
        print("\n" + "=" * 60)
        print("å¼€å§‹æ•°æ®è’¸é¦ä»»åŠ¡")
        print("=" * 60)
        
        df = load_csv(self.dataset_path)
        
        # æ ¹æ®æµ‹è¯•æ¨¡å¼é‡‡æ ·æ•°æ®
        if self.test_mode != 'all':
            print(f"\nğŸ“Š æµ‹è¯•æ¨¡å¼: {self.test_mode}, é‡‡æ · {self.test_size} æ¡æ•°æ®")
            df = sample_data(df, mode=self.test_mode, n=self.test_size, random_seed=self.random_seed)
        
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(df)} æ¡æ•°æ®...")
        print(f"   å¹¶è¡Œçº¿ç¨‹æ•°: {self.parallel_workers}")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"   æ‰¹æ¬¡å»¶è¿Ÿ: {self.batch_delay}ç§’")
        print(f"   æ£€æŸ¥ç‚¹é—´éš”: {self.checkpoint_interval}æ¡")
        print("=" * 60 + "\n")
        
        # é‡ç½®ç»“æœ
        self.distilled_dataset = []
        self.failed_indices = []
        self.reset_stats()
        
        # å¤„ç†æ•°æ®
        start_time = time.time()
        
        if self.parallel_workers == 1:
            # ä¸²è¡Œå¤„ç†
            self._run_serial(df)
        else:
            # å¹¶è¡Œå¤„ç†
            self._run_parallel(df)
        
        elapsed_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        # å¦‚æœä½¿ç”¨äº†APIåŒ¹é…ï¼Œæ•°æ®å·²åŒ…å«idå’Œfew_shot_examplesï¼Œä¿å­˜ä¸ºexternalç‰ˆæœ¬
        # å¦åˆ™åªä¿å­˜æ ‡å‡†ç‰ˆæœ¬
        if self.api_matcher is not None:
            # ä½¿ç”¨APIåŒ¹é…æ—¶ï¼Œä¿å­˜ä¸ºexternalç‰ˆæœ¬ï¼ˆåŒ…å«idå’Œfew-shot examplesï¼‰
            output_file_external = self.output_dir / f"{output_name}_external.json"
            save_json(self.distilled_dataset, output_file_external)
            
            # åŒæ—¶ä¿å­˜ä¸€ä¸ªä¸å¸¦é¢å¤–ä¿¡æ¯çš„æ ‡å‡†ç‰ˆæœ¬ï¼ˆç”¨äºè®­ç»ƒï¼‰
            print("\nğŸ”„ ç”Ÿæˆæ ‡å‡†è®­ç»ƒæ•°æ®é›†ï¼ˆç§»é™¤é¢å¤–ä¿¡æ¯ï¼‰...")
            dataset_standard = []
            for item in self.distilled_dataset:
                standard_item = {
                    'instruction': item['instruction'],
                    'input': item['input'],
                    'output': item['output']
                }
                dataset_standard.append(standard_item)
            output_file = self.output_dir / f"{output_name}.json"
            save_json(dataset_standard, output_file)
        else:
            # æœªä½¿ç”¨APIåŒ¹é…æ—¶ï¼Œåªä¿å­˜æ ‡å‡†ç‰ˆæœ¬
            output_file = self.output_dir / f"{output_name}.json"
            save_json(self.distilled_dataset, output_file)
            output_file_external = None
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("è’¸é¦ä»»åŠ¡å®Œæˆ")
        print("=" * 60)
        print(f"âœ“ æˆåŠŸ: {len(self.distilled_dataset)} æ¡")
        print(f"âœ— å¤±è´¥: {len(self.failed_indices)} æ¡")
        print(f"â± è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {len(df) / elapsed_time:.2f} æ¡/ç§’")
        if self.api_matcher is not None:
            print(f"ğŸ“ æ ‡å‡†è¾“å‡º: {output_file}")
            print(f"ğŸ“ é¢å¤–ä¿¡æ¯è¾“å‡º: {output_file_external}")
        else:
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print("=" * 60)
        
        # æ‰“å°APIç»Ÿè®¡
        self.print_stats()
        
        # æ˜¾ç¤ºç¤ºä¾‹
        if self.distilled_dataset:
            print("\nç¤ºä¾‹æ•°æ®:")
            print("=" * 60)
            import json
            print(json.dumps(self.distilled_dataset[0], ensure_ascii=False, indent=2))
            print("=" * 60)
        
        return {
            "success_count": len(self.distilled_dataset),
            "failed_count": len(self.failed_indices),
            "failed_indices": self.failed_indices,
            "elapsed_time": elapsed_time,
            "output_file": str(output_file),
            "output_file_external": str(output_file_external) if output_file_external else None,
            "api_stats": self.get_stats()
        }
    
    def _run_serial(self, df: pd.DataFrame) -> None:
        """
        ä¸²è¡Œå¤„ç†æ•°æ®
        
        Args:
            df: è¦å¤„ç†çš„æ•°æ®æ¡†
        """
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è¿›åº¦"):
            # å¦‚æœå¯ç”¨APIåŒ¹é…ï¼Œç›´æ¥ç”ŸæˆåŒ…å«é¢å¤–ä¿¡æ¯çš„ç‰ˆæœ¬
            include_id = self.api_matcher is not None
            alpaca_item = self.process_single_row(idx, row, include_id=include_id)
            
            if alpaca_item:
                self.distilled_dataset.append(alpaca_item)
            
            # æ‰¹æ¬¡å»¶è¿Ÿ
            if (idx + 1) % self.batch_size == 0:
                time.sleep(self.batch_delay)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (idx + 1) % self.checkpoint_interval == 0:
                self.save_checkpoint('temp_checkpoint')
                print(f"\nâœ“ å·²å¤„ç† {idx + 1} æ¡ï¼Œæ£€æŸ¥ç‚¹å·²ä¿å­˜")
    
    def _run_parallel(self, df: pd.DataFrame) -> None:
        """
        å¹¶è¡Œå¤„ç†æ•°æ®
        
        Args:
            df: è¦å¤„ç†çš„æ•°æ®æ¡†
        """
        # å¦‚æœå¯ç”¨APIåŒ¹é…ï¼Œç›´æ¥ç”ŸæˆåŒ…å«é¢å¤–ä¿¡æ¯çš„ç‰ˆæœ¬
        include_id = self.api_matcher is not None
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = [(idx, row, include_id) for idx, row in df.iterrows()]
        results = {}  # å­˜å‚¨ç»“æœï¼Œä¿æŒåŸå§‹é¡ºåº
        processed_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.parallel_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(self.process_single_row_with_index, task): task[0] 
                for task in tasks
            }
            
            # ä½¿ç”¨è¿›åº¦æ¡
            with tqdm(total=len(tasks), desc="å¤„ç†è¿›åº¦") as pbar:
                for future in as_completed(future_to_task):
                    idx, alpaca_item = future.result()
                    results[idx] = alpaca_item
                    processed_count += 1
                    pbar.update(1)
                    
                    # æ‰¹æ¬¡å»¶è¿Ÿï¼ˆæ¯å¤„ç†ä¸€æ‰¹åæš‚åœï¼‰
                    if processed_count % self.batch_size == 0:
                        time.sleep(self.batch_delay)
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if processed_count % self.checkpoint_interval == 0:
                        # æŒ‰ç´¢å¼•é¡ºåºæ•´ç†å½“å‰ç»“æœ
                        sorted_results = [
                            results[i] for i in sorted(results.keys()) 
                            if results[i] is not None
                        ]
                        with self._lock:
                            self.distilled_dataset = sorted_results
                            self.save_checkpoint('temp_checkpoint')
                        print(f"\nâœ“ å·²å¤„ç† {processed_count} æ¡ï¼Œæ£€æŸ¥ç‚¹å·²ä¿å­˜")
        
        # æŒ‰åŸå§‹ç´¢å¼•é¡ºåºæ•´ç†æœ€ç»ˆç»“æœ
        self.distilled_dataset = [
            results[idx] for idx in sorted(results.keys()) 
            if results[idx] is not None
        ]
