"""
å¿«é€Ÿå¯åŠ¨è„šæœ¬
æä¾›äº¤äº’å¼ç•Œé¢æ¥è¿è¡Œä¸åŒçš„Agentä»»åŠ¡
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from agents import DistillationAgent, DataExplainerAgent
from evaluation import Evaluator
from utils import load_csv, split_dataset, save_split_datasets
from config import DATASET_PATH, OUTPUT_DIR


def print_menu():
    """æ‰“å°èœå•"""
    print("\n" + "=" * 60)
    print("Flaky Teståˆ†æç³»ç»Ÿ - å¿«é€Ÿå¯åŠ¨")
    print("=" * 60)
    print("1. æ•°æ®è’¸é¦ï¼ˆæµ‹è¯•æ¨¡å¼ - æœ€å10æ¡ï¼Œå•çº¿ç¨‹ï¼‰")
    print("2. æ•°æ®è’¸é¦ï¼ˆæµ‹è¯•æ¨¡å¼ - å‰10æ¡ï¼Œå•çº¿ç¨‹ï¼‰")
    print("3. æ•°æ®è’¸é¦ï¼ˆæµ‹è¯•æ¨¡å¼ - éšæœº10æ¡ï¼Œå•çº¿ç¨‹ï¼‰")
    print("4. æ•°æ®è’¸é¦ï¼ˆæµ‹è¯•æ¨¡å¼ - å¹¶è¡Œæ¨ç†ï¼‰")
    print("5. æ•°æ®è®²è§£ï¼ˆ20ä¸ªæ ·æœ¬ï¼‰")
    print("6. æ•°æ®è®²è§£ï¼ˆè‡ªå®šä¹‰æ ·æœ¬æ•°ï¼‰")
    print("7. è¯„ä¼°é¢„æµ‹ç»“æœ")
    print("8. æ•°æ®é›†åˆ’åˆ†ï¼ˆè®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼‰")
    print("9. é€€å‡º")
    print("=" * 60)


def run_distillation_test(mode='last', parallel_workers=1):
    """è¿è¡Œæ•°æ®è’¸é¦æµ‹è¯•"""
    worker_text = f"{parallel_workers}çº¿ç¨‹å¹¶è¡Œ" if parallel_workers > 1 else "å•çº¿ç¨‹"
    print(f"\nğŸš€ å¯åŠ¨æ•°æ®è’¸é¦Agentï¼ˆæµ‹è¯•æ¨¡å¼: {mode}ï¼Œ{worker_text}ï¼‰...")
    
    agent = DistillationAgent(
        test_mode=mode,
        test_size=10,
        batch_size=5,
        batch_delay=1,
        parallel_workers=parallel_workers
    )
    
    output_name = f'distillation_test_{mode}{"_parallel" if parallel_workers > 1 else ""}'
    result = agent.run(output_name=output_name)
    
    print(f"\nâœ“ è’¸é¦å®Œæˆ!")
    print(f"  æˆåŠŸ: {result['success_count']} æ¡")
    print(f"  å¤±è´¥: {result['failed_count']} æ¡")
    print(f"  è¾“å‡º: {result['output_file']}")


def run_distillation_parallel():
    """è¿è¡Œå¹¶è¡Œæ•°æ®è’¸é¦"""
    print("\n" + "=" * 60)
    print("å¹¶è¡Œæ•°æ®è’¸é¦é…ç½®")
    print("=" * 60)
    
    try:
        # é€‰æ‹©æµ‹è¯•æ¨¡å¼
        print("\næµ‹è¯•æ¨¡å¼:")
        print("1. æœ€åNæ¡")
        print("2. å‰Næ¡")
        print("3. éšæœºNæ¡")
        mode_choice = input("é€‰æ‹©æ¨¡å¼ (1-3): ").strip()
        
        mode_map = {'1': 'last', '2': 'first', '3': 'random'}
        mode = mode_map.get(mode_choice, 'last')
        
        # è¾“å…¥æ•°æ®é‡
        test_size = int(input("è¯·è¾“å…¥æµ‹è¯•æ•°æ®é‡ (é»˜è®¤10): ").strip() or "10")
        
        # è¾“å…¥å¹¶è¡Œçº¿ç¨‹æ•°
        parallel_workers = int(input("è¯·è¾“å…¥å¹¶è¡Œçº¿ç¨‹æ•° (1-10ï¼Œæ¨è3-5): ").strip() or "3")
        parallel_workers = max(1, min(10, parallel_workers))  # é™åˆ¶åœ¨1-10ä¹‹é—´
        
        print(f"\né…ç½®:")
        print(f"  æ¨¡å¼: {mode}")
        print(f"  æ•°æ®é‡: {test_size}")
        print(f"  å¹¶è¡Œçº¿ç¨‹: {parallel_workers}")
        
        confirm = input("\nç¡®è®¤å¼€å§‹ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆ")
            return
        
        # åˆ›å»ºAgentå¹¶è¿è¡Œ
        agent = DistillationAgent(
            test_mode=mode,
            test_size=test_size,
            batch_size=5,
            batch_delay=0.5 if parallel_workers > 1 else 1,
            parallel_workers=parallel_workers
        )
        
        output_name = f'distillation_{mode}_{test_size}samples_parallel{parallel_workers}'
        result = agent.run(output_name=output_name)
        
        print(f"\nâœ“ è’¸é¦å®Œæˆ!")
        print(f"  æˆåŠŸ: {result['success_count']} æ¡")
        print(f"  å¤±è´¥: {result['failed_count']} æ¡")
        print(f"  è€—æ—¶: {result['elapsed_time']:.2f} ç§’")
        print(f"  è¾“å‡º: {result['output_file']}")
        
    except ValueError as e:
        print(f"âœ— è¾“å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âœ— å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def run_data_explainer(sample_size=20):
    """è¿è¡Œæ•°æ®è®²è§£"""
    print(f"\nğŸš€ å¯åŠ¨æ•°æ®è®²è§£Agentï¼ˆæ ·æœ¬æ•°: {sample_size}ï¼‰...")
    
    agent = DataExplainerAgent(
        sample_size=sample_size,
        random_seed=42
    )
    
    result = agent.run(output_name='dataset_analysis')
    
    if result['success']:
        print(f"\nâœ“ åˆ†æå®Œæˆ!")
        print(f"  JSON: {result['json_file']}")
        print(f"  æ–‡æœ¬: {result['txt_file']}")


def run_evaluation():
    """è¿è¡Œè¯„ä¼°ä»»åŠ¡"""
    print("\n" + "=" * 60)
    print("è¯„ä¼°é¢„æµ‹ç»“æœ")
    print("=" * 60)
    
    from pathlib import Path
    output_dir = Path(__file__).parent / 'output'
    
    # åˆ—å‡ºoutputç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶ï¼ˆä¼˜å…ˆæ˜¾ç¤ºå¸¦_with_idçš„æ–‡ä»¶ï¼‰
    json_files = list(output_dir.glob('*_with_id.json'))
    if not json_files:
        # å¦‚æœæ²¡æœ‰å¸¦_with_idçš„æ–‡ä»¶ï¼Œåˆ™æ˜¾ç¤ºæ‰€æœ‰JSONæ–‡ä»¶
        json_files = list(output_dir.glob('*.json'))
    
    if not json_files:
        print("\nâœ— outputç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
        print("æç¤º: è¯·å…ˆè¿è¡Œæ•°æ®è’¸é¦ä»»åŠ¡ç”Ÿæˆé¢„æµ‹ç»“æœï¼ˆå»ºè®®ä½¿ç”¨å¸¦_with_idçš„æ–‡ä»¶è¿›è¡Œè¯„ä¼°ï¼‰")
        return
    
    print(f"\næ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶:")
    for i, file in enumerate(json_files, 1):
        # æ ‡æ³¨å“ªäº›æ˜¯å¸¦IDçš„æ–‡ä»¶
        marker = " âœ“ (æ¨è)" if "_with_id" in file.name else ""
        print(f"  {i}. {file.name}{marker}")
    
    try:
        # é€‰æ‹©è¦è¯„ä¼°çš„æ–‡ä»¶
        file_choice = input("\nè¯·é€‰æ‹©è¦è¯„ä¼°çš„æ–‡ä»¶ç¼–å·: ").strip()
        file_idx = int(file_choice) - 1
        
        if file_idx < 0 or file_idx >= len(json_files):
            print("âœ— æ— æ•ˆçš„æ–‡ä»¶ç¼–å·")
            return
        
        prediction_file = json_files[file_idx]
        print(f"\né€‰æ‹©çš„æ–‡ä»¶: {prediction_file.name}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¸¦IDçš„æ–‡ä»¶
        if "_with_id" not in prediction_file.name:
            print("âš  è­¦å‘Š: è¯¥æ–‡ä»¶ä¸åŒ…å«IDå­—æ®µï¼Œè¯„ä¼°å¯èƒ½ä¸å‡†ç¡®")
            confirm = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
            if confirm != 'y':
                print("å·²å–æ¶ˆ")
                return
        
        # ä½¿ç”¨é»˜è®¤çš„ground truthæ–‡ä»¶
        ground_truth_file = Path(__file__).parent / 'dataset' / 'FlakyLens_dataset_with_nonflaky_indented.csv'
        
        if not ground_truth_file.exists():
            print(f"âœ— æœªæ‰¾åˆ°ground truthæ–‡ä»¶: {ground_truth_file}")
            return
        
        # åˆ›å»ºè¯„ä¼°å™¨ï¼ˆé€šè¿‡IDå­—æ®µåŒ¹é…ï¼‰
        print("\nğŸš€ å¼€å§‹è¯„ä¼°...")
        print("ğŸ“Œ è¯„ä¼°æ–¹å¼: é€šè¿‡IDå­—æ®µåŒ¹é…é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾")
        evaluator = Evaluator(
            prediction_file=prediction_file,
            ground_truth_file=ground_truth_file,
            label_column='label',
            id_column='id'
        )
        
        # è¿è¡Œè¯„ä¼°
        eval_output_dir = output_dir / 'evaluation'
        metrics = evaluator.run(
            output_dir=eval_output_dir,
            save_report=True,
            detailed=True
        )
        
        print(f"\nâœ“ è¯„ä¼°å®Œæˆ!")
        print(f"  æŠ¥å‘Šå·²ä¿å­˜åˆ°: {eval_output_dir}")
        
    except ValueError as e:
        print(f"âœ— è¾“å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âœ— è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def run_dataset_split():
    """è¿è¡Œæ•°æ®é›†åˆ’åˆ†ä»»åŠ¡"""
    print("\n" + "=" * 60)
    print("æ•°æ®é›†åˆ’åˆ† - è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†")
    print("=" * 60)
    
    try:
        # è¾“å…¥åˆ’åˆ†æ¯”ä¾‹
        print("\nè¯·è¾“å…¥åˆ’åˆ†æ¯”ä¾‹ï¼ˆé»˜è®¤ 7:2:1ï¼‰")
        train_ratio = float(input("  è®­ç»ƒé›†æ¯”ä¾‹ (0-1, é»˜è®¤0.7): ").strip() or "0.7")
        val_ratio = float(input("  éªŒè¯é›†æ¯”ä¾‹ (0-1, é»˜è®¤0.2): ").strip() or "0.2")
        test_ratio = float(input("  æµ‹è¯•é›†æ¯”ä¾‹ (0-1, é»˜è®¤0.1): ").strip() or "0.1")
        
        # éªŒè¯æ¯”ä¾‹æ€»å’Œ
        total = train_ratio + val_ratio + test_ratio
        if abs(total - 1.0) > 1e-6:
            print(f"âœ— æ¯”ä¾‹æ€»å’Œå¿…é¡»ä¸º 1ï¼Œå½“å‰ä¸º: {total}")
            return
        
        # æ˜¯å¦ä½¿ç”¨åˆ†å±‚é‡‡æ ·
        use_stratify = input("\næ˜¯å¦ä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼ˆåŸºäºlabelåˆ—ï¼‰ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower()
        stratify_column = 'label' if use_stratify != 'n' else None
        
        # éšæœºç§å­
        random_seed = int(input("éšæœºç§å­ (é»˜è®¤42): ").strip() or "42")
        
        # è¾“å‡ºæ ¼å¼
        print("\nè¾“å‡ºæ ¼å¼:")
        print("1. CSV")
        print("2. JSON")
        print("3. ä¸¤è€…éƒ½ä¿å­˜")
        format_choice = input("é€‰æ‹©æ ¼å¼ (1-3, é»˜è®¤1): ").strip() or "1"
        
        print(f"\né…ç½®:")
        print(f"  è®­ç»ƒé›†: {train_ratio*100:.1f}%")
        print(f"  éªŒè¯é›†: {val_ratio*100:.1f}%")
        print(f"  æµ‹è¯•é›†: {test_ratio*100:.1f}%")
        print(f"  åˆ†å±‚é‡‡æ ·: {'æ˜¯ (åŸºäºlabel)' if stratify_column else 'å¦'}")
        print(f"  éšæœºç§å­: {random_seed}")
        
        confirm = input("\nç¡®è®¤å¼€å§‹åˆ’åˆ†ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆ")
            return
        
        # åŠ è½½æ•°æ®é›†
        print(f"\nğŸ“‚ åŠ è½½æ•°æ®é›†: {DATASET_PATH}")
        df = load_csv(DATASET_PATH)
        
        # åˆ’åˆ†æ•°æ®é›†
        print("\nğŸ”€ å¼€å§‹åˆ’åˆ†æ•°æ®é›†...")
        splits = split_dataset(
            df,
            train_ratio=train_ratio,
            val_ratio=val_ratio,
            test_ratio=test_ratio,
            stratify_column=stratify_column,
            random_seed=random_seed,
            shuffle=True
        )
        
        # ä¿å­˜åˆ’åˆ†åçš„æ•°æ®é›†
        output_dir = OUTPUT_DIR / 'splits'
        
        if format_choice in ['1', '3']:
            print("\nğŸ’¾ ä¿å­˜CSVæ ¼å¼...")
            csv_files = save_split_datasets(
                splits,
                output_dir=output_dir,
                base_name='flaky_dataset',
                format='csv'
            )
        
        if format_choice in ['2', '3']:
            print("\nğŸ’¾ ä¿å­˜JSONæ ¼å¼...")
            json_files = save_split_datasets(
                splits,
                output_dir=output_dir,
                base_name='flaky_dataset',
                format='json'
            )
        
        print(f"\nâœ“ æ•°æ®é›†åˆ’åˆ†å®Œæˆ!")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        
    except ValueError as e:
        print(f"âœ— è¾“å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âœ— åˆ’åˆ†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    while True:
        print_menu()
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-9): ").strip()
        
        if choice == '1':
            run_distillation_test(mode='last', parallel_workers=1)
        elif choice == '2':
            run_distillation_test(mode='first', parallel_workers=1)
        elif choice == '3':
            run_distillation_test(mode='random', parallel_workers=1)
        elif choice == '4':
            run_distillation_parallel()
        elif choice == '5':
            run_data_explainer(sample_size=20)
        elif choice == '6':
            try:
                size = int(input("è¯·è¾“å…¥æ ·æœ¬æ•°: ").strip())
                run_data_explainer(sample_size=size)
            except ValueError:
                print("âœ— æ— æ•ˆçš„æ•°å­—")
        elif choice == '7':
            run_evaluation()
        elif choice == '8':
            run_dataset_split()
        elif choice == '9':
            print("\nğŸ‘‹ å†è§!")
            break
        else:
            print("\nâœ— æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡è¯•")
        
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºå·²ä¸­æ–­ï¼Œå†è§!")
    except Exception as e:
        print(f"\nâœ— å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
