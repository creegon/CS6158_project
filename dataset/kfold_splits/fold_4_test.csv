id,project,test_name,full_code,label,category
20,cdapio_cdap,WorkflowHttpHandlerTest.testWorkflowForkFailure,"@Test
public void testWorkflowForkFailure() throws Exception {
    Assert.assertEquals(200, deploy(WorkflowFailureInForkApp.class).getStatusLine().getStatusCode());
    Id.Application appId = Application.from(DEFAULT, NAME);
    Id.Workflow workflowId = Workflow.from(appId, NAME);
    Id.Program firstMRId = Program.from(appId, MAPREDUCE, FIRST_MAPREDUCE_NAME);
    Id.Program secondMRId = Program.from(appId, MAPREDUCE, SECOND_MAPREDUCE_NAME);
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    File fileToSync = new File(tmpFolder.newFolder() + ""/sync.file"");
    File fileToWait = new File(tmpFolder.newFolder() + ""/wait.file"");
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInput(""testWorkflowForkFailureInput""), ""outputPath"", outputPath, ""sync.file"", fileToSync.getAbsolutePath(), ""wait.file"", fileToWait.getAbsolutePath(), (""mapreduce."" + WorkflowFailureInForkApp.SECOND_MAPREDUCE_NAME) + "".throw.exception"", ""true""));
    waitState(workflowId, RUNNING.name());
    waitState(workflowId, STOPPED.name());
    verifyProgramRuns(workflowId, ""failed"");
    List<RunRecord> mapReduceProgramRuns = getProgramRuns(firstMRId, KILLED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(secondMRId, FAILED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
}",concurrency,1
48,cdapio_cdap,PreviewDataPipelineTest.testLogicalTypePreviewRun,"@Test
public void testLogicalTypePreviewRun(Engine engine) throws Exception {
    PreviewManager previewManager = getPreviewManager();
    String sourceTableName = ""singleInput"";
    String sinkTableName = ""singleOutput"";
    Schema schema = Schema.recordOf(
    ""testRecord"",
    Schema.Field.of(""name"", Schema.of(Schema.Type.STRING)),
    Schema.Field.of(""date"", Schema.of(Schema.LogicalType.DATE)),
    Schema.Field.of(""ts"", Schema.of(Schema.LogicalType.TIMESTAMP_MILLIS))
    );
    ETLBatchConfig etlConfig = ETLBatchConfig.builder()
    .addStage(new ETLStage(""source"", MockSource.getPlugin(sourceTableName, schema)))
    .addStage(new ETLStage(""transform"", IdentityTransform.getPlugin()))
    .addStage(new ETLStage(""sink"", MockSink.getPlugin(sinkTableName)))
    .addConnection(""source"", ""transform"")
    .addConnection(""transform"", ""sink"")
    .setEngine(engine)
    .setNumOfRecordsPreview(100)
    .build();
    PreviewConfig previewConfig = new PreviewConfig(SmartWorkflow.NAME, ProgramType.WORKFLOW,
    Collections.<String, String>emptyMap(), 10);
    addDatasetInstance(Table.class.getName(), sourceTableName,
    DatasetProperties.of(ImmutableMap.of(""schema"", schema.toString())));
    DataSetManager<Table> inputManager = getDataset(NamespaceId.DEFAULT.dataset(sourceTableName));
    ZonedDateTime expectedMillis = ZonedDateTime.of(2018, 11, 11, 11, 11, 11, 123 * 1000 * 1000,
    ZoneId.ofOffset(""UTC"", ZoneOffset.UTC));
    StructuredRecord recordSamuel = StructuredRecord.builder(schema).set(""name"", ""samuel"")
    .setDate(""date"", LocalDate.of(2002, 11, 18)).setTimestamp(""ts"", expectedMillis).build();
    StructuredRecord recordBob = StructuredRecord.builder(schema).set(""name"", ""bob"")
    .setDate(""date"", LocalDate.of(2003, 11, 18)).setTimestamp(""ts"", expectedMillis).build();
    MockSource.writeInput(inputManager, ImmutableList.of(recordSamuel, recordBob));
    AppRequest<ETLBatchConfig> appRequest = new AppRequest<>(APP_ARTIFACT_RANGE, etlConfig, previewConfig);
    ApplicationId previewId = previewManager.start(NamespaceId.DEFAULT, appRequest);
    Tasks.waitFor(PreviewStatus.Status.COMPLETED, new Callable<PreviewStatus.Status>() {
        @Override
        public PreviewStatus.Status call() throws Exception {
            PreviewStatus status = previewManager.getStatus(previewId);
            return status == null ? null : status.getStatus();
        }
    }, 5, TimeUnit.MINUTES);
    checkPreviewStore(previewManager, previewId, ""source"", 2);
    List<JsonElement> data = previewManager.getData(previewId, ""source"").get(DATA_TRACER_PROPERTY);
    StructuredRecord actualRecordSamuel = GSON.fromJson(data.get(0), StructuredRecord.class);
    Assert.assertEquals(actualRecordSamuel.get(""date""), ""2002-11-18"");
    Assert.assertEquals(actualRecordSamuel.get(""ts""), ""2018-11-11T11:11:11.123Z[UTC]"");
    StructuredRecord actualRecordBob = GSON.fromJson(data.get(1), StructuredRecord.class);
    Assert.assertEquals(actualRecordBob.get(""date""), ""2003-11-18"");
    Assert.assertEquals(actualRecordBob.get(""ts""), ""2018-11-11T11:11:11.123Z[UTC]"");
    checkPreviewStore(previewManager, previewId, ""transform"", 2);
    checkPreviewStore(previewManager, previewId, ""sink"", 2);
    validateMetric(2, previewId, ""source.records.in"", previewManager);
    validateMetric(2, previewId, ""source.records.out"", previewManager);
    validateMetric(2, previewId, ""transform.records.in"", previewManager);
    validateMetric(2, previewId, ""transform.records.out"", previewManager);
    validateMetric(2, previewId, ""sink.records.out"", previewManager);
    validateMetric(2, previewId, ""sink.records.in"", previewManager);
    DataSetManager<Table> sinkManager = getDataset(sinkTableName);
    Assert.assertNull(sinkManager.get());
    deleteDatasetInstance(NamespaceId.DEFAULT.dataset(sourceTableName));
    Assert.assertNotNull(previewManager.getRunId(previewId));
}",time,2
98,cdapio_cdap,ProgramLifecycleHttpHandlerTest.testStartProgramWithDisabledRuntimeArgs,"@Test
public void testStartProgramWithDisabledRuntimeArgs() throws Exception {
    ProfileId profileId = new NamespaceId(TEST_NAMESPACE1).profile(""MyProfile"");
    Profile profile = new Profile(""MyProfile"", Profile.NATIVE.getLabel(), Profile.NATIVE.getDescription(),Profile.NATIVE.getScope(), Profile.NATIVE.getProvisioner());
    putProfile(profileId, profile, 200);
    disableProfile(profileId, 200);
    deploy(AppWithWorkflow.class, 200, Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
    ProgramId programId = new NamespaceId(TEST_NAMESPACE1).app(APP_WITH_WORKFLOW_APP_ID).workflow(APP_WITH_WORKFLOW_WORKFLOW_NAME);
    Assert.assertEquals(STOPPED, getProgramStatus(programId));
    startProgram(programId, Collections.singletonMap(SystemArguments.PROFILE_NAME, profileId.getScopedName()), 409);
    Assert.assertEquals(STOPPED, getProgramStatus(programId));
    startProgram(programId, Collections.singletonMap(SystemArguments.PROFILE_NAME, ProfileId.NATIVE.getScopedName()),200);
    waitState(programId, STOPPED);
}",async wait,0
139,cdapio_cdap,WorkflowClientTestRun.testWorkflowClient,"@Test
public void testWorkflowClient() throws Exception {
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    Map<String, String> runtimeArgs = ImmutableMap.of(""inputPath"", createInput(""input""),
    ""outputPath"", outputPath);
    Id.Workflow workflowId = Id.Workflow.from(appId, AppWithWorkflow.SampleWorkflow.NAME);
    programClient.start(workflowId, false, runtimeArgs);
    programClient.waitForStatus(workflowId, ""STOPPED"", 60, TimeUnit.SECONDS);
    List<RunRecord> workflowRuns = programClient.getProgramRuns(workflowId, ProgramRunStatus.COMPLETED.name(), 0,
    Long.MAX_VALUE, 10);
    Assert.assertEquals(1, workflowRuns.size());
    Id.Run workflowRunId = new Id.Run(workflowId, workflowRuns.get(0).getPid());
    try {
        workflowClient.getWorkflowToken(new Id.Run(Id.Workflow.from(appId, ""random""), workflowRunId.getId()));
        Assert.fail(""Should not find a workflow token for a non-existing workflow"");
    } catch (NotFoundException expected) {
    }
    try {
        workflowClient.getWorkflowToken(new Id.Run(workflowId, RunIds.generate().getId()));
        Assert.fail(""Should not find a workflow token for a random run id"");
    } catch (NotFoundException expected) {
    }
    WorkflowTokenDetail workflowToken = workflowClient.getWorkflowToken(workflowRunId);
    Assert.assertEquals(3, workflowToken.getTokenData().size());
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, WorkflowToken.Scope.SYSTEM);
    Assert.assertTrue(workflowToken.getTokenData().size() > 0);
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, ""start_time"");
    Map<String, List<WorkflowTokenDetail.NodeValueDetail>> tokenData = workflowToken.getTokenData();
    Assert.assertEquals(AppWithWorkflow.WordCountMapReduce.NAME, tokenData.get(""start_time"").get(0).getNode());
    Assert.assertTrue(Long.parseLong(tokenData.get(""start_time"").get(0).getValue()) < System.currentTimeMillis());
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, WorkflowToken.Scope.USER, ""action_type"");
    tokenData = workflowToken.getTokenData();
    Assert.assertEquals(AppWithWorkflow.WordCountMapReduce.NAME, tokenData.get(""action_type"").get(0).getNode());
    Assert.assertEquals(""MapReduce"", tokenData.get(""action_type"").get(0).getValue());
    String nodeName = AppWithWorkflow.SampleWorkflow.firstActionName;
    WorkflowTokenNodeDetail workflowTokenAtNode =
    workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName);
    Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,
    workflowTokenAtNode.getTokenDataAtNode().get(AppWithWorkflow.DummyAction.TOKEN_KEY));
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName, WorkflowToken.Scope.SYSTEM);
    Assert.assertEquals(0, workflowTokenAtNode.getTokenDataAtNode().size());
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName,
    AppWithWorkflow.DummyAction.TOKEN_KEY);
    Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,
    workflowTokenAtNode.getTokenDataAtNode().get(AppWithWorkflow.DummyAction.TOKEN_KEY));
    String reduceOutputRecordsCounter = ""org.apache.hadoop.mapreduce.TaskCounter.REDUCE_OUTPUT_RECORDS"";
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, AppWithWorkflow.WordCountMapReduce.NAME,
    WorkflowToken.Scope.SYSTEM, reduceOutputRecordsCounter);
    Assert.assertEquals(6, Integer.parseInt(workflowTokenAtNode.getTokenDataAtNode().get(reduceOutputRecordsCounter)));
}",async wait,0
154,cdapio_cdap,TestFrameworkTestRun.testWorkerInstances,"@Test
public void testWorkerInstances() throws Exception {
    ApplicationManager applicationManager = deployApplication(testSpace, AppUsingGetServiceURL.class);
    WorkerManager workerManager = applicationManager.getWorkerManager(PINGING_WORKER).start();
    workerManager.waitForStatus(true);
    workerInstancesCheck(workerManager, 5);
    workerManager.setInstances(10);
    workerInstancesCheck(workerManager, 10);
    workerManager.setInstances(2);
    workerInstancesCheck(workerManager, 2);
    workerManager.setInstances(2);
    workerInstancesCheck(workerManager, 2);
    WorkerManager lifecycleWorkerManager = applicationManager.getWorkerManager(LIFECYCLE_WORKER).start();
    lifecycleWorkerManager.waitForStatus(true);
    lifecycleWorkerManager.setInstances(5);
    workerInstancesCheck(lifecycleWorkerManager, 5);
    for (int i = 0; i < 5; i++) {
        kvTableKeyCheck(testSpace, WORKER_INSTANCES_DATASET, Bytes.toBytes(String.format(""init.%d"", i)));
    }
    lifecycleWorkerManager.stop();
    lifecycleWorkerManager.waitForStatus(false);
    if (workerManager.isRunning()) {
        workerManager.stop();
    }
    workerManager.waitForStatus(false);
    workerInstancesCheck(lifecycleWorkerManager, 5);
    workerInstancesCheck(workerManager, 2);
    assertWorkerDatasetWrites(Bytes.toBytes(""init""), Bytes.stopKeyForPrefix(Bytes.toBytes(""init.2"")), 3, 3);
    assertWorkerDatasetWrites(Bytes.toBytes(""init.3""), Bytes.stopKeyForPrefix(Bytes.toBytes(""init"")), 2, 5);
    byte[] startRow = Bytes.toBytes(""stop"");
    assertWorkerDatasetWrites(startRow, Bytes.stopKeyForPrefix(startRow), 5, 5);
}",async wait,0
202,cdapio_cdap,ProvisioningServiceTest.testCancelDeprovision,"@Test
public void testCancelDeprovision() throws Exception {
    ProvisionerInfo provisionerInfo = new MockProvisioner.PropertyBuilder().waitDelete(1, TimeUnit.MINUTES).build();
    TaskFields taskFields = testProvision(ProvisioningOp.Status.CREATED, provisionerInfo);
    Runnable task = Transactionals.execute(transactional, dsContext -> {
        return provisioningService.deprovision(taskFields.programRunId, dsContext);
    });
    task.run();
    Assert.assertTrue(provisioningService.cancelDeprovisionTask(taskFields.programRunId).isPresent());
    ProvisioningTaskKey taskKey = new ProvisioningTaskKey(taskFields.programRunId, ProvisioningOp.Type.DEPROVISION);
    waitForExpectedProvisioningState(taskKey, ProvisioningOp.Status.CANCELLED);
}",test order dependency,4
214,cdapio_cdap,MetadataSubscriberServiceTest.testSubscriber,"@Test
public void testSubscriber() throws InterruptedException, ExecutionException, TimeoutException {
    LineageWriter lineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId run1 = service1.run(RunIds.generate());
    lineageWriter.addAccess(run1, dataset1, AccessType.READ);
    lineageWriter.addAccess(run1, dataset2, AccessType.WRITE);
    LineageStoreReader lineageReader = getInjector().getInstance(LineageStoreReader.class);
    ProgramRunId run1 = service1.run(RunIds.generate());
    Set<NamespacedEntityId> entities = lineageReader.getEntitiesForRun(run1);
    Assert.assertTrue(entities.isEmpty());
    LineageWriter lineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    lineageWriter.addAccess(run1, dataset1, AccessType.READ);
    lineageWriter.addAccess(run1, dataset2, AccessType.WRITE);
    FieldLineageWriter fieldLineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId spark1Run1 = spark1.run(RunIds.generate(100));
    ReadOperation read = new ReadOperation(""read"", ""some read"", EndPoint.of(""ns"", ""endpoint1""), ""offset"", ""body"");
    TransformOperation parse = new TransformOperation(""parse"", ""parse body"",
    Collections.singletonList(InputField.of(""read"", ""body"")),
    ""name"", ""address"");
    WriteOperation write = new WriteOperation(""write"", ""write data"", EndPoint.of(""ns"", ""endpoint2""),
    Arrays.asList(InputField.of(""read"", ""offset""),
    InputField.of(""parse"", ""name""),
    InputField.of(""parse"", ""address"")));
    List<Operation> operations = new ArrayList<>();
    operations.add(read);
    operations.add(write);
    operations.add(parse);
    FieldLineageInfo info1 = new FieldLineageInfo(operations);
    fieldLineageWriter.write(spark1Run1, info1);
    ProgramRunId spark1Run2 = spark1.run(RunIds.generate(200));
    fieldLineageWriter.write(spark1Run2, info1);
    List<Operation> operations2 = new ArrayList<>();
    operations2.add(read);
    operations2.add(parse);
    TransformOperation normalize = new TransformOperation(""normalize"", ""normalize address"",
    Collections.singletonList(InputField.of(""parse"", ""address"")),
    ""address"");
    operations2.add(normalize);
    WriteOperation anotherWrite = new WriteOperation(""anotherwrite"", ""write data"", EndPoint.of(""ns"", ""endpoint2""),
    Arrays.asList(InputField.of(""read"", ""offset""),
    InputField.of(""parse"", ""name""),
    InputField.of(""normalize"", ""address"")));
    operations2.add(anotherWrite);
    FieldLineageInfo info2 = new FieldLineageInfo(operations2);
    ProgramRunId spark1Run3 = spark1.run(RunIds.generate(300));
    fieldLineageWriter.write(spark1Run3, info2);
    UsageWriter usageWriter = getInjector().getInstance(MessagingUsageWriter.class);
    usageWriter.register(spark1, dataset1);
    usageWriter.registerAll(Collections.singleton(spark1), dataset3);
    Set<NamespacedEntityId> expectedLineage = new HashSet<>(Arrays.asList(run1.getParent(), dataset1, dataset2));
    Tasks.waitFor(true, () -> expectedLineage.equals(lineageReader.getEntitiesForRun(run1)),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    Assert.assertTrue(lineageReader.getRelations(spark1, 0L, Long.MAX_VALUE, x -> true).isEmpty());
    FieldLineageReader fieldLineageReader = getInjector().getInstance(FieldLineageReader.class);
    Set<Operation> expectedOperations = new HashSet<>();
    expectedOperations.add(read);
    expectedOperations.add(anotherWrite);
    List<ProgramRunOperations> expected = new ArrayList<>();
    expected.add(new ProgramRunOperations(Collections.singleton(spark1Run3), expectedOperations));
    expectedOperations = new HashSet<>();
    expectedOperations.add(read);
    expectedOperations.add(write);
    expected.add(new ProgramRunOperations(new HashSet<>(Arrays.asList(spark1Run1, spark1Run2)),
    expectedOperations));
    EndPointField endPointField = new EndPointField(EndPoint.of(""ns"", ""endpoint2""), ""offset"");
    Tasks.waitFor(expected, () -> fieldLineageReader.getIncomingOperations(endPointField, 1L, Long.MAX_VALUE - 1),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    Set<EntityId> expectedUsage = new HashSet<>(Arrays.asList(dataset1, dataset3));
    UsageRegistry usageRegistry = getInjector().getInstance(UsageRegistry.class);
    Tasks.waitFor(true, () -> expectedUsage.equals(usageRegistry.getDatasets(spark1)),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
}",async wait,0
262,cdapio_cdap,WorkflowHttpHandlerTest.testWorkflowTokenPut,"@Test
public void testWorkflowTokenPut() throws Exception {
    Assert.assertEquals(200, deploy(WorkflowTokenTestPutApp.class).getStatusLine().getStatusCode());
    Id.Application appId = Id.Application.from(Id.Namespace.DEFAULT, WorkflowTokenTestPutApp.NAME);
    Id.Workflow workflowId = Id.Workflow.from(appId, WorkflowTokenTestPutApp.WorkflowTokenTestPut.NAME);
    Id.Program mapReduceId = Id.Program.from(appId, ProgramType.MAPREDUCE, WorkflowTokenTestPutApp.RecordCounter.NAME);
    Id.Program sparkId = Id.Program.from(appId, ProgramType.SPARK, WorkflowTokenTestPutApp.SparkTestApp.NAME);
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""firstInput""),
    ""outputPath"", outputPath, ""put.in.mapper.initialize"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    List<RunRecord> workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
    List<RunRecord> mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""secondInput""),
    ""outputPath"", outputPath, ""put.in.map"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(2, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(2, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""thirdInput""),
    ""outputPath"", outputPath, ""put.in.reducer.initialize"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(3, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(3, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""fourthInput""),
    ""outputPath"", outputPath, ""put.in.reduce"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(4, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(4, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""fifthInput""),
    ""outputPath"", outputPath, ""closurePutToken"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(5, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    List<RunRecord> sparkProgramRuns = getProgramRuns(sparkId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, sparkProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""sixthInput""),
    ""outputPath"", outputPath));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
    workflowProgramRuns = getProgramRuns(sparkId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
}",async wait,0
325,cdapio_cdap,MetadataHttpHandlerTestRun.testSystemMetadataRetrieval,"@Test
public void testSystemMetadataRetrieval() throws Exception {
    appClient.deploy(DEFAULT, createAppJarFile(AllProgramsApp.class));
    Id.Stream streamId = Stream.from(DEFAULT, STREAM_NAME);
    Set<String> streamSystemTags = getTags(streamId, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(STREAM_NAME), streamSystemTags);
    Map<String, String> streamSystemProperties = getProperties(streamId, SYSTEM);
    final String creationTime = ""creation-time"";
    String description = ""description"";
    String schema = ""schema"";
    String ttl = ""ttl"";
    Assert.assertTrue(""Expected creation time to exist but it does not"", streamSystemProperties.containsKey(creationTime));
    long createTime = Long.parseLong(streamSystemProperties.get(creationTime));
    Assert.assertTrue(""Stream create time should be within the last hour - "" + createTime, createTime > (System.currentTimeMillis() - TimeUnit.HOURS.toMillis(1)));
    Assert.assertEquals(ImmutableMap.of(schema, Schema.recordOf(""stringBody"", Field.of(""body"", Schema.of(STRING))).toString(), ttl, String.valueOf(Long.MAX_VALUE), description, ""test stream"", creationTime, String.valueOf(createTime)), streamSystemProperties);
    long newTtl = 100000L;
    streamClient.setStreamProperties(streamId, new StreamProperties(newTtl, null, null));
    streamSystemProperties = getProperties(streamId, SYSTEM);
    Assert.assertEquals(ImmutableMap.of(schema, Schema.recordOf(""stringBody"", Field.of(""body"", Schema.of(STRING))).toString(), ttl, String.valueOf(newTtl * 1000), description, ""test stream"", creationTime, String.valueOf(createTime)), streamSystemProperties);
    Set<MetadataRecord> streamSystemMetadata = getMetadata(streamId, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(streamId, MetadataScope.SYSTEM, streamSystemProperties, streamSystemTags)), streamSystemMetadata);
    Id.Stream.View view = View.from(streamId, ""view"");
    Schema viewSchema = Schema.recordOf(""record"", Field.of(""viewBody"", Schema.nullableOf(Schema.of(BYTES))));
    streamViewClient.createOrUpdate(view, new ViewSpecification(new FormatSpecification(""format"", viewSchema)));
    Set<String> viewSystemTags = getTags(view, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(""view"", STREAM_NAME), viewSystemTags);
    Map<String, String> viewSystemProperties = getProperties(view, SYSTEM);
    Assert.assertEquals(viewSchema.toString(), viewSystemProperties.get(schema));
    ImmutableSet<String> viewUserTags = ImmutableSet.of(""viewTag"");
    addTags(view, viewUserTags);
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(view, MetadataScope.USER, ImmutableMap.<String, String>of(), viewUserTags), new MetadataRecord(view, MetadataScope.SYSTEM, viewSystemProperties, viewSystemTags)), getMetadata(view));
    Id.DatasetInstance datasetInstance = DatasetInstance.from(DEFAULT, DATASET_NAME);
    Set<String> dsSystemTags = getTags(datasetInstance, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(DATASET_NAME, BATCH_TAG, EXPLORE_TAG), dsSystemTags);
    Map<String, String> dsSystemProperties = getProperties(datasetInstance, SYSTEM);
    Assert.assertTrue(""Expected creation time to exist but it does not"", dsSystemProperties.containsKey(creationTime));
    createTime = Long.parseLong(dsSystemProperties.get(creationTime));
    Assert.assertTrue(""Dataset create time should be within the last hour - "" + createTime, createTime > (System.currentTimeMillis() - TimeUnit.HOURS.toMillis(1)));
    Assert.assertEquals(ImmutableMap.of(""type"", KeyValueTable.class.getName(), description, ""test dataset"", creationTime, String.valueOf(createTime)), dsSystemProperties);
    datasetClient.update(datasetInstance, ImmutableMap.of(PROPERTY_TTL, ""100000""));
    dsSystemProperties = getProperties(datasetInstance, SYSTEM);
    Assert.assertEquals(ImmutableMap.of(""type"", KeyValueTable.class.getName(), description, ""test dataset"", ttl, ""100000"", creationTime, String.valueOf(createTime)), dsSystemProperties);
    Id.Artifact artifactId = getArtifactId();
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(artifactId, MetadataScope.SYSTEM, ImmutableMap.<String, String>of(), ImmutableSet.of(AllProgramsApp.class.getSimpleName()))), getMetadata(artifactId, SYSTEM));
    Id.Application app = Application.from(DEFAULT, NAME);
    Assert.assertEquals(ImmutableMap.builder().put((FLOW.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpFlow.NAME, NAME).put((MAPREDUCE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpMR.NAME, NAME).put((MAPREDUCE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpMR2.NAME, NAME).put((SERVICE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpService.NAME, NAME).put((SPARK.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpSpark.NAME, NAME).put((WORKER.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpWorker.NAME, NAME).put((WORKFLOW.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpWorkflow.NAME, NAME).put((""schedule"" + MetadataDataset.KEYVALUE_SEPARATOR) + AllProgramsApp.SCHEDULE_NAME, (AllProgramsApp.SCHEDULE_NAME + MetadataDataset.KEYVALUE_SEPARATOR) + AllProgramsApp.SCHEDULE_DESCRIPTION).build(), getProperties(app, SYSTEM));
    Assert.assertEquals(ImmutableSet.of(AllProgramsApp.class.getSimpleName(), NAME), getTags(app, SYSTEM));
    assertProgramSystemMetadata(Program.from(app, FLOW, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, WORKER, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, SERVICE, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, MAPREDUCE, NAME), ""Batch"");
    assertProgramSystemMetadata(Program.from(app, SPARK, NAME), ""Batch"");
    assertProgramSystemMetadata(Program.from(app, WORKFLOW, NAME), ""Batch"");
}",async wait,0
342,cdapio_cdap,TestFrameworkTestRun.testAppWithServices,"@Test
public void testAppWithServices() throws Exception {
    ApplicationManager applicationManager = deployApplication(AppWithServices.class);
    LOG.info(""Deployed."");
    ServiceManager serviceManager = applicationManager.getServiceManager(AppWithServices.SERVICE_NAME).start();
    serviceManager.waitForStatus(true);
    LOG.info(""Service Started"");
    URL serviceURL = serviceManager.getServiceURL(15, TimeUnit.SECONDS);
    Assert.assertNotNull(serviceURL);
    URL url = new URL(serviceURL, ""ping2"");
    HttpRequest request = HttpRequest.get(url).build();
    HttpResponse response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    url = new URL(serviceURL, ""failure"");
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(500, response.getResponseCode());
    Assert.assertTrue(response.getResponseBodyAsString().contains(""Exception""));
    url = new URL(serviceURL, ""verifyClassLoader"");
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    RuntimeMetrics serviceMetrics = serviceManager.getMetrics();
    serviceMetrics.waitForinput(3, 5, TimeUnit.SECONDS);
    Assert.assertEquals(3, serviceMetrics.getInput());
    Assert.assertEquals(2, serviceMetrics.getProcessed());
    Assert.assertEquals(1, serviceMetrics.getException());
    RuntimeMetrics handlerMetrics = getMetricsManager().getServiceHandlerMetrics(Id.Namespace.DEFAULT.getId(),
    AppWithServices.APP_NAME,
    AppWithServices.SERVICE_NAME,
    AppWithServices.SERVICE_NAME);
    handlerMetrics.waitForinput(3, 5, TimeUnit.SECONDS);
    Assert.assertEquals(3, handlerMetrics.getInput());
    Assert.assertEquals(2, handlerMetrics.getProcessed());
    Assert.assertEquals(1, handlerMetrics.getException());
    LOG.info(""DatasetUpdateService Started"");
    Map<String, String> args
    = ImmutableMap.of(AppWithServices.WRITE_VALUE_RUN_KEY, AppWithServices.DATASET_TEST_VALUE,
    AppWithServices.WRITE_VALUE_STOP_KEY, AppWithServices.DATASET_TEST_VALUE_STOP);
    ServiceManager datasetWorkerServiceManager = applicationManager
    .getServiceManager(AppWithServices.DATASET_WORKER_SERVICE_NAME).start(args);
    WorkerManager datasetWorker =
    applicationManager.getWorkerManager(AppWithServices.DATASET_UPDATE_WORKER).start(args);
    datasetWorkerServiceManager.waitForStatus(true);
    ServiceManager noopManager = applicationManager.getServiceManager(""NoOpService"").start();
    serviceManager.waitForStatus(true, 2, 1);
    String result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY);
    String decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE, decodedResult);
    handlerMetrics = getMetricsManager().getServiceHandlerMetrics(Id.Namespace.DEFAULT.getId(),
    AppWithServices.APP_NAME,
    ""NoOpService"",
    ""NoOpHandler"");
    handlerMetrics.waitForinput(1, 5, TimeUnit.SECONDS);
    Assert.assertEquals(1, handlerMetrics.getInput());
    Assert.assertEquals(1, handlerMetrics.getProcessed());
    Assert.assertEquals(0, handlerMetrics.getException());
    String path = String.format(""discover/%s/%s"",
    AppWithServices.APP_NAME, AppWithServices.DATASET_WORKER_SERVICE_NAME);
    url = new URL(serviceURL, path);
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    datasetWorker.stop();
    datasetWorkerServiceManager.stop();
    datasetWorkerServiceManager.waitForStatus(false);
    LOG.info(""DatasetUpdateService Stopped"");
    serviceManager.stop();
    serviceManager.waitForStatus(false);
    LOG.info(""ServerService Stopped"");
    result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY_STOP);
    decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE_STOP, decodedResult);
    result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY_STOP_2);
    decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE_STOP_2, decodedResult);
}",concurrency,1
356,cdapio_cdap,MetricsQueryTestRun.testingUserServiceGaugeMetrics,"@Test
public void testingUserServiceGaugeMetrics() throws Exception {
    MetricsCollector collector =
    collectionService.getCollector(getUserServiceContext(Constants.DEFAULT_NAMESPACE, ""WordCount"", ""CounterService"",
    ""CountRunnable""));
    collector.increment(""gmetric"", 1);
    collector.gauge(""gmetric"", 10);
    collector.increment(""gmetric"", 1);
    TimeUnit.SECONDS.sleep(1);
    collector.gauge(""gmetric"", 10);
    TimeUnit.SECONDS.sleep(2);
    String runnableRequest =
    ""/system/apps/WordCount/services/CounterService/runnables/CountRunnable/gmetric?aggregate=true"";
    String serviceRequest =
    ""/system/apps/WordCount/services/CounterService/gmetric?aggregate=true"";
    testSingleMetric(runnableRequest, 10);
    testSingleMetric(serviceRequest, 10);
}",async wait,0
35660,cdapio_cdap,Spark2Test.testSpark2Service,"  @Test
  public void testSpark2Service() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, Spark2TestApp.class);
    SparkManager manager = applicationManager.getSparkManager(ScalaSparkServiceProgram.class.getSimpleName()).start();

    URL url = manager.getServiceURL(5, TimeUnit.MINUTES);
    Assert.assertNotNull(url);

    // GET request to sum n numbers.
    URL sumURL = url.toURI().resolve(""sum?n="" + Joiner.on(""&n="").join(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).toURL();
    HttpURLConnection urlConn = (HttpURLConnection) sumURL.openConnection();
    Assert.assertEquals(HttpURLConnection.HTTP_OK, urlConn.getResponseCode());
    try (InputStream is = urlConn.getInputStream()) {
      Assert.assertEquals(55, Integer.parseInt(new String(ByteStreams.toByteArray(is), StandardCharsets.UTF_8)));
    }
  }
",non-flaky,5
35661,cdapio_cdap,Spark2Test.call,"  @Test
  public void testSparkWithObjectStore() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    DataSetManager<ObjectStore<String>> keysManager = getDataset(""keys"");
    prepareInputData(keysManager);

    SparkManager sparkManager = applicationManager.getSparkManager(CharCountProgram.class.getSimpleName()).start();
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);

    // validate that the table emitted metrics
    // one read + one write in beforeSubmit(), increment (= read + write) in main -> 4
    Tasks.waitFor(4L, new Callable<Long>() {
      @Override
      public Long call() throws Exception {
        Collection<MetricTimeSeries> metrics =
          getMetricsManager().query(new MetricDataQuery(
            0,
            System.currentTimeMillis() / 1000L,
            Integer.MAX_VALUE,
            ""system."" + Constants.Metrics.Name.Dataset.OP_COUNT,
            AggregationFunction.SUM,
            ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, DefaultId.NAMESPACE.getNamespace(),
                            Constants.Metrics.Tag.APP, SparkAppUsingObjectStore.class.getSimpleName(),
                            Constants.Metrics.Tag.SPARK, CharCountProgram.class.getSimpleName(),
                            Constants.Metrics.Tag.DATASET, ""totals""),
            Collections.<String>emptyList()));
        if (metrics.isEmpty()) {
          return 0L;
        }
        Assert.assertEquals(1, metrics.size());
        MetricTimeSeries ts = metrics.iterator().next();
        Assert.assertEquals(1, ts.getTimeValues().size());
        return ts.getTimeValues().get(0).getValue();
      }
",non-flaky,5
35662,cdapio_cdap,Spark2Test.testScalaSparkWithObjectStore,"  @Test
  public void testScalaSparkWithObjectStore() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    DataSetManager<ObjectStore<String>> keysManager = getDataset(""keys"");
    prepareInputData(keysManager);

    SparkManager sparkManager = applicationManager.getSparkManager(ScalaCharCountProgram.class.getSimpleName()).start();
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);
  }
",non-flaky,5
35663,cdapio_cdap,Spark2Test.testScalaSparkCrossNSStream,"  @Test
  public void testScalaSparkCrossNSStream() throws Exception {
    // create a namespace for input and create a file set instance
    NamespaceMeta inputNSMeta = new NamespaceMeta.Builder().setName(""inputSpaceForSpark"").build();
    getNamespaceAdmin().create(inputNSMeta);
    DatasetId inputDatasetId = inputNSMeta.getNamespaceId().dataset(""input"");
    addDatasetInstance(FileSet.class.getName(), inputDatasetId,
                       FileSetProperties.builder().setInputFormat(TextInputFormat.class).build());

    // create a namespace for dataset and add the dataset instance in it
    NamespaceMeta outputNSMeta = new NamespaceMeta.Builder().setName(""crossNSDataset"").build();
    getNamespaceAdmin().create(outputNSMeta);
    addDatasetInstance(outputNSMeta.getNamespaceId().dataset(""count""), ""keyValueTable"");

    // write something to the input dataset
    Location inputFile = this.<FileSet>getDataset(inputDatasetId).get().getLocation(""inputFile"");
    try (PrintStream printer = new PrintStream(inputFile.getOutputStream(), true, ""UTF-8"")) {
      for (int i = 0; i < 50; i++) {
        printer.println(String.valueOf(i));
      }
    }

    // deploy the spark app in another namespace (default)
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    Map<String, String> args = new HashMap<>();
    args.put(ScalaCrossNSProgram.INPUT_NAMESPACE(), inputNSMeta.getNamespaceId().getNamespace());
    args.put(ScalaCrossNSProgram.OUTPUT_NAMESPACE(), outputNSMeta.getNamespaceId().getNamespace());
    args.put(ScalaCrossNSProgram.OUTPUT_NAME(), ""count"");

    FileSetArguments.setInputPath(args, ""inputFile"");

    SparkManager sparkManager =
      applicationManager.getSparkManager(ScalaCrossNSProgram.class.getSimpleName()).start(args);
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    // get the dataset from the other namespace where we expect it to exist and compare the data
    DataSetManager<KeyValueTable> countManager = getDataset(outputNSMeta.getNamespaceId().dataset(""count""));
    KeyValueTable results = countManager.get();
    for (int i = 0; i < 50; i++) {
      byte[] key = String.valueOf(i).getBytes(Charsets.UTF_8);
      Assert.assertArrayEquals(key, results.read(key));
    }
  }
",non-flaky,5
35664,cdapio_cdap,Spark2Test.testScalaSparkCrossNSDataset,"  @Test
  public void testScalaSparkCrossNSDataset() throws Exception {
    // Deploy and create a dataset in namespace datasetSpaceForSpark
    NamespaceMeta inputDSNSMeta = new NamespaceMeta.Builder().setName(""datasetSpaceForSpark"").build();
    getNamespaceAdmin().create(inputDSNSMeta);
    deploy(inputDSNSMeta.getNamespaceId(), SparkAppUsingObjectStore.class);
    DataSetManager<ObjectStore<String>> keysManager = getDataset(inputDSNSMeta.getNamespaceId().dataset(""keys""));
    prepareInputData(keysManager);

    Map<String, String> args = ImmutableMap.of(ScalaCharCountProgram.INPUT_DATASET_NAMESPACE(),
                                               inputDSNSMeta.getNamespaceId().getNamespace(),
                                               ScalaCharCountProgram.INPUT_DATASET_NAME(), ""keys"");

    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);
    SparkManager sparkManager =
      applicationManager.getSparkManager(ScalaCharCountProgram.class.getSimpleName()).start(args);
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);
  }
",non-flaky,5
35665,cdapio_cdap,Spark2Test.testSparkWithLocalFiles,"  @Test
  public void testSparkWithLocalFiles() throws Exception {
    testSparkWithLocalFiles(SparkAppUsingLocalFiles.class,
                            SparkAppUsingLocalFiles.JavaSparkUsingLocalFiles.class.getSimpleName(), ""java"");
    testSparkWithLocalFiles(SparkAppUsingLocalFiles.class,
                            SparkAppUsingLocalFiles.ScalaSparkUsingLocalFiles.class.getSimpleName(), ""scala"");
  }
",non-flaky,5
35666,cdapio_cdap,Spark2Test.testPySpark,"  @Test
  public void testPySpark() throws Exception {
    if (TestBase.getCurrentSparkCompat() == SparkCompat.SPARK3_2_12) {
      //For spark 3 we need python 3, otherwise skip test
      try {
        Process python = new ProcessBuilder(""python3"", ""-V"").start();
        int resultCode = python.waitFor();
        assumeTrue(""Python3 returned error, result code: "" + resultCode,
                   resultCode == 0);
      } catch (IOException e) {
        assumeNoException(""Python3 can't be started"", e);
      }
    }
    ApplicationManager appManager = deploy(NamespaceId.DEFAULT, Spark2TestApp.class);

    // Write some data to a local file
    File inputFile = TEMP_FOLDER.newFile();
    try (BufferedWriter writer = Files.newBufferedWriter(inputFile.toPath(), StandardCharsets.UTF_8)) {
      for (int i = 0; i < 100; i++) {
        writer.write(""Event "" + i);
        writer.newLine();
      }
    }

    File outputDir = new File(TMP_FOLDER.newFolder(), ""output"");
    appManager.getSparkManager(PythonSpark2.class.getSimpleName()).startAndWaitForGoodRun(
      ImmutableMap.of(""input.file"", inputFile.getAbsolutePath(),
                      ""output.path"", outputDir.getAbsolutePath()),
      ProgramRunStatus.COMPLETED, 2, TimeUnit.MINUTES);

    // Verify the result
    File resultFile = DirUtils.listFiles(outputDir).stream()
      .filter(f -> !f.getName().endsWith("".crc""))
      .filter(f -> !f.getName().startsWith(""_SUCCESS""))
      .findFirst()
      .orElse(null);
    Assert.assertNotNull(resultFile);

    List<String> lines = Files.readAllLines(resultFile.toPath(), StandardCharsets.UTF_8);
    Assert.assertFalse(lines.isEmpty());

    // Expected only even number
    int count = 0;
    for (String line : lines) {
      line = line.trim();
      if (!line.isEmpty()) {
        Assert.assertEquals(""Event "" + count, line);
        count += 2;
      }
    }

    Assert.assertEquals(100, count);

    final Map<String, String> tags = ImmutableMap.of(
      Constants.Metrics.Tag.NAMESPACE, NamespaceId.DEFAULT.getNamespace(),
      Constants.Metrics.Tag.APP, Spark2TestApp.class.getSimpleName(),
      Constants.Metrics.Tag.SPARK, PythonSpark2.class.getSimpleName());

    Tasks.waitFor(100L, () ->  getMetricsManager().getTotalMetric(tags, ""user.body""),
                  5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
  }
",non-flaky,5
35667,cdapio_cdap,IntegrationTestBaseTest.testDeployApplicationInNamespace,"  @Test
  public void testDeployApplicationInNamespace() throws Exception {
    NamespaceId namespace = new NamespaceId(""Test1"");
    NamespaceMeta namespaceMeta = new NamespaceMeta.Builder().setName(namespace).build();
    getNamespaceClient().create(namespaceMeta);
    ClientConfig clientConfig = new ClientConfig.Builder(getClientConfig()).build();
    deployApplication(namespace, AllProgramsApp.class);

    // Check the default namespaces applications to see whether the application wasn't made in the default namespace
    ClientConfig defaultClientConfig = new ClientConfig.Builder(getClientConfig()).build();
    Assert.assertTrue(new ApplicationClient(defaultClientConfig).list(NamespaceId.DEFAULT).isEmpty());

    ApplicationClient applicationClient = new ApplicationClient(clientConfig);
    Assert.assertEquals(AllProgramsApp.NAME, applicationClient.list(namespace).get(0).getName());
    applicationClient.delete(namespace.app(AllProgramsApp.NAME));
    Assert.assertTrue(new ApplicationClient(clientConfig).list(namespace).isEmpty());

  }
",non-flaky,5
35668,cdapio_cdap,IntegrationTestBaseTest.testSQLQuery,"  @Test
  public void testSQLQuery() throws Exception {
    getTestManager().deployDatasetModule(NamespaceId.DEFAULT.datasetModule(""my-kv""), AppUsingCustomModule.Module.class);

    DatasetAdmin dsAdmin = getTestManager().addDatasetInstance(""myKeyValueTable"",
                                                               NamespaceId.DEFAULT.dataset(""myTable""));
    Assert.assertTrue(dsAdmin.exists());

    ApplicationManager appManager = deployApplication(NamespaceId.DEFAULT, AppUsingCustomModule.class);
    ServiceManager serviceManager = appManager.getServiceManager(""MyService"").start();
    serviceManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);

    put(serviceManager, ""a"", ""1"");
    put(serviceManager, ""b"", ""2"");
    put(serviceManager, ""c"", ""1"");

    try (
      Connection connection = getTestManager().getQueryClient(NamespaceId.DEFAULT);
      // the value (character) ""1"" corresponds to the decimal 49. In hex, that is 31.
      ResultSet results = connection.prepareStatement(""select key from dataset_mytable where hex(value) = '31'"")
        .executeQuery()
    ) {
      // run a query over the dataset
      Assert.assertTrue(results.next());
      Assert.assertEquals(""a"", results.getString(1));
      Assert.assertTrue(results.next());
      Assert.assertEquals(""c"", results.getString(1));
      Assert.assertFalse(results.next());
    }

    dsAdmin.drop();
    Assert.assertFalse(dsAdmin.exists());
  }
",non-flaky,5
35669,cdapio_cdap,LogBufferWriterTest.testLogBufferWriter,"  @Test
  public void testLogBufferWriter() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    LogBufferWriter writer = new LogBufferWriter(absolutePath, 100000, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterator<LogBufferEvent> writtenEvents = writer.write(events.iterator()).iterator();
    writer.close();

    int i = 0;
    int startPos = 0;
    // verify if correct offsets were set without file rotation
    while (writtenEvents.hasNext()) {
      LogBufferEvent bufferEvent = writtenEvents.next();
      Assert.assertEquals(String.valueOf(i++), bufferEvent.getLogEvent().getMessage());
      // There will not be any rotation.
      Assert.assertEquals(bufferEvent.getOffset().getFilePos(), startPos);
      startPos = startPos + Bytes.SIZEOF_INT + serializer.toBytes(bufferEvent.getLogEvent()).length;
    }

    // verify if the events were serialized and written correctly
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (byte[] eventBytes : events) {
        ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
        Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
      }
    }
  }
",non-flaky,5
35670,cdapio_cdap,LogBufferWriterTest.testFileRotation,"  @Test
  public void testFileRotation() throws Exception {
    // Make sure rotation happens after every event is written
    LogBufferWriter writer = new LogBufferWriter(TMP_FOLDER.newFolder().getAbsolutePath(), 10, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterator<LogBufferEvent> writtenEvents = writer.write(events.iterator()).iterator();
    writer.close();

    int i = 0;
    // verify if correct offsets and file id were set with file rotation
    while (writtenEvents.hasNext()) {
      LogBufferEvent bufferEvent = writtenEvents.next();
      Assert.assertEquals(bufferEvent.getLogEvent().getMessage(), String.valueOf(i));
      // There will be 2 events in one file.
      Assert.assertEquals(i++ + "".buf"", bufferEvent.getOffset().getFileId() + "".buf"");
      Assert.assertEquals(0, bufferEvent.getOffset().getFilePos());
    }
  }
",non-flaky,5
35671,cdapio_cdap,LogBufferWriterTest.testWritesOnClosedWriter,"  @Test (expected = IOException.class)
  public void testWritesOnClosedWriter() throws IOException {
    LogBufferWriter writer = new LogBufferWriter(TMP_FOLDER.newFolder().getAbsolutePath(), 100000, () -> { });
    writer.close();
    // should throw IOException
    writer.write(ImmutableList.of(
      serializer.toBytes(createLoggingEvent(""test.logger"", Level.INFO, ""0"", 1,
                                            new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"",
                                                                     ""instance1"")))).iterator());
  }
",non-flaky,5
35672,cdapio_cdap,LogBufferRecoveryServiceTest.testLogBufferRecoveryService,"  @Test
  public void testLogBufferRecoveryService() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    // create and start pipeline
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);

    // start the pipeline
    pipeline.startAndWait();

    // write directly to log buffer
    LogBufferWriter writer = new LogBufferWriter(absolutePath, 250, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    writer.write(events.iterator()).iterator();
    writer.close();

    // start log buffer reader to read log events from files. keep the batch size as 2 so that there are more than 1
    // iterations
    LogBufferRecoveryService service = new LogBufferRecoveryService(ImmutableList.of(pipeline),
                                                                    ImmutableList.of(checkpointManager),
                                                                    absolutePath, 2, new AtomicBoolean(true));
    service.startAndWait();

    Tasks.waitFor(5, () -> appender.getEvents().size(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    service.stopAndWait();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35673,cdapio_cdap,ConcurrentLogBufferWriterTest.testWrites,"  @Test
  public void testWrites() throws Exception {
    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    writer.process(new LogBufferRequest(0, events));

    // verify if the events were written to log buffer
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (byte[] eventBytes : events) {
        ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
        Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
      }
    }

    // verify if the pipeline has processed the messages.
    Tasks.waitFor(5, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35674,cdapio_cdap,ConcurrentLogBufferWriterTest.testConcurrentWrites,"  @Test
  public void testConcurrentWrites() throws Exception {
    int threadCount = 20;

    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();

    ExecutorService executor = Executors.newFixedThreadPool(threadCount);
    final CyclicBarrier barrier = new CyclicBarrier(threadCount + 1);
    for (int i = 0; i < threadCount; i++) {
      executor.submit(() -> {
        try {
          barrier.await();
          writer.process(new LogBufferRequest(0, events));
        } catch (Exception e) {
          LOG.error(""Exception raised when processing log events."", e);
        }
      });
    }

    barrier.await();
    executor.shutdown();
    Assert.assertTrue(executor.awaitTermination(1, TimeUnit.MINUTES));

    // verify if the events were written to log buffer
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (int i = 0; i < threadCount; i++) {
        for (byte[] eventBytes : events) {
          ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
          Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
        }
      }
    }

    // verify if the pipeline has processed the messages.
    Tasks.waitFor(100, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35675,cdapio_cdap,LogBufferHandlerTest.testHandler,"  @Test
  public void testHandler() throws Exception {
    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);

    LogBufferProcessorPipeline pipeline = getLogPipeline(loggerContext);
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });

    NettyHttpService httpService = NettyHttpService.builder(""RemoteAppenderTest"")
      .setHttpHandlers(new LogBufferHandler(writer))
      .setExceptionHandler(new HttpExceptionHandler())
      .build();

    httpService.start();

    RemoteLogAppender remoteLogAppender = getRemoteAppender(cConf, httpService);
    remoteLogAppender.start();

    List<ILoggingEvent> events = getLoggingEvents();
    WorkerLoggingContext loggingContext =
      new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"", ""instance1"");
    for (int i = 0; i < 1000; i++) {
      remoteLogAppender.append(new LogMessage(events.get(i % events.size()), loggingContext));
    }

    Tasks.waitFor(1000, () -> appender.getEvents().size(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    remoteLogAppender.stop();
    httpService.stop();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35676,cdapio_cdap,LogBufferCleanerTest.testLogBufferCleanerService,"  @Test
  public void testLogBufferCleanerService() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    // update checkpoints
    checkpointManager.saveCheckpoints(ImmutableMap.of(0, new TestCheckpoint(2L, 0L, 1L)));

    // write directly to log buffer, keep file size 10 bytes so that more files are created
    LogBufferWriter writer = new LogBufferWriter(absolutePath, 10,
                                                 new LogBufferCleaner(ImmutableList.of(checkpointManager),
                                                                      absolutePath, new AtomicBoolean(true)));
    ImmutableList<byte[]> events = getLoggingEvents();
    List<byte[]> subset = new ArrayList<>();
    subset.add(events.get(0));
    subset.add(events.get(1));
    subset.add(events.get(2));
    writer.write(subset.iterator()).iterator();

    // should delete file 0 an1
    File file0 = new File(absolutePath, ""0.buff"");
    File file1 = new File(absolutePath, ""1.buff"");
    Tasks.waitFor(true, () -> !file0.exists() && !file1.exists(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // update checkpoints
    checkpointManager.saveCheckpoints(ImmutableMap.of(0, new TestCheckpoint(5L, 0L, 1L)));

    subset.add(events.get(3));
    subset.add(events.get(4));
    subset.add(events.get(5));
    writer.write(subset.iterator()).iterator();
    writer.close();

    // should delete file 2, 3 and 4
    File file2 = new File(absolutePath, ""2.buff"");
    File file3 = new File(absolutePath, ""3.buff"");
    File file4 = new File(absolutePath, ""4.buff"");
    Tasks.waitFor(true, () -> !file2.exists() && !file3.exists() && !file4.exists(), 120, TimeUnit.SECONDS,
                  100, TimeUnit.MILLISECONDS);

  }
",non-flaky,5
35677,cdapio_cdap,LogBufferReaderTest.testLogReader,"  @Test
  public void testLogReader() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    LogBufferWriter writer = new LogBufferWriter(absolutePath, 250, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterable<LogBufferEvent> writtenEvents = writer.write(events.iterator());
    writer.close();

    List<LogBufferEvent> logBufferEvents = new LinkedList<>();
    // read from start positions, tests case where no checkpoints are persisted
    LogBufferReader reader = new LogBufferReader(absolutePath, 2, 3, -1, -1);
    Iterator<LogBufferEvent> iterator = writtenEvents.iterator();
    verifyEvents(logBufferEvents, reader, iterator);
    reader.close();

    // this should skip first and second event, this is because log buffer offsets are offset for event that is
    // already stored. so in this case, skip first event and skip second event as second event is the last stored event
    reader = new LogBufferReader(absolutePath, 2, 3, 0, 145);
    iterator = writtenEvents.iterator();
    iterator.next();
    iterator.next();
    verifyEvents(logBufferEvents, reader, iterator);
    reader.close();
  }
",non-flaky,5
35678,cdapio_cdap,LogBufferProcessorPipelineTest.testSingleAppender,"  @Test
  public void testSingleAppender() throws Exception {
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    // start thread to write to incomingEventQueue
    List<ILoggingEvent> events = getLoggingEvents();
    AtomicInteger i = new AtomicInteger(0);
    List<LogBufferEvent> bufferEvents = events.stream().map(event -> {
      LogBufferEvent lbe = new LogBufferEvent(event, serializer.toBytes(event).length,
                                              new LogBufferFileOffset(0, i.get()));
      i.incrementAndGet();
      return lbe;
    }).collect(Collectors.toList());

    // start a thread to send log buffer events to pipeline
    ExecutorService executorService = Executors.newSingleThreadExecutor();
    executorService.execute(() -> {
      for (int count = 0; count < 40; count++) {
        pipeline.processLogEvents(bufferEvents.iterator());
        try {
          Thread.sleep(100);
        } catch (InterruptedException e) {
          // should not happen
        }
      }
    });

    // wait for pipeline to append all the logs to appender. The DEBUG message should get filtered out.
    Tasks.waitFor(200, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    executorService.shutdown();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35679,cdapio_cdap,KafkaLogProcessorPipelineTest.testBasicSort,"  @Test
  public void testBasicSort() throws Exception {
    String topic = ""testPipeline"";
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 300L, 1048576, 500L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.DEBUG, ""hidden"", now - 600),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""4"", now - 100))
    );

    // Since the messages are published in one batch, the processor should be able to fetch all of them,
    // hence the sorting order should be deterministic.
    // The DEBUG message should get filtered out
    Tasks.waitFor(5, () -> appender.getEvents().size(), 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    for (int i = 0; i < 5; i++) {
      Assert.assertEquals(Integer.toString(i), appender.getEvents().poll().getMessage());
    }

    // Now publish large messages that exceed the maximum queue size (1024). It should trigger writing regardless of
    // the event timestamp
    List<ILoggingEvent> events = new ArrayList<>(500);
    now = System.currentTimeMillis();
    for (int i = 0; i < 500; i++) {
      // The event timestamp is 10 seconds in future.
      events.add(LogPipelineTestUtil.createLoggingEvent(""test.large.logger"",
                                                        Level.WARN, ""Large logger "" + i, now + 10000));
    }
    publishLog(topic, events);

    Tasks.waitFor(true, () -> !appender.getEvents().isEmpty(), 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    events.clear();
    events.addAll(appender.getEvents());

    for (int i = 0; i < events.size(); i++) {
      Assert.assertEquals(""Large logger "" + i, events.get(i).getMessage());
    }

    pipeline.stopAndWait();
    loggerContext.stop();

    Assert.assertNull(appender.getEvents());
  }
",non-flaky,5
35680,cdapio_cdap,KafkaLogProcessorPipelineTest.testRegularFlush,"  @Test
  public void testRegularFlush() throws Exception {
    String topic = ""testFlush"";
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    TestCheckpointManager checkpointManager = new TestCheckpointManager();

    // Use a longer checkpoint time and a short event delay. Should expect flush called at least once
    // per event delay.
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024, 100, 1048576, 2000);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Even when there is no event, the flush should still get called.
    Tasks.waitFor(5, appender::getFlushCount, 3, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // Publish some logs
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 300),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now + 100)
    ));

    // Wait until getting all logs.
    Tasks.waitFor(3, () -> appender.getEvents().size(), 3, TimeUnit.SECONDS, 200, TimeUnit.MILLISECONDS);

    pipeline.stopAndWait();

    // Should get at least 20 flush calls, since the checkpoint is every 2 seconds
    Assert.assertTrue(appender.getFlushCount() >= 20);
  }
",non-flaky,5
35681,cdapio_cdap,KafkaLogProcessorPipelineTest.testMetricsAppender,"  @Test
  public void testMetricsAppender() throws Exception {
    Injector injector = KAFKA_TESTER.getInjector();
    MetricsCollectionService collectionService = injector.getInstance(MetricsCollectionService.class);
    collectionService.startAndWait();
    LoggerContext loggerContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                           injector.getInstance(LocationFactory.class),
                                                           injector.getInstance(MetricsCollectionService.class));
    final File logDir = TEMP_FOLDER.newFolder();
    loggerContext.putProperty(""logDirectory"", logDir.getAbsolutePath());

    LogPipelineConfigurator configurator = new LogPipelineConfigurator(CConfiguration.create());
    configurator.setContext(loggerContext);
    URL configURL = getClass().getClassLoader().getResource(""pipeline-metric-appender.xml"");
    Assert.assertNotNull(configURL);
    configurator.doConfigure(configURL);

    String topic = ""metricsPipeline"";
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 100L, 1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""testMetricAppender"",
                                      loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka
    long now = System.currentTimeMillis();
    WorkerLoggingContext loggingContext =
      new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"", ""instance1"");
    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 500),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.DEBUG, ""hidden"", now - 600),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""4"", now - 100)),
               loggingContext
    );

    WorkflowProgramLoggingContext workflowProgramLoggingContext =
      new WorkflowProgramLoggingContext(""default"", ""app1"", ""wflow1"", ""run1"", ProgramType.MAPREDUCE, ""mr1"", ""mrun1"");

    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.WARN, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.WARN, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.TRACE, ""3"", now - 500)),
               workflowProgramLoggingContext
    );

    ServiceLoggingContext serviceLoggingContext =
      new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(), Constants.Logging.COMPONENT_NAME,
                                Constants.Service.TRANSACTION);
    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""3"", now - 500),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900)),
               serviceLoggingContext
    );

    final MetricStore metricStore = injector.getInstance(MetricStore.class);
    try {
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.info"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(loggingContext),
                                                 new ArrayList<>()), 5L);

      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.debug"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(loggingContext),
                                                 new ArrayList<>()), 1L);

      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.warn"",
                                                 AggregationFunction.SUM,
                                                 // mapreduce metrics context
                                                 ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, ""default"",
                                                                 Constants.Metrics.Tag.APP, ""app1"",
                                                                 Constants.Metrics.Tag.MAPREDUCE, ""mr1"",
                                                                 Constants.Metrics.Tag.RUN_ID, ""mrun1""),
                                                 new ArrayList<>()), 2L);
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.trace"",
                                                 AggregationFunction.SUM,
                                                 // workflow metrics context
                                                 ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, ""default"",
                                                                 Constants.Metrics.Tag.APP, ""app1"",
                                                                 Constants.Metrics.Tag.WORKFLOW, ""wflow1"",
                                                                 Constants.Metrics.Tag.RUN_ID, ""run1""),
                                                 new ArrayList<>()), 1L);
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.services.log.error"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(serviceLoggingContext),
                                                 new ArrayList<>()), 3L);
    } finally {
      pipeline.stopAndWait();
      loggerContext.stop();
      collectionService.stopAndWait();
    }
  }
",non-flaky,5
35682,cdapio_cdap,KafkaLogProcessorPipelineTest.testMultiAppenders,"  @Test
  public void testMultiAppenders() throws Exception {
    final File logDir = TEMP_FOLDER.newFolder();
    LoggerContext loggerContext = new LoggerContext();
    loggerContext.putProperty(""logDirectory"", logDir.getAbsolutePath());

    LogPipelineConfigurator configurator = new LogPipelineConfigurator(CConfiguration.create());
    configurator.setContext(loggerContext);
    URL configURL = getClass().getClassLoader().getResource(""pipeline-multi-appenders.xml"");
    Assert.assertNotNull(configURL);
    configurator.doConfigure(configURL);

    String topic = ""testMultiAppenders"";
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 100L, 1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""testMultiAppenders"",
                                      loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka using a non-specific logger
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""logger.trace"", Level.TRACE, ""TRACE"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""logger.debug"", Level.DEBUG, ""DEBUG"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""logger.info"", Level.INFO, ""INFO"", now - 800),
      LogPipelineTestUtil.createLoggingEvent(""logger.warn"", Level.WARN, ""WARN"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""logger.error"", Level.ERROR, ""ERROR"", now - 600)
    ));

    // All logs should get logged to the default.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""default.log"");
      List<String> lines = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR"").equals(lines);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // Publish some more log messages via the non-additive ""test.info"" logger.
    now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.info.trace"", Level.TRACE, ""TRACE"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.info.debug"", Level.DEBUG, ""DEBUG"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.info"", Level.INFO, ""INFO"", now - 800),
      LogPipelineTestUtil.createLoggingEvent(""test.info.warn"", Level.WARN, ""WARN"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.info.error"", Level.ERROR, ""ERROR"", now - 600)
    ));

    // Only logs with INFO or above level should get written to the info.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""info.log"");
      List<String> lines = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""INFO"", ""WARN"", ""ERROR"").equals(lines);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // The default.log file shouldn't be changed, because the test.info logger is non additive
    File defaultLogFile = new File(logDir, ""default.log"");
    List<String> lines = Files.readAllLines(defaultLogFile.toPath(), StandardCharsets.UTF_8);
    Assert.assertEquals(Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR""), lines);

    // Publish a log messages via the additive ""test.error"" logger.
    now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.error.1.2"", Level.ERROR, ""ERROR"", now - 1000)
    ));

    // Expect the log get appended to both the error.log file as well as the default.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""error.log"");
      List<String> lines1 = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      if (!Collections.singletonList(""ERROR"").equals(lines1)) {
        return false;
      }

      logFile = new File(logDir, ""default.log"");
      lines1 = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR"", ""ERROR"").equals(lines1);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35683,cdapio_cdap,KafkaOffsetResolverTest.testOutOfOrderEvents,"  @Test
  public void testOutOfOrderEvents() throws Exception {
    String topic = ""testOutOfOrderEvents"";
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, EVENT_DELAY_MILLIS,
                                                         1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    // Publish log messages to Kafka and wait for all messages to be published
    long baseTime = System.currentTimeMillis() - 2 * EVENT_DELAY_MILLIS;
    List<ILoggingEvent> outOfOrderEvents = ImmutableList.of(
      createLoggingEvent(""test.logger"", Level.INFO, ""0"", baseTime - 20 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""0"", baseTime - 20 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 7 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""2"", baseTime - 9 * 100),
      createLoggingEvent(""test.logger"", Level.INFO, ""3"", baseTime - 500),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000 + EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000 - EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 10 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 600),
      createLoggingEvent(""test.logger"", Level.INFO, ""5"", baseTime - 20 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""5"", baseTime - 20 * 1000 + EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""6"", baseTime - 600),
      createLoggingEvent(""test.logger"", Level.INFO, ""6"", baseTime - 10 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""7"", baseTime - 2 * 1000 + EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""8"", baseTime - 7 * 1000 + EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""4"", baseTime - 100 + EVENT_DELAY_MILLIS));
    publishLog(topic, outOfOrderEvents);
    waitForAllLogsPublished(topic, outOfOrderEvents.size());

    KafkaOffsetResolver offsetResolver = new KafkaOffsetResolver(KAFKA_TESTER.getBrokerService(), config);
    // Use every event's timestamp as target time and assert that found offset with target timestamp
    // matches the expected offset
    for (ILoggingEvent event : outOfOrderEvents) {
      assertOffsetResolverResult(offsetResolver, outOfOrderEvents, event.getTimeStamp(), baseTime);
    }

    // Try to find the offset with an event time that has timestamp earlier than all event timestamps in Kafka
    assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                               baseTime - 10 * EVENT_DELAY_MILLIS, baseTime);

    // Try to find the offset with an event time that has timestamp larger than all event timestamps in Kafka
    assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                               baseTime + 10 * EVENT_DELAY_MILLIS, baseTime);

    // Use a random number between (timestamp - EVENT_DELAY_MILLIS, timestamp + EVENT_DELAY_MILLIS) as target time
    // and assert that found offset with target timestamp matches the expected offset
    for (int i = 0; i < 10; i++) {
      for (ILoggingEvent event : outOfOrderEvents) {
        assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                                   event.getTimeStamp() + RANDOM.nextInt() % EVENT_DELAY_MILLIS, baseTime);
      }
    }
  }
",non-flaky,5
35684,cdapio_cdap,KafkaOffsetResolverTest.testInOrderEvents,"  @Test
  public void testInOrderEvents() throws InterruptedException, IOException {
    String topic = ""testInOrderEvents"";
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, EVENT_DELAY_MILLIS,
                                                         1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    // Publish log messages to Kafka and wait for all messages to be published
    long baseTime = System.currentTimeMillis() - EVENT_DELAY_MILLIS;
    List<ILoggingEvent> inOrderEvents = new ArrayList<>();
    for (int i = 0; i < 20; i++) {
      inOrderEvents.add(createLoggingEvent(""test.logger"", Level.INFO, Integer.toString(i), baseTime + i));
    }
    publishLog(topic, inOrderEvents);
    waitForAllLogsPublished(topic, inOrderEvents.size());

    KafkaOffsetResolver offsetResolver = new KafkaOffsetResolver(KAFKA_TESTER.getBrokerService(), config);
    // Use every event's timestamp as target time and assert that found offset is the next offset of the current offset
    for (int i = 0; i < inOrderEvents.size(); i++) {
      long targetTime = inOrderEvents.get(i).getTimeStamp();
      long offset = offsetResolver.getStartOffset(Long.MAX_VALUE, targetTime, 0);
      Assert.assertEquals(""Failed to find the expected event with the target time: "" + targetTime,
                          i + 1, offset);
    }
  }
",non-flaky,5
35685,cdapio_cdap,TimeEventQueueTest.compare,"  @Test
  public void testOrdering() {
    TimeEventQueue<TimestampedEvent, Integer> eventQueue = new TimeEventQueue<>(ImmutableSet.of(1, 3));
    List<TimestampedEvent> expected = new ArrayList<>();

    // Put 10 events to partition 1, with both increasing timestamps and offsets
    for (int i = 0; i < 10; i++) {
      TimestampedEvent event = new TimestampedEvent(i, ""m1"" + i);
      eventQueue.add(event, i, 10, 1, i);
      expected.add(event);
    }

    // Put 10 events to partition 3, with increasing offsets, but non-sorted, some duplicated timestamps
    for (int i = 0; i < 10; i++) {
      long timestamp = i % 3;
      TimestampedEvent event = new TimestampedEvent(timestamp, ""m3"" + i);
      eventQueue.add(event, timestamp, 10, 3, i);
      expected.add(event);
    }

    // Sort the expected result based on timestamp. Since Collections.sort is stable sort,
    // partition 1 events will always be before partition 3 if the timestamps are the same
    Collections.sort(expected, new Comparator<TimestampedEvent>() {
      @Override
      public int compare(TimestampedEvent o1, TimestampedEvent o2) {
        return Long.compare(o1.getTimestamp(), o2.getTimestamp());
      }
",non-flaky,5
35686,cdapio_cdap,TimeEventQueueTest.testKafkaOffset,"  @Test
  public void testKafkaOffset() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(ImmutableSet.of(1, 2));

    // Insert 6 events, with timestamps going back and forth
    eventQueue.add(""m7"", 7L, 10, 1, 0);
    eventQueue.add(""m5"", 5L, 10, 2, 1);
    eventQueue.add(""m10"", 10L, 10, 1, 2);
    eventQueue.add(""m11"", 11L, 10, 2, 3);
    eventQueue.add(""m2"", 2L, 10, 1, 4);
    eventQueue.add(""m8"", 8L, 10, 2, 5);

    // Have 6 events of size 10, so total size should be 60
    Assert.assertEquals(60, eventQueue.getEventSize());

    // Events should be time ordered when getting from iterator
    TimeEventQueue.EventIterator<String, Integer> iterator = eventQueue.iterator();

    Assert.assertEquals(""m2"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(4, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(50, eventQueue.getEventSize());
    Assert.assertEquals(0, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(1, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m5"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(1, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(40, eventQueue.getEventSize());
    Assert.assertEquals(0, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m7"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(0, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(30, eventQueue.getEventSize());
    Assert.assertEquals(2, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m8"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(5, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(20, eventQueue.getEventSize());
    Assert.assertEquals(2, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m10"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(2, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(10, eventQueue.getEventSize());
    Assert.assertTrue(eventQueue.isEmpty(1));

    Assert.assertEquals(""m11"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(3, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(0, eventQueue.getEventSize());
    Assert.assertTrue(eventQueue.isEmpty(2));

    Assert.assertTrue(eventQueue.isEmpty());
  }
",non-flaky,5
35687,cdapio_cdap,TimeEventQueueTest.testInvalidPartition,"  @Test (expected = IllegalArgumentException.class)
  public void testInvalidPartition() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(Collections.singleton(1));
    eventQueue.add(""test"", 1L, 10, 2, 0);
  }
",non-flaky,5
35688,cdapio_cdap,TimeEventQueueTest.testIllegalRemove,"  @Test (expected = IllegalStateException.class)
  public void testIllegalRemove() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(Collections.singleton(1));
    eventQueue.add(""test"", 1L, 10, 1, 0);
    Iterator<String> iterator = eventQueue.iterator();
    iterator.next();
    iterator.remove();
    iterator.remove();
  }
",non-flaky,5
35689,cdapio_cdap,TimeEventQueueProcessorTest.test,"  @Test
  public void test() throws Exception {
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    LogProcessorPipelineContext context = new LogProcessorPipelineContext(CConfiguration.create(),
                                                                          ""test"", loggerContext, NO_OP_METRICS_CONTEXT,
                                                                          0);
    context.start();
    TimeEventQueueProcessor<TestOffset> processor = new TimeEventQueueProcessor<>(context, 50, 1,
                                                                                  ImmutableList.of(0));
    long now = System.currentTimeMillis();
    List<ILoggingEvent> events = ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""5"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""4"", now - 600),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""6"", now - 100));

    ProcessedEventMetadata<TestOffset> metadata = processor.process(0, new TransformingIterator(events.iterator()));
    // all 6 events should be processed. This is because when the buffer is full after 5 events, time event queue
    // processor should append existing buffered events and enqueue 6th event
    Assert.assertEquals(6, metadata.getTotalEventsProcessed());
    for (Map.Entry<Integer, Checkpoint<TestOffset>> entry : metadata.getCheckpoints().entrySet()) {
      Checkpoint<TestOffset> value = entry.getValue();
      // offset should be max offset processed so far
      Assert.assertEquals(6, value.getOffset().getOffset());
    }
  }
",non-flaky,5
35690,cdapio_cdap,LoggersTest.testEffectiveLevel,"  @Test
  public void testEffectiveLevel() throws Exception {
    LoggerContext context = new LoggerContext();
    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(context);
    configurator.doConfigure(new InputSource(new StringReader(generateLogback(""WARN"", ImmutableMap.of(
      ""test"", ""INFO"",
      ""test.a"", ""ERROR"",
      ""test.a.X"", ""DEBUG"",
      ""test.a.X$1"", ""OFF""
    )))));

    Assert.assertSame(context.getLogger(""test""), Loggers.getEffectiveLogger(context, ""test""));
    Assert.assertSame(context.getLogger(""test.a""), Loggers.getEffectiveLogger(context, ""test.a""));
    Assert.assertSame(context.getLogger(""test.a.X""), Loggers.getEffectiveLogger(context, ""test.a.X""));
    Assert.assertSame(context.getLogger(""test.a.X$1""), Loggers.getEffectiveLogger(context, ""test.a.X$1""));

    Assert.assertSame(context.getLogger(Logger.ROOT_LOGGER_NAME),
                      Loggers.getEffectiveLogger(context, ""defaultToRoot""));
    Assert.assertSame(context.getLogger(""test""),
                      Loggers.getEffectiveLogger(context, ""test.defaultToTest""));
    Assert.assertSame(context.getLogger(""test.a""),
                      Loggers.getEffectiveLogger(context, ""test.a.defaultToTestDotA""));
    Assert.assertSame(context.getLogger(""test.a.X""),
                      Loggers.getEffectiveLogger(context, ""test.a.X.defaultToTestDotADotX""));
  }
",non-flaky,5
35691,cdapio_cdap,FileMetadataTest.testFileMetadataReadWrite,"  @Test
  public void testFileMetadataReadWrite() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    LogPathIdentifier logPathIdentifier =
      new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(), ""testApp"", ""testFlow"");
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
    long currentTime = System.currentTimeMillis();
    for (int i = 10; i <= 100; i += 10) {
      // i is the event time
      fileMetaDataWriter.writeMetaData(logPathIdentifier, i, currentTime,
                                       location.append(Integer.toString(i)));
    }

    // for the timestamp 80, add new new log path id with different current time.

    fileMetaDataWriter.writeMetaData(logPathIdentifier, 80, currentTime + 1,
                                     location.append(""81""));

    fileMetaDataWriter.writeMetaData(logPathIdentifier, 80, currentTime + 2,
                                     location.append(""82""));

    // reader test
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);

    Assert.assertEquals(12, fileMetadataReader.listFiles(logPathIdentifier, 0, 100).size());
    Assert.assertEquals(5, fileMetadataReader.listFiles(logPathIdentifier, 20, 50).size());
    Assert.assertEquals(2, fileMetadataReader.listFiles(logPathIdentifier, 100, 150).size());

    // should include the latest file with event start time 80.
    List<LogLocation> locationList = fileMetadataReader.listFiles(logPathIdentifier, 81, 85);
    Assert.assertEquals(1, locationList.size());
    Assert.assertEquals(80, locationList.get(0).getEventTimeMs());
    Assert.assertEquals(location.append(""82""), locationList.get(0).getLocation());

    Assert.assertEquals(1, fileMetadataReader.listFiles(logPathIdentifier, 150, 1000).size());
  }
",non-flaky,5
35692,cdapio_cdap,FileMetadataTest.testFileMetadataReadWriteAcrossFormats,"  @Test
  public void testFileMetadataReadWriteAcrossFormats() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    LogPathIdentifier logPathIdentifier =
      new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(), ""testApp"", ""testFlow"");
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
    long currentTime = System.currentTimeMillis();

    long eventTime = currentTime + 20;
    long newCurrentTime = currentTime + 100;
    // 10 files in new format
    for (int i = 1; i <= 10; i++) {
      fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTime + i, newCurrentTime + i,
                                       location.append(""testFileNew"" + Integer.toString(i)));
    }
    // reader test
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);

    // scan only in new files time range
    List<LogLocation> locations = fileMetadataReader.listFiles(logPathIdentifier, eventTime + 2, eventTime + 6);
    // should include files from currentTime (1..6)
    Assert.assertEquals(6, locations.size());
    for (LogLocation logLocation : locations) {
      Assert.assertEquals(LogLocation.VERSION_1, logLocation.getFrameworkVersion());
    }

    // scan time range across formats
    locations = fileMetadataReader.listFiles(logPathIdentifier, currentTime + 2, eventTime + 6);
    // should include files from new range (1..6)
    Assert.assertEquals(6, locations.size());

    for (int i = 0; i < locations.size(); i++) {
      Assert.assertEquals(LogLocation.VERSION_1, locations.get(i).getFrameworkVersion());
      Assert.assertEquals(location.append(""testFileNew"" + Integer.toString(i + 1)), locations.get(i).getLocation());
    }
  }
",non-flaky,5
35693,cdapio_cdap,DistributedLogFrameworkTest.testFramework,"  @Test
  public void testFramework() throws Exception {
    DistributedLogFramework framework = injector.getInstance(DistributedLogFramework.class);
    CConfiguration cConf = injector.getInstance(CConfiguration.class);

    framework.startAndWait();

    // Send some logs to Kafka.
    LoggingContext context = new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(),
                                                       Constants.Logging.COMPONENT_NAME,
                                                       ""test"");

    // Make sure all events get flushed in the same batch
    long eventTimeBase = System.currentTimeMillis() + cConf.getInt(Constants.Logging.PIPELINE_EVENT_DELAY_MS);
    final int msgCount = 50;
    for (int i = 0; i < msgCount; i++) {
      // Publish logs in descending timestamp order
      publishLog(
        cConf.get(Constants.Logging.KAFKA_TOPIC), context,
        ImmutableList.of(
          createLoggingEvent(""io.cdap.test."" + i, Level.INFO, ""Testing "" + i, eventTimeBase - i)
        )
      );
    }

    // Read the logs back. They should be sorted by timestamp order.
    final FileMetaDataReader metaDataReader = injector.getInstance(FileMetaDataReader.class);
    Tasks.waitFor(true, () -> {
      List<LogLocation> locations = metaDataReader.listFiles(new LogPathIdentifier(NamespaceId.SYSTEM.getNamespace(),
                                                                                    Constants.Logging.COMPONENT_NAME,
                                                                                   ""test""), 0, Long.MAX_VALUE);
      if (locations.size() != 1) {
        return false;
      }
      LogLocation location = locations.get(0);
      int i = 0;
      try {
        try (CloseableIterator<LogEvent> iter = location.readLog(Filter.EMPTY_FILTER, 0, Long.MAX_VALUE, msgCount)) {
          while (iter.hasNext()) {
            String expectedMsg = ""Testing "" + (msgCount - i - 1);
            LogEvent event = iter.next();
            if (!expectedMsg.equals(event.getLoggingEvent().getMessage())) {
              return false;
            }
            i++;
          }
          return i == msgCount;
        }
      } catch (Exception e) {
        // It's possible the file is an invalid Avro file due to a race between creation of the meta data
        // and the time when actual content are flushed to the file
        return false;
      }
    }, 10, TimeUnit.SECONDS, msgCount, TimeUnit.MILLISECONDS);

    framework.stopAndWait();

    String kafkaTopic = cConf.get(Constants.Logging.KAFKA_TOPIC);
    // Check the checkpoint is persisted correctly. Since all messages are processed,
    // the checkpoint should be the same as the message count.
    CheckpointManager<KafkaOffset> checkpointManager = getCheckpointManager(kafkaTopic);
    Checkpoint<KafkaOffset> checkpoint = checkpointManager.getCheckpoint(0);
    Assert.assertEquals(msgCount, checkpoint.getOffset().getNextOffset());
  }
",non-flaky,5
35694,cdapio_cdap,LoggingEventSerializerTest.testEmptySerialization,"  @Test
  public void testEmptySerialization() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventSerializerTest.class);
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, ""message"", null, null);
    iLoggingEvent.setThreadName(""thread-1"");
    iLoggingEvent.setTimeStamp(10000000L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
",non-flaky,5
35695,cdapio_cdap,LoggingEventSerializerTest.testSerialization,"  @Test
  public void testSerialization() throws Exception {
    Map<String, String> mdcMap = Maps.newHashMap();
    mdcMap.put(""mdc1"", ""mdc-val1"");
    mdcMap.put(""mdc2"", null);
    mdcMap.put(null, null);

    Map<String, String> contextMap = Maps.newHashMap();
    contextMap.put(""p1"", ""ctx-val1"");
    contextMap.put(""p2"", null);
    contextMap.put(null, null);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent();
    iLoggingEvent.setThreadName(""threadName1"");
    iLoggingEvent.setLevel(Level.INFO);
    iLoggingEvent.setMessage(""Log message1"");
    iLoggingEvent.setArgumentArray(new Object[]{null, ""arg2"", ""100"", null});
    iLoggingEvent.setLoggerName(""loggerName1"");

    iLoggingEvent.setLoggerContextRemoteView(new LoggerContextVO(""logger_context1"", contextMap, 12345634234L));

    Exception e1 = new Exception(null, null);
    Exception e2 = new Exception(""Test Exception2"", e1);
    iLoggingEvent.setThrowableProxy(new ThrowableProxy(e2));
    iLoggingEvent.prepareForDeferredProcessing();
    ((ThrowableProxy) iLoggingEvent.getThrowableProxy()).calculatePackagingData();

    iLoggingEvent.setCallerData(new StackTraceElement[]{
      new StackTraceElement(""com.Class1"", ""methodName1"", ""fileName1"", 10),
      null,
      new StackTraceElement(""com.Class2"", ""methodName2"", ""fileName2"", 20),
      new StackTraceElement(""com.Class3"",  ""methodName3"", null, 30),
      null
    });

    iLoggingEvent.setMarker(null);
    iLoggingEvent.getMDCPropertyMap().putAll(mdcMap);
    iLoggingEvent.setTimeStamp(1234567890L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));
    System.out.println(actualEvent);
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
",non-flaky,5
35696,cdapio_cdap,LoggingEventSerializerTest.testOldSystemLoggingContext,"  @Test
  public void testOldSystemLoggingContext() throws Exception {
    // see: CDAP-7482
    Map<String, String> mdcMap = new HashMap<>();
    mdcMap.put(ServiceLoggingContext.TAG_SYSTEM_ID, ""ns1"");
    mdcMap.put(ComponentLoggingContext.TAG_COMPONENT_ID, ""comp1"");
    mdcMap.put(ServiceLoggingContext.TAG_SERVICE_ID, ""ser1"");

    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent();
    iLoggingEvent.setCallerData(new StackTraceElement[] { null });
    iLoggingEvent.setMDCPropertyMap(mdcMap);

    Assert.assertTrue(LoggingContextHelper.getLoggingContext(iLoggingEvent.getMDCPropertyMap())
                        instanceof ServiceLoggingContext);
  }
",non-flaky,5
35697,cdapio_cdap,LoggingEventSerializerTest.testNullSerialization,"  @Test
  public void testNullSerialization() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventSerializerTest.class);
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      null, (ch.qos.logback.classic.Logger) logger, null, null, null, null);
    iLoggingEvent.setThreadName(null);
    iLoggingEvent.setLevel(null);
    iLoggingEvent.setMessage(null);
    iLoggingEvent.setArgumentArray(null);
    iLoggingEvent.setLoggerName(null);
    iLoggingEvent.setLoggerContextRemoteView(null);
    iLoggingEvent.setThrowableProxy(null);
    iLoggingEvent.setCallerData(null);
    iLoggingEvent.setMarker(null);
    iLoggingEvent.setMDCPropertyMap(null);
    iLoggingEvent.setTimeStamp(10000000L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));

    iLoggingEvent.setLevel(Level.ERROR);
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
",non-flaky,5
35698,cdapio_cdap,LoggingEventSerializerTest.testDecodeTimestamp,"  @Test
  public void testDecodeTimestamp() throws IOException {
    long timestamp = System.currentTimeMillis();

    ch.qos.logback.classic.spi.LoggingEvent event = new ch.qos.logback.classic.spi.LoggingEvent();
    event.setLevel(Level.INFO);
    event.setLoggerName(""test.logger"");
    event.setMessage(""Some test"");
    event.setTimeStamp(timestamp);

    // Serialize it
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] bytes = serializer.toBytes(event);

    // Decode timestamp
    Assert.assertEquals(timestamp, serializer.decodeEventTimestamp(ByteBuffer.wrap(bytes)));
  }
",non-flaky,5
35699,cdapio_cdap,LoggingEventTest.testSerialize,"  @Test
  public void testSerialize() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventTest.class);
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, ""Log message1"", null,
      new Object[] {""arg1"", ""arg2"", ""100""});
    iLoggingEvent.setLoggerName(""loggerName1"");
    iLoggingEvent.setThreadName(""threadName1"");
    iLoggingEvent.setTimeStamp(1234567890L);
    iLoggingEvent.setLoggerContextRemoteView(new LoggerContextVO(""loggerContextRemoteView"",
                                                                 ImmutableMap.of(""key1"", ""value1"", ""key2"", ""value2""),
                                                                 100000L));
    Map<String, String> mdcMap = Maps.newHashMap(ImmutableMap.of(""mdck1"", ""mdcv1"", ""mdck2"", ""mdck2""));
    iLoggingEvent.setMDCPropertyMap(mdcMap);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] encoded = serializer.toBytes(new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext()));

    ILoggingEvent decodedEvent = serializer.fromBytes(ByteBuffer.wrap(encoded));
    LoggingEventSerializerTest.assertLoggingEventEquals(iLoggingEvent, decodedEvent);
  }
",non-flaky,5
35700,cdapio_cdap,LoggingEventTest.testEmptySerialize,"  @Test
  public void testEmptySerialize() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventTest.class);
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, null, null, null);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] encoded = serializer.toBytes(new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext()));

    ILoggingEvent decodedEvent = serializer.fromBytes(ByteBuffer.wrap(encoded));
    LoggingEventSerializerTest.assertLoggingEventEquals(iLoggingEvent, decodedEvent);
  }
",non-flaky,5
35701,cdapio_cdap,FileMetadataCleanerTest.testScanAndDeleteNewMetadata,"  @Test
  public void testScanAndDeleteNewMetadata() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      long currentTime = System.currentTimeMillis();
      long eventTimestamp = currentTime - 100;
      LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testFlow"");
      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      List<String> expected = new ArrayList<>();
      for (int i = 0; i < 100; i++) {
        Location location = locationFactory.create(""testFlowFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      long tillTime = currentTime + 50;
      List<FileMetadataCleaner.DeletedEntry> deletedEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51 rows, till time is inclusive
      Assert.assertEquals(51, deletedEntries.size());
      int count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }
      // now add 10 entries for spark
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testSpark"");
      expected = new ArrayList<>();

      for (int i = 0; i < 10; i++) {
        Location location = locationFactory.create(""testSparkFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      // lets keep the same till time - this should only delete the spark entries now
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51 rows, till time is inclusive
      Assert.assertEquals(10, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now add 10 entries in mr context in time range 60-70
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testMr"");
      expected = new ArrayList<>();

      // flow should come up at the beginning in the expected list
      for (int i = 51; i <= 70; i++) {
        expected.add(locationFactory.create(""testFlowFile"" + i).toURI().getPath());
      }

      for (int i = 0; i < 10; i++) {
        Location location = locationFactory.create(""testMrFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      List<String> nextExpected = new ArrayList<>();
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testCustomAction"");
      for (int i = 90; i < 100; i++) {
        Location location = locationFactory.create(""testActionFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        nextExpected.add(location.toURI().getPath());
      }

      tillTime = currentTime + 70;
      // lets delete till 70.
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51-70 files of flow and 0-9 files of spark files in that order and 0 files of action.
      Assert.assertEquals(30, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now delete till currentTime + 100, this should delete all remaining entries.
      // custom action should come first and then flow entries

      tillTime = currentTime + 100;
      // lets delete till 100.
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 90-99 of custom action(10) 71-99 (29) files of flow.
      for (int i = 71; i < 100; i++) {
        nextExpected.add(locationFactory.create(""testFlowFile"" + i).toURI().getPath());
      }
      Assert.assertEquals(39, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(nextExpected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now lets do a delete with till time  = currentTime + 1000, this should return empty result
      tillTime = currentTime + 1000;
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      Assert.assertEquals(0, deletedEntries.size());
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
",non-flaky,5
35702,cdapio_cdap,FileMetadataCleanerTest.testFileMetadataWithCommonContextPrefix,"  @Test
  public void testFileMetadataWithCommonContextPrefix() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      List<LogPathIdentifier> logPathIdentifiers = new ArrayList<>();
      // we write entries where program id is of format testFlow{1..20},
      // this should be able to scan and delete common prefix programs like testFlow1, testFlow10 during clenaup.
      for (int i = 1; i <= 20; i++) {
        logPathIdentifiers.add(new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(),
                                                     ""testApp"", String.format(""testFlow%s"", i)));
      }

      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
      long currentTime = System.currentTimeMillis();
      long newCurrentTime = currentTime + 100;

      for (int i = 1; i <= 20; i++) {
        LogPathIdentifier identifier = logPathIdentifiers.get(i - 1);
        for (int j = 0; j < 10; j++) {
          fileMetaDataWriter.writeMetaData(identifier, newCurrentTime + j, newCurrentTime + j,
                                           location.append(""testFileNew"" + Integer.toString(j)));
        }
      }

      List<LogLocation> locations;
      for (int i = 1; i <= 20; i++) {
        locations = fileMetadataReader.listFiles(logPathIdentifiers.get(i - 1),
                                                 newCurrentTime, newCurrentTime + 10);
        // should include files from currentTime (0..9)
        Assert.assertEquals(10, locations.size());
      }

      long tillTime = newCurrentTime + 4;
      List<FileMetadataCleaner.DeletedEntry> deleteEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // 20 context, 5 entries each
      Assert.assertEquals(100, deleteEntries.size());
      for (int i = 1; i <= 20; i++) {
        locations = fileMetadataReader.listFiles(logPathIdentifiers.get(i - 1),
                                                 newCurrentTime, newCurrentTime + 10);
        // should include files from time (5..9)
        Assert.assertEquals(5, locations.size());
        int startIndex = 5;
        for (LogLocation logLocation : locations) {
          Assert.assertEquals(String.format(""testFileNew%s"", startIndex), logLocation.getLocation().getName());
          startIndex++;
        }
      }
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
",non-flaky,5
35703,cdapio_cdap,FileMetadataCleanerTest.testWithBatchSizeLargerThanNumOfFiles,"  @Test
  public void testWithBatchSizeLargerThanNumOfFiles() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      LogPathIdentifier identifier = new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(),
                                                           ""testApp"", String.format(""testFlow%s"", 0));

      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
      long currentTime = System.currentTimeMillis();
      long newCurrentTime = currentTime + 100;

      for (int j = 0; j < 10; j++) {
        fileMetaDataWriter.writeMetaData(identifier, newCurrentTime + j, newCurrentTime + j,
                                         location.append(""testFileNew"" + Integer.toString(j)));
      }

      List<LogLocation> locations;
      locations = fileMetadataReader.listFiles(identifier, newCurrentTime, newCurrentTime + 10);
      // should include files from currentTime (0..9)
      Assert.assertEquals(10, locations.size());

      long tillTime = newCurrentTime + 4;
      List<FileMetadataCleaner.DeletedEntry> deleteEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 1000);
      Assert.assertEquals(5, deleteEntries.size());
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
",non-flaky,5
35704,cdapio_cdap,LogCleanerTest.testLogCleanup,"  @Test
  public void testLogCleanup() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    long currentTime = System.currentTimeMillis();
    LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""testNs"", ""testApp"", ""testEntity"");
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    long startTime = currentTime - 5000;
    Location dirLocation = locationFactory.create(""logs"");
    dirLocation.mkdirs();
    // create 20 files, add them in past time range
    for (int i = 0; i < 20; i++) {
      Location location = dirLocation.append(""test"" + i);
      location.createNew();
      fileMetaDataWriter.writeMetaData(logPathIdentifier, startTime + i, startTime + i, location);
    }

    Assert.assertEquals(20, dirLocation.list().size());
    LogCleaner logCleaner = new LogCleaner(fileMetadataCleaner, locationFactory, 100, 60);
    logCleaner.run();
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    // all meta data should be deleted
    Assert.assertEquals(0, fileMetaDataReader.listFiles(logPathIdentifier, 0, System.currentTimeMillis()).size());
    // we are not asserting file existence as the delete could fail and we don't guarantee file deletion.
  }
",non-flaky,5
35705,cdapio_cdap,TestFileLogging.testGetLogNext,"  @Test
  public void testGetLogNext() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logReader = injector.getInstance(FileLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetNext(logReader, loggingContext);
  }
",non-flaky,5
35706,cdapio_cdap,TestFileLogging.testGetLogPrev,"  @Test
  public void testGetLogPrev() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logReader = injector.getInstance(FileLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetPrev(logReader, loggingContext);
  }
",non-flaky,5
35707,cdapio_cdap,TestFileLogging.testGetLog,"  @Test
  public void testGetLog() throws Exception {
    // LogReader.getLog is tested in LogSaverTest for distributed mode
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logTail = injector.getInstance(FileLogReader.class);
    LoggingTester.LogCallback logCallback1 = new LoggingTester.LogCallback();
    logTail.getLogPrev(loggingContext, ReadRange.LATEST, 60, Filter.EMPTY_FILTER,
                       logCallback1);
    List<LogEvent> allEvents = logCallback1.getEvents();
    Assert.assertEquals(60, allEvents.size());

    List<LogEvent> events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(10).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(15).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));

    Assert.assertEquals(5, events.size());
    Assert.assertEquals(allEvents.get(10).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(14).getLoggingEvent().getFormattedMessage(),
                        events.get(4).getLoggingEvent().getFormattedMessage());


    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(0).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(59).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(59, events.size());
    Assert.assertEquals(allEvents.get(0).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(58).getLoggingEvent().getFormattedMessage(),
                        events.get(58).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(12).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(41).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(29, events.size());
    Assert.assertEquals(allEvents.get(12).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(40).getLoggingEvent().getFormattedMessage(),
                        events.get(28).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(22).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(38).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(16, events.size());
    Assert.assertEquals(allEvents.get(22).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(37).getLoggingEvent().getFormattedMessage(),
                        events.get(15).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(41).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(59).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(18, events.size());
    Assert.assertEquals(allEvents.get(41).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(58).getLoggingEvent().getFormattedMessage(),
                        events.get(17).getLoggingEvent().getFormattedMessage());

    // Try with null run id, should get all logs for WORKER_1
    LoggingContext loggingContext1 = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", null, ""INSTANCE1"");
    events =
      Lists.newArrayList(logTail.getLog(loggingContext1, 0, Long.MAX_VALUE, Filter.EMPTY_FILTER));
    Assert.assertEquals(120, events.size());
  }
",non-flaky,5
35708,cdapio_cdap,CDAPLogAppenderTest.testCDAPLogAppender,"  @Test
  public void testCDAPLogAppender() {
    int syncInterval = 1024 * 1024;
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(TimeUnit.DAYS.toMillis(1));
    cdapLogAppender.setMaxFileSizeInBytes(104857600);
    cdapLogAppender.setDirPermissions(""700"");
    cdapLogAppender.setFilePermissions(""600"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    LoggingEvent event =
      new LoggingEvent(""io.cdap.Test"",
                       (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                       Level.ERROR , ""test message"", null, null);
    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""default"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    event.setMDCPropertyMap(properties);

    cdapLogAppender.doAppend(event);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(1, files.size());
      LogLocation logLocation = files.get(0);
      Assert.assertEquals(LogLocation.VERSION_1, logLocation.getFrameworkVersion());
      Assert.assertTrue(logLocation.getLocation().exists());
      CloseableIterator<LogEvent> logEventCloseableIterator =
        logLocation.readLog(Filter.EMPTY_FILTER, 0, Long.MAX_VALUE, Integer.MAX_VALUE);
      int logCount = 0;
      while (logEventCloseableIterator.hasNext()) {
        logCount++;
        LogEvent logEvent = logEventCloseableIterator.next();
        Assert.assertEquals(event.getMessage(), logEvent.getLoggingEvent().getMessage());
      }
      logEventCloseableIterator.close();
      Assert.assertEquals(1, logCount);
      // checking permission
      String expectedPermissions = ""rw-------"";
      for (LogLocation file : files) {
        Location location = file.getLocation();
        Assert.assertEquals(expectedPermissions, location.getPermissions());
      }
    } catch (Exception e) {
      Assert.fail();
    }
  }
",non-flaky,5
35709,cdapio_cdap,CDAPLogAppenderTest.testCDAPLogAppenderRotation,"  @Test
  public void testCDAPLogAppenderRotation() throws Exception {
    int syncInterval = 1024 * 1024;
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(500);
    cdapLogAppender.setMaxFileSizeInBytes(104857600);
    cdapLogAppender.setDirPermissions(""750"");
    cdapLogAppender.setFilePermissions(""640"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""testTimeRotation"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    long currentTimeMillisEvent1 = System.currentTimeMillis();

    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"", properties);

    event1.setTimeStamp(currentTimeMillisEvent1);
    cdapLogAppender.doAppend(event1);

    // Pause pass the max file lifetime ms
    TimeUnit.MILLISECONDS.sleep(500);

    long currentTimeMillisEvent2 = System.currentTimeMillis();

    LoggingEvent event2 = getLoggingEvent(""io.cdap.Test2"",
                                          (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(
                                            Logger.ROOT_LOGGER_NAME), Level.ERROR , ""test message 2"", properties);
    event2.setTimeStamp(currentTimeMillisEvent1 + 1000);
    cdapLogAppender.doAppend(event2);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(2, files.size());
      assertLogEventDetails(event1, files.get(0));
      assertLogEventDetails(event2, files.get(1));
      Assert.assertEquals(currentTimeMillisEvent1, files.get(0).getEventTimeMs());
      Assert.assertEquals(currentTimeMillisEvent1 + 1000, files.get(1).getEventTimeMs());
      Assert.assertTrue(files.get(0).getFileCreationTimeMs() >= currentTimeMillisEvent1);
      Assert.assertTrue(files.get(1).getFileCreationTimeMs() >= currentTimeMillisEvent2);

      // checking permission
      String expectedPermissions = ""rw-r-----"";
      for (LogLocation file : files) {
        Location location = file.getLocation();
        Assert.assertEquals(expectedPermissions, location.getPermissions());
      }
    } catch (Exception e) {
      Assert.fail();
    }
  }
",non-flaky,5
35710,cdapio_cdap,CDAPLogAppenderTest.testCDAPLogAppenderSizeBasedRotation,"  @Test
  public void testCDAPLogAppenderSizeBasedRotation() throws Exception {
    int syncInterval = 1024 * 1024;
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(TimeUnit.DAYS.toMillis(1));
    cdapLogAppender.setMaxFileSizeInBytes(500);
    cdapLogAppender.setDirPermissions(""750"");
    cdapLogAppender.setFilePermissions(""640"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""testSizeRotation"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    long currentTimeMillisEvent1 = System.currentTimeMillis();

    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"", properties);

    event1.setTimeStamp(currentTimeMillisEvent1);
    cdapLogAppender.doAppend(event1);
    // sync updates the file size
    cdapLogAppender.sync();

    long currentTimeMillisEvent2 = System.currentTimeMillis();
    LoggingEvent event2 = getLoggingEvent(""io.cdap.Test2"",
                                          (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(
                                            Logger.ROOT_LOGGER_NAME), Level.ERROR , ""test message 2"", properties);
    event2.setTimeStamp(currentTimeMillisEvent2);
    // one new append, we will rotate to new file as the file size limit is very low and last append exceeded that.
    cdapLogAppender.doAppend(event2);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(2, files.size());
      assertLogEventDetails(event1, files.get(0));
      assertLogEventDetails(event2, files.get(1));
      Assert.assertEquals(currentTimeMillisEvent1, files.get(0).getEventTimeMs());
      Assert.assertEquals(currentTimeMillisEvent2, files.get(1).getEventTimeMs());
      Assert.assertTrue(files.get(0).getFileCreationTimeMs() >= currentTimeMillisEvent1);
      Assert.assertTrue(files.get(1).getFileCreationTimeMs() >= currentTimeMillisEvent2);
    } catch (Exception e) {
      Assert.fail();
    }
  }
",non-flaky,5
35711,cdapio_cdap,LogFileManagerTest.testLogFileManager,"  @Test
  public void testLogFileManager() throws Exception {
    int syncInterval = 1024 * 1024;
    long maxLifeTimeMs = 50;
    long maxFileSizeInBytes = 104857600;
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(injector.getInstance(TransactionRunner.class));
    LogFileManager logFileManager = new LogFileManager(""700"", ""600"", maxLifeTimeMs, maxFileSizeInBytes, syncInterval,
                                                       fileMetaDataWriter,
                                                       injector.getInstance(LocationFactory.class));
    LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""test"", ""testApp"", ""testFlow"");
    long timestamp = System.currentTimeMillis();
    LogFileOutputStream outputStream = logFileManager.getLogFileOutputStream(logPathIdentifier, timestamp);
    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"");
    outputStream.append(event1);
    // we are doing this, instead of calling getLogFileOutputStream to avoid race, if test machine can be slow.
    Assert.assertNotNull((logFileManager.getActiveOutputStream(logPathIdentifier)));
    TimeUnit.MILLISECONDS.sleep(60);
    logFileManager.flush();
    // should be closed on flush, should return null
    Assert.assertNull((logFileManager.getActiveOutputStream(logPathIdentifier)));
    LogFileOutputStream newLogOutStream = logFileManager.getLogFileOutputStream(logPathIdentifier, timestamp);
    // make sure the new location we got is different
    Assert.assertNotEquals(outputStream.getLocation(), newLogOutStream.getLocation());
  }
",non-flaky,5
35712,cdapio_cdap,TestDistributedLogReader.testDistributedLogPrevBoth,"  @Test
  public void testDistributedLogPrevBoth() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_BOTH, 16, 4, ""TestDistributedLogReader Log message1 "", 60);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_BOTH, 16, 4, ""TestDistributedLogReader Log message1 "", 60);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_BOTH, 9, 8, ""TestDistributedLogReader Log message1 "", 60);
  }
",non-flaky,5
35713,cdapio_cdap,TestDistributedLogReader.testDistributedLogNextBoth,"  @Test
  public void testDistributedLogNextBoth() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_BOTH, 20, 3, ""TestDistributedLogReader Log message1 "", 60, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_BOTH, 20, 3, ""TestDistributedLogReader Log message1 "", 60, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_BOTH, 1, 3,
                           ""TestDistributedLogReader Log message1 "", 3, 57);
  }
",non-flaky,5
35714,cdapio_cdap,TestDistributedLogReader.testDistributedLogPrevFile,"  @Test
  public void testDistributedLogPrevFile() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogPrev(readRange, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);
  }
",non-flaky,5
35715,cdapio_cdap,TestDistributedLogReader.testDistributedLogNextFile,"  @Test
  public void testDistributedLogNextFile() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogNext(readRange, LOGGING_CONTEXT_FILE, 14, 3, ""TestDistributedLogReader Log message2 "", 40, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_FILE, 14, 3, ""TestDistributedLogReader Log message2 "", 40, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_FILE, 1, 5,
                           ""TestDistributedLogReader Log message2 "", 5, 35);
  }
",non-flaky,5
35716,cdapio_cdap,TestDistributedLogReader.testDistributedLogPrevKafka,"  @Test
  public void testDistributedLogPrevKafka() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogPrev(readRange, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);
  }
",non-flaky,5
35717,cdapio_cdap,TestDistributedLogReader.testDistributedLogNextKafka,"  @Test
  public void testDistributedLogNextKafka() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_KAFKA, 10, 3, ""TestDistributedLogReader Log message3 "", 30, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_KAFKA, 10, 3, ""TestDistributedLogReader Log message3 "", 30, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_KAFKA, 1, 8,
                           ""TestDistributedLogReader Log message3 "", 8, 22);
  }
",non-flaky,5
35718,cdapio_cdap,TestKafkaLogging.testGetNext,"  @Test
  public void testGetNext() throws Exception {
    // Check with null runId and null instanceId
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_1"", ""APP_1"", ""FLOW_1"", ""RUN1"", ""INSTANCE1"");
    KafkaLogReader logReader = KAFKA_TESTER.getInjector().getInstance(KafkaLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetNext(logReader, loggingContext);
  }
",non-flaky,5
35719,cdapio_cdap,TestKafkaLogging.testGetPrev,"  @Test
  public void testGetPrev() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_1"", ""APP_1"", ""FLOW_1"", ""RUN1"", ""INSTANCE1"");
    KafkaLogReader logReader = KAFKA_TESTER.getInjector().getInstance(KafkaLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetPrev(logReader, loggingContext);
  }
",non-flaky,5
35720,cdapio_cdap,TestKafkaLogging.apply,"  @Test
  public void testPartitionKey() throws Exception {
    CConfiguration cConf = KAFKA_TESTER.getCConf();
    // set kafka partition key to application
    cConf.set(Constants.Logging.LOG_PUBLISH_PARTITION_KEY, ""application"");

    Logger logger = LoggerFactory.getLogger(""TestKafkaLogging"");
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_2"", ""APP_2"", ""FLOW_2"", ""RUN2"", ""INSTANCE2"");
    LoggingContextAccessor.setLoggingContext(loggingContext);
    for (int i = 0; i < 40; ++i) {
      logger.warn(""TKL_NS_2 Test log message {} {} {}"", i, ""arg1"", ""arg2"", new Exception(""test exception""));
    }

    loggingContext = new WorkerLoggingContext(""TKL_NS_2"", ""APP_2"", ""FLOW_3"", ""RUN3"", ""INSTANCE3"");
    LoggingContextAccessor.setLoggingContext(loggingContext);
    for (int i = 0; i < 40; ++i) {
      logger.warn(""TKL_NS_2 Test log message {} {} {}"", i, ""arg1"", ""arg2"", new Exception(""test exception""));
    }

    final Multimap<Integer, String> actual = ArrayListMultimap.create();

    KAFKA_TESTER.getPublishedMessages(KAFKA_TESTER.getCConf().get(Constants.Logging.KAFKA_TOPIC),
                                      ImmutableSet.of(0, 1), 40, new Function<FetchedMessage, String>() {
        @Override
        public String apply(final FetchedMessage input) {
          try {
            Map.Entry<Integer, String> entry = convertFetchedMessage(input);
            actual.put(entry.getKey(), entry.getValue());
          } catch (IOException e) {
            // should never happen
          }
          return """";
        }
",non-flaky,5
35721,cdapio_cdap,LocalLogAppenderResilientTest.addStatusEvent,"  @Test
  public void testResilientLogging() throws Exception {
    Configuration hConf = new Configuration();
    CConfiguration cConf = CConfiguration.create();

    File datasetDir = new File(tmpFolder.newFolder(), ""datasetUser"");
    //noinspection ResultOfMethodCallIgnored
    datasetDir.mkdirs();

    cConf.set(Constants.Dataset.Manager.OUTPUT_DIR, datasetDir.getAbsolutePath());
    cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS, ""localhost"");

    cConf.set(Constants.Dataset.Executor.ADDRESS, ""localhost"");
    cConf.setInt(Constants.Dataset.Executor.PORT, Networks.getRandomPort());

    cConf.set(Constants.CFG_LOCAL_DATA_DIR, tmpFolder.newFolder().getAbsolutePath());

    Injector injector = Guice.createInjector(
      new ConfigModule(cConf, hConf),
      new IOModule(),
      new ZKClientModule(),
      new KafkaClientModule(),
      new InMemoryDiscoveryModule(),
      new NonCustomLocationUnitTestModule(),
      new DataFabricModules().getInMemoryModules(),
      new DataSetsModules().getStandaloneModules(),
      new DataSetServiceModules().getInMemoryModules(),
      new TransactionMetricsModule(),
      new ExploreClientModule(),
      new LocalLogAppenderModule(),
      new NamespaceAdminTestModule(),
      new AuthorizationTestModule(),
      new AuthorizationEnforcementModule().getInMemoryModules(),
      new AuthenticationContextModules().getMasterModule(),
      new AbstractModule() {
        @Override
        protected void configure() {
          bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
          bind(OwnerAdmin.class).to(NoOpOwnerAdmin.class);
          bind(MetadataServiceClient.class).to(NoOpMetadataServiceClient.class);
        }
      });

    TransactionManager txManager = injector.getInstance(TransactionManager.class);
    txManager.startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);

    DatasetOpExecutorService opExecutorService = injector.getInstance(DatasetOpExecutorService.class);
    opExecutorService.startAndWait();

    // Start the logging before starting the service.
    LoggingContextAccessor.setLoggingContext(new WorkerLoggingContext(""TRL_ACCT_1"", ""APP_1"", ""WORKER_1"",
                                                                      ""RUN"", ""INSTANCE""));
    String logBaseDir = ""trl-log/log_files_"" + new Random(System.currentTimeMillis()).nextLong();

    cConf.set(LoggingConfiguration.LOG_BASE_DIR, logBaseDir);
    cConf.setInt(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES, 20 * 1024);
    final LogAppender appender = injector.getInstance(LocalLogAppender.class);
    new LogAppenderInitializer(appender).initialize(""TestResilientLogging"");

    int failureMsgCount = 3;
    final CountDownLatch failureLatch = new CountDownLatch(failureMsgCount);
    LoggerContext loggerContext = (LoggerContext) LoggerFactory.getILoggerFactory();
    loggerContext.getStatusManager().add(new StatusListener() {
      @Override
      public void addStatusEvent(Status status) {
        if (status.getLevel() != Status.ERROR || status.getOrigin() != appender) {
          return;
        }
        Throwable cause = status.getThrowable();
        if (cause != null) {
          Throwable rootCause = Throwables.getRootCause(cause);
          if (rootCause instanceof ServiceUnavailableException) {
            String serviceName = ((ServiceUnavailableException) rootCause).getServiceName();
            if (Constants.Service.DATASET_MANAGER.equals(serviceName)) {
              failureLatch.countDown();
            }
          }
        }
      }
",non-flaky,5
35722,cdapio_cdap,LoggingContextMDCTest.testMDC,"  @Test
  public void testMDC() {
    LoggingContext context = new TestLoggingContext(""namespace"", ""app"", ""run"", ""instance"");

    // Put an entry in the user mdc. It shouldn't override what's in the system tags.
    Map<String, String> userMDC = new HashMap<>();
    userMDC.put(Constants.Logging.TAG_APPLICATION_ID, ""userApp"");

    Map<String, String> mdc = new LoggingContextMDC(context.getSystemTagsAsString(), userMDC);

    Assert.assertEquals(4, mdc.size());

    Map<String, String> copiedMDC = new HashMap<>();
    for (Map.Entry<String, String> entry : mdc.entrySet()) {
      copiedMDC.put(entry.getKey(), entry.getValue());
    }

    Assert.assertEquals(4, copiedMDC.size());
    Assert.assertEquals(""namespace"", copiedMDC.get(Constants.Logging.TAG_NAMESPACE_ID));
    Assert.assertEquals(""app"", copiedMDC.get(Constants.Logging.TAG_APPLICATION_ID));
    Assert.assertEquals(""run"", copiedMDC.get(Constants.Logging.TAG_RUN_ID));
    Assert.assertEquals(""instance"", copiedMDC.get(Constants.Logging.TAG_INSTANCE_ID));

    // Should be able to set user property
    mdc.put(""user"", ""test"");
    Assert.assertEquals(5, mdc.size());
    Assert.assertEquals(5, mdc.entrySet().size());

    // This should fail with exception
    try {
      mdc.put(Constants.Logging.TAG_APPLICATION_ID, ""newApp"");
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }
  }
",non-flaky,5
35723,cdapio_cdap,LoggingContextAccessorTest.run,"  @Test
  public void testReset() {
    Cancellable cancellable = LoggingContextAccessor.setLoggingContext(
      new GenericLoggingContext(OLD_NS, OLD_APP, OLD_ENTITY));
    Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID), OLD_NS);
    Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID), OLD_APP);
    Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID), OLD_ENTITY);

    final Cancellable cancellable2 =
      LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(NS, APP, ENTITY));

    Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID), NS);
    Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID), APP);
    Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID), ENTITY);

    // Verify a different thread cannot change context
    Thread thread = new Thread(new Runnable() {
      @Override
      public void run() {
        cancellable2.cancel();
      }
",non-flaky,5
35724,cdapio_cdap,LogAppenderInitializerTest.testConfigUpdate,"  @Test (timeout = 10000L)
  public void testConfigUpdate() throws Exception {
    File logbackFile = createLogbackFile(new File(TEMP_FOLDER.newFolder(), ""logback.xml""), """");

    // Create a logger context from the generated logback.xml file
    LoggerContext loggerContext = (LoggerContext) LoggerFactory.getILoggerFactory();
    loggerContext.reset();

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(loggerContext);
    configurator.doConfigure(logbackFile);

    TestLogAppender testAppender = new TestLogAppender();
    testAppender.setName(""TestAppender"");
    LogAppenderInitializer initializer = new LogAppenderInitializer(testAppender);
    initializer.initialize();

    LoggingContextAccessor.setLoggingContext(new TestLoggingContext(""ns"", ""app"", ""run"", ""instance""));

    Logger logger = loggerContext.getLogger(LogAppenderInitializerTest.class);
    logger.info(""Testing"");

    Assert.assertEquals(""Testing"", testAppender.getLastMessage());

    // Update the logback file
    createLogbackFile(logbackFile, ""<logger name=\""io.cdap.cdap\"" level=\""INFO\"" />"");

    // Wait till the test appender stop() is called. This will happen when logback detected the changes.
    while (!testAppender.isStoppedOnce()) {
      // We need to keep logging because there is some internal thresold in logback implementation to trigger the
      // config reload thread.
      logger.info(""Waiting stop"");
      TimeUnit.MILLISECONDS.sleep(150);
      // Update the last modified time of the logback file to make sure logback can pick it with.
      logbackFile.setLastModified(System.currentTimeMillis());
    }

    // The appender should get automatically started again
    Tasks.waitFor(true, testAppender::isStarted, 5, TimeUnit.SECONDS);
    logger.info(""Reattached"");

    Assert.assertEquals(""Reattached"", testAppender.getLastMessage());
    loggerContext.stop();
  }
",non-flaky,5
35725,cdapio_cdap,TestTMSLogging.testTmsLogAppender,"  @Test
  public void testTmsLogAppender() throws Exception {
    // setup TMSLogAppender and log messages to it
    LogAppenderInitializer logAppenderInitializer = new LogAppenderInitializer(tmsLogAppender);
    logAppenderInitializer.initialize(""TestTMSLogging"");

    Logger logger = LoggerFactory.getLogger(""TestTMSLogging"");
    LoggingTester loggingTester = new LoggingTester();

    LoggingContext loggingContext = new MapReduceLoggingContext(""TKL_NS_1"", ""APP_1"", ""MR_1"", ""RUN1"");
    loggingTester.generateLogs(logger, loggingContext);

    logAppenderInitializer.close();

    // fetch and deserialize all the logs from TMS
    LoggingEventSerializer loggingEventSerializer = new LoggingEventSerializer();

    Map<Integer, List<ILoggingEvent>> partitionedFetchedLogs = new HashMap<>();
    int totalFetchedLogs = 0;

    for (Map.Entry<Integer, TopicId> topicId : topicIds.entrySet()) {
      List<ILoggingEvent> fetchedLogs = new ArrayList<>();
      MessageFetcher messageFetcher = client.prepareFetch(topicId.getValue());
      try (CloseableIterator<RawMessage> messages = messageFetcher.fetch()) {
        while (messages.hasNext()) {
          RawMessage message = messages.next();
          ILoggingEvent iLoggingEvent = loggingEventSerializer.fromBytes(ByteBuffer.wrap(message.getPayload()));
          fetchedLogs.add(iLoggingEvent);
        }
      }

      totalFetchedLogs += fetchedLogs.size();
      partitionedFetchedLogs.put(topicId.getKey(), fetchedLogs);
    }

    // LoggingTester emits 240 logs in total
    Assert.assertEquals(240, totalFetchedLogs);

    // Read the partition that our LoggingContext maps to and filter the logs in there to the logs that correspond
    // to our LoggingContext.
    LogPartitionType logPartitionType =
            LogPartitionType.valueOf(cConf.get(Constants.Logging.LOG_PUBLISH_PARTITION_KEY).toUpperCase());
    String partitionKey = logPartitionType.getPartitionKey(loggingContext);
    int partition = TMSLogAppender.partition(partitionKey, cConf.getInt(Constants.Logging.NUM_PARTITIONS));
    Filter logFilter = LoggingContextHelper.createFilter(loggingContext);

    List<ILoggingEvent> filteredLogs =
            partitionedFetchedLogs.get(partition).stream().filter(logFilter::match).collect(Collectors.toList());

    // LoggingTester emits 60 logs with the given LoggingContext
    Assert.assertEquals(60, filteredLogs.size());

    for (int i = 0; i < filteredLogs.size(); i++) {
      ILoggingEvent loggingEvent = filteredLogs.get(i);
      Assert.assertEquals(String.format(""Test log message %s arg1 arg2"", i), loggingEvent.getFormattedMessage());
    }
  }
",non-flaky,5
35726,cdapio_cdap,RollingLocationLogAppenderTest.testRollingLocationLogAppender,"  @Test
  public void testRollingLocationLogAppender() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNamespace"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 5);
    Map<LocationIdentifier, LocationOutputStream> activeFiles = rollingAppender.getLocationManager()
      .getActiveLocations();
    Assert.assertEquals(1, activeFiles.size());
    verifyFileOutput(activeFiles, 5);

    // different program should go to different directory
    addTagsToMdc(""testNamespace"", ""testApp1"");
    ingestLogs(logger, 5);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(2, activeFiles.size());
    verifyFileOutput(activeFiles, 5);

    // different program should go to different directory because namespace is different
    addTagsToMdc(""testNamespace1"", ""testApp1"");
    ingestLogs(logger, 5);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(3, activeFiles.size());
    verifyFileOutput(activeFiles, 5);
  }
",non-flaky,5
35727,cdapio_cdap,RollingLocationLogAppenderTest.testRollOver,"  @Test
  public void testRollOver() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNs"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 20000);
    Map<LocationIdentifier, LocationOutputStream> activeFiles = rollingAppender.getLocationManager()
      .getActiveLocations();
    Assert.assertEquals(1, activeFiles.size());
    LocationOutputStream locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs"", ""testApp""));
    Location parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());

    // different program should go to different directory
    addTagsToMdc(""testNs"", ""testApp1"");
    ingestLogs(logger, 20000);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(2, activeFiles.size());
    locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs"", ""testApp1""));
    parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());

    // different program should go to different directory because namespace is different
    addTagsToMdc(""testNs1"", ""testApp1"");
    ingestLogs(logger, 20000);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(3, activeFiles.size());
    locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs1"", ""testApp1""));
    parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());
  }
",non-flaky,5
35728,cdapio_cdap,RollingLocationLogAppenderTest.testFileClose,"  @Test
  public void testFileClose() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNs"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 20);

    // wait for 500 ms so that file is eligible for closing
    Thread.sleep(500);
    // flush to make sure file is closed
    rollingAppender.flush();
    Assert.assertEquals(0, rollingAppender.getLocationManager().getActiveLocations().size());
  }
",non-flaky,5
35729,cdapio_cdap,MetricsAdminSubscriberServiceTest.test,"  @Test
  public void test() throws Exception {
    MetricsAdminSubscriberService adminService = injector.getInstance(MetricsAdminSubscriberService.class);
    adminService.startAndWait();

    // publish a metrics
    MetricsContext metricsContext = metricsCollectionService.getContext(
      Collections.singletonMap(Constants.Metrics.Tag.NAMESPACE, NamespaceId.SYSTEM.getNamespace()));
    metricsContext.increment(""test.increment"", 10L);
    metricsContext.gauge(""test.gauge"", 20L);

    MetricsSystemClient systemClient = injector.getInstance(RemoteMetricsSystemClient.class);

    // Search for metrics names
    Tasks.waitFor(true, () -> {
      Set<String> names = new HashSet<>(systemClient.search(metricsContext.getTags()));
      return names.contains(""system.test.increment"") && names.contains(""system.test.gauge"");
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    // Query for metrics values
    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // Find and match the values for the increment and gauge
      boolean incMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 10L)
        .isPresent();

      boolean gaugeMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 20L)
        .isPresent();

      return incMatched && gaugeMatched;
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    // Emit more metrics
    metricsContext.increment(""test.increment"", 40L);
    metricsContext.gauge(""test.gauge"", 40L);

    // Query for metrics values. Should see the latest aggregates
    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // Find and match the values for the increment and gauge
      boolean incMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 50L)
        .isPresent();

      boolean gaugeMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 40L)
        .isPresent();

      return incMatched && gaugeMatched;
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);


    // Delete the increment metrics
    systemClient.delete(new MetricDeleteQuery(0, Integer.MAX_VALUE,
                                              Collections.emptySet(),
                                              metricsContext.getTags(),
                                              new ArrayList<>(metricsContext.getTags().keySet())));

    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // increment should be missing
      boolean foundInc = values.stream()
        .anyMatch(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""));

      // Find and match the values for gauge
      boolean foundGauge = values.stream()
        .anyMatch(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""));

      return !foundInc && !foundGauge;
    }, 1000, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    adminService.stopAndWait();
  }
",non-flaky,5
35730,cdapio_cdap,MessagingMetricsProcessorManagerServiceTest.persistMetricsTests,"  @Test
  public void persistMetricsTests() throws Exception {

    injector.getInstance(TransactionManager.class).startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);
    injector.getInstance(DatasetOpExecutorService.class).startAndWait();
    injector.getInstance(DatasetService.class).startAndWait();

    Set<Integer> partitions = IntStream.range(0, cConf.getInt(Constants.Metrics.MESSAGING_TOPIC_NUM))
      .boxed().collect(Collectors.toSet());

    long startTime = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());

    for (int iteration = 0; iteration < 50; iteration++) {
      // First publish all metrics before MessagingMetricsProcessorManagerService starts, so that fetchers of
      // different topics
      // will fetch metrics concurrently.
      for (int i = 0; i < 50; i++) {
        // TOPIC_PREFIX + (i % PARTITION_SIZE) decides which topic the metric is published to
        publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, """", MetricType.COUNTER);
      }
      for (int i = 50; i < 100; i++) {
        // TOPIC_PREFIX + (i % PARTITION_SIZE) decides which topic the metric is published to
        publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, """", MetricType.GAUGE);
      }

      final MockMetricStore metricStore = new MockMetricStore();
      // Create new MessagingMetricsProcessorManagerService instance every time because the same instance cannot be
      // started
      // again after it's stopped
      MessagingMetricsProcessorManagerService messagingMetricsProcessorManagerService =
        new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                    messagingService,
                                                    injector.getInstance(SchemaGenerator.class),
                                                    injector.getInstance(DatumReaderFactory.class), metricStore,
                                                    injector.getInstance(MetricsWriterProvider.class),
                                                    partitions, new NoopMetricsContext(), 50, 0);
      messagingMetricsProcessorManagerService.startAndWait();

      // Wait for the 1 aggregated counter metric (with value 50) and 50 gauge metrics to be stored in the metricStore
      Tasks.waitFor(51, () -> metricStore.getAllMetrics().size(), 15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      assertMetricsResult(expected, metricStore.getAllMetrics());

      // validate metrics processor metrics
      // 50 counter and 50 gauge metrics are emitted in each iteration above
      Tasks.waitFor(100L, () -> metricStore.getMetricsProcessedByMetricsProcessor(),
                    15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      // publish a dummy metric
      // this is to force the metrics processor to publish delay metrics for all the topics
      publishMessagingMetrics(100, startTime, METRICS_CONTEXT, expected, """", MetricType.GAUGE);
      // validate the newly published metric
      Tasks.waitFor(101L, () -> metricStore.getMetricsProcessedByMetricsProcessor(),
                    15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      // in MessagingMetricsProcessorManagerService, before persisting the metrics and topic metas, a copy of the
      // topic metas
      // containing the metrics processor delay metrics is made before making a copy of metric values.
      // Therefore, there can be a very small chance where all metric values are persisted but the corresponding
      // topic metas are not yet persisted. Wait for all topic metas to be persisted
      Tasks.waitFor(true, metricStore::isMetricsProcessorDelayEmitted, 15, TimeUnit.SECONDS);

      // Clear metricStore and expected results for the next iteration
      metricStore.deleteAll();
      expected.clear();
      // Stop messagingMetricsProcessorManagerService
      messagingMetricsProcessorManagerService.stopAndWait();
    }
  }
",non-flaky,5
35731,cdapio_cdap,MetricsProcessorServiceTest.call,"  @Test
  public void testMetricsProcessor() throws Exception {
    injector.getInstance(TransactionManager.class).startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);
    injector.getInstance(DatasetOpExecutorService.class).startAndWait();
    injector.getInstance(DatasetService.class).startAndWait();

    final MetricStore metricStore = injector.getInstance(MetricStore.class);

    Set<Integer> partitions = new HashSet<>();
    for (int i = 0; i < cConf.getInt(Constants.Metrics.MESSAGING_TOPIC_NUM); i++) {
      partitions.add(i);
    }

    // Start KafkaMetricsProcessorService after metrics are published to Kafka

    // Intentionally set queue size to a small value, so that MessagingMetricsProcessorManagerService
    // internally can persist metrics when more messages are to be fetched
    MessagingMetricsProcessorManagerService messagingMetricsProcessorManagerService =
      new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                  messagingService, injector.getInstance(SchemaGenerator.class),
                                                  injector.getInstance(DatumReaderFactory.class),
                                                  metricStore, injector.getInstance(MetricsWriterProvider.class),
                                                  partitions, new NoopMetricsContext(), 50, 0);
    messagingMetricsProcessorManagerService.startAndWait();

    long startTime = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
    // Publish metrics with messaging service and record expected metrics
    for (int i = 10; i < 20; i++) {
      publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, SYSTEM_METRIC_PREFIX, MetricType.COUNTER);
    }

    Thread.sleep(500);
    // Stop and restart messagingMetricsProcessorManagerService
    messagingMetricsProcessorManagerService.stopAndWait();
    // Intentionally set queue size to a large value, so that MessagingMetricsProcessorManagerService
    // internally only persists metrics during terminating.
    messagingMetricsProcessorManagerService =
      new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                  messagingService, injector.getInstance(SchemaGenerator.class),
                                                  injector.getInstance(DatumReaderFactory.class),
                                                  metricStore, injector.getInstance(MetricsWriterProvider.class),
                                                  partitions, new NoopMetricsContext(), 50, 0);
    messagingMetricsProcessorManagerService.startAndWait();

    // Publish metrics after MessagingMetricsProcessorManagerService restarts and record expected metrics
    for (int i = 20; i < 30; i++) {
      publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, SYSTEM_METRIC_PREFIX, MetricType.GAUGE);
    }

    final List<String> missingMetricNames = new ArrayList<>();
    // Wait until all expected metrics can be queried from the metric store. If not all expected metrics
    // are retrieved when timeout occurs, print out the missing metrics
    try {
      Tasks.waitFor(true, new Callable<Boolean>() {
        @Override
        public Boolean call() throws Exception {
          return canQueryAllMetrics(metricStore, METRICS_CONTEXT, expected, missingMetricNames);
        }
",non-flaky,5
35732,cdapio_cdap,MetricsQueryHelperTest.testGetResolution,"  @Test
  public void testGetResolution() {
    MetricsQueryHelper helper = new MetricsQueryHelper(null, CConfiguration.create());
    try {
      // test  start > end time
      helper.getResolution(""auto"", 10000L, 100L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test resolution is auto, but not both start and end time are provided
      helper.getResolution(""auto"", null, 200L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test resolution is auto, but not both start and end time are provided
      helper.getResolution(""auto"", 200L, null);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test non-existing resolution
      helper.getResolution(""6s"", 400L, 600L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    // test specific resolution
    Assert.assertEquals(1, helper.getResolution(""1s"", 100L, 10000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""1m"", 1000L, 100000L).intValue());
    Assert.assertEquals(3600, helper.getResolution(""1h"", 100L, 10000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""60s"", 100L, 10000L).intValue());

    // test resolution is auto
    // if 0 < ts diff <= 600, second resolution will be used
    Assert.assertEquals(1, helper.getResolution(""auto"", 0L, 300L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 10000L, 10300L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 0L, 600L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 1000L, 1600L).intValue());

    // if 600 < ts diff <= 36000, minute resolution will be used
    Assert.assertEquals(60, helper.getResolution(""auto"", 0L, 601L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 10000L, 10601L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 0L, 36000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 10000L, 46000L).intValue());

    // if ts > 36000, hour resolution will be used
    Assert.assertEquals(3600, helper.getResolution(""auto"", 0L, 36001L).intValue());
    Assert.assertEquals(3600, helper.getResolution(""auto"", 1000L, 10000000L).intValue());

    // if resolution is null, and both start and end time provided, the logic should be same as auto
    Assert.assertEquals(1, helper.getResolution(null, 0L, 300L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 10000L, 10300L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 0L, 600L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 1000L, 1600L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 0L, 601L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 10000L, 10601L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 0L, 36000L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 10000L, 46000L).intValue());
    Assert.assertEquals(3600, helper.getResolution(null, 0L, 36001L).intValue());
    Assert.assertEquals(3600, helper.getResolution(null, 1000L, 10000000L).intValue());

    // if resolution is null, and either timestamp is not specified, minimum resolution will be used
    Assert.assertEquals(1, helper.getResolution(null, 0L, null).intValue());
    Assert.assertEquals(1, helper.getResolution(null, null, 10000000L).intValue());
  }
",non-flaky,5
35733,cdapio_cdap,TimeseriesIdTest.testEquality,"  @Test
  public void testEquality() {
    TimeseriesId id1 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", null, ""0"");
    TimeseriesId id2 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", null, ""0"");
    Assert.assertTrue(id1.equals(id2));
    Assert.assertTrue(id2.equals(id1));
    Assert.assertEquals(id1.hashCode(), id2.hashCode());

    id1 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", ""tag1"", ""0"");
    id2 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", ""tag1"", ""0"");
    Assert.assertTrue(id1.equals(id2));
    Assert.assertTrue(id2.equals(id1));
    Assert.assertEquals(id1.hashCode(), id2.hashCode());
  }
",non-flaky,5
35734,cdapio_cdap,AggregatedMetricsCollectionServiceTest.testPublish,"  @Test
  public void testPublish() throws InterruptedException {
    final BlockingQueue<MetricValues> published = new LinkedBlockingQueue<>();

    AggregatedMetricsCollectionService service = new AggregatedMetricsCollectionService(1000L) {
      @Override
      protected void publish(Iterator<MetricValues> metrics) {
        Iterators.addAll(published, metrics);
      }
    };

    service.startAndWait();

    // non-empty tags.
    final Map<String, String> baseTags = ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, NAMESPACE,
                                                         Constants.Metrics.Tag.APP, APP,
                                                         Constants.Metrics.Tag.SERVICE, SERVICE,
                                                         Constants.Metrics.Tag.RUN_ID, RUNID);

    try {
      // The first section tests with empty tags.
      // Publish couple metrics with empty tags, they should be aggregated.
      service.getContext(EMPTY_TAGS).increment(METRIC, Integer.MAX_VALUE);
      service.getContext(EMPTY_TAGS).increment(METRIC, 2);
      service.getContext(EMPTY_TAGS).increment(METRIC, 3);
      service.getContext(EMPTY_TAGS).increment(METRIC, 4);

      verifyCounterMetricsValue(published, ImmutableMap.of(0, ImmutableMap.of(METRIC, 9L + Integer.MAX_VALUE)));

      // No publishing for 0 value metrics
      Assert.assertNull(published.poll(3, TimeUnit.SECONDS));

      //update the metrics multiple times with gauge.
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 1);
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 2);
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 3);

      // gauge just updates the value, so polling should return the most recent value written
      verifyGaugeMetricsValue(published, ImmutableMap.of(0, 3L));

      // define collectors for non-empty tags
      MetricsContext baseCollector = service.getContext(baseTags);
      MetricsContext metricsContext = baseCollector.childContext(Constants.Metrics.Tag.HANDLER, HANDLER)
        .childContext(Constants.Metrics.Tag.INSTANCE_ID, INSTANCE);

      // increment metrics for various collectors
      baseCollector.increment(METRIC, Integer.MAX_VALUE);
      metricsContext.increment(METRIC, 5);
      baseCollector.increment(METRIC, 10);
      baseCollector.increment(METRIC, 3);
      metricsContext.increment(METRIC, 2);
      metricsContext.increment(METRIC, 4);
      metricsContext.increment(METRIC, 3);
      metricsContext.increment(METRIC, 1);

      // there are two collectors, verify their metrics values
      verifyCounterMetricsValue(published, ImmutableMap.of(4, ImmutableMap.of(METRIC, 13L + Integer.MAX_VALUE),
                                                           6, ImmutableMap.of(METRIC, 15L)));

      // No publishing for 0 value metrics
      Assert.assertNull(published.poll(3, TimeUnit.SECONDS));

      // gauge metrics for various collectors
      baseCollector.gauge(GAUGE_METRIC, Integer.MAX_VALUE);
      baseCollector.gauge(GAUGE_METRIC, 3);
      metricsContext.gauge(GAUGE_METRIC, 6);
      metricsContext.gauge(GAUGE_METRIC, 2);
      baseCollector.gauge(GAUGE_METRIC, 1);
      metricsContext.gauge(GAUGE_METRIC, Integer.MAX_VALUE);

      // gauge just updates the value, so polling should return the most recent value written
      verifyGaugeMetricsValue(published, ImmutableMap.of(4, 1L, 6, (long) Integer.MAX_VALUE));

      metricsContext.gauge(GAUGE_METRIC, 0);
      verifyCounterMetricsValue(published, ImmutableMap.of(6, ImmutableMap.of(GAUGE_METRIC, 0L)));
    } finally {
      service.stopAndWait();
    }
  }
",non-flaky,5
35735,cdapio_cdap,MessagingMetricsCollectionServiceTest.testMessagingPublish,"  @Test
  public void testMessagingPublish() throws TopicNotFoundException {

    MetricsCollectionService collectionService = new MessagingMetricsCollectionService(CConfiguration.create(),
                                                                                       messagingService,
                                                                                       recordWriter);
    collectionService.startAndWait();

    // publish metrics for different context
    for (int i = 1; i <= 3; i++) {
      collectionService.getContext(ImmutableMap.of(""tag"", """" + i)).increment(""processed"", i);
    }

    collectionService.stopAndWait();

    // <Context, metricName, value>
    Table<String, String, Long> expected = HashBasedTable.create();
    expected.put(""tag.1"", ""processed"", 1L);
    expected.put(""tag.2"", ""processed"", 2L);
    expected.put(""tag.3"", ""processed"", 3L);

    ReflectionDatumReader<MetricValues> recordReader = new ReflectionDatumReader<>(schema, metricValueType);
    assertMetricsFromMessaging(schema, recordReader, expected);
  }
",non-flaky,5
35736,cdapio_cdap,RoutingToDataSetsTest.testTypeHandlerRequests,"  @Test
  public void testTypeHandlerRequests() throws Exception {
    Assert.assertEquals(""listModules"", doRequest(""/namespaces/myspace/data/modules"", ""GET""));
    Assert.assertEquals(""post:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""POST""));
    Assert.assertEquals(""delete:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""DELETE""));
    Assert.assertEquals(""get:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""GET""));
    Assert.assertEquals(""listTypes"", doRequest(""/namespaces/myspace/data/types"", ""GET""));
    Assert.assertEquals(""getType:myType"", doRequest(""/namespaces/myspace/data/types/myType"", ""GET""));
  }
",non-flaky,5
35737,cdapio_cdap,RoutingToDataSetsTest.testInstanceHandlerRequests,"  @Test
  public void testInstanceHandlerRequests() throws Exception {
    Assert.assertEquals(""list"", doRequest(""/namespaces/myspace/data/datasets"", ""GET""));
    Assert.assertEquals(""post:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""POST""));
    Assert.assertEquals(""delete:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""DELETE""));
    Assert.assertEquals(""get:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""GET""));
  }
",non-flaky,5
35738,cdapio_cdap,RouterAuditLookUpTest.testCorrectNumberInClassPath,"  @Test
  public void testCorrectNumberInClassPath() throws Exception {
    Assert.assertEquals(ExpectedNumberOfAuditPolicyPaths.EXPECTED_PATH_NUMBER, AUDIT_LOOK_UP.getNumberOfPaths());
  }
",non-flaky,5
35739,cdapio_cdap,RouterAuditLookUpTest.testDataFabricEndpoints,"  @Test
  public void testDataFabricEndpoints() throws Exception {
    // endpoints from DatasetInstanceHandler
    assertContent(""/v3/namespaces/default/data/datasets/myDataset"", DEFAULT_AUDIT);
    // endpoints from DatasetTypeHandler
    assertContent(""/v3/namespaces/default/data/modules/myModule"",
                  new AuditLogConfig(HttpMethod.PUT, false, false, ImmutableList.of(""X-Class-Name"")));
  }
",non-flaky,5
35740,cdapio_cdap,RouterAuditLookUpTest.testAppFabricEndpoints,"  @Test
  public void testAppFabricEndpoints() throws Exception {
    // endpoints from AppLifecycleHttpHandler
    assertContent(""/v3/namespaces/default/apps/myApp"", DEFAULT_AUDIT);
    assertContent(""/v3/namespaces/default/apps"",
                  new AuditLogConfig(HttpMethod.POST, false, true,
                                     ImmutableList.of(AbstractAppFabricHttpHandler.ARCHIVE_NAME_HEADER,
                                                       AbstractAppFabricHttpHandler.APP_CONFIG_HEADER,
                                                       AbstractAppFabricHttpHandler.PRINCIPAL_HEADER,
                                                       AbstractAppFabricHttpHandler.SCHEDULES_HEADER)));
    // endpoints from ArtifactHttpHandler
    assertContent(""/v3/namespaces/default/artifacts/myArtifact/versions/1.0/properties"", DEFAULT_AUDIT);
    assertContent(""/v3/namespaces/default/artifacts/myArtifact"",
                  new AuditLogConfig(HttpMethod.POST, false, false,
                                     ImmutableList.of(""Artifact-Version"", ""Artifact-Extends"", ""Artifact-Plugins"")));
    // endpoints from AuthorizationHandler
    assertContent(""/v3/security/authorization/privileges/grant"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from ConsoleSettingsHttpHandler
    assertContent(""/v3/configuration/user/"", DEFAULT_AUDIT);
    // endpoints from MetadataHttpHandler
    assertContent(""/v3/namespaces/default/apps/app1/metadata/properties"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from MonitorHttpHandler
    assertContent(""/v3/system/services/appfabric/instances"", DEFAULT_AUDIT);
    // endpoints from NamespaceHttpHandler
    assertContent(""/v3/namespaces/default"", DEFAULT_AUDIT);
    // endpoints from PreferencesHttpHandler
    assertContent(""/v3/preferences"", DEFAULT_AUDIT);
    // endpoints from ProgramLifecycleHttpHandler
    assertContent(""/v3/namespaces/default/stop"", new AuditLogConfig(HttpMethod.POST, true, true, EMPTY_HEADERS));
    // endpoints from SecureStoreHandler
    assertContent(""/v3/namespaces/default/securekeys/myKey"", DEFAULT_AUDIT);
    // endpoints from TransactionHttpHandler
    assertContent(""/v3/transactions/invalid/remove/until"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
  }
",non-flaky,5
35741,cdapio_cdap,RouterAuditLookUpTest.testExploreEndpoints,"  @Test
  public void testExploreEndpoints() throws Exception {
    // endpoints from ExploreExecutorHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/datasets/myDataset/update"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from NamespacedExploreMetadataHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/jdbc/tables"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from NamespacedExploreQueryExecutorHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/queries"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
  }
",non-flaky,5
35742,cdapio_cdap,NettyRouterPipelineAuthTest.testRouterAuthBypass,"  @Test
  public void testRouterAuthBypass() throws Exception {
    // mock token validator passes for any token other than ""Bearer failme""
    testGet(200, ""hello"", ""/v1/echo/hello"", ImmutableMap.of(""Authorization"", ""Bearer x""));
    // so this should fail
    testGet(401, null, ""/v1/echo/hello"");
    testGet(401, null, ""/v1/echo/hello"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // but /v1/echo/dontfail is configured to bypass auth
    testGet(200, ""dontfail"", ""/v1/echo/dontfail"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // it only bypasses on exact match, not prefix match
    testGet(401, null, ""/v1/echo/dontfailme"", ImmutableMap.of(""Authorization"", ""Bearer failme""));

    // /v1/repeat is configured to bypass auth validation, on prefix match
    testGet(200, ""hello"", ""/v1/repeat/hello"");
    testGet(200, ""hello"", ""/v1/repeat/hello"", ImmutableMap.of(""Authorization"", ""Bearer x""));
    testGet(200, ""hello"", ""/v1/repeat/hello"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // even with a token that fails validation, we get the correct status code 404
    testGet(404, null, ""/v1/repeat/dontfail/me"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
  }
",non-flaky,5
35743,cdapio_cdap,AuditLogTest.testAuditLog,"  @Test
  public void testAuditLog() throws IOException {
    HttpURLConnection urlConn = createURLConnection(""/get"", HttpMethod.GET);
    Assert.assertEquals(200, urlConn.getResponseCode());
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/put"", HttpMethod.PUT);
    urlConn.getOutputStream().write(""Test Put"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Test Put"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/post"", HttpMethod.POST);
    urlConn.getOutputStream().write(""Test Post"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Test Post"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/postHeaders"", HttpMethod.POST);
    urlConn.setRequestProperty(""user-id"", ""cdap"");
    urlConn.getOutputStream().write(""Post Headers"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Post Headers"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    List<String> loggedMessages = TestLogAppender.INSTANCE.getLoggedMessages();
    Assert.assertEquals(4, loggedMessages.size());

    Assert.assertTrue(loggedMessages.get(0).endsWith(""\""GET /get HTTP/1.1\"" - - 200 0 -""));
    Assert.assertTrue(loggedMessages.get(1).endsWith(""\""PUT /put HTTP/1.1\"" - Test Put 200 8 -""));
    Assert.assertTrue(loggedMessages.get(2).endsWith(""\""POST /post HTTP/1.1\"" - Test Post 200 9 Test Post""));
    Assert.assertTrue(
      loggedMessages.get(3).endsWith(""\""POST /postHeaders HTTP/1.1\"" {user-id=cdap} Post Headers 200 12 Post Headers""));
  }
",non-flaky,5
35744,cdapio_cdap,RouterMainTest.testGuiceInjection,"  @Test
  public void testGuiceInjection() {
    CConfiguration cConf = CConfiguration.create();

    Injector injector = RouterMain.createGuiceInjector(cConf);
    Assert.assertNotNull(injector);

    NettyRouter router = injector.getInstance(NettyRouter.class);
    Assert.assertNotNull(router);
  }
",non-flaky,5
35745,cdapio_cdap,NettyRouterPipelineTest.onCompleted,"  @Test
  public void testChunkRequestSuccess() throws Exception {

    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    byte [] requestBody = generatePostData();
    InetSocketAddress address = ROUTER.getRouterAddress();
    final Request request = new RequestBuilder(""POST"")
      .setUrl(String.format(""http://%s:%d%s"", address.getHostName(), address.getPort(), ""/v1/upload""))
      .setContentLength(requestBody.length)
      .setBody(new ByteEntityWriter(requestBody))
      .build();

    final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
    Future<Void> future = asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
      @Override
      public Void onCompleted(Response response) {
        return null;
      }
",non-flaky,5
35746,cdapio_cdap,NettyRouterPipelineTest.testDeployNTimes,"  @Test
  public void testDeployNTimes() throws Exception {
    // regression tests for race condition during multiple deploys.
    deploy(100);
  }
",non-flaky,5
35747,cdapio_cdap,NettyRouterPipelineTest.channelRead,"  @Test
  public void testHttpPipelining() throws Exception {
    final BlockingQueue<HttpResponseStatus> responseStatuses = new LinkedBlockingQueue<>();
    EventLoopGroup eventGroup = new NioEventLoopGroup();

    Bootstrap bootstrap = new Bootstrap()
      .channel(NioSocketChannel.class)
      .group(eventGroup)
      .handler(new ChannelInitializer<SocketChannel>() {
        @Override
        protected void initChannel(SocketChannel ch) {
          ChannelPipeline pipeline = ch.pipeline();
          pipeline.addLast(""codec"", new HttpClientCodec());
          pipeline.addLast(""aggregator"", new HttpObjectAggregator(1048576));
          pipeline.addLast(""handler"", new ChannelInboundHandlerAdapter() {
            @Override
            public void channelRead(ChannelHandlerContext ctx, Object msg) {
              if (msg instanceof HttpResponse) {
                responseStatuses.add(((HttpResponse) msg).status());
              }
              ReferenceCountUtil.release(msg);
            }
",non-flaky,5
35748,cdapio_cdap,AuthServerAnnounceTest.testEmptyAnnounceAddressURLsConfig,"  @Test
  public void testEmptyAnnounceAddressURLsConfig() throws Exception {
    HttpRouterService routerService = new AuthServerAnnounceTest.HttpRouterService(HOSTNAME, DISCOVERY_SERVICE);
    routerService.startUp();
    try {
      Assert.assertEquals(Collections.EMPTY_LIST, getAuthURI(routerService));
    } finally {
      routerService.shutDown();
    }
  }
",non-flaky,5
35749,cdapio_cdap,AuthServerAnnounceTest.testAnnounceURLsConfig,"  @Test
  public void testAnnounceURLsConfig() throws Exception {
    HttpRouterService routerService = new AuthServerAnnounceTest.HttpRouterService(HOSTNAME, DISCOVERY_SERVICE);
    routerService.cConf.set(Constants.Security.AUTH_SERVER_ANNOUNCE_URLS, ANNOUNCE_URLS);
    routerService.startUp();
    try {
      List<String> expected = Stream.of(ANNOUNCE_URLS.split("",""))
        .map(url -> String.format(""%s/%s"", url, GrantAccessToken.Paths.GET_TOKEN))
        .collect(Collectors.toList());
      Assert.assertEquals(expected, getAuthURI(routerService));
    } finally {
      routerService.shutDown();
    }
  }
",non-flaky,5
35750,cdapio_cdap,NettyRouterTestBase.testRouterSync,"  @Test
  public void testRouterSync() throws Exception {
    testSync(25);
    // sticky endpoint strategy used so the sum should be 25
    Assert.assertEquals(25, defaultServer1.getNumRequests() + defaultServer2.getNumRequests());
  }
",non-flaky,5
35751,cdapio_cdap,NettyRouterTestBase.onCompleted,"  @Test
  public void testRouterAsync() throws Exception {
    int numElements = 123;
    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    final CountDownLatch latch = new CountDownLatch(numElements);
    final AtomicInteger numSuccessfulRequests = new AtomicInteger(0);
    for (int i = 0; i < numElements; ++i) {
      final int elem = i;
      final Request request = new RequestBuilder(""GET"")
        .setUrl(resolveURI(String.format(""%s/%s-%d"", ""/v1/echo"", ""async"", i)))
        .build();
      asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
        @Override
        public Void onCompleted(Response response) throws Exception {
          latch.countDown();
          Assert.assertEquals(HttpResponseStatus.OK.code(), response.getStatusCode());
          String responseBody = response.getResponseBody();
          LOG.trace(""Got response {}"", responseBody);
          Assert.assertEquals(""async-"" + elem, responseBody);
          numSuccessfulRequests.incrementAndGet();
          return null;
        }
",non-flaky,5
35752,cdapio_cdap,NettyRouterTestBase.testRouterOneServerDown,"  @Test
  public void testRouterOneServerDown() throws Exception {
    // Bring down defaultServer1
    defaultServer1.cancelRegistration();

    testSync(25);
    Assert.assertEquals(0, defaultServer1.getNumRequests());
    Assert.assertTrue(defaultServer2.getNumRequests() > 0);

    defaultServer1.registerServer();
  }
",non-flaky,5
35753,cdapio_cdap,NettyRouterTestBase.testRouterAllServersDown,"  @Test
  public void testRouterAllServersDown() throws Exception {
    // Bring down all servers
    defaultServer1.cancelRegistration();
    defaultServer2.cancelRegistration();

    testSyncServiceUnavailable();
    Assert.assertEquals(0, defaultServer1.getNumRequests());
    Assert.assertEquals(0, defaultServer2.getNumRequests());

    defaultServer1.registerServer();
    defaultServer2.registerServer();
  }
",non-flaky,5
35754,cdapio_cdap,NettyRouterTestBase.testHostForward,"  @Test
  public void testHostForward() throws Exception {
    // Test defaultService
    HttpResponse response = get(resolveURI(String.format(""%s/%s"", ""/v1/ping"", ""sync"")));
    Assert.assertEquals(HttpResponseStatus.OK.code(), response.getStatusLine().getStatusCode());
    Assert.assertEquals(APP_FABRIC_SERVICE, EntityUtils.toString(response.getEntity()));
  }
",non-flaky,5
35755,cdapio_cdap,NettyRouterTestBase.onCompleted,"  @Test
  public void testUpload() throws Exception {
    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    byte [] requestBody = generatePostData();
    final Request request = new RequestBuilder(""POST"")
      .setUrl(resolveURI(""/v1/upload""))
      .setContentLength(requestBody.length)
      .setBody(new ByteEntityWriter(requestBody))
      .build();

    final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
    Future<Void> future = asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
      @Override
      public Void onCompleted(Response response) {
        return null;
      }
",non-flaky,5
35756,cdapio_cdap,NettyRouterTestBase.testConnectionClose,"  @Test
  public void testConnectionClose() throws Exception {
    URL[] urls = new URL[] {
      new URL(resolveURI(""/abc/v1/status"")),
      new URL(resolveURI(""/def/v1/status""))
    };

    // Make bunch of requests to one service to 2 difference urls, with the first one keep-alive, second one not.
    // This make router creates two backend service connections on the same inbound connection
    // This is to verify on the close of the second one, it won't close the the inbound if there is an
    // in-flight request happening already (if reached another round of the following for-loop).
    int times = 1000;
    boolean keepAlive = true;
    for (int i = 0; i < times; i++) {
      HttpURLConnection urlConn = openURL(urls[i % urls.length]);
      try {
        urlConn.setRequestProperty(HttpHeaderNames.CONNECTION.toString(),
                                   (keepAlive ? HttpHeaderValues.KEEP_ALIVE : HttpHeaderValues.CLOSE).toString());
        Assert.assertEquals(HttpURLConnection.HTTP_OK, urlConn.getResponseCode());
      } finally {
        urlConn.getInputStream().close();
        keepAlive = !keepAlive;
        urlConn.disconnect();
      }
    }

    Assert.assertEquals(times, defaultServer1.getNumRequests() + defaultServer2.getNumRequests());
  }
",non-flaky,5
35757,cdapio_cdap,NettyRouterTestBase.testConnectionIdleTimeout,"  @Test
  public void testConnectionIdleTimeout() throws Exception {
    // Only use server1
    defaultServer2.cancelRegistration();

    String path = ""/v2/ping"";
    URI uri = new URI(resolveURI(path));
    Socket socket = getSocketFactory().createSocket(uri.getHost(), uri.getPort());
    PrintWriter out = new PrintWriter(socket.getOutputStream(), true);
    InputStream inputStream = socket.getInputStream();

    // make a request
    String firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // sleep for 500 ms below the configured idle timeout; the connection on server side should not get closed by then
    // Hence it should be reusing the same server side connection
    TimeUnit.MILLISECONDS.sleep(TimeUnit.SECONDS.toMillis(CONNECTION_IDLE_TIMEOUT_SECS) - 500);
    firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // sleep for 500 ms over the configured idle timeout; the connection on server side should get closed by then
    // Hence it should be create a new server side connection
    TimeUnit.MILLISECONDS.sleep(TimeUnit.SECONDS.toMillis(CONNECTION_IDLE_TIMEOUT_SECS) + 500);
    // Due to timeout the client connection will be closed, and hence this request should not go to the server
    firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // assert that the connection is closed on the server side
    Assert.assertEquals(3, defaultServer1.getNumRequests());
    Assert.assertEquals(2, defaultServer1.getNumConnectionsOpened());
    Assert.assertEquals(1, defaultServer1.getNumConnectionsClosed());
  }
",non-flaky,5
35758,cdapio_cdap,NettyRouterTestBase.testConnectionIdleTimeoutWithMultipleServers,"  @Test
  public void testConnectionIdleTimeoutWithMultipleServers() throws Exception {
    defaultServer2.cancelRegistration();

    URL url = new URL(resolveURI(""/v2/ping""));
    HttpURLConnection urlConnection = openURL(url);
    Assert.assertEquals(200, urlConnection.getResponseCode());
    urlConnection.getInputStream().close();
    urlConnection.disconnect();

    // requests past this point will go to defaultServer2
    defaultServer1.cancelRegistration();
    defaultServer2.registerServer();

    for (int i = 0; i < 4; i++) {
      // this is an assumption that CONNECTION_IDLE_TIMEOUT_SECS is more than 1 second
      TimeUnit.SECONDS.sleep(1);
      url = new URL(resolveURI(""/v1/ping/"" + i));
      urlConnection = openURL(url);
      Assert.assertEquals(200, urlConnection.getResponseCode());
      urlConnection.getInputStream().close();
      urlConnection.disconnect();
    }

    // for the past 4 seconds, we've been making requests to defaultServer2; therefore, defaultServer1 will have closed
    // its single connection
    Assert.assertEquals(1, defaultServer1.getNumConnectionsOpened());
    Assert.assertEquals(1, defaultServer1.getNumConnectionsClosed());

    // however, the connection to defaultServer2 is not timed out, because we've been making requests to it
    Assert.assertEquals(1, defaultServer2.getNumConnectionsOpened());
    Assert.assertEquals(0, defaultServer2.getNumConnectionsClosed());

    defaultServer2.registerServer();
    defaultServer1.cancelRegistration();
    url = new URL(resolveURI(""/v2/ping""));
    urlConnection = openURL(url);
    Assert.assertEquals(200, urlConnection.getResponseCode());
    urlConnection.getInputStream().close();
    urlConnection.disconnect();
  }
",non-flaky,5
35759,cdapio_cdap,NettyRouterTestBase.testExpectContinue,"  @Test (timeout = 5000L)
  public void testExpectContinue() throws Exception {
    URL url = new URL(resolveURI(""/v2/upload""));
    HttpURLConnection urlConn = openURL(url);
    urlConn.setRequestMethod(""POST"");
    urlConn.setRequestProperty(HttpHeaderNames.EXPECT.toString(), HttpHeaderValues.CONTINUE.toString());
    urlConn.setDoOutput(true);

    // Forces sending small chunks to have the netty server receives multiple chunks
    urlConn.setChunkedStreamingMode(10);
    String msg = Strings.repeat(""Message"", 100);
    urlConn.getOutputStream().write(msg.getBytes(StandardCharsets.UTF_8));

    Assert.assertEquals(200, urlConn.getResponseCode());
    String result = new String(ByteStreams.toByteArray(urlConn.getInputStream()), StandardCharsets.UTF_8);
    Assert.assertEquals(msg, result);
  }
",non-flaky,5
61,graylog2_graylog2-server,indexCreationDateReturnsIndexCreationDateOfExistingIndexAsDateTime,"@Test
public void indexCreationDateReturnsIndexCreationDateOfExistingIndexAsDateTime() {
    final DateTime now = DateTime.now(DateTimeZone.UTC);
    final String indexName = client().createRandomIndex(""indices_it_"");
    final Optional<DateTime> indexCreationDate = indices.indexCreationDate(indexName);
    assertThat(indexCreationDate).isNotEmpty()
    .hasValueSatisfying(date -> Assertions.assertThat(date).isEqualToIgnoringMillis(now));
}",time,2
136,graylog2_graylog2-server,KafkaJournalTest.serverStatusUnthrottledIfJournalUtilizationIsLowerThanThreshold,"@Test
public void serverStatusUnthrottledIfJournalUtilizationIsLowerThanThreshold() throws Exception {
    serverStatus.throttle();
    final Size segmentSize = Size.kilobytes(1L);
    final KafkaJournal journal = new KafkaJournal(journalDirectory, scheduler, segmentSize, Duration.standardSeconds(1L), Size.kilobytes(4L), Duration.standardSeconds(1L), 1000000, Duration.standardSeconds(1L), 90, new MetricRegistry(), serverStatus);
    journal.flushDirtyLogs();
    journal.cleanupLogs();
    assertThat(serverStatus.getLifecycle()).isEqualTo(RUNNING);
}",concurrency,1
219,graylog2_graylog2-server,KafkaJournalTest.serverStatusThrottledIfJournalUtilizationIsHigherThanThreshold,"@Test
public void serverStatusThrottledIfJournalUtilizationIsHigherThanThreshold() throws Exception {
    serverStatus.running();
    final Size segmentSize = Size.kilobytes(1L);
    final KafkaJournal journal = new KafkaJournal(journalDirectory, scheduler, segmentSize, Duration.standardSeconds(1L), Size.kilobytes(4L), Duration.standardSeconds(1L), 1000000, Duration.standardSeconds(1L), 90, new MetricRegistry(), serverStatus);
    createBulkChunks(journal, segmentSize, 4);
    journal.flushDirtyLogs();
    journal.cleanupLogs();
    assertThat(serverStatus.getLifecycle()).isEqualTo(THROTTLED);
}",concurrency,1
303,graylog2_graylog2-server,ContentPackTest.shouldDeserializeSerializedContentPack,"@Test
public void shouldDeserializeSerializedContentPack() throws Exception {
    final ContentPack contentPack = createTestContentPack();
    final URL contentPackURL = ContentPackTest.class.getResource(""expected_content_pack.json"");
    Path path = Paths.get(contentPackURL.toURI());
    String expectedJSON = String.join("""", Files.readAllLines(path)).replace(""\n"", """").replace(""\r"", """");
    final String jsonTxt = objectMapper.writeValueAsString(contentPack);
    assertThat(jsonTxt).isEqualTo(expectedJSON);
    final ContentPack readContentPack = objectMapper.readValue(jsonTxt, ContentPack.class);
    assertThat(readContentPack.id()).isEqualTo(contentPack.id());
    assertThat(readContentPack.version()).isEqualTo(contentPack.version());
    assertThat(readContentPack.revision()).isEqualTo(contentPack.revision());
}",unordered collections,3
86038,graylog2_graylog2-server,NotificationResourceHandlerTest.testExecution,"    @Test
    public void testExecution() throws EventNotificationException {
        notificationResourceHandler.test(getHttpNotification(), ""testUser"");

        ArgumentCaptor<EventNotificationContext> captor = ArgumentCaptor.forClass(EventNotificationContext.class);
        verify(eventNotification, times(1)).execute(captor.capture());

        assertThat(captor.getValue()).satisfies(ctx -> {
            assertThat(ctx.event().message()).isEqualTo(""Notification test message triggered from user <testUser>"");
            assertThat(ctx.notificationId()).isEqualTo(NotificationTestData.TEST_NOTIFICATION_ID);
            assertThat(ctx.notificationConfig().type()).isEqualTo(HTTPEventNotificationConfig.TYPE_NAME);
            assertThat(ctx.eventDefinition().get().title()).isEqualTo(""Event Definition Test Title"");
        });
    }
",non-flaky,5
86039,graylog2_graylog2-server,NotificationDtoTest.testValidateWithEmptyTitle,"    @Test
    public void testValidateWithEmptyTitle() {
        final NotificationDto invalidNotification = getHttpNotification().toBuilder().title("""").build();
        final ValidationResult validationResult = invalidNotification.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""title"");
    }
",non-flaky,5
86040,graylog2_graylog2-server,NotificationDtoTest.testValidateWithEmptyConfig,"    @Test
    public void testValidateWithEmptyConfig() {
        final NotificationDto invalidNotification = NotificationDto.builder()
                .title(""Foo"")
                .description("""")
                .config(new EventNotificationConfig.FallbackNotificationConfig())
                .build();
        final ValidationResult validationResult = invalidNotification.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""config"");
    }
",non-flaky,5
86041,graylog2_graylog2-server,NotificationDtoTest.testValidateHttpWithEmptyConfigParameters,"    @Test
    public void testValidateHttpWithEmptyConfigParameters() {
        final HTTPEventNotificationConfig emptyConfig = HTTPEventNotificationConfig.Builder.create()
                .url("""")
                .build();
        final NotificationDto emptyNotification = getHttpNotification().toBuilder().config(emptyConfig).build();
        final ValidationResult validationResult = emptyNotification.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""url"");
    }
",non-flaky,5
86042,graylog2_graylog2-server,NotificationDtoTest.testValidateEmailWithEmptyConfigParameters,"    @Test
    public void testValidateEmailWithEmptyConfigParameters() {
        final EmailEventNotificationConfig emptyConfig = EmailEventNotificationConfig.Builder.create()
                .sender("""")
                .subject("""")
                .bodyTemplate("""")
                .build();
        final NotificationDto emptyNotification = getEmailNotification().toBuilder().config(emptyConfig).build();
        final ValidationResult validationResult = emptyNotification.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors().size()).isEqualTo(4);
        assertThat(validationResult.getErrors()).containsOnlyKeys(""subject"", ""sender"", ""body_template"", ""recipients"");
    }
",non-flaky,5
86043,graylog2_graylog2-server,NotificationDtoTest.testValidateLegacyWithEmptyConfigParameters,"    @Test
    public void testValidateLegacyWithEmptyConfigParameters() {
        final LegacyAlarmCallbackEventNotificationConfig emptyConfig = LegacyAlarmCallbackEventNotificationConfig.Builder.create()
                .callbackType("""")
                .configuration(new HashMap<>())
                .build();
        final NotificationDto emptyNotification = getLegacyNotification().toBuilder().config(emptyConfig).build();
        final ValidationResult validationResult = emptyNotification.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""callback_type"");
    }
",non-flaky,5
86044,graylog2_graylog2-server,NotificationDtoTest.testValidHttpNotification,"    @Test
    public void testValidHttpNotification() {
        final NotificationDto validNotification = getHttpNotification();

        final ValidationResult validationResult = validNotification.validate();
        assertThat(validationResult.failed()).isFalse();
        assertThat(validationResult.getErrors().size()).isEqualTo(0);
    }
",non-flaky,5
86045,graylog2_graylog2-server,NotificationDtoTest.testValidEmailNotification,"    @Test
    public void testValidEmailNotification() {
        final NotificationDto validNotification = getEmailNotification();

        final ValidationResult validationResult = validNotification.validate();
        assertThat(validationResult.failed()).isFalse();
        assertThat(validationResult.getErrors().size()).isEqualTo(0);
    }
",non-flaky,5
86046,graylog2_graylog2-server,NotificationDtoTest.testValidLegacyNotification,"    @Test
    public void testValidLegacyNotification() {
        final NotificationDto validNotification = getLegacyNotification();

        final ValidationResult validationResult = validNotification.validate();
        assertThat(validationResult.failed()).isFalse();
        assertThat(validationResult.getErrors().size()).isEqualTo(0);
    }
",non-flaky,5
86047,graylog2_graylog2-server,EventProcessorDependencyCheckTest.canProcessTimerange,"    @Test
    public void canProcessTimerange() {
        final DateTime now = DateTime.now(DateTimeZone.UTC);

        final EventProcessorStateDto stateDto1 = EventProcessorStateDto.builder()
                .eventDefinitionId(""a"")
                .minProcessedTimestamp(now.minusDays(1))
                .maxProcessedTimestamp(now)
                .build();
        final EventProcessorStateDto stateDto2 = EventProcessorStateDto.builder()
                .eventDefinitionId(""b"")
                .minProcessedTimestamp(now.minusDays(1))
                .maxProcessedTimestamp(now.minusHours(1))
                .build();
        final EventProcessorStateDto stateDto3 = EventProcessorStateDto.builder()
                .eventDefinitionId(""c"")
                .minProcessedTimestamp(now.minusDays(1))
                .maxProcessedTimestamp(now.minusHours(2))
                .build();

        // No state objects yet
        assertThat(dependencyCheck.canProcessTimerange(now, ImmutableSet.of(""a""))).isFalse();

        stateService.setState(stateDto1);
        stateService.setState(stateDto2);
        stateService.setState(stateDto3);

        // No state object has processedTimerageEnd >= now + 1h
        assertThat(dependencyCheck.canProcessTimerange(now.plusHours(1), ImmutableSet.of(""a""))).isFalse();

        // Only processor ""a"" has been processed at ""now""
        assertThat(dependencyCheck.canProcessTimerange(now, ImmutableSet.of(""a""))).isTrue();
        assertThat(dependencyCheck.canProcessTimerange(now, ImmutableSet.of(""a"", ""b""))).isFalse();
        assertThat(dependencyCheck.canProcessTimerange(now, ImmutableSet.of(""a"", ""c""))).isFalse();
        assertThat(dependencyCheck.canProcessTimerange(now, ImmutableSet.of(""a"", ""b"", ""c""))).isFalse();
        assertThat(dependencyCheck.canProcessTimerange(now, ImmutableSet.of(""b""))).isFalse();
        assertThat(dependencyCheck.canProcessTimerange(now, ImmutableSet.of(""c""))).isFalse();

        // Only processors ""a"" and ""b"" have been processed at now - 1h
        assertThat(dependencyCheck.canProcessTimerange(now.minusHours(1), ImmutableSet.of(""a"", ""b""))).isTrue();
        assertThat(dependencyCheck.canProcessTimerange(now.minusHours(1), ImmutableSet.of(""a"", ""c""))).isFalse();

        // Processors ""a"", ""b"" and ""c"" have been processed at now - 2h
        assertThat(dependencyCheck.canProcessTimerange(now.minusHours(2), ImmutableSet.of(""a"", ""b"", ""c""))).isTrue();
        assertThat(dependencyCheck.canProcessTimerange(now.minusHours(2), ImmutableSet.of(""a"", ""b""))).isTrue();
        assertThat(dependencyCheck.canProcessTimerange(now.minusHours(2), ImmutableSet.of(""a"", ""c""))).isTrue();
        assertThat(dependencyCheck.canProcessTimerange(now.minusHours(2), ImmutableSet.of(""a""))).isTrue();
        assertThat(dependencyCheck.canProcessTimerange(now.minusHours(2), ImmutableSet.of(""b""))).isTrue();
        assertThat(dependencyCheck.canProcessTimerange(now.minusHours(2), ImmutableSet.of(""c""))).isTrue();
    }
",non-flaky,5
86048,graylog2_graylog2-server,EventProcessorDependencyCheckTest.hasMessagesIndexedUpTo,"    @Test
    public void hasMessagesIndexedUpTo() {
        final DateTime timestamp = DateTime.now(DateTimeZone.UTC);

        when(dbProcessingStatusService.earliestPostIndexingTimestamp()).thenReturn(Optional.of(timestamp));

        assertThat(dependencyCheck.hasMessagesIndexedUpTo(timestamp)).isTrue();
        assertThat(dependencyCheck.hasMessagesIndexedUpTo(timestamp.minusHours(1))).isTrue();
        assertThat(dependencyCheck.hasMessagesIndexedUpTo(timestamp.plusHours(1))).isFalse();

        // The method should always return false if there is no value for the max indexed timestamp available
        when(dbProcessingStatusService.earliestPostIndexingTimestamp()).thenReturn(Optional.empty());

        assertThat(dependencyCheck.hasMessagesIndexedUpTo(timestamp)).isFalse();
        assertThat(dependencyCheck.hasMessagesIndexedUpTo(timestamp.minusHours(1))).isFalse();
        assertThat(dependencyCheck.hasMessagesIndexedUpTo(timestamp.plusHours(1))).isFalse();
    }
",non-flaky,5
86049,graylog2_graylog2-server,NotificationGracePeriodServiceTest.falseWithDisabledGracePeriod,"    @Test
    public void falseWithDisabledGracePeriod() {
        final NotificationGracePeriodService notificationGracePeriodService = new NotificationGracePeriodService();

        when(settings.gracePeriodMs()).thenReturn(0L);
        when(definition.notificationSettings()).thenReturn(settings);
        when(definition.id()).thenReturn(""1234"");

        final Event event = new TestEvent();
        event.setKeyTuple(ImmutableList.of(""testkey""));
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
    }
",non-flaky,5
86050,graylog2_graylog2-server,NotificationGracePeriodServiceTest.withinGracePeriod,"    @Test
    public void withinGracePeriod() {
        final NotificationGracePeriodService notificationGracePeriodService = new NotificationGracePeriodService();

        when(settings.gracePeriodMs()).thenReturn(10L);
        when(definition.notificationSettings()).thenReturn(settings);
        when(definition.id()).thenReturn(""1234"");

        final Event event = new TestEvent();
        event.setKeyTuple(ImmutableList.of(""testkey""));
        final Event event2 = new TestEvent();
        event2.setKeyTuple(ImmutableList.of(""testkey""));
        event2.setEventTimestamp(event.getEventTimestamp().plus(5L));
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event2)).isTrue();
    }
",non-flaky,5
86051,graylog2_graylog2-server,NotificationGracePeriodServiceTest.outsideGracePeriod,"    @Test
    public void outsideGracePeriod() {
        final NotificationGracePeriodService notificationGracePeriodService = new NotificationGracePeriodService();

        when(settings.gracePeriodMs()).thenReturn(10L);
        when(definition.notificationSettings()).thenReturn(settings);
        when(definition.id()).thenReturn(""1234"");

        final Event event = new TestEvent();
        event.setKeyTuple(ImmutableList.of(""testkey""));
        final Event event2 = new TestEvent();
        event2.setKeyTuple(ImmutableList.of(""testkey""));
        event2.setEventTimestamp(event.getEventTimestamp().plus(11L));
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event2)).isFalse();
    }
",non-flaky,5
86052,graylog2_graylog2-server,NotificationGracePeriodServiceTest.insideThenInsideGracePeriod,"    @Test
    public void insideThenInsideGracePeriod() {
        final NotificationGracePeriodService notificationGracePeriodService = new NotificationGracePeriodService();

        when(settings.gracePeriodMs()).thenReturn(10L);
        when(definition.notificationSettings()).thenReturn(settings);
        when(definition.id()).thenReturn(""1234"");

        final Event event = new TestEvent(DateTime.now(UTC), ""testkey"");
        final Event event2 = new TestEvent(event.getEventTimestamp().plus(5L), ""testkey"");
        final Event event3 = new TestEvent(event2.getEventTimestamp().plus(4L), ""testkey"");

        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event2)).isTrue();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event3)).isTrue();
    }
",non-flaky,5
86053,graylog2_graylog2-server,NotificationGracePeriodServiceTest.insideOutsideInsideGracePeriod,"    @Test
    public void insideOutsideInsideGracePeriod() {
        final NotificationGracePeriodService notificationGracePeriodService = new NotificationGracePeriodService();

        when(settings.gracePeriodMs()).thenReturn(10L);
        when(definition.notificationSettings()).thenReturn(settings);
        when(definition.id()).thenReturn(""1234"");

        final Event event = new TestEvent(DateTime.now(UTC), ""testkey"");
        final Event event2 = new TestEvent(event.getEventTimestamp().plus(5L), ""testkey"");
        final Event event3 = new TestEvent(event2.getEventTimestamp().plus(6L), ""testkey"");
        final Event event4 = new TestEvent(event3.getEventTimestamp().plus(6L), ""testkey"");

        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event2)).isTrue();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event3)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event4)).isTrue();
    }
",non-flaky,5
86054,graylog2_graylog2-server,NotificationGracePeriodServiceTest.differentKey,"    @Test
    public void differentKey() {
        final NotificationGracePeriodService notificationGracePeriodService = new NotificationGracePeriodService();

        when(settings.gracePeriodMs()).thenReturn(10L);
        when(definition.notificationSettings()).thenReturn(settings);
        when(definition.id()).thenReturn(""1234"");

        final Event event = new TestEvent();
        event.setKeyTuple(ImmutableList.of(""testkey""));
        final Event event2 = new TestEvent();
        event2.setKeyTuple(ImmutableList.of(""otherkey""));
        event2.setEventTimestamp(event.getEventTimestamp().plus(1L));
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event2)).isFalse();
    }
",non-flaky,5
86055,graylog2_graylog2-server,NotificationGracePeriodServiceTest.differentNotification,"    @Test
    public void differentNotification() {
        final NotificationGracePeriodService notificationGracePeriodService = new NotificationGracePeriodService();

        when(settings.gracePeriodMs()).thenReturn(10L);
        when(definition.notificationSettings()).thenReturn(settings);
        when(definition.id()).thenReturn(""1234"");

        final Event event = new TestEvent();
        event.setKeyTuple(ImmutableList.of(""testkey""));
        final Event event2 = new TestEvent();
        event2.setKeyTuple(ImmutableList.of(""testkey""));
        event2.setEventTimestamp(event.getEventTimestamp().plus(1L));
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""4242"", event2)).isFalse();
    }
",non-flaky,5
86056,graylog2_graylog2-server,NotificationGracePeriodServiceTest.emptyKey,"    @Test
    public void emptyKey() {
        final NotificationGracePeriodService notificationGracePeriodService = new NotificationGracePeriodService();

        when(settings.gracePeriodMs()).thenReturn(10L);
        when(definition.notificationSettings()).thenReturn(settings);
        when(definition.id()).thenReturn(""1234"");

        final Event event = new TestEvent();
        event.setKeyTuple(ImmutableList.of());
        final Event event2 = new TestEvent();
        event.setKeyTuple(ImmutableList.of());
        event2.setEventTimestamp(event.getEventTimestamp().plus(1L));
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event)).isFalse();
        assertThat(notificationGracePeriodService.inGracePeriod(definition, ""5678"", event2)).isTrue();
    }
",non-flaky,5
86057,graylog2_graylog2-server,AggregationFunctionTest.testFunctionMapping,"    @Test
    public void testFunctionMapping() {
        testToSeriesSpec(AggregationFunction.AVG, Average.class);
        testToSeriesSpec(AggregationFunction.CARD, Cardinality.class);
        testToSeriesSpec(AggregationFunction.COUNT, Count.class);
        testToSeriesSpec(AggregationFunction.MAX, Max.class);
        testToSeriesSpec(AggregationFunction.MIN, Min.class);
        testToSeriesSpec(AggregationFunction.STDDEV, StdDev.class);
        testToSeriesSpec(AggregationFunction.SUM, Sum.class);
        testToSeriesSpec(AggregationFunction.SUMOFSQUARES, SumOfSquares.class);
        testToSeriesSpec(AggregationFunction.VARIANCE, Variance.class);
    }
",non-flaky,5
86058,graylog2_graylog2-server,AggregationFunctionTest.fieldRequirements,"    @Test
    public void fieldRequirements() {
        assertThatCode(() -> AggregationFunction.AVG.toSeriesSpec(""a"", null))
                .hasMessageContaining(""<avg>"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatCode(() -> AggregationFunction.CARD.toSeriesSpec(""a"", null))
                .hasMessageContaining(""<card>"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatCode(() -> AggregationFunction.COUNT.toSeriesSpec(""a"", null))
                .doesNotThrowAnyException();

        assertThatCode(() -> AggregationFunction.MAX.toSeriesSpec(""a"", null))
                .hasMessageContaining(""<max>"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatCode(() -> AggregationFunction.MIN.toSeriesSpec(""a"", null))
                .hasMessageContaining(""<min>"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatCode(() -> AggregationFunction.STDDEV.toSeriesSpec(""a"", null))
                .hasMessageContaining(""<stddev>"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatCode(() -> AggregationFunction.SUM.toSeriesSpec(""a"", null))
                .hasMessageContaining(""<sum>"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatCode(() -> AggregationFunction.SUMOFSQUARES.toSeriesSpec(""a"", null))
                .hasMessageContaining(""<sumofsquares>"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatCode(() -> AggregationFunction.VARIANCE.toSeriesSpec(""a"", null))
                .hasMessageContaining(""<variance>"")
                .isInstanceOf(IllegalArgumentException.class);
    }
",non-flaky,5
86059,graylog2_graylog2-server,AggregationEventProcessorConfigTest.toJobSchedulerConfig,"    @Test
    public void toJobSchedulerConfig() {
        final EventDefinitionDto dto = dbService.get(""54e3deadbeefdeadbeefaffe"").orElse(null);

        assertThat(dto).isNotNull();

        assertThat(dto.config().toJobSchedulerConfig(dto, clock)).isPresent().get().satisfies(schedulerConfig -> {
            assertThat(schedulerConfig.jobDefinitionConfig()).satisfies(jobDefinitionConfig -> {
                assertThat(jobDefinitionConfig).isInstanceOf(EventProcessorExecutionJob.Config.class);

                final EventProcessorExecutionJob.Config config = (EventProcessorExecutionJob.Config) jobDefinitionConfig;

                assertThat(config.eventDefinitionId()).isEqualTo(dto.id());
                assertThat(config.processingWindowSize()).isEqualTo(300000);
                assertThat(config.processingHopSize()).isEqualTo(300000);
                assertThat(config.parameters()).isEqualTo(AggregationEventProcessorParameters.builder()
                        .timerange(AbsoluteRange.create(clock.nowUTC().minus(300000), clock.nowUTC()))
                        .build());
            });

            assertThat(schedulerConfig.schedule()).satisfies(schedule -> {
                assertThat(schedule).isInstanceOf(IntervalJobSchedule.class);

                final IntervalJobSchedule config = (IntervalJobSchedule) schedule;

                assertThat(config.interval()).isEqualTo(300000);
                assertThat(config.unit()).isEqualTo(TimeUnit.MILLISECONDS);
            });
        });
    }
",non-flaky,5
86060,graylog2_graylog2-server,AggregationEventProcessorConfigTest.testValidateWithInvalidTimeRange,"    @Test
    public void testValidateWithInvalidTimeRange() {
        final AggregationEventProcessorConfig invalidConfig1 = getConfig().toBuilder()
            .searchWithinMs(-1)
            .build();

        final ValidationResult validationResult1 = invalidConfig1.validate();
        assertThat(validationResult1.failed()).isTrue();
        assertThat(validationResult1.getErrors()).containsOnlyKeys(""search_within_ms"");

        final AggregationEventProcessorConfig invalidConfig2 = invalidConfig1.toBuilder()
            .searchWithinMs(0)
            .build();

        final ValidationResult validationResult2 = invalidConfig2.validate();
        assertThat(validationResult2.failed()).isTrue();
        assertThat(validationResult2.getErrors()).containsOnlyKeys(""search_within_ms"");
    }
",non-flaky,5
86061,graylog2_graylog2-server,AggregationEventProcessorConfigTest.testValidateWithInvalidExecutionTime,"    @Test
    public void testValidateWithInvalidExecutionTime() {
        final AggregationEventProcessorConfig invalidConfig1 = getConfig().toBuilder()
            .executeEveryMs(-1)
            .build();

        final ValidationResult validationResult1 = invalidConfig1.validate();
        assertThat(validationResult1.failed()).isTrue();
        assertThat(validationResult1.getErrors()).containsOnlyKeys(""execute_every_ms"");

        final AggregationEventProcessorConfig invalidConfig2 = invalidConfig1.toBuilder()
            .executeEveryMs(0)
            .build();

        final ValidationResult validationResult2 = invalidConfig2.validate();
        assertThat(validationResult2.failed()).isTrue();
        assertThat(validationResult2.getErrors()).containsOnlyKeys(""execute_every_ms"");
    }
",non-flaky,5
86062,graylog2_graylog2-server,AggregationEventProcessorConfigTest.testValidateWithIncompleteAggregationOptions,"    @Test
    public void testValidateWithIncompleteAggregationOptions() {
        AggregationEventProcessorConfig invalidConfig = getConfig().toBuilder()
            .groupBy(ImmutableList.of(""foo""))
            .build();

        ValidationResult validationResult = invalidConfig.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""series"", ""conditions"");

        invalidConfig = getConfig().toBuilder()
            .series(ImmutableList.of(this.getSeries()))
            .build();

        validationResult = invalidConfig.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""conditions"");

        invalidConfig = getConfig().toBuilder()
            .conditions(this.getConditions())
            .build();

        validationResult = invalidConfig.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""series"");
    }
",non-flaky,5
86063,graylog2_graylog2-server,AggregationEventProcessorConfigTest.testValidConfiguration,"    @Test
    public void testValidConfiguration() {
        final ValidationResult validationResult = getConfig().validate();
        assertThat(validationResult.failed()).isFalse();
        assertThat(validationResult.getErrors().size()).isEqualTo(0);
    }
",non-flaky,5
86064,graylog2_graylog2-server,AggregationEventProcessorConfigTest.testValidFilterConfiguration,"    @Test
    public void testValidFilterConfiguration() {
        final AggregationEventProcessorConfig config = getConfig().toBuilder()
            .query(""foo"")
            .streams(ImmutableSet.of(""1"", ""2""))
            .build();

        final ValidationResult validationResult = config.validate();
        assertThat(validationResult.failed()).isFalse();
        assertThat(validationResult.getErrors().size()).isEqualTo(0);
    }
",non-flaky,5
86065,graylog2_graylog2-server,AggregationEventProcessorConfigTest.testValidAggregationConfiguration,"    @Test
    public void testValidAggregationConfiguration() {
        final AggregationEventProcessorConfig config = getConfig().toBuilder()
            .groupBy(ImmutableList.of(""bar""))
            .series(ImmutableList.of(this.getSeries()))
            .conditions(this.getConditions())
            .build();

        final ValidationResult validationResult = config.validate();
        assertThat(validationResult.failed()).isFalse();
        assertThat(validationResult.getErrors().size()).isEqualTo(0);
    }
",non-flaky,5
86066,graylog2_graylog2-server,AggregationEventProcessorConfigTest.requiredPermissions,"    @Test
    public void requiredPermissions() {
        assertThat(dbService.get(""54e3deadbeefdeadbeefaffe"")).get().satisfies(definition -> {
            assertThat(definition.config().requiredPermissions()).containsOnly(""streams:read:stream-a"", ""streams:read:stream-b"");
        });
    }
",non-flaky,5
86067,graylog2_graylog2-server,AggregationEventProcessorConfigTest.requiredPermissionsWithEmptyStreams,"    @Test
    public void requiredPermissionsWithEmptyStreams() {
        assertThat(dbService.get(""54e3deadbeefdeadbeefafff"")).get().satisfies(definition -> {
            assertThat(definition.config().requiredPermissions()).containsOnly(""streams:read"");
        });
    }
",non-flaky,5
86068,graylog2_graylog2-server,AggregationEventProcessorTest.testEventsFromAggregationResult,"    @Test
    public void testEventsFromAggregationResult() {
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final AbsoluteRange timerange = AbsoluteRange.create(now.minusHours(1), now.plusHours(1));

        // We expect to get the end of the aggregation timerange as event time
        final TestEvent event1 = new TestEvent(timerange.to());
        final TestEvent event2 = new TestEvent(timerange.to());
        when(eventFactory.createEvent(any(EventDefinition.class), eq(now), anyString()))
                .thenReturn(event1)  // first invocation return value
                .thenReturn(event2); // second invocation return value

        final EventDefinitionDto eventDefinitionDto = EventDefinitionDto.builder()
                .id(""dto-id-1"")
                .title(""Test Aggregation"")
                .description(""A test aggregation event processors"")
                .priority(1)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .config(AggregationEventProcessorConfig.builder()
                        .query("""")
                        .streams(ImmutableSet.of(""stream-2""))
                        .groupBy(ImmutableList.of(""group_field_one"", ""group_field_two""))
                        .series(ImmutableList.of())
                        .conditions(null)
                        .searchWithinMs(30000)
                        .executeEveryMs(30000)
                        .build())
                .keySpec(ImmutableList.of())
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .timerange(timerange)
                .build();

        final AggregationEventProcessor eventProcessor = new AggregationEventProcessor(eventDefinitionDto, searchFactory, eventProcessorDependencyCheck, stateService, moreSearch, streamService, messages);

        final AggregationResult result = AggregationResult.builder()
                .effectiveTimerange(timerange)
                .totalAggregatedMessages(1)
                .sourceStreams(ImmutableSet.of(""stream-1"", ""stream-2""))
                .keyResults(ImmutableList.of(
                        AggregationKeyResult.builder()
                                .key(ImmutableList.of(""one"", ""two""))
                                .timestamp(now)
                                .seriesValues(ImmutableList.of(
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(42.0d)
                                                .series(AggregationSeries.builder()
                                                        .id(""abc123"")
                                                        .function(AggregationFunction.COUNT)
                                                        .field(""source"")
                                                        .build())
                                                .build(),
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(23.0d)
                                                .series(AggregationSeries.builder()
                                                        .id(""abc123-no-field"")
                                                        .function(AggregationFunction.COUNT)
                                                        .build())
                                                .build(),
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(1.0d)
                                                .series(AggregationSeries.builder()
                                                        .id(""xyz789"")
                                                        .function(AggregationFunction.CARD)
                                                        .field(""source"")
                                                        .build())
                                                .build()
                                ))
                                .build()
                ))
                .build();

        final ImmutableList<EventWithContext> eventsWithContext = eventProcessor.eventsFromAggregationResult(eventFactory, parameters, result);

        assertThat(eventsWithContext).hasSize(1);

        assertThat(eventsWithContext.get(0)).satisfies(eventWithContext -> {
            final Event event = eventWithContext.event();

            assertThat(event.getId()).isEqualTo(event1.getId());
            assertThat(event.getMessage()).isEqualTo(event1.getMessage());
            assertThat(event.getEventTimestamp()).isEqualTo(timerange.to());
            assertThat(event.getTimerangeStart()).isEqualTo(timerange.from());
            assertThat(event.getTimerangeEnd()).isEqualTo(timerange.to());
            // Should only contain the streams that have been configured in event definition
            assertThat(event.getSourceStreams()).containsOnly(""stream-2"");

            final Message message = eventWithContext.messageContext().orElse(null);

            assertThat(message).isNotNull();
            assertThat(message.getField(""group_field_one"")).isEqualTo(""one"");
            assertThat(message.getField(""group_field_two"")).isEqualTo(""two"");
            assertThat(message.getField(""aggregation_key"")).isEqualTo(""one|two"");
            assertThat(message.getField(""aggregation_value_count_source"")).isEqualTo(42.0d);
            // Make sure that the count with a ""null"" field doesn't include the field in the name
            assertThat(message.getField(""aggregation_value_count"")).isEqualTo(23.0d);
            assertThat(message.getField(""aggregation_value_card_source"")).isEqualTo(1.0d);
        });
    }
",non-flaky,5
86069,graylog2_graylog2-server,AggregationEventProcessorTest.testEventsFromAggregationResultWithConditions,"    @Test
    public void testEventsFromAggregationResultWithConditions() {
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final AbsoluteRange timerange = AbsoluteRange.create(now.minusHours(1), now.plusHours(1));

        // We expect to get the end of the aggregation timerange as event time
        final TestEvent event1 = new TestEvent(timerange.to());
        final TestEvent event2 = new TestEvent(timerange.to());
        when(eventFactory.createEvent(any(EventDefinition.class), eq(now), anyString()))
                .thenReturn(event1)  // first invocation return value
                .thenReturn(event2); // second invocation return value

        // There should only be one result because the second result's ""abc123"" value is less than 40. (it is 23)
        // See result builder below
        final AggregationConditions conditions = AggregationConditions.builder()
                .expression(Expr.And.create(
                        Expr.Greater.create(Expr.NumberReference.create(""abc123""), Expr.NumberValue.create(40.0d)),
                        Expr.Lesser.create(Expr.NumberReference.create(""xyz789""), Expr.NumberValue.create(2.0d))
                ))
                .build();

        final EventDefinitionDto eventDefinitionDto = EventDefinitionDto.builder()
                .id(""dto-id-1"")
                .title(""Test Aggregation"")
                .description(""A test aggregation event processors"")
                .priority(1)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .config(AggregationEventProcessorConfig.builder()
                        .query("""")
                        .streams(ImmutableSet.of())
                        .groupBy(ImmutableList.of(""group_field_one"", ""group_field_two""))
                        .series(ImmutableList.of())
                        .conditions(conditions)
                        .searchWithinMs(30000)
                        .executeEveryMs(30000)
                        .build())
                .keySpec(ImmutableList.of())
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .timerange(timerange)
                .build();

        final AggregationEventProcessor eventProcessor = new AggregationEventProcessor(eventDefinitionDto, searchFactory, eventProcessorDependencyCheck, stateService, moreSearch, streamService, messages);

        final AggregationResult result = AggregationResult.builder()
                .effectiveTimerange(timerange)
                .totalAggregatedMessages(1)
                .sourceStreams(ImmutableSet.of(""stream-1"", ""stream-2"", ""stream-3""))
                .keyResults(ImmutableList.of(
                        AggregationKeyResult.builder()
                                .key(ImmutableList.of(""one"", ""two""))
                                .timestamp(now)
                                .seriesValues(ImmutableList.of(
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(42.0d)
                                                .series(AggregationSeries.builder()
                                                        .id(""abc123"")
                                                        .function(AggregationFunction.COUNT)
                                                        .field(""source"")
                                                        .build())
                                                .build(),
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(1.0d)
                                                .series(AggregationSeries.builder()
                                                        .id(""xyz789"")
                                                        .function(AggregationFunction.CARD)
                                                        .field(""source"")
                                                        .build())
                                                .build()
                                ))
                                .build(),
                        AggregationKeyResult.builder()
                                .key(ImmutableList.of(now.toString(), ""one"", ""two""))
                                .seriesValues(ImmutableList.of(
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(23.0d) // Doesn't match condition
                                                .series(AggregationSeries.builder()
                                                        .id(""abc123"")
                                                        .function(AggregationFunction.COUNT)
                                                        .field(""source"")
                                                        .build())
                                                .build(),
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(1.0d)
                                                .series(AggregationSeries.builder()
                                                        .id(""xyz789"")
                                                        .function(AggregationFunction.CARD)
                                                        .field(""source"")
                                                        .build())
                                                .build()
                                ))
                                .build()
                ))
                .build();

        final ImmutableList<EventWithContext> eventsWithContext = eventProcessor.eventsFromAggregationResult(eventFactory, parameters, result);

        assertThat(eventsWithContext).hasSize(1);

        assertThat(eventsWithContext.get(0)).satisfies(eventWithContext -> {
            final Event event = eventWithContext.event();

            assertThat(event.getId()).isEqualTo(event1.getId());
            assertThat(event.getMessage()).isEqualTo(event1.getMessage());
            assertThat(event.getEventTimestamp()).isEqualTo(timerange.to());
            assertThat(event.getTimerangeStart()).isEqualTo(timerange.from());
            assertThat(event.getTimerangeEnd()).isEqualTo(timerange.to());
            // Should contain all streams because when config.streams is empty, we search in all streams
            assertThat(event.getSourceStreams()).containsOnly(""stream-1"", ""stream-2"", ""stream-3"");

            final Message message = eventWithContext.messageContext().orElse(null);

            assertThat(message).isNotNull();
            assertThat(message.getField(""group_field_one"")).isEqualTo(""one"");
            assertThat(message.getField(""group_field_two"")).isEqualTo(""two"");
            assertThat(message.getField(""aggregation_key"")).isEqualTo(""one|two"");
            assertThat(message.getField(""aggregation_value_count_source"")).isEqualTo(42.0d);
            assertThat(message.getField(""aggregation_value_card_source"")).isEqualTo(1.0d);
        });
    }
",non-flaky,5
86070,graylog2_graylog2-server,AggregationEventProcessorTest.createEventsWithFilter,"    @Test
    public void createEventsWithFilter() throws Exception {
        when(eventProcessorDependencyCheck.hasMessagesIndexedUpTo(any(DateTime.class))).thenReturn(true);

        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final AbsoluteRange timerange = AbsoluteRange.create(now.minusHours(1), now.plusHours(1));

        final AggregationEventProcessorConfig config = AggregationEventProcessorConfig.builder()
                .query("""")
                .streams(ImmutableSet.of())
                .groupBy(ImmutableList.of())
                .series(ImmutableList.of())
                .conditions(null)
                .searchWithinMs(30000)
                .executeEveryMs(30000)
                .build();
        final EventDefinitionDto eventDefinitionDto = EventDefinitionDto.builder()
                .id(""dto-id-1"")
                .title(""Test Aggregation"")
                .description(""A test aggregation event processors"")
                .priority(1)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .config(config)
                .keySpec(ImmutableList.of())
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .timerange(timerange)
                .build();

        final AggregationEventProcessor eventProcessor = new AggregationEventProcessor(eventDefinitionDto, searchFactory, eventProcessorDependencyCheck, stateService, moreSearch, streamService, messages);

        assertThatCode(() -> eventProcessor.createEvents(eventFactory, parameters, (events) -> {})).doesNotThrowAnyException();

        verify(moreSearch, times(1)).scrollQuery(
                eq(config.query()),
                eq(config.streams()),
                eq(config.queryParameters()),
                eq(parameters.timerange()),
                eq(parameters.batchSize()),
                any(MoreSearch.ScrollCallback.class)
        );
        verify(searchFactory, never()).create(eq(config), eq(parameters), any(String.class), eq(eventDefinitionDto));
    }
",non-flaky,5
86071,graylog2_graylog2-server,AggregationEventProcessorTest.createEventsWithoutRequiredMessagesBeingIndexed,"    @Test
    public void createEventsWithoutRequiredMessagesBeingIndexed() throws Exception {
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final AbsoluteRange timerange = AbsoluteRange.create(now.minusHours(1), now.plusHours(1));

        final AggregationEventProcessorConfig config = AggregationEventProcessorConfig.builder()
                .query("""")
                .streams(ImmutableSet.of())
                .groupBy(ImmutableList.of())
                .series(ImmutableList.of())
                .conditions(null)
                .searchWithinMs(30000)
                .executeEveryMs(30000)
                .build();
        final EventDefinitionDto eventDefinitionDto = EventDefinitionDto.builder()
                .id(""dto-id-1"")
                .title(""Test Aggregation"")
                .description(""A test aggregation event processors"")
                .priority(1)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .config(config)
                .keySpec(ImmutableList.of())
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .timerange(timerange)
                .build();

        final AggregationEventProcessor eventProcessor = new AggregationEventProcessor(eventDefinitionDto, searchFactory, eventProcessorDependencyCheck, stateService, moreSearch, streamService, messages);

        // If the dependency check returns true, there should be no exception raised and the state service should be called
        when(eventProcessorDependencyCheck.hasMessagesIndexedUpTo(timerange.to())).thenReturn(true);

        assertThatCode(() -> eventProcessor.createEvents(eventFactory, parameters, (events) -> {})).doesNotThrowAnyException();

        verify(stateService, times(1)).setState(""dto-id-1"", timerange.from(), timerange.to());
        verify(moreSearch, times(1)).scrollQuery(
                eq(config.query()),
                eq(config.streams()),
                eq(config.queryParameters()),
                eq(parameters.timerange()),
                eq(parameters.batchSize()),
                any(MoreSearch.ScrollCallback.class)
        );

        reset(stateService, moreSearch, searchFactory); // Rest mocks so we can verify it again

        // If the dependency check returns false, a precondition exception should be raised and the state service not be called
        when(eventProcessorDependencyCheck.hasMessagesIndexedUpTo(timerange.to())).thenReturn(false);

        assertThatCode(() -> eventProcessor.createEvents(eventFactory, parameters, (events) -> {}))
                .hasMessageContaining(eventDefinitionDto.title())
                .hasMessageContaining(eventDefinitionDto.id())
                .hasMessageContaining(timerange.from().toString())
                .hasMessageContaining(timerange.to().toString())
                .isInstanceOf(EventProcessorPreconditionException.class);

        verify(stateService, never()).setState(any(String.class), any(DateTime.class), any(DateTime.class));
        verify(searchFactory, never()).create(any(), any(), any(), any());
        verify(moreSearch, never()).scrollQuery(
                eq(config.query()),
                eq(config.streams()),
                eq(config.queryParameters()),
                eq(parameters.timerange()),
                eq(parameters.batchSize()),
                any(MoreSearch.ScrollCallback.class)
        );
    }
",non-flaky,5
86072,graylog2_graylog2-server,AggregationEventProcessorTest.testEventsFromAggregationResultWithEmptyResultUsesEventDefinitionStreamAsSourceStreams,"    @Test
    public void testEventsFromAggregationResultWithEmptyResultUsesEventDefinitionStreamAsSourceStreams() {
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final AbsoluteRange timerange = AbsoluteRange.create(now.minusHours(1), now.plusHours(1));

        // We expect to get the end of the aggregation timerange as event time
        final TestEvent event1 = new TestEvent(timerange.to());
        final TestEvent event2 = new TestEvent(timerange.to());
        when(eventFactory.createEvent(any(EventDefinition.class), eq(now), anyString()))
                .thenReturn(event1)  // first invocation return value
                .thenReturn(event2); // second invocation return value

        final EventDefinitionDto eventDefinitionDto = EventDefinitionDto.builder()
                .id(""dto-id-1"")
                .title(""Test Aggregation"")
                .description(""A test aggregation event processors"")
                .priority(1)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .config(AggregationEventProcessorConfig.builder()
                        .query("""")
                        .streams(ImmutableSet.of(""stream-2""))
                        .groupBy(ImmutableList.of(""group_field_one"", ""group_field_two""))
                        .series(ImmutableList.of())
                        .conditions(null)
                        .searchWithinMs(30000)
                        .executeEveryMs(30000)
                        .build())
                .keySpec(ImmutableList.of())
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .timerange(timerange)
                .build();

        final AggregationEventProcessor eventProcessor = new AggregationEventProcessor(eventDefinitionDto, searchFactory, eventProcessorDependencyCheck, stateService, moreSearch, streamService, messages);

        final AggregationResult result = AggregationResult.builder()
                .effectiveTimerange(timerange)
                .totalAggregatedMessages(0)
                .sourceStreams(ImmutableSet.of()) // No streams in result
                .keyResults(ImmutableList.of(
                        AggregationKeyResult.builder()
                                .key(ImmutableList.of(""one"", ""two""))
                                .timestamp(now)
                                .seriesValues(ImmutableList.of(
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(0.0d)
                                                .series(AggregationSeries.builder()
                                                        .id(""abc123"")
                                                        .function(AggregationFunction.COUNT)
                                                        .build())
                                                .build()
                                ))
                                .build()
                ))
                .build();

        final ImmutableList<EventWithContext> eventsWithContext = eventProcessor.eventsFromAggregationResult(eventFactory, parameters, result);

        assertThat(eventsWithContext).hasSize(1);

        assertThat(eventsWithContext.get(0)).satisfies(eventWithContext -> {
            final Event event = eventWithContext.event();

            assertThat(event.getId()).isEqualTo(event1.getId());
            assertThat(event.getMessage()).isEqualTo(event1.getMessage());
            assertThat(event.getEventTimestamp()).isEqualTo(timerange.to());
            assertThat(event.getTimerangeStart()).isEqualTo(timerange.from());
            assertThat(event.getTimerangeEnd()).isEqualTo(timerange.to());
            // Must contain the stream from the event definition because there is none in the result
            assertThat(event.getSourceStreams()).containsOnly(""stream-2"");

            final Message message = eventWithContext.messageContext().orElse(null);

            assertThat(message).isNotNull();
            assertThat(message.getField(""group_field_one"")).isEqualTo(""one"");
            assertThat(message.getField(""group_field_two"")).isEqualTo(""two"");
            assertThat(message.getField(""aggregation_key"")).isEqualTo(""one|two"");
            assertThat(message.getField(""aggregation_value_count"")).isEqualTo(0.0d);
        });
    }
",non-flaky,5
86073,graylog2_graylog2-server,AggregationEventProcessorTest.testEventsFromAggregationResultWithEmptyResultAndNoConfiguredStreamsUsesAllStreamsAsSourceStreams,"    @Test
    public void testEventsFromAggregationResultWithEmptyResultAndNoConfiguredStreamsUsesAllStreamsAsSourceStreams() {
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final AbsoluteRange timerange = AbsoluteRange.create(now.minusHours(1), now.plusHours(1));

        // We expect to get the end of the aggregation timerange as event time
        final TestEvent event1 = new TestEvent(timerange.to());
        final TestEvent event2 = new TestEvent(timerange.to());
        when(eventFactory.createEvent(any(EventDefinition.class), eq(now), anyString()))
                .thenReturn(event1)  // first invocation return value
                .thenReturn(event2); // second invocation return value

        when(streamService.loadAll()).thenReturn(ImmutableList.of(
                new StreamMock(Collections.singletonMap(""_id"", ""stream-1""), Collections.emptyList()),
                new StreamMock(Collections.singletonMap(""_id"", ""stream-2""), Collections.emptyList()),
                new StreamMock(Collections.singletonMap(""_id"", ""stream-3""), Collections.emptyList()),
                new StreamMock(Collections.singletonMap(""_id"", StreamImpl.DEFAULT_STREAM_ID), Collections.emptyList()),
                new StreamMock(Collections.singletonMap(""_id"", StreamImpl.DEFAULT_EVENTS_STREAM_ID), Collections.emptyList()),
                new StreamMock(Collections.singletonMap(""_id"", StreamImpl.DEFAULT_SYSTEM_EVENTS_STREAM_ID), Collections.emptyList())
        ));

        final EventDefinitionDto eventDefinitionDto = EventDefinitionDto.builder()
                .id(""dto-id-1"")
                .title(""Test Aggregation"")
                .description(""A test aggregation event processors"")
                .priority(1)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .config(AggregationEventProcessorConfig.builder()
                        .query("""")
                        .streams(ImmutableSet.of()) // No configured streams!
                        .groupBy(ImmutableList.of(""group_field_one"", ""group_field_two""))
                        .series(ImmutableList.of())
                        .conditions(null)
                        .searchWithinMs(30000)
                        .executeEveryMs(30000)
                        .build())
                .keySpec(ImmutableList.of())
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .timerange(timerange)
                .build();

        final AggregationEventProcessor eventProcessor = new AggregationEventProcessor(eventDefinitionDto, searchFactory, eventProcessorDependencyCheck, stateService, moreSearch, streamService, messages);

        final AggregationResult result = AggregationResult.builder()
                .effectiveTimerange(timerange)
                .totalAggregatedMessages(0)
                .sourceStreams(ImmutableSet.of()) // No streams in result
                .keyResults(ImmutableList.of(
                        AggregationKeyResult.builder()
                                .key(ImmutableList.of(""one"", ""two""))
                                .timestamp(now)
                                .seriesValues(ImmutableList.of(
                                        AggregationSeriesValue.builder()
                                                .key(ImmutableList.of(""a""))
                                                .value(0.0d)
                                                .series(AggregationSeries.builder()
                                                        .id(""abc123"")
                                                        .function(AggregationFunction.COUNT)
                                                        .build())
                                                .build()
                                ))
                                .build()
                ))
                .build();

        final ImmutableList<EventWithContext> eventsWithContext = eventProcessor.eventsFromAggregationResult(eventFactory, parameters, result);

        assertThat(eventsWithContext).hasSize(1);

        assertThat(eventsWithContext.get(0)).satisfies(eventWithContext -> {
            final Event event = eventWithContext.event();

            assertThat(event.getId()).isEqualTo(event1.getId());
            assertThat(event.getMessage()).isEqualTo(event1.getMessage());
            assertThat(event.getEventTimestamp()).isEqualTo(timerange.to());
            assertThat(event.getTimerangeStart()).isEqualTo(timerange.from());
            assertThat(event.getTimerangeEnd()).isEqualTo(timerange.to());
            // Must contain all existing streams but the default event streams!
            assertThat(event.getSourceStreams()).containsOnly(
                    ""stream-1"",
                    ""stream-2"",
                    ""stream-3"",
                    StreamImpl.DEFAULT_STREAM_ID
            );

            final Message message = eventWithContext.messageContext().orElse(null);

            assertThat(message).isNotNull();
            assertThat(message.getField(""group_field_one"")).isEqualTo(""one"");
            assertThat(message.getField(""group_field_two"")).isEqualTo(""two"");
            assertThat(message.getField(""aggregation_key"")).isEqualTo(""one|two"");
            assertThat(message.getField(""aggregation_value_count"")).isEqualTo(0.0d);
        });
    }
",non-flaky,5
86074,graylog2_graylog2-server,PivotAggregationSearchTest.testExtractValuesWithGroupBy,"    @Test
    public void testExtractValuesWithGroupBy() throws Exception {
        final AbsoluteRange timerange = AbsoluteRange.create(DateTime.now(DateTimeZone.UTC).minusSeconds(3600), DateTime.now(DateTimeZone.UTC));
        final AggregationSeries seriesCount = AggregationSeries.create(""abc123"", AggregationFunction.COUNT, ""source"");
        final AggregationSeries seriesCard = AggregationSeries.create(""abc123"", AggregationFunction.CARD, ""source"");
        final AggregationEventProcessorConfig config = AggregationEventProcessorConfig.builder()
                .query("""")
                .streams(Collections.emptySet())
                .groupBy(Collections.emptyList())
                .series(ImmutableList.of(seriesCount, seriesCard))
                .conditions(null)
                .searchWithinMs(30000)
                .executeEveryMs(30000)
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .streams(Collections.emptySet())
                .timerange(timerange)
                .batchSize(500)
                .build();

        final PivotAggregationSearch pivotAggregationSearch = new PivotAggregationSearch(
                config,
                parameters,
                ""test"",
                eventDefinition,
                searchJobService,
                queryEngine,
                EventsConfigurationTestProvider.create(),
                moreSearch,
                permittedStreams);

        final String toString = timerange.getTo().toString();
        final PivotResult pivotResult = PivotResult.builder()
                .id(""test"")
                .effectiveTimerange(timerange)
                .total(1)
                .addRow(PivotResult.Row.builder()
                        .key(ImmutableList.of(toString, ""a"", ""b""))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/count/source/abc123""), 42, true, ""row-leaf""))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/card/source/abc123""), 1, true, ""row-leaf""))
                        .source(""leaf"")
                        .build())
                .addRow(PivotResult.Row.builder()
                        .key(ImmutableList.of(toString, ""a""))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/count/source/abc123""), 84, true, ""row-inner""))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/card/source/abc123""), 1, true, ""row-inner""))
                        .source(""non-leaf"")
                        .build())
                .addRow(PivotResult.Row.builder()
                        .key(ImmutableList.of(toString, ""a"", ""c""))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/count/source/abc123""), 42, true, ""row-leaf""))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/card/source/abc123""), 1, true, ""row-leaf""))
                        .source(""leaf"")
                        .build())
                .build();

        final ImmutableList<AggregationKeyResult> results = pivotAggregationSearch.extractValues(pivotResult);

        assertThat(results.size()).isEqualTo(2);

        assertThat(results.get(0)).isEqualTo(AggregationKeyResult.builder()
                .timestamp(timerange.getTo())
                .key(ImmutableList.of(""a"", ""b""))
                .seriesValues(ImmutableList.of(
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of(""a"", ""b""))
                                .value(42.0)
                                .series(seriesCount)
                                .build(),
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of(""a"", ""b""))
                                .value(1.0)
                                .series(seriesCard)
                                .build()
                ))
                .build());

        assertThat(results.get(1)).isEqualTo(AggregationKeyResult.builder()
                .timestamp(timerange.getTo())
                .key(ImmutableList.of(""a"", ""c""))
                .seriesValues(ImmutableList.of(
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of(""a"", ""c""))
                                .value(42.0)
                                .series(seriesCount)
                                .build(),
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of(""a"", ""c""))
                                .value(1.0)
                                .series(seriesCard)
                                .build()
                ))
                .build());
    }
",non-flaky,5
86075,graylog2_graylog2-server,PivotAggregationSearchTest.testExtractValuesWithoutGroupBy,"    @Test
    public void testExtractValuesWithoutGroupBy() throws Exception {
        final AbsoluteRange timerange = AbsoluteRange.create(DateTime.now(DateTimeZone.UTC).minusSeconds(3600), DateTime.now(DateTimeZone.UTC));
        final AggregationSeries seriesCount = AggregationSeries.create(""abc123"", AggregationFunction.COUNT, ""source"");
        final AggregationSeries seriesCountNoField = AggregationSeries.create(""abc123"", AggregationFunction.COUNT, """");
        final AggregationSeries seriesCard = AggregationSeries.create(""abc123"", AggregationFunction.CARD, ""source"");
        final AggregationEventProcessorConfig config = AggregationEventProcessorConfig.builder()
                .query("""")
                .streams(Collections.emptySet())
                .groupBy(Collections.emptyList())
                .series(ImmutableList.of(seriesCount, seriesCountNoField, seriesCard))
                .conditions(null)
                .searchWithinMs(30000)
                .executeEveryMs(30000)
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .streams(Collections.emptySet())
                .timerange(timerange)
                .batchSize(500)
                .build();

        final PivotAggregationSearch pivotAggregationSearch = new PivotAggregationSearch(
                config,
                parameters,
                ""test"",
                eventDefinition,
                searchJobService,
                queryEngine,
                EventsConfigurationTestProvider.create(),
                moreSearch,
                permittedStreams);

        final PivotResult pivotResult = PivotResult.builder()
                .id(""test"")
                .effectiveTimerange(timerange)
                .total(1)
                .addRow(PivotResult.Row.builder()
                        .key(ImmutableList.of(timerange.getTo().toString()))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/count/source/abc123""), 42, true, ""row-leaf""))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/count/<no-field>/abc123""), 23, true, ""row-leaf""))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/card/source/abc123""), 1, true, ""row-leaf""))
                        .source(""leaf"")
                        .build())
                .build();

        final ImmutableList<AggregationKeyResult> results = pivotAggregationSearch.extractValues(pivotResult);

        assertThat(results.size()).isEqualTo(1);

        assertThat(results.get(0)).isEqualTo(AggregationKeyResult.builder()
                .key(ImmutableList.of())
                .timestamp(timerange.getTo())
                .seriesValues(ImmutableList.of(
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of())
                                .value(42.0)
                                .series(seriesCount)
                                .build(),
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of())
                                .value(23.0)
                                .series(seriesCountNoField)
                                .build(),
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of())
                                .value(1.0)
                                .series(seriesCard)
                                .build()
                ))
                .build());
    }
",non-flaky,5
86076,graylog2_graylog2-server,PivotAggregationSearchTest.testExtractValuesWithNullValues,"    @Test
    public void testExtractValuesWithNullValues() throws Exception {
        final AbsoluteRange timerange = AbsoluteRange.create(DateTime.now(DateTimeZone.UTC).minusSeconds(3600), DateTime.now(DateTimeZone.UTC));
        final AggregationSeries seriesCount = AggregationSeries.create(""abc123"", AggregationFunction.COUNT, ""source"");
        final AggregationSeries seriesAvg = AggregationSeries.create(""abc123"", AggregationFunction.AVG, ""some_field"");
        final AggregationEventProcessorConfig config = AggregationEventProcessorConfig.builder()
                .query("""")
                .streams(Collections.emptySet())
                .groupBy(Collections.emptyList())
                .series(ImmutableList.of(seriesCount, seriesAvg))
                .conditions(null)
                .searchWithinMs(30000)
                .executeEveryMs(30000)
                .build();
        final AggregationEventProcessorParameters parameters = AggregationEventProcessorParameters.builder()
                .streams(Collections.emptySet())
                .timerange(timerange)
                .batchSize(500)
                .build();

        final PivotAggregationSearch pivotAggregationSearch = new PivotAggregationSearch(
                config,
                parameters,
                ""test"",
                eventDefinition,
                searchJobService,
                queryEngine,
                EventsConfigurationTestProvider.create(),
                moreSearch,
                permittedStreams);

        final PivotResult pivotResult = PivotResult.builder()
                .id(""test"")
                .effectiveTimerange(timerange)
                .total(1)
                .addRow(PivotResult.Row.builder()
                        .key(ImmutableList.of(timerange.getTo().toString()))
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/count/source/abc123""), 42, true, ""row-leaf""))
                        // A ""null"" value can happen with some Elasticsearch aggregations (e.g. avg on a non-existent field)
                        .addValue(PivotResult.Value.create(ImmutableList.of(""metric/avg/some_field/abc123""), null, true, ""row-leaf""))
                        .source(""leaf"")
                        .build())
                .build();

        final ImmutableList<AggregationKeyResult> results = pivotAggregationSearch.extractValues(pivotResult);

        assertThat(results.size()).isEqualTo(1);

        assertThat(results.get(0)).isEqualTo(AggregationKeyResult.builder()
                .key(ImmutableList.of())
                .timestamp(timerange.getTo())
                .seriesValues(ImmutableList.of(
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of())
                                .value(42.0)
                                .series(seriesCount)
                                .build(),
                        AggregationSeriesValue.builder()
                                .key(ImmutableList.of())
                                .value(Double.NaN) // For ""null"" we expect NaN
                                .series(seriesAvg)
                                .build()
                ))
                .build());
    }
",non-flaky,5
86077,graylog2_graylog2-server,PivotAggregationSearchTest.testDateRangeBucketWithOneTumblingWindow,"    @Test
    public void testDateRangeBucketWithOneTumblingWindow() {
        final long processingWindowSize = Duration.standardSeconds(60).getMillis();
        final long processingHopSize = Duration.standardSeconds(60).getMillis();
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final DateTime from = now;
        final DateTime to = now.plusMillis((int) processingWindowSize);
        TimeRange timeRange = AbsoluteRange.create(from, to);
        final DateRangeBucket rangeBucket = PivotAggregationSearch.buildDateRangeBuckets(timeRange, processingWindowSize, processingHopSize);

        assertThat(rangeBucket.ranges()).containsExactly(DateRange.create(from, to));
    }
",non-flaky,5
86078,graylog2_graylog2-server,PivotAggregationSearchTest.testDateRangeBucketWithCatchUpTumblingWindows,"    @Test
    public void testDateRangeBucketWithCatchUpTumblingWindows() {
        final long processingWindowSize = Duration.standardSeconds(60).getMillis();
        final long processingHopSize = Duration.standardSeconds(60).getMillis();
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final DateTime from = now;
        // We are 3 full processingWindows behind
        final DateTime to = now.plusMillis((int) processingWindowSize * 3);
        TimeRange timeRange = AbsoluteRange.create(from, to);
        final DateRangeBucket rangeBucket = PivotAggregationSearch.buildDateRangeBuckets(timeRange, processingWindowSize, processingHopSize);

        assertThat(rangeBucket.ranges()).containsExactly(
                DateRange.create(from.plusMillis((int) (processingWindowSize * 0)), from.plusMillis((int) (processingWindowSize * 1))),
                DateRange.create(from.plusMillis((int) (processingWindowSize * 1)), from.plusMillis((int) (processingWindowSize * 2))),
                DateRange.create(from.plusMillis((int) (processingWindowSize * 2)), from.plusMillis((int) (processingWindowSize * 3)))
        );
    }
",non-flaky,5
86079,graylog2_graylog2-server,PivotAggregationSearchTest.testDateRangeBucketWithSlidingWindow,"    @Test
    public void testDateRangeBucketWithSlidingWindow() {
        final long processingWindowSize = Duration.standardSeconds(3600).getMillis();
        final long processingHopSize = Duration.standardSeconds(60).getMillis();
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final DateTime from = now;
        final DateTime to = now.plusMillis((int) processingWindowSize);
        TimeRange timeRange = AbsoluteRange.create(from, to);
        final DateRangeBucket rangeBucket = PivotAggregationSearch.buildDateRangeBuckets(timeRange, processingWindowSize, processingHopSize);

        assertThat(rangeBucket.ranges()).containsExactly(
                DateRange.create(from, to)
        );
    }
",non-flaky,5
86080,graylog2_graylog2-server,PivotAggregationSearchTest.testDateRangeBucketWithCatchUpSlidingWindows,"    @Test
    public void testDateRangeBucketWithCatchUpSlidingWindows() {
        final int processingWindowSizeSec = 120;
        final int processingHopSizeSec = 60;
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final DateTime from = now;
        // We are 3 full processingWindows behind
        final DateTime to = now.plusSeconds(processingWindowSizeSec * 3);
        TimeRange timeRange = AbsoluteRange.create(from, to);
        final DateRangeBucket rangeBucket = PivotAggregationSearch.buildDateRangeBuckets(timeRange, processingWindowSizeSec * 1000, processingHopSizeSec * 1000);

        assertThat(rangeBucket.ranges()).containsExactly(
                DateRange.create(from.plusSeconds(processingHopSizeSec * 0), from.plusSeconds(processingWindowSizeSec)),
                DateRange.create(from.plusSeconds(processingHopSizeSec * 1), from.plusSeconds(processingHopSizeSec * 1).plusSeconds(processingWindowSizeSec)),
                DateRange.create(from.plusSeconds(processingHopSizeSec * 2), from.plusSeconds(processingHopSizeSec * 2).plusSeconds(processingWindowSizeSec)),
                DateRange.create(from.plusSeconds(processingHopSizeSec * 3), from.plusSeconds(processingHopSizeSec * 3).plusSeconds(processingWindowSizeSec)),
                DateRange.create(from.plusSeconds(processingHopSizeSec * 4), to)
        );
    }
",non-flaky,5
86081,graylog2_graylog2-server,EventDefinitionHandlerTest.create,"    @Test
    public void create() {
        final EventDefinitionDto newDto = EventDefinitionDto.builder()
                .title(""Test"")
                .description(""A test event definition"")
                .config(TestEventProcessorConfig.builder()
                        .message(""This is a test event processor"")
                        .searchWithinMs(300000)
                        .executeEveryMs(60001)
                        .build())
                .priority(3)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .keySpec(ImmutableList.of(""a"", ""b""))
                .notifications(ImmutableList.of())
                .build();

        final EventDefinitionDto dto = handler.create(newDto, Optional.empty());

        // Handler should create the event definition
        assertThat(eventDefinitionService.get(dto.id())).isPresent();

        final Optional<JobDefinitionDto> jobDefinition = jobDefinitionService.getByConfigField(""event_definition_id"", dto.id());

        // Handler also should create the job definition for the event definition/processor
        assertThat(jobDefinition).isPresent().get().satisfies(definition -> {
            assertThat(definition.title()).isEqualTo(""Test"");
            assertThat(definition.description()).isEqualTo(""A test event definition"");
            assertThat(definition.config()).isInstanceOf(EventProcessorExecutionJob.Config.class);

            final EventProcessorExecutionJob.Config config = (EventProcessorExecutionJob.Config) definition.config();


            assertThat(config.processingWindowSize()).isEqualTo(300000);
            assertThat(config.processingHopSize()).isEqualTo(60001);
        });

        // And the handler should also create a job trigger for the created job definition
        final Optional<JobTriggerDto> jobTrigger = jobTriggerService.nextRunnableTrigger();

        assertThat(jobTrigger).isPresent().get().satisfies(trigger -> {
            assertThat(trigger.jobDefinitionId()).isEqualTo(jobDefinition.get().id());
            assertThat(trigger.schedule()).isInstanceOf(IntervalJobSchedule.class);

            final IntervalJobSchedule schedule = (IntervalJobSchedule) trigger.schedule();

            assertThat(schedule.interval()).isEqualTo(60001);
            assertThat(schedule.unit()).isEqualTo(TimeUnit.MILLISECONDS);
        });
    }
",non-flaky,5
86082,graylog2_graylog2-server,EventDefinitionHandlerTest.createWithoutSchedule,"    @Test
    public void createWithoutSchedule() {
        final EventDefinitionDto newDto = EventDefinitionDto.builder()
                .title(""Test"")
                .description(""A test event definition"")
                .config(TestEventProcessorConfig.builder()
                        .message(""This is a test event processor"")
                        .searchWithinMs(300000)
                        .executeEveryMs(60001)
                        .build())
                .priority(3)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .keySpec(ImmutableList.of(""a"", ""b""))
                .notifications(ImmutableList.of())
                .build();

        final EventDefinitionDto dto = handler.createWithoutSchedule(newDto, Optional.empty());

        // Handler should create the event definition
        assertThat(eventDefinitionService.get(dto.id())).isPresent();

        // Handler should NOT create a job definition for the event definition/processor
        assertThat(jobDefinitionService.getByConfigField(""event_definition_id"", dto.id())).isNotPresent();

        // And the handler should also NOT create a job trigger for the created job definition
        assertThat(jobTriggerService.nextRunnableTrigger()).isNotPresent();
    }
",non-flaky,5
86083,graylog2_graylog2-server,EventDefinitionHandlerTest.update,"    @Test
    public void update() {
        final String newTitle = ""A NEW TITLE "" + DateTime.now(DateTimeZone.UTC).toString();
        final String newDescription = ""A NEW DESCRIPTION "" + DateTime.now(DateTimeZone.UTC).toString();

        final EventDefinitionDto existingDto = eventDefinitionService.get(""54e3deadbeefdeadbeef0000"").orElse(null);
        final JobDefinitionDto existingJobDefinition = jobDefinitionService.get(""54e3deadbeefdeadbeef0001"").orElse(null);
        final JobTriggerDto existingTrigger = jobTriggerService.get(""54e3deadbeefdeadbeef0002"").orElse(null);
        final TestEventProcessorConfig existingConfig = (TestEventProcessorConfig) existingDto.config();
        final TestEventProcessorConfig newConfig = existingConfig.toBuilder()
                .executeEveryMs(550000)
                .searchWithinMs(800000)
                .build();
        final EventProcessorExecutionJob.Data existingTriggerData = (EventProcessorExecutionJob.Data) existingTrigger.data().orElseThrow(AssertionError::new);

        assertThat(existingDto).isNotNull();
        assertThat(existingJobDefinition).isNotNull();
        assertThat(existingTrigger).isNotNull();

        final EventDefinitionDto updatedDto = existingDto.toBuilder()
                .title(newTitle)
                .description(newDescription)
                .config(newConfig)
                .build();

        assertThat(handler.update(updatedDto, true)).isNotEqualTo(existingDto);

        assertThat(eventDefinitionService.get(existingDto.id())).isPresent().get().satisfies(dto -> {
            assertThat(dto.id()).isEqualTo(existingDto.id());
            assertThat(dto.title()).isEqualTo(newTitle);
            assertThat(dto.description()).isEqualTo(newDescription);
        });

        // Test that the schedule is updated to the new config
        final JobDefinitionDto newJobDefinition = jobDefinitionService.get(""54e3deadbeefdeadbeef0001"").orElseThrow(AssertionError::new);
        assertThat(newJobDefinition.title()).isEqualTo(newTitle);
        assertThat(newJobDefinition.description()).isEqualTo(newDescription);
        assertThat(((EventProcessorExecutionJob.Config) newJobDefinition.config()).processingHopSize()).isEqualTo(550000);
        assertThat(((EventProcessorExecutionJob.Config) newJobDefinition.config()).processingWindowSize()).isEqualTo(800000);

        // Test if the EventDefinition update removed the old trigger data
        // and reset the job definition timerange to the new parameters
        final EventProcessorExecutionJob.Config newJobConfig = (EventProcessorExecutionJob.Config) newJobDefinition.config();
        final TimeRange newTimeRange = newJobConfig.parameters().timerange();
        assertThat(newTimeRange.getFrom()).isEqualTo(clock.nowUTC().minus(newConfig.searchWithinMs()));
        assertThat(newTimeRange.getTo()).isEqualTo(clock.nowUTC());

        assertThat(jobTriggerService.get(""54e3deadbeefdeadbeef0002"")).isPresent().get().satisfies(trigger -> {
            assertThat(trigger.data()).isEmpty();
            assertThat(trigger.nextTime()).isEqualTo(clock.nowUTC());
        });
    }
",non-flaky,5
86084,graylog2_graylog2-server,EventDefinitionHandlerTest.updateWithSchedulingDisabled,"    @Test
    public void updateWithSchedulingDisabled() {
        final String newTitle = ""A NEW TITLE "" + DateTime.now(DateTimeZone.UTC).toString();
        final String newDescription = ""A NEW DESCRIPTION "" + DateTime.now(DateTimeZone.UTC).toString();

        final EventDefinitionDto existingDto = eventDefinitionService.get(""54e3deadbeefdeadbeef0000"").orElse(null);
        final JobDefinitionDto existingJobDefinition = jobDefinitionService.get(""54e3deadbeefdeadbeef0001"").orElse(null);
        final JobTriggerDto existingTrigger = jobTriggerService.get(""54e3deadbeefdeadbeef0002"").orElse(null);
        final TestEventProcessorConfig existingConfig = (TestEventProcessorConfig) existingDto.config();
        final TestEventProcessorConfig newConfig = existingConfig.toBuilder()
                .executeEveryMs(550000)
                .searchWithinMs(800000)
                .build();

        assertThat(existingDto).isNotNull();
        assertThat(existingJobDefinition).isNotNull();
        assertThat(existingTrigger).isNotNull();

        final EventDefinitionDto updatedDto = existingDto.toBuilder()
                .title(newTitle)
                .description(newDescription)
                .config(newConfig)
                .build();

        assertThat(handler.update(updatedDto, false)).isNotEqualTo(existingDto);

        assertThat(eventDefinitionService.get(existingDto.id())).isPresent().get().satisfies(dto -> {
            assertThat(dto.id()).isEqualTo(existingDto.id());
            assertThat(dto.title()).isEqualTo(newTitle);
            assertThat(dto.description()).isEqualTo(newDescription);
        });

        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isNotPresent();
        assertThat(jobTriggerService.get(""54e3deadbeefdeadbeef0002"")).isNotPresent();
    }
",non-flaky,5
86085,graylog2_graylog2-server,EventDefinitionHandlerTest.updateWithSchedulingReEnabled,"    @Test
    public void updateWithSchedulingReEnabled() {
        final String newTitle = ""A NEW TITLE "" + DateTime.now(DateTimeZone.UTC).toString();
        final String newDescription = ""A NEW DESCRIPTION "" + DateTime.now(DateTimeZone.UTC).toString();

        final EventDefinitionDto existingDto = eventDefinitionService.get(""54e3deadbeefdeadbeef0000"").orElse(null);
        final TestEventProcessorConfig existingConfig = (TestEventProcessorConfig) existingDto.config();
        final TestEventProcessorConfig newConfig = existingConfig.toBuilder()
                .executeEveryMs(550000)
                .searchWithinMs(800000)
                .build();

        assertThat(existingDto).isNotNull();

        final EventDefinitionDto updatedDto = existingDto.toBuilder()
                .title(newTitle)
                .description(newDescription)
                .config(newConfig)
                .build();

        assertThat(handler.update(updatedDto, true)).isNotEqualTo(existingDto);

        assertThat(eventDefinitionService.get(existingDto.id())).isPresent().get().satisfies(dto -> {
            assertThat(dto.id()).isEqualTo(existingDto.id());
            assertThat(dto.title()).isEqualTo(newTitle);
            assertThat(dto.description()).isEqualTo(newDescription);
        });

        final JobDefinitionDto newJobDefinition = jobDefinitionService.getByConfigField(""event_definition_id"", existingDto.id())
                .orElseThrow(AssertionError::new);
        assertThat(newJobDefinition.title()).isEqualTo(newTitle);
        assertThat(newJobDefinition.description()).isEqualTo(newDescription);
        assertThat(((EventProcessorExecutionJob.Config) newJobDefinition.config()).processingHopSize()).isEqualTo(550000);

        assertThat(jobTriggerService.getForJob(newJobDefinition.id()).get(0)).satisfies(trigger -> {
            final IntervalJobSchedule schedule = (IntervalJobSchedule) trigger.schedule();
            assertThat(schedule.interval()).isEqualTo(550000);
        });
    }
",non-flaky,5
86086,graylog2_graylog2-server,EventDefinitionHandlerTest.updateWithErrors,"    @Test
    public void updateWithErrors() {
        final String newTitle = ""A NEW TITLE "" + DateTime.now(DateTimeZone.UTC).toString();
        final String newDescription = ""A NEW DESCRIPTION "" + DateTime.now(DateTimeZone.UTC).toString();

        final EventDefinitionDto existingDto = eventDefinitionService.get(""54e3deadbeefdeadbeef0000"").orElse(null);
        final JobDefinitionDto existingJobDefinition = jobDefinitionService.get(""54e3deadbeefdeadbeef0001"").orElse(null);
        final JobTriggerDto existingTrigger = jobTriggerService.get(""54e3deadbeefdeadbeef0002"").orElse(null);

        assertThat(existingDto).isNotNull();
        assertThat(existingJobDefinition).isNotNull();
        assertThat(existingTrigger).isNotNull();

        final EventDefinitionDto updatedDto = existingDto.toBuilder()
                .title(newTitle)
                .description(newDescription)
                .build();

        doThrow(new NullPointerException(""yolo1"")).when(eventDefinitionService).save(any());

        assertThatCode(() -> handler.update(updatedDto, true))
                .isInstanceOf(NullPointerException.class)
                .hasMessageContaining(""yolo1"");

        assertThat(eventDefinitionService.get(existingDto.id())).isPresent().get().satisfies(dto -> {
            assertThat(dto.id()).isEqualTo(existingDto.id());
            assertThat(dto.title()).isEqualTo(existingDto.title());
            assertThat(dto.description()).isEqualTo(existingDto.description());
        });

        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isPresent().get().satisfies(definition -> {
            assertThat(definition.title()).isEqualTo(existingJobDefinition.title());
            assertThat(definition.description()).isEqualTo(existingJobDefinition.description());
        });

        // Reset all before doing new stubs
        reset(eventDefinitionService);
        reset(jobDefinitionService);
        reset(jobTriggerService);

        doThrow(new NullPointerException(""yolo2"")).when(jobDefinitionService).save(any());

        assertThatCode(() -> handler.update(updatedDto, true))
                .isInstanceOf(NullPointerException.class)
                .hasMessageContaining(""yolo2"");

        assertThat(eventDefinitionService.get(existingDto.id())).isPresent().get().satisfies(dto -> {
            assertThat(dto.id()).isEqualTo(existingDto.id());
            assertThat(dto.title()).isEqualTo(existingDto.title());
            assertThat(dto.description()).isEqualTo(existingDto.description());
        });

        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isPresent().get().satisfies(definition -> {
            assertThat(definition.title()).isEqualTo(existingJobDefinition.title());
            assertThat(definition.description()).isEqualTo(existingJobDefinition.description());
        });

        // Reset all before doing new stubs
        reset(eventDefinitionService);
        reset(jobDefinitionService);
        reset(jobTriggerService);

        doThrow(new NullPointerException(""yolo3"")).when(jobTriggerService).update(any());

        assertThatCode(() -> handler.update(updatedDto, true))
                .isInstanceOf(NullPointerException.class)
                .hasMessageContaining(""yolo3"");

        assertThat(eventDefinitionService.get(existingDto.id())).isPresent().get().satisfies(dto -> {
            assertThat(dto.id()).isEqualTo(existingDto.id());
            assertThat(dto.title()).isEqualTo(existingDto.title());
            assertThat(dto.description()).isEqualTo(existingDto.description());
        });

        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isPresent().get().satisfies(definition -> {
            assertThat(definition.title()).isEqualTo(existingJobDefinition.title());
            assertThat(definition.description()).isEqualTo(existingJobDefinition.description());
        });
    }
",non-flaky,5
86087,graylog2_graylog2-server,EventDefinitionHandlerTest.delete,"    @Test
    public void delete() {
        assertThat(eventDefinitionService.get(""54e3deadbeefdeadbeef0000"")).isPresent();
        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isPresent();
        assertThat(jobTriggerService.get(""54e3deadbeefdeadbeef0002"")).isPresent();

        assertThat(handler.delete(""54e3deadbeefdeadbeef0000"")).isTrue();

        assertThat(eventDefinitionService.get(""54e3deadbeefdeadbeef0000"")).isNotPresent();
        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isNotPresent();
        assertThat(jobTriggerService.get(""54e3deadbeefdeadbeef0002"")).isNotPresent();
    }
",non-flaky,5
86088,graylog2_graylog2-server,EventDefinitionHandlerTest.schedule,"    @Test
    public void schedule() {
        assertThat(eventDefinitionService.get(""54e3deadbeefdeadbeef0000"")).isPresent();
        assertThat(jobDefinitionService.streamAll().count()).isEqualTo(0);
        assertThat(jobTriggerService.all()).isEmpty();

        handler.schedule(""54e3deadbeefdeadbeef0000"");

        assertThat(eventDefinitionService.get(""54e3deadbeefdeadbeef0000"")).isPresent();

        assertThat(jobDefinitionService.getByConfigField(""event_definition_id"", ""54e3deadbeefdeadbeef0000""))
                .get()
                .satisfies(definition -> {
                    assertThat(definition.title()).isEqualTo(""Test"");
                    assertThat(definition.description()).isEqualTo(""A test event definition"");
                    assertThat(definition.config()).isInstanceOf(EventProcessorExecutionJob.Config.class);

                    final EventProcessorExecutionJob.Config config = (EventProcessorExecutionJob.Config) definition.config();


                    assertThat(config.processingWindowSize()).isEqualTo(300000);
                    assertThat(config.processingHopSize()).isEqualTo(60000);

                    assertThat(jobTriggerService.nextRunnableTrigger()).get().satisfies(trigger -> {
                        assertThat(trigger.jobDefinitionId()).isEqualTo(definition.id());
                        assertThat(trigger.schedule()).isInstanceOf(IntervalJobSchedule.class);

                        final IntervalJobSchedule schedule = (IntervalJobSchedule) trigger.schedule();

                        assertThat(schedule.interval()).isEqualTo(60000);
                        assertThat(schedule.unit()).isEqualTo(TimeUnit.MILLISECONDS);
                    });
                });


        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isNotPresent();
        assertThat(jobTriggerService.get(""54e3deadbeefdeadbeef0002"")).isNotPresent();
    }
",non-flaky,5
86089,graylog2_graylog2-server,EventDefinitionHandlerTest.scheduleWithMissingEventDefinition,"    @Test
    public void scheduleWithMissingEventDefinition() {
        final String id = ""54e3deadbeefdeadbeef9999"";

        // The event definition should not exist so our test works
        assertThat(eventDefinitionService.get(id)).isNotPresent();

        assertThatThrownBy(() -> handler.schedule(id))
                .hasMessageContaining(""doesn't exist"")
                .isInstanceOf(IllegalArgumentException.class);
    }
",non-flaky,5
86090,graylog2_graylog2-server,EventDefinitionHandlerTest.unschedule,"    @Test
    public void unschedule() {
        assertThat(eventDefinitionService.get(""54e3deadbeefdeadbeef0000"")).isPresent();
        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isPresent();
        assertThat(jobTriggerService.get(""54e3deadbeefdeadbeef0002"")).isPresent();

        handler.unschedule(""54e3deadbeefdeadbeef0000"");

        // Unschedule should NOT delete the event definition!
        assertThat(eventDefinitionService.get(""54e3deadbeefdeadbeef0000"")).isPresent();

        // Only the job definition and the trigger
        assertThat(jobDefinitionService.get(""54e3deadbeefdeadbeef0001"")).isNotPresent();
        assertThat(jobTriggerService.get(""54e3deadbeefdeadbeef0002"")).isNotPresent();
    }
",non-flaky,5
86091,graylog2_graylog2-server,EventDefinitionHandlerTest.unscheduleWithMissingEventDefinition,"    @Test
    public void unscheduleWithMissingEventDefinition() {
        final String id = ""54e3deadbeefdeadbeef9999"";

        // The event definition should not exist so our test works
        assertThat(eventDefinitionService.get(id)).isNotPresent();

        assertThatThrownBy(() -> handler.unschedule(id))
                .hasMessageContaining(""doesn't exist"")
                .isInstanceOf(IllegalArgumentException.class);
    }
",non-flaky,5
86092,graylog2_graylog2-server,EventDefinitionDtoTest.testValidateWithEmptyTitle,"    @Test
    public void testValidateWithEmptyTitle() {
        final EventDefinitionDto invalidEventDefinition = testSubject.toBuilder()
            .title("""")
            .build();
        final ValidationResult validationResult = invalidEventDefinition.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""title"");
    }
",non-flaky,5
86093,graylog2_graylog2-server,EventDefinitionDtoTest.testValidateWithEmptyConfigType,"    @Test
    public void testValidateWithEmptyConfigType() {
        final EventDefinitionDto invalidEventDefinition = testSubject.toBuilder()
            .config(new EventProcessorConfig.FallbackConfig())
            .build();
        final ValidationResult validationResult = invalidEventDefinition.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""config"");
    }
",non-flaky,5
86094,graylog2_graylog2-server,EventDefinitionDtoTest.testValidateWithInvalidConfig,"    @Test
    public void testValidateWithInvalidConfig() {
        final AggregationEventProcessorConfig configMock = mock(AggregationEventProcessorConfig.class);
        final ValidationResult mockedValidationResult = new ValidationResult();
        mockedValidationResult.addError(""foo"", ""bar"");
        when(configMock.validate()).thenReturn(mockedValidationResult);

        final EventDefinitionDto invalidEventDefinition = testSubject.toBuilder()
            .config(configMock)
            .build();
        final ValidationResult validationResult = invalidEventDefinition.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""foo"");
    }
",non-flaky,5
86095,graylog2_graylog2-server,EventDefinitionDtoTest.testValidateWithInvalidFieldName,"    @Test
    public void testValidateWithInvalidFieldName() {
        final EventFieldSpec fieldSpecMock = mock(EventFieldSpec.class);
        final EventDefinitionDto invalidEventDefinition = testSubject.toBuilder()
            .fieldSpec(ImmutableMap.of(""foo\\bar"", fieldSpecMock, ""$yo&^a"", fieldSpecMock))
            .build();
        final ValidationResult validationResult = invalidEventDefinition.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""field_spec"");
        final List<String> fieldValidation = (List<String>) validationResult.getErrors().get(""field_spec"");
        assertThat(fieldValidation.size()).isEqualTo(2);
        assertThat(fieldValidation.get(0)).contains(""foo\\bar"");
        assertThat(fieldValidation.get(1)).contains(""$yo&^a"");
    }
",non-flaky,5
86096,graylog2_graylog2-server,EventDefinitionDtoTest.testValidateWithKeySpecNotInFieldSpec,"    @Test
    public void testValidateWithKeySpecNotInFieldSpec() {
        final EventFieldSpec fieldSpecMock = mock(EventFieldSpec.class);
        final EventDefinitionDto invalidEventDefinition = testSubject.toBuilder()
            .fieldSpec(ImmutableMap.of(""bar"", fieldSpecMock, ""baz"", fieldSpecMock))
            .keySpec(ImmutableList.of(""foo""))
            .build();
        final ValidationResult validationResult = invalidEventDefinition.validate();
        assertThat(validationResult.failed()).isTrue();
        assertThat(validationResult.getErrors()).containsOnlyKeys(""key_spec"");
    }
",non-flaky,5
86097,graylog2_graylog2-server,EventDefinitionDtoTest.testValidEventDefinition,"    @Test
    public void testValidEventDefinition() {
        final ValidationResult validationResult = testSubject.validate();
        assertThat(validationResult.failed()).isFalse();
        assertThat(validationResult.getErrors().size()).isEqualTo(0);
    }
",non-flaky,5
86098,graylog2_graylog2-server,EventDefinitionDtoTest.testValidEventDefinitionWithKeySpecInFieldSpec,"    @Test
    public void testValidEventDefinitionWithKeySpecInFieldSpec() {
        final EventFieldSpec fieldSpecMock = mock(EventFieldSpec.class);
        final EventDefinitionDto invalidEventDefinition = testSubject.toBuilder()
            .fieldSpec(ImmutableMap.of(""foo"", fieldSpecMock, ""bar"", fieldSpecMock))
            .keySpec(ImmutableList.of(""foo"", ""bar""))
            .build();
        final ValidationResult validationResult = invalidEventDefinition.validate();
        assertThat(validationResult.failed()).isFalse();
        assertThat(validationResult.getErrors().size()).isEqualTo(0);
    }
",non-flaky,5
86099,graylog2_graylog2-server,EventProcessorDtoTest.type,"    @Test
    public void automaticallyAddsPersistToStreamsStorageHandler() {
        final EventStorageHandler.Config testStorageHandlerConfig = new EventStorageHandler.Config() {
            @Override
            public String type() {
                return ""storage-test"";
            }
",non-flaky,5
86100,graylog2_graylog2-server,DBEventProcessorServiceTest.loadPersisted,"    @Test
    public void loadPersisted() {
        final List<EventDefinitionDto> dtos = dbService.streamAll().collect(Collectors.toList());

        assertThat(dtos).hasSize(1);

        assertThat(dtos.get(0)).satisfies(dto -> {
            assertThat(dto.id()).isNotBlank();
            assertThat(dto.title()).isEqualTo(""Test"");
            assertThat(dto.description()).isEqualTo(""A test event definition"");
            assertThat(dto.priority()).isEqualTo(2);
            assertThat(dto.keySpec()).isEqualTo(ImmutableList.of(""username""));
            assertThat(dto.fieldSpec()).isEmpty();
            assertThat(dto.notifications()).isEmpty();
            assertThat(dto.storage()).hasSize(1);

            assertThat(dto.config()).isInstanceOf(TestEventProcessorConfig.class);
            assertThat(dto.config()).satisfies(abstractConfig -> {
                final TestEventProcessorConfig config = (TestEventProcessorConfig) abstractConfig;

                assertThat(config.type()).isEqualTo(""__test_event_processor_config__"");
                assertThat(config.message()).isEqualTo(""This is a test event processor"");
            });
        });
    }
",non-flaky,5
86101,graylog2_graylog2-server,DBEventProcessorServiceTest.save,"    @Test
    public void save() {
        final EventDefinitionDto newDto = EventDefinitionDto.builder()
                .title(""Test"")
                .description(""A test event definition"")
                .config(TestEventProcessorConfig.builder()
                        .message(""This is a test event processor"")
                        .searchWithinMs(1000)
                        .executeEveryMs(1000)
                        .build())
                .priority(3)
                .alert(false)
                .notificationSettings(EventNotificationSettings.withGracePeriod(60000))
                .keySpec(ImmutableList.of(""a"", ""b""))
                .notifications(ImmutableList.of())
                .build();

        final EventDefinitionDto dto = dbService.save(newDto);

        assertThat(dto.id()).isNotBlank();
        assertThat(dto.title()).isEqualTo(""Test"");
        assertThat(dto.description()).isEqualTo(""A test event definition"");
        assertThat(dto.priority()).isEqualTo(3);
        assertThat(dto.keySpec()).isEqualTo(ImmutableList.of(""a"", ""b""));
        assertThat(dto.fieldSpec()).isEmpty();
        assertThat(dto.notifications()).isEmpty();
        assertThat(dto.storage()).hasSize(1);
        // We will always add a persist-to-streams handler for now
        assertThat(dto.storage()).containsOnly(PersistToStreamsStorageHandler.Config.createWithDefaultEventsStream());
    }
",non-flaky,5
86102,graylog2_graylog2-server,DBEventProcessorStateServiceTest.persistence,"    @Test
    public void persistence() {
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final DateTime min = now.minusHours(1);
        final DateTime max = now;

        final EventProcessorStateDto stateDto = EventProcessorStateDto.builder()
                .eventDefinitionId(""abc123"")
                .minProcessedTimestamp(min)
                .maxProcessedTimestamp(max)
                .build();

        assertThat(stateService.setState(stateDto)).isPresent().get().satisfies(dto -> {
            assertThat(dto.id()).isNotBlank();
            assertThat(dto.eventDefinitionId()).isEqualTo(""abc123"");
            assertThat(dto.minProcessedTimestamp()).isEqualTo(min);
            assertThat(dto.maxProcessedTimestamp()).isEqualTo(max);
        });

        assertThatThrownBy(() -> stateService.setState("""", min, max))
                .hasMessageContaining(""eventDefinitionId"")
                .isInstanceOf(IllegalArgumentException.class);
        assertThatThrownBy(() -> stateService.setState(null, min, max))
                .hasMessageContaining(""eventDefinitionId"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatThrownBy(() -> stateService.setState(""a"", null, max))
                .hasMessageContaining(""minProcessedTimestamp"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatThrownBy(() -> stateService.setState(""a"", min, null))
                .hasMessageContaining(""maxProcessedTimestamp"")
                .isInstanceOf(IllegalArgumentException.class);

        // A max timestamp that is older than the min timestamp is an error! (e.g. mixing up arguments)
        assertThatThrownBy(() -> stateService.setState(""a"", max, min))
                .hasMessageContaining(""minProcessedTimestamp"")
                .hasMessageContaining(""maxProcessedTimestamp"")
                .isInstanceOf(IllegalArgumentException.class);
    }
",non-flaky,5
86103,graylog2_graylog2-server,DBEventProcessorStateServiceTest.loading,"    @Test
    public void loading() {
        final Optional<EventProcessorStateDto> stateDto = stateService.findByEventDefinitionId(""54e3deadbeefdeadbeefaff3"");

        assertThat(stateDto).isPresent().get().satisfies(dto -> {
            assertThat(dto.id()).isEqualTo(""54e3deadbeefdeadbeefaffe"");
            assertThat(dto.eventDefinitionId()).isEqualTo(""54e3deadbeefdeadbeefaff3"");
            assertThat(dto.minProcessedTimestamp()).isEqualTo(DateTime.parse(""2019-01-01T00:00:00.000Z""));
            assertThat(dto.maxProcessedTimestamp()).isEqualTo(DateTime.parse(""2019-01-01T01:00:00.000Z""));
        });
    }
",non-flaky,5
86104,graylog2_graylog2-server,DBEventProcessorStateServiceTest.findByEventProcessorId,"    @Test
    public void findByEventProcessorId() {
        assertThat(stateService.findByEventDefinitionId(""54e3deadbeefdeadbeefaff3"")).isPresent();

        assertThat(stateService.findByEventDefinitionId(""nope"")).isNotPresent();

        assertThatThrownBy(() -> stateService.findByEventDefinitionId(null))
                .hasMessageContaining(""eventDefinitionId"")
                .isInstanceOf(IllegalArgumentException.class);

        assertThatThrownBy(() -> stateService.findByEventDefinitionId(""""))
                .hasMessageContaining(""eventDefinitionId"")
                .isInstanceOf(IllegalArgumentException.class);
    }
",non-flaky,5
86105,graylog2_graylog2-server,DBEventProcessorStateServiceTest.findByEventProcessorsAndMaxTimestamp,"    @Test
    public void findByEventProcessorsAndMaxTimestamp() {
        assertThat(stateService.findByEventDefinitionId(""54e3deadbeefdeadbeefaff3"")).isPresent().get().satisfies(dto -> {
            final DateTime maxTs = dto.maxProcessedTimestamp();
            final String id = dto.eventDefinitionId();

            assertThat(stateService.findByEventDefinitionsAndMaxTimestamp(ImmutableSet.of(id), maxTs))
                    .hasSize(1);
            assertThat(stateService.findByEventDefinitionsAndMaxTimestamp(ImmutableSet.of(id), maxTs.minusHours(1)))
                    .hasSize(1);
            assertThat(stateService.findByEventDefinitionsAndMaxTimestamp(ImmutableSet.of(id), maxTs.plusHours(1)))
                    .hasSize(0);

            assertThatThrownBy(() -> stateService.findByEventDefinitionsAndMaxTimestamp(ImmutableSet.of(), maxTs))
                    .isInstanceOf(IllegalArgumentException.class);
            assertThatThrownBy(() -> stateService.findByEventDefinitionsAndMaxTimestamp(null, maxTs))
                    .isInstanceOf(IllegalArgumentException.class);
            assertThatThrownBy(() -> stateService.findByEventDefinitionsAndMaxTimestamp(ImmutableSet.of(id), null))
                    .isInstanceOf(IllegalArgumentException.class);

            assertThat(stateService.findByEventDefinitionsAndMaxTimestamp(ImmutableSet.of(""nope""), maxTs))
                    .hasSize(0);
            assertThat(stateService.findByEventDefinitionsAndMaxTimestamp(ImmutableSet.of(id, ""nope""), maxTs))
                    .hasSize(1);
        });
    }
",non-flaky,5
86106,graylog2_graylog2-server,DBEventProcessorStateServiceTest.setState,"    @Test
    public void setState() {
        final DateTime now = DateTime.now(DateTimeZone.UTC);

        // Before we set the state, there should be no record
        assertThat(stateService.findByEventDefinitionId(""yolo"")).isNotPresent();

        assertThat(stateService.setState(""yolo"", now.minusHours(1), now))
                .isPresent()
                .get()
                .satisfies(dto1 -> {
                    assertThat(dto1.minProcessedTimestamp()).isEqualTo(now.minusHours(1));
                    assertThat(dto1.maxProcessedTimestamp()).isEqualTo(now);
                    assertThat(dto1.eventDefinitionId()).isEqualTo(""yolo"");

                    assertThat(stateService.setState(""yolo"", now, now.plusHours(1)))
                            .isPresent()
                            .get()
                            .satisfies(dto2 -> {
                                // The second setState call should update the existing one
                                assertThat(dto2.id()).isEqualTo(dto1.id());
                                assertThat(dto2.eventDefinitionId()).isEqualTo(""yolo"");
                                assertThat(dto2.minProcessedTimestamp()).isEqualTo(dto1.minProcessedTimestamp());
                                assertThat(dto2.maxProcessedTimestamp()).isEqualTo(dto1.maxProcessedTimestamp().plusHours(1));
                            });
                });
    }
",non-flaky,5
86107,graylog2_graylog2-server,DBEventProcessorStateServiceTest.setStateKeepsMinMaxTimestamp,"    @Test
    public void setStateKeepsMinMaxTimestamp() {
        final DateTime now = DateTime.now(DateTimeZone.UTC);
        final DateTime min = now.minusHours(1);
        final DateTime max = now;

        // Before we set the state, there should be no record
        assertThat(stateService.findByEventDefinitionId(""yolo"")).isNotPresent();

        // Create state
        stateService.setState(""yolo"", min, now);

        // Check that it has been created
        assertThat(stateService.findByEventDefinitionId(""yolo""))
                .isPresent()
                .get()
                .satisfies(dto -> {
                    assertThat(dto.minProcessedTimestamp()).isEqualTo(min);
                    assertThat(dto.maxProcessedTimestamp()).isEqualTo(now);
                });

        // Overwrite state with an EARLIER max timestamp
        stateService.setState(""yolo"", min, max.minusMinutes(10));

        // Max timestamp should NOT be overwritten by older timestamp
        assertThat(stateService.findByEventDefinitionId(""yolo""))
                .isPresent()
                .get()
                .satisfies(dto -> {
                    assertThat(dto.minProcessedTimestamp()).isEqualTo(min);
                    assertThat(dto.maxProcessedTimestamp()).isEqualTo(max);
                });

        // Overwrite state with a LATER min timestamp
        stateService.setState(""yolo"", min.plusMinutes(5), max);

        // Min timestamp should NOT be overwritten by younger timestamp
        assertThat(stateService.findByEventDefinitionId(""yolo""))
                .isPresent()
                .get()
                .satisfies(dto -> {
                    assertThat(dto.minProcessedTimestamp()).isEqualTo(min);
                    assertThat(dto.maxProcessedTimestamp()).isEqualTo(max);
                });

        // Overwrite state with a NEWER max timestamp
        stateService.setState(""yolo"", min, max.plusDays(10));

        // Max timestamp is now set to the newer one
        assertThat(stateService.findByEventDefinitionId(""yolo""))
                .isPresent()
                .get()
                .satisfies(dto -> {
                    assertThat(dto.minProcessedTimestamp()).isEqualTo(min);
                    assertThat(dto.maxProcessedTimestamp()).isEqualTo(max.plusDays(10));
                });

        // Overwrite state with an OLDER min timestamp
        stateService.setState(""yolo"", min.minusDays(100), max.plusDays(10));

        // Min timestamp is now set to the older one
        assertThat(stateService.findByEventDefinitionId(""yolo""))
                .isPresent()
                .get()
                .satisfies(dto -> {
                    assertThat(dto.minProcessedTimestamp()).isEqualTo(min.minusDays(100));
                    assertThat(dto.maxProcessedTimestamp()).isEqualTo(max.plusDays(10));
                });
    }
",non-flaky,5
86108,graylog2_graylog2-server,DBEventProcessorStateServiceTest.deleteByEventProcessorId,"    @Test
    public void deleteByEventProcessorId() {
        assertThat(stateService.deleteByEventDefinitionId(""54e3deadbeefdeadbeefaff3"")).isEqualTo(1);
        assertThat(stateService.deleteByEventDefinitionId(""nope"")).isEqualTo(0);
    }
",non-flaky,5
86109,graylog2_graylog2-server,NotificationFacadeTest.exportEntity,"    @Test
    public void exportEntity() {
        final ModelId id = ModelId.of(""5d4d33753d27460ad18e0c4d"");
        final EntityDescriptor descriptor = EntityDescriptor.create(id, ModelTypes.NOTIFICATION_V1);
        final EntityDescriptorIds entityDescriptorIds = EntityDescriptorIds.of(descriptor);
        final Optional<Entity> entity = facade.exportEntity(descriptor, entityDescriptorIds);
        assertThat(entity).isPresent();
        final EntityV1 entityV1 = (EntityV1) entity.get();
        final NotificationEntity notificationEntity = objectMapper.convertValue(entityV1.data(),
                NotificationEntity.class);
        assertThat(notificationEntity.title().asString()).isEqualTo(""title"");
        assertThat(notificationEntity.description().asString()).isEqualTo(""description"");
        assertThat(notificationEntity.config().type()).isEqualTo(""email-notification-v1"");
    }
",non-flaky,5
86110,graylog2_graylog2-server,NotificationFacadeTest.createNativeEntity,"    @Test
    public void createNativeEntity() {
        final EntityV1 entityV1 = createTestEntity();
        final JobDefinitionDto jobDefinitionDto = mock(JobDefinitionDto.class);

        when(jobDefinitionService.save(any(JobDefinitionDto.class))).thenReturn(jobDefinitionDto);
        final UserImpl kmerzUser = new UserImpl(mock(PasswordAlgorithmFactory.class), new Permissions(ImmutableSet.of()), ImmutableMap.of(""username"", ""kmerz""));
        when(userService.load(""kmerz"")).thenReturn(kmerzUser);

        final NativeEntity<NotificationDto> nativeEntity = facade.createNativeEntity(
            entityV1,
            ImmutableMap.of(),
            ImmutableMap.of(),
            ""kmerz"");
        assertThat(nativeEntity).isNotNull();

        final NotificationDto notificationDto = nativeEntity.entity();
        assertThat(notificationDto.title()).isEqualTo(""title"");
        assertThat(notificationDto.description()).isEqualTo(""descriptions"");
        assertThat(notificationDto.config().type()).isEqualTo(""http-notification-v1"");
    }
",non-flaky,5
86111,graylog2_graylog2-server,NotificationFacadeTest.loadNativeEntity,"    @Test
    public void loadNativeEntity() {
        final NativeEntityDescriptor nativeEntityDescriptor = NativeEntityDescriptor.create(
                ModelId.of(""content-pack-id""),
                ModelId.of(""5d4d33753d27460ad18e0c4d""),
                ModelTypes.NOTIFICATION_V1,
                ""title"");
        final Optional<NativeEntity<NotificationDto>> optionalNativeEntity = facade.loadNativeEntity(
                nativeEntityDescriptor);
        assertThat(optionalNativeEntity).isPresent();
        final NativeEntity<NotificationDto> nativeEntity = optionalNativeEntity.get();
        assertThat(nativeEntity.entity()).isNotNull();
        final NotificationDto notificationDto = nativeEntity.entity();
        assertThat(notificationDto.id()).isEqualTo(""5d4d33753d27460ad18e0c4d"");
    }
",non-flaky,5
86112,graylog2_graylog2-server,NotificationFacadeTest.createExcerpt,"    @Test
    public void createExcerpt() {
        final Optional<NotificationDto> notificationDto = notificationService.get(
                ""5d4d33753d27460ad18e0c4d"");
        assertThat(notificationDto).isPresent();
        final EntityExcerpt excerpt = facade.createExcerpt(notificationDto.get());
        assertThat(excerpt.title()).isEqualTo(""title"");
        assertThat(excerpt.id()).isEqualTo(ModelId.of(""5d4d33753d27460ad18e0c4d""));
        assertThat(excerpt.type()).isEqualTo(ModelTypes.NOTIFICATION_V1);
    }
",non-flaky,5
86113,graylog2_graylog2-server,NotificationFacadeTest.listExcerpts,"    @Test
    public void listExcerpts() {
        final Set<EntityExcerpt> excerpts = facade.listEntityExcerpts();
        final EntityExcerpt excerpt = excerpts.iterator().next();
        assertThat(excerpt.title()).isEqualTo(""title"");
        assertThat(excerpt.id()).isEqualTo(ModelId.of(""5d4d33753d27460ad18e0c4d""));
        assertThat(excerpt.type()).isEqualTo(ModelTypes.NOTIFICATION_V1);
    }
",non-flaky,5
86114,graylog2_graylog2-server,NotificationFacadeTest.delete,"    @Test
    public void delete() {
        long countBefore = notificationService.streamAll().count();
        assertThat(countBefore).isEqualTo(1);

        final Optional<NotificationDto> notificationDto = notificationService.get(
                ""5d4d33753d27460ad18e0c4d"");
        assertThat(notificationDto).isPresent();
        facade.delete(notificationDto.get());

        long countAfter = notificationService.streamAll().count();
        assertThat(countAfter).isEqualTo(0);
    }
",non-flaky,5
86115,graylog2_graylog2-server,EventDefinitionFacadeTest.exportEntity,"    @Test
    public void exportEntity() {
        final ModelId id = ModelId.of(""5d4032513d2746703d1467f6"");

        when(jobDefinitionService.getByConfigField(eq(""event_definition_id""), eq(id.id())))
                .thenReturn(Optional.of(mock(JobDefinitionDto.class)));

        final EntityDescriptor descriptor = EntityDescriptor.create(id, ModelTypes.EVENT_DEFINITION_V1);
        final EntityDescriptorIds entityDescriptorIds = EntityDescriptorIds.of(descriptor);
        final Optional<Entity> entity = facade.exportEntity(descriptor, entityDescriptorIds);
        assertThat(entity).isPresent();
        final EntityV1 entityV1 = (EntityV1) entity.get();
        final EventDefinitionEntity eventDefinitionEntity = objectMapper.convertValue(entityV1.data(),
                EventDefinitionEntity.class);
        assertThat(eventDefinitionEntity.title().asString()).isEqualTo(""title"");
        assertThat(eventDefinitionEntity.description().asString()).isEqualTo(""description"");
        assertThat(eventDefinitionEntity.config().type()).isEqualTo(AggregationEventProcessorConfigEntity.TYPE_NAME);
        assertThat(eventDefinitionEntity.isScheduled().asBoolean(ImmutableMap.of())).isTrue();
    }
",non-flaky,5
86116,graylog2_graylog2-server,EventDefinitionFacadeTest.exportEntityWithoutScheduling,"    @Test
    public void exportEntityWithoutScheduling() {
        final ModelId id = ModelId.of(""5d4032513d2746703d1467f6"");

        when(jobDefinitionService.getByConfigField(eq(""event_definition_id""), eq(id.id())))
                .thenReturn(Optional.empty());

        final EntityDescriptor descriptor = EntityDescriptor.create(id, ModelTypes.EVENT_DEFINITION_V1);
        final EntityDescriptorIds entityDescriptorIds = EntityDescriptorIds.of(descriptor);
        final Optional<Entity> entity = facade.exportEntity(descriptor, entityDescriptorIds);
        assertThat(entity).isPresent();
        final EntityV1 entityV1 = (EntityV1) entity.get();
        final EventDefinitionEntity eventDefinitionEntity = objectMapper.convertValue(entityV1.data(),
                EventDefinitionEntity.class);
        assertThat(eventDefinitionEntity.title().asString()).isEqualTo(""title"");
        assertThat(eventDefinitionEntity.description().asString()).isEqualTo(""description"");
        assertThat(eventDefinitionEntity.config().type()).isEqualTo(AggregationEventProcessorConfigEntity.TYPE_NAME);
        assertThat(eventDefinitionEntity.isScheduled().asBoolean(ImmutableMap.of())).isFalse();
    }
",non-flaky,5
86117,graylog2_graylog2-server,EventDefinitionFacadeTest.createNativeEntity,"    @Test
    public void createNativeEntity() {
        final EntityV1 entityV1 = createTestEntity();
        final NotificationDto notificationDto = NotificationDto.builder()
                .config(HTTPEventNotificationConfig.builder().url(""https://hulud.net"").build())
                .title(""Notify me Senpai"")
                .description(""A notification for senpai"")
                .id(""dead-beef"")
                .build();
        final EntityDescriptor entityDescriptor = EntityDescriptor.create(""123123"", ModelTypes.NOTIFICATION_V1);
        final ImmutableMap<EntityDescriptor, Object> nativeEntities = ImmutableMap.of(
                entityDescriptor, notificationDto);

        final JobDefinitionDto jobDefinitionDto = mock(JobDefinitionDto.class);
        final JobTriggerDto jobTriggerDto = mock(JobTriggerDto.class);
        when(jobDefinitionDto.id()).thenReturn(""job-123123"");
        when(jobSchedulerClock.nowUTC()).thenReturn(DateTime.now(DateTimeZone.UTC));
        when(jobDefinitionService.save(any(JobDefinitionDto.class))).thenReturn(jobDefinitionDto);
        when(jobTriggerService.create(any(JobTriggerDto.class))).thenReturn(jobTriggerDto);
        final UserImpl kmerzUser = new UserImpl(mock(PasswordAlgorithmFactory.class), new Permissions(ImmutableSet.of()), ImmutableMap.of(""username"", ""kmerz""));
        when(userService.load(""kmerz"")).thenReturn(kmerzUser);


        final NativeEntity<EventDefinitionDto> nativeEntity = facade.createNativeEntity(
                entityV1,
                ImmutableMap.of(),
                nativeEntities,
                ""kmerz"");
        assertThat(nativeEntity).isNotNull();

        final EventDefinitionDto eventDefinitionDto = nativeEntity.entity();
        assertThat(eventDefinitionDto.title()).isEqualTo(""title"");
        assertThat(eventDefinitionDto.description()).isEqualTo(""description"");
        assertThat(eventDefinitionDto.config().type()).isEqualTo(""aggregation-v1"");
        // verify that ownership was registered for this entity
        verify(entityOwnershipService, times(1)).registerNewEventDefinition(nativeEntity.entity().id(), kmerzUser);
    }
",non-flaky,5
86118,graylog2_graylog2-server,EventDefinitionFacadeTest.loadNativeEntity,"    @Test
    public void loadNativeEntity() {
        final NativeEntityDescriptor nativeEntityDescriptor = NativeEntityDescriptor
                .create(ModelId.of(""content-pack-id""),
                        ModelId.of(""5d4032513d2746703d1467f6""),
                        ModelTypes.EVENT_DEFINITION_V1,
                        ""title"");
        final Optional<NativeEntity<EventDefinitionDto>> optionalNativeEntity = facade.loadNativeEntity(nativeEntityDescriptor);
        assertThat(optionalNativeEntity).isPresent();
        final NativeEntity<EventDefinitionDto> nativeEntity = optionalNativeEntity.get();
        assertThat(nativeEntity.entity()).isNotNull();
        final EventDefinitionDto eventDefinition = nativeEntity.entity();
        assertThat(eventDefinition.id()).isEqualTo(""5d4032513d2746703d1467f6"");
    }
",non-flaky,5
86119,graylog2_graylog2-server,EventDefinitionFacadeTest.createExcerpt,"    @Test
    public void createExcerpt() {
        final Optional<EventDefinitionDto> eventDefinitionDto = eventDefinitionService.get(
                ""5d4032513d2746703d1467f6"");
        assertThat(eventDefinitionDto).isPresent();
        final EntityExcerpt excerpt = facade.createExcerpt(eventDefinitionDto.get());
        assertThat(excerpt.title()).isEqualTo(""title"");
        assertThat(excerpt.id()).isEqualTo(ModelId.of(""5d4032513d2746703d1467f6""));
        assertThat(excerpt.type()).isEqualTo(ModelTypes.EVENT_DEFINITION_V1);
    }
",non-flaky,5
86120,graylog2_graylog2-server,EventDefinitionFacadeTest.listExcerpts,"    @Test
    public void listExcerpts() {
        final Set<EntityExcerpt> excerpts = facade.listEntityExcerpts();
        final EntityExcerpt excerpt = excerpts.iterator().next();
        assertThat(excerpt.title()).isEqualTo(""title"");
        assertThat(excerpt.id()).isEqualTo(ModelId.of(""5d4032513d2746703d1467f6""));
        assertThat(excerpt.type()).isEqualTo(ModelTypes.EVENT_DEFINITION_V1);
    }
",non-flaky,5
86121,graylog2_graylog2-server,EventDefinitionFacadeTest.delete,"    @Test
    public void delete() {
        long countBefore = eventDefinitionService.streamAll().count();
        assertThat(countBefore).isEqualTo(1);

        final Optional<EventDefinitionDto> eventDefinitionDto = eventDefinitionService.get(
                ""5d4032513d2746703d1467f6"");
        assertThat(eventDefinitionDto).isPresent();
        facade.delete(eventDefinitionDto.get());

        long countAfter = eventDefinitionService.streamAll().count();
        assertThat(countAfter).isEqualTo(0);
    }
",non-flaky,5
86122,graylog2_graylog2-server,EventDefinitionFacadeTest.resolveNativeEntity,"    @Test
    public void resolveNativeEntity() {
        EntityDescriptor eventDescriptor = EntityDescriptor
                .create(""5d4032513d2746703d1467f6"", ModelTypes.EVENT_DEFINITION_V1);
        EntityDescriptor streamDescriptor = EntityDescriptor
                .create(""5cdab2293d27467fbe9e8a72"", ModelTypes.STREAM_V1);
        Set<EntityDescriptor> expectedNodes = ImmutableSet.of(eventDescriptor, streamDescriptor);
        Graph<EntityDescriptor> graph = facade.resolveNativeEntity(eventDescriptor);
        assertThat(graph).isNotNull();
        Set<EntityDescriptor> nodes = graph.nodes();
        assertThat(nodes).isEqualTo(expectedNodes);
    }
",non-flaky,5
86123,graylog2_graylog2-server,EventDefinitionFacadeTest.resolveForInstallation,"    @Test
    public void resolveForInstallation() {
        EntityV1 eventEntityV1 = createTestEntity();

        final NotificationEntity notificationEntity = NotificationEntity.builder()
                .title(ValueReference.of(""title""))
                .description(ValueReference.of(""description""))
                .config(HttpEventNotificationConfigEntity.builder()
                        .url(ValueReference.of(""http://url"")).build())
                .build();
        final JsonNode data = objectMapper.convertValue(notificationEntity, JsonNode.class);
        final EntityV1 notificationV1 = EntityV1.builder()
                .data(data)
                .id(ModelId.of(""123123""))
                .type(ModelTypes.EVENT_DEFINITION_V1)
                .build();

        final EntityDescriptor entityDescriptor = EntityDescriptor.create(""123123"", ModelTypes.NOTIFICATION_V1);

        Map<String, ValueReference> parameters = ImmutableMap.of();
        Map<EntityDescriptor, Entity> entities = ImmutableMap.of(entityDescriptor, notificationV1);

        Graph<Entity> graph = facade.resolveForInstallation(eventEntityV1, parameters, entities);
        assertThat(graph).isNotNull();
        Set<Entity> expectedNodes = ImmutableSet.of(eventEntityV1, notificationV1);
        assertThat(graph.nodes()).isEqualTo(expectedNodes);
    }
",non-flaky,5
86124,graylog2_graylog2-server,LegacyAlertConditionMigratorTest.run,"    @Test
    public void run() {
        final int migratedConditions = 10;
        final int migratedCallbacks = 4;

        assertThat(migrator.run(Collections.emptySet(), Collections.emptySet())).satisfies(result -> {
            assertThat(result.completedAlertConditions()).containsOnly(
                    ""00000000-0000-0000-0000-000000000001"",
                    ""00000000-0000-0000-0000-000000000002"",
                    ""00000000-0000-0000-0000-000000000003"",
                    ""00000000-0000-0000-0000-000000000004"",
                    ""00000000-0000-0000-0000-000000000005"",
                    ""00000000-0000-0000-0000-000000000006"",
                    ""00000000-0000-0000-0000-000000000007"",
                    ""00000000-0000-0000-0000-000000000008"",
                    ""00000000-0000-0000-0000-000000000009"",
                    ""00000000-0000-0000-0000-000000000010""
            );
            assertThat(result.completedAlarmCallbacks()).containsOnly(
                    ""54e3deadbeefdeadbeef0001"",
                    ""54e3deadbeefdeadbeef0002"",
                    ""54e3deadbeefdeadbeef0003"",
                    ""54e3deadbeefdeadbeef0004""
            );
        });

        // Make sure we use the EventDefinitionHandler to create the event definitions
        verify(eventDefinitionHandler, times(migratedConditions)).create(any(EventDefinitionDto.class), any(Optional.class));

        // Make sure we use the NotificationResourceHandler to create the notifications
        verify(notificationResourceHandler, times(migratedCallbacks)).create(any(NotificationDto.class), any(Optional.class));

        assertThat(eventDefinitionService.streamAll().count()).isEqualTo(migratedConditions);
        assertThat(notificationService.streamAll().count()).isEqualTo(migratedCallbacks);

        final NotificationDto httpNotification = notificationService.streamAll()
                .filter(n -> n.title().equals(""HTTP Callback Test""))
                .findFirst()
                .orElse(null);

        assertThat(httpNotification).isNotNull();
        assertThat(httpNotification.title()).isEqualTo(""HTTP Callback Test"");
        assertThat(httpNotification.description()).isEqualTo(""Migrated legacy alarm callback"");
        assertThat(httpNotification.config()).isInstanceOf(LegacyAlarmCallbackEventNotificationConfig.class);
        assertThat((LegacyAlarmCallbackEventNotificationConfig) httpNotification.config()).satisfies(config -> {
            assertThat(config.callbackType()).isEqualTo(""org.graylog2.alarmcallbacks.HTTPAlarmCallback"");
            assertThat(config.configuration().get(""url"")).isEqualTo(""http://localhost:11000/"");
        });

        final NotificationDto httpNotificationWithoutTitle = notificationService.streamAll()
                .filter(n -> n.title().equals(""Untitled""))
                .findFirst()
                .orElse(null);

        assertThat(httpNotificationWithoutTitle).isNotNull();
        assertThat(httpNotificationWithoutTitle.title()).isEqualTo(""Untitled"");
        assertThat(httpNotificationWithoutTitle.description()).isEqualTo(""Migrated legacy alarm callback"");
        assertThat(httpNotificationWithoutTitle.config()).isInstanceOf(LegacyAlarmCallbackEventNotificationConfig.class);
        assertThat((LegacyAlarmCallbackEventNotificationConfig) httpNotificationWithoutTitle.config()).satisfies(config -> {
            assertThat(config.callbackType()).isEqualTo(""org.graylog2.alarmcallbacks.HTTPAlarmCallback"");
            assertThat(config.configuration().get(""url"")).isEqualTo(""http://localhost:11000/"");
        });

        final NotificationDto emailNotification = notificationService.streamAll()
                .filter(n -> n.title().equals(""Email Callback Test""))
                .findFirst()
                .orElse(null);

        assertThat(emailNotification).isNotNull();
        assertThat(emailNotification.title()).isEqualTo(""Email Callback Test"");
        assertThat(emailNotification.description()).isEqualTo(""Migrated legacy alarm callback"");
        assertThat(emailNotification.config()).isInstanceOf(LegacyAlarmCallbackEventNotificationConfig.class);
        assertThat((LegacyAlarmCallbackEventNotificationConfig) emailNotification.config()).satisfies(config -> {
            assertThat(config.callbackType()).isEqualTo(""org.graylog2.alarmcallbacks.EmailAlarmCallback"");
            assertThat(config.configuration().get(""sender"")).isEqualTo(""graylog@example.org"");
            assertThat(config.configuration().get(""subject"")).isEqualTo(""Graylog alert for stream: ${stream.title}: ${check_result.resultDescription}"");
            assertThat((String) config.configuration().get(""body"")).contains(""Alert Description: ${check_result.resultDescription}\nDate: "");
            assertThat(config.configuration().get(""user_receivers"")).isEqualTo(Collections.emptyList());
            assertThat(config.configuration().get(""email_receivers"")).isEqualTo(Collections.singletonList(""jane@example.org""));
        });

        final NotificationDto slackNotification = notificationService.streamAll()
                .filter(n -> n.title().equals(""Slack Callback Test""))
                .findFirst()
                .orElse(null);

        assertThat(slackNotification).isNotNull();
        assertThat(slackNotification.title()).isEqualTo(""Slack Callback Test"");
        assertThat(slackNotification.description()).isEqualTo(""Migrated legacy alarm callback"");
        assertThat(slackNotification.config()).isInstanceOf(LegacyAlarmCallbackEventNotificationConfig.class);
        assertThat((LegacyAlarmCallbackEventNotificationConfig) slackNotification.config()).satisfies(config -> {
            assertThat(config.callbackType()).isEqualTo(""org.graylog2.plugins.slack.callback.SlackAlarmCallback"");
            assertThat(config.configuration().get(""icon_url"")).isEqualTo("""");
            assertThat(config.configuration().get(""graylog2_url"")).isEqualTo("""");
            assertThat(config.configuration().get(""link_names"")).isEqualTo(true);
            assertThat(config.configuration().get(""webhook_url"")).isEqualTo(""http://example.com/slack-hook"");
            assertThat(config.configuration().get(""color"")).isEqualTo(""#FF0000"");
            assertThat(config.configuration().get(""icon_emoji"")).isEqualTo("""");
            assertThat(config.configuration().get(""user_name"")).isEqualTo(""Graylog"");
            assertThat(config.configuration().get(""backlog_items"")).isEqualTo(5);
            assertThat(config.configuration().get(""custom_fields"")).isEqualTo("""");
            assertThat(config.configuration().get(""proxy_address"")).isEqualTo("""");
            assertThat(config.configuration().get(""channel"")).isEqualTo(""#channel"");
            assertThat(config.configuration().get(""notify_channel"")).isEqualTo(false);
            assertThat(config.configuration().get(""add_attachment"")).isEqualTo(true);
            assertThat(config.configuration().get(""short_mode"")).isEqualTo(false);
        });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Message Count - MORE"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(120000);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(10);

                    assertThat(eventDefinition.notifications()).hasSize(2);
                    assertThat(eventDefinition.notifications().stream().map(EventNotificationHandler.Config::notificationId).collect(Collectors.toList()))
                            .containsOnly(httpNotification.id(), httpNotificationWithoutTitle.id());

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0001"");
                        assertThat(config.query()).isEqualTo(""hello:world"");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(10 * 60 * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.COUNT);
                        assertThat(config.series().get(0).field()).isNotPresent();

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Greater.class);

                                final Expr.Greater greater = (Expr.Greater) expression;

                                assertThat(greater.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(greater.right()).isEqualTo(Expr.NumberValue.create(1));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Message Count - LESS"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(0);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(0);

                    assertThat(eventDefinition.notifications()).hasSize(2);
                    assertThat(eventDefinition.notifications().stream().map(EventNotificationHandler.Config::notificationId).collect(Collectors.toList()))
                            .containsOnly(httpNotification.id(), httpNotificationWithoutTitle.id());

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0001"");
                        assertThat(config.query()).isEmpty();
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(4 * 60 * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.COUNT);
                        assertThat(config.series().get(0).field()).isNotPresent();

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Lesser.class);

                                final Expr.Lesser lesser = (Expr.Lesser) expression;

                                assertThat(lesser.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(lesser.right()).isEqualTo(Expr.NumberValue.create(42));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Field Value - HIGHER - MEAN"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(60000);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(15);
                    assertThat(eventDefinition.notifications()).isEmpty();

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0002"");
                        assertThat(config.query()).isEqualTo(""*"");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(5 * 60 * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.AVG);
                        assertThat(config.series().get(0).field()).get().isEqualTo(""test_field_1"");

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Greater.class);

                                final Expr.Greater greater = (Expr.Greater) expression;

                                assertThat(greater.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(greater.right()).isEqualTo(Expr.NumberValue.create(23));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Field Value - LOWER - SUM"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(60000);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(15);
                    assertThat(eventDefinition.notifications()).isEmpty();

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0002"");
                        assertThat(config.query()).isEqualTo(""*"");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(5 * 60 * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.SUM);
                        assertThat(config.series().get(0).field()).get().isEqualTo(""test_field_1"");

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Lesser.class);

                                final Expr.Lesser lesser = (Expr.Lesser) expression;

                                assertThat(lesser.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(lesser.right()).isEqualTo(Expr.NumberValue.create(23));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Field Value - LOWER - MIN"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(60000);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(15);
                    assertThat(eventDefinition.notifications()).isEmpty();

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0002"");
                        assertThat(config.query()).isEqualTo(""*"");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(5 * 60 * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.MIN);
                        assertThat(config.series().get(0).field()).get().isEqualTo(""test_field_1"");

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Lesser.class);

                                final Expr.Lesser lesser = (Expr.Lesser) expression;

                                assertThat(lesser.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(lesser.right()).isEqualTo(Expr.NumberValue.create(23));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Field Value - LOWER - MAX"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(60000);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(15);
                    assertThat(eventDefinition.notifications()).isEmpty();

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0002"");
                        assertThat(config.query()).isEqualTo(""*"");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(5 * 60 * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.MAX);
                        assertThat(config.series().get(0).field()).get().isEqualTo(""test_field_1"");

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Lesser.class);

                                final Expr.Lesser lesser = (Expr.Lesser) expression;

                                assertThat(lesser.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(lesser.right()).isEqualTo(Expr.NumberValue.create(23));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Field Value - LOWER - STDDEV"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(60000);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(15);
                    assertThat(eventDefinition.notifications()).isEmpty();

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0002"");
                        assertThat(config.query()).isEqualTo(""*"");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(5 * 60 * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.STDDEV);
                        assertThat(config.series().get(0).field()).get().isEqualTo(""test_field_1"");

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Greater.class);

                                final Expr.Greater greater = (Expr.Greater) expression;

                                assertThat(greater.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(greater.right()).isEqualTo(Expr.NumberValue.create(23));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Field Content - WITHOUT QUERY"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(120000);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(100);

                    assertThat(eventDefinition.notifications()).hasSize(2);
                    assertThat(eventDefinition.notifications().stream().map(EventNotificationHandler.Config::notificationId).collect(Collectors.toSet()))
                            .containsOnly(emailNotification.id(), slackNotification.id());

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0003"");
                        assertThat(config.query()).isEqualTo(""test_field_2:\""hello\"""");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(CHECK_INTERVAL * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.COUNT);
                        assertThat(config.series().get(0).field()).isNotPresent();

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Greater.class);

                                final Expr.Greater greater = (Expr.Greater) expression;

                                assertThat(greater.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(greater.right()).isEqualTo(Expr.NumberValue.create(0));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Field Content - WITH QUERY"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(0);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(0);

                    assertThat(eventDefinition.notifications()).hasSize(2);
                    assertThat(eventDefinition.notifications().stream().map(EventNotificationHandler.Config::notificationId).collect(Collectors.toSet()))
                            .containsOnly(emailNotification.id(), slackNotification.id());

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0003"");
                        assertThat(config.query()).isEqualTo(""test_field_3:\""foo\"" AND foo:bar"");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(CHECK_INTERVAL * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.COUNT);
                        assertThat(config.series().get(0).field()).isNotPresent();

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Greater.class);

                                final Expr.Greater greater = (Expr.Greater) expression;

                                assertThat(greater.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(greater.right()).isEqualTo(Expr.NumberValue.create(0));
                            });
                        });
                    });
                });

        assertThat(eventDefinitionService.streamAll().filter(ed -> ed.title().equals(""Untitled"")).findFirst())
                .get()
                .satisfies(eventDefinition -> {
                    assertThat(eventDefinition.alert()).isTrue();
                    assertThat(eventDefinition.priority()).isEqualTo(2);
                    assertThat(eventDefinition.keySpec()).isEmpty();
                    assertThat(eventDefinition.notificationSettings().gracePeriodMs()).isEqualTo(0);
                    assertThat(eventDefinition.notificationSettings().backlogSize()).isEqualTo(0);

                    assertThat(eventDefinition.notifications()).hasSize(2);
                    assertThat(eventDefinition.notifications().stream().map(EventNotificationHandler.Config::notificationId).collect(Collectors.toSet()))
                            .containsOnly(emailNotification.id(), slackNotification.id());

                    assertThat((AggregationEventProcessorConfig) eventDefinition.config()).satisfies(config -> {
                        assertThat(config.streams()).containsExactly(""54e3deadbeefdeadbeef0003"");
                        assertThat(config.query()).isEqualTo(""test_field_3:\""foo\"" AND foo:bar"");
                        assertThat(config.groupBy()).isEmpty();
                        assertThat(config.searchWithinMs()).isEqualTo(CHECK_INTERVAL * 1000);
                        assertThat(config.executeEveryMs()).isEqualTo(CHECK_INTERVAL * 1000);

                        assertThat(config.series()).hasSize(1);
                        assertThat(config.series().get(0).id()).isNotBlank();
                        assertThat(config.series().get(0).function()).isEqualTo(AggregationFunction.COUNT);
                        assertThat(config.series().get(0).field()).isNotPresent();

                        assertThat(config.conditions()).get().satisfies(conditions -> {
                            assertThat(conditions.expression()).get().satisfies(expression -> {
                                assertThat(expression).isInstanceOf(Expr.Greater.class);

                                final Expr.Greater greater = (Expr.Greater) expression;

                                assertThat(greater.left()).isEqualTo(Expr.NumberReference.create(config.series().get(0).id()));
                                assertThat(greater.right()).isEqualTo(Expr.NumberValue.create(0));
                            });
                        });
                    });
                });
    }
",non-flaky,5
86125,graylog2_graylog2-server,LegacyAlertConditionMigratorTest.runWithMigrationStatus,"    @Test
    public void runWithMigrationStatus() {
        final int migratedConditions = 9; // Only 8 because we pass one migrated condition in
        final int migratedCallbacks = 3;  // Only 2 because we pass one migrated callback in

        assertThat(migrator.run(Collections.singleton(""00000000-0000-0000-0000-000000000002""), Collections.singleton(""54e3deadbeefdeadbeef0001""))).satisfies(result -> {
            assertThat(result.completedAlertConditions()).containsOnly(
                    ""00000000-0000-0000-0000-000000000001"",
                    ""00000000-0000-0000-0000-000000000002"",
                    ""00000000-0000-0000-0000-000000000003"",
                    ""00000000-0000-0000-0000-000000000004"",
                    ""00000000-0000-0000-0000-000000000005"",
                    ""00000000-0000-0000-0000-000000000006"",
                    ""00000000-0000-0000-0000-000000000007"",
                    ""00000000-0000-0000-0000-000000000008"",
                    ""00000000-0000-0000-0000-000000000009"",
                    ""00000000-0000-0000-0000-000000000010""
            );
            assertThat(result.completedAlarmCallbacks()).containsOnly(
                    ""54e3deadbeefdeadbeef0001"",
                    ""54e3deadbeefdeadbeef0002"",
                    ""54e3deadbeefdeadbeef0003"",
                    ""54e3deadbeefdeadbeef0004""
            );
        });

        // Make sure we use the EventDefinitionHandler to create the event definitions
        verify(eventDefinitionHandler, times(migratedConditions)).create(any(EventDefinitionDto.class), any(Optional.class));

        // Make sure we use the NotificationResourceHandler to create the notifications
        verify(notificationResourceHandler, times(migratedCallbacks)).create(any(NotificationDto.class), any(Optional.class));

        assertThat(eventDefinitionService.streamAll().count()).isEqualTo(migratedConditions);
        assertThat(notificationService.streamAll().count()).isEqualTo(migratedCallbacks);
    }
",non-flaky,5
86126,graylog2_graylog2-server,LookupTableFieldValueProviderTest.testWithMessageContext,"    @Test
    public void testWithMessageContext() {
        final String fieldValueString = ""world"";
        final String expectedLookupValue = ""lookup-world"";

        final TestEvent event = new TestEvent();
        final Message message = newMessage(ImmutableMap.of(""hello"", fieldValueString));
        final EventWithContext eventWithContext = EventWithContext.create(event, message);

        final LookupTableFieldValueProvider.Config config = newConfig(""test"", ""hello"");

        setupMocks(""test"");
        when(lookupTableFunction.lookup(""world"")).thenReturn(LookupResult.single(""lookup-"" + message.getField(""hello"")));

        final FieldValue fieldValue = newProvider(config).doGet(""test"", eventWithContext);

        assertThat(fieldValue.value()).isEqualTo(expectedLookupValue);
    }
",non-flaky,5
86127,graylog2_graylog2-server,LookupTableFieldValueProviderTest.testWithEventContext,"    @Test
    public void testWithEventContext() {
        final String fieldValueString = ""event"";
        final String expectedLookupValue = ""lookup-event"";

        final TestEvent event = new TestEvent();
        final TestEvent eventContext = new TestEvent();

        eventContext.setField(""hello"", FieldValue.string(fieldValueString));

        final EventWithContext eventWithContext = EventWithContext.create(event, eventContext);

        final LookupTableFieldValueProvider.Config config = newConfig(""test"", ""hello"");

        setupMocks(""test"");
        when(lookupTableFunction.lookup(fieldValueString)).thenReturn(LookupResult.single(""lookup-"" + eventContext.getField(""hello"").value()));

        final FieldValue fieldValue = newProvider(config).doGet(""test"", eventWithContext);

        assertThat(fieldValue.value()).isEqualTo(expectedLookupValue);
    }
",non-flaky,5
86128,graylog2_graylog2-server,LookupTableFieldValueProviderTest.testWithMissingLookupTable,"    @Test
    public void testWithMissingLookupTable() {
        final TestEvent event = new TestEvent();
        final EventWithContext eventWithContext = EventWithContext.create(event, newMessage(ImmutableMap.of(""hello"", ""world"")));

        final LookupTableFieldValueProvider.Config config = newConfig(""test-doesntexist"", ""hello"");

        setupMocks(""test"");
        when(lookupTableFunction.lookup(""world"")).thenReturn(LookupResult.single(""lookup-world""));

        assertThatThrownBy(() -> newProvider(config).doGet(""test"", eventWithContext))
                .hasMessageContaining(""test-doesntexist"")
                .isInstanceOf(IllegalArgumentException.class);
    }
",non-flaky,5
86129,graylog2_graylog2-server,TemplateFieldValueProviderTest.templateWithMessageContext,"    @Test
    public void templateWithMessageContext() {
        final TestEvent event = new TestEvent();
        final EventWithContext eventWithContext = EventWithContext.create(event, newMessage(ImmutableMap.of(""hello"", ""world"")));

        final FieldValue fieldValue = newTemplate(""hello: ${source.hello}"").doGet(""test"", eventWithContext);

        assertThat(fieldValue.value()).isEqualTo(""hello: world"");
    }
",non-flaky,5
86130,graylog2_graylog2-server,TemplateFieldValueProviderTest.templateWithEventContext,"    @Test
    public void templateWithEventContext() {
        final TestEvent event = new TestEvent();
        final TestEvent eventContext = new TestEvent();

        eventContext.setField(""hello"", FieldValue.string(""event""));

        final EventWithContext eventWithContext = EventWithContext.create(event, eventContext);

        final FieldValue fieldValue = newTemplate(""hello: ${source.hello}"").doGet(""test"", eventWithContext);

        assertThat(fieldValue.value()).isEqualTo(""hello: event"");
    }
",non-flaky,5
86131,graylog2_graylog2-server,TemplateFieldValueProviderTest.templateWithError,"    @Test
    public void templateWithError() {
        final TestEvent event = new TestEvent();
        final EventWithContext eventWithContext = EventWithContext.create(event, newMessage(ImmutableMap.of(""hello"", ""world"")));

        final FieldValue fieldValue = newTemplate(""hello: ${source.yolo}"", true).doGet(""test"", eventWithContext);

        assertThat(fieldValue.dataType()).isEqualTo(FieldValueType.ERROR);
    }
",non-flaky,5
86132,graylog2_graylog2-server,TemplateFieldValueProviderTest.templateCalculation,"    @Test
    @Ignore(""template engine doesn't support expressions"")
    public void templateCalculation() {
        final TestEvent event = new TestEvent();
        final EventWithContext eventWithContext = EventWithContext.create(event, newMessage(ImmutableMap.of(""bytes"", 1024)));

        final FieldValue fieldValue = newTemplate(""${source.bytes / 1024}"").doGet(""test"", eventWithContext);

        assertThat(fieldValue.value()).isEqualTo(""1"");
    }
",non-flaky,5
86133,graylog2_graylog2-server,TemplateFieldValueProviderTest.templateNumberFormatting,"    @Test
    public void templateNumberFormatting() {
        final TestEvent event = new TestEvent();
        final EventWithContext eventWithContext = EventWithContext.create(event, newMessage(ImmutableMap.of(""count"", 10241234, ""avg"", 1024.42)));

        final FieldValue fieldValue = newTemplate(""count: ${source.count} avg: ${source.avg}"").doGet(""test"", eventWithContext);

        assertThat(fieldValue.value()).isEqualTo(""count: 10241234 avg: 1024.42"");
    }
",non-flaky,5
86134,graylog2_graylog2-server,TemplateFieldValueProviderTest.templateDateFormatting,"    @Test
    public void templateDateFormatting() {
        final TestEvent event = new TestEvent();
        final EventWithContext eventWithContext = EventWithContext.create(event, newMessage(ImmutableMap.of(""timestamp"", DateTime.parse(""2019-07-02T12:21:00.123Z""))));

        final FieldValue fieldValue = newTemplate(""timestamp: ${source.timestamp}"").doGet(""test"", eventWithContext);

        assertThat(fieldValue.value()).isEqualTo(""timestamp: 2019-07-02T12:21:00.123Z"");
    }
",non-flaky,5
86135,graylog2_graylog2-server,TemplateFieldValueProviderTest.templateBooleanFormatting,"    @Test
    public void templateBooleanFormatting() {
        final TestEvent event = new TestEvent();
        final EventWithContext eventWithContext = EventWithContext.create(event, newMessage(ImmutableMap.of(""success"", true)));

        final FieldValue fieldValue = newTemplate(""success: ${source.success}"").doGet(""test"", eventWithContext);

        assertThat(fieldValue.value()).isEqualTo(""success: true"");
    }
",non-flaky,5
86136,graylog2_graylog2-server,ESMongoDateTimeDeserializerTest.deserializeDateTime,"    @Test
    public void deserializeDateTime() throws Exception {
        final String json = ""{\""date_time\"":\""2016-12-13 14:00:00.000\""}"";
        final DTO value = objectMapper.readValue(json, DTO.class);
        assertThat(value.dateTime).isEqualTo(new DateTime(2016, 12, 13, 14, 0, DateTimeZone.UTC));
    }
",non-flaky,5
86137,graylog2_graylog2-server,ESMongoDateTimeDeserializerTest.deserializeIsoDateTime,"    @Test
    public void deserializeIsoDateTime() throws Exception {
        final String json = ""{\""date_time\"":\""2016-12-13T14:00:00.000\""}"";
        final DTO value = objectMapper.readValue(json, DTO.class);
        assertThat(value.dateTime).isEqualTo(new DateTime(2016, 12, 13, 14, 0, DateTimeZone.UTC));
    }
",non-flaky,5
59,apache_beam,testBacklogLimiter,"@Test
public void testBacklogLimiter() {
    long duration = runWithRate(2 * RateLimiting.DEFAULT_MAX_PARALLELISM,-1.0 , new DelayFn<Integer>());
    Assert.assertThat(duration,greaterThan(2 * DelayFn.DELAY_MS));
}",time,2
131,apache_beam,testClientConnecting,"@Test
public void testClientConnecting() throws Exception {
    PipelineOptions options = PipelineOptionsFactory.create();
    Endpoints.ApiServiceDescriptor descriptor = findOpenPort();
    BeamFnControlService service =
    new BeamFnControlService(
    descriptor,
    ServerStreamObserverFactory.fromOptions(options)::from,
    GrpcContextHeaderAccessorProvider.getHeaderAccessor());
    Server server =
    ServerFactory.fromOptions(options).create(descriptor, ImmutableList.of(service));
    String url = service.getApiServiceDescriptor().getUrl();
    BeamFnControlGrpc.BeamFnControlStub clientStub =
    BeamFnControlGrpc.newStub(ManagedChannelBuilder.forTarget(url).usePlaintext(true).build());
    clientStub.control(requestObserver);
    try (FnApiControlClient client = service.get()) {
        assertNotNull(client);
    }
    server.shutdown();
    server.awaitTermination(1, TimeUnit.SECONDS);
    server.shutdownNow();
    verify(requestObserver).onCompleted();
    verifyNoMoreInteractions(requestObserver);
}",async wait,0
175,apache_beam,testRateLimitingMax,"@Test
public void testRateLimitingMax() {
    int n = 10;
    double rate = 10.0;
    long duration = runWithRate(n, rate, new IdentityFn<Integer>());
    long perElementPause = (long) (1000L / rate);
    long minDuration = (n - 1) * perElementPause;
    Assert.assertThat(duration, greaterThan(minDuration));
}",time,2
78232,apache_beam,SimplePushbackSideInputDoFnRunnerTest.startFinishBundleDelegates,"  @Test
  public void startFinishBundleDelegates() {
    PushbackSideInputDoFnRunner runner = createRunner(ImmutableList.of(singletonView));

    assertThat(underlying.started, is(true));
    assertThat(underlying.finished, is(false));
    runner.finishBundle();
    assertThat(underlying.finished, is(true));
  }
",non-flaky,5
78233,apache_beam,SimplePushbackSideInputDoFnRunnerTest.processElementSideInputNotReady,"  @Test
  public void processElementSideInputNotReady() {
    when(reader.isReady(Mockito.eq(singletonView), Mockito.any(BoundedWindow.class)))
        .thenReturn(false);

    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =
        createRunner(ImmutableList.of(singletonView));

    WindowedValue<Integer> oneWindow =
        WindowedValue.of(
            2,
            new Instant(-2),
            new IntervalWindow(new Instant(-500L), new Instant(0L)),
            PaneInfo.ON_TIME_AND_ONLY_FIRING);
    Iterable<WindowedValue<Integer>> oneWindowPushback =
        runner.processElementInReadyWindows(oneWindow);
    assertThat(oneWindowPushback, containsInAnyOrder(oneWindow));
    assertThat(underlying.inputElems, emptyIterable());
  }
",non-flaky,5
78234,apache_beam,SimplePushbackSideInputDoFnRunnerTest.processElementSideInputNotReadyMultipleWindows,"  @Test
  public void processElementSideInputNotReadyMultipleWindows() {
    when(reader.isReady(Mockito.eq(singletonView), Mockito.any(BoundedWindow.class)))
        .thenReturn(false);

    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =
        createRunner(ImmutableList.of(singletonView));

    WindowedValue<Integer> multiWindow =
        WindowedValue.of(
            2,
            new Instant(-2),
            ImmutableList.of(
                new IntervalWindow(new Instant(-500L), new Instant(0L)),
                new IntervalWindow(BoundedWindow.TIMESTAMP_MIN_VALUE, new Instant(250L)),
                GlobalWindow.INSTANCE),
            PaneInfo.ON_TIME_AND_ONLY_FIRING);
    Iterable<WindowedValue<Integer>> multiWindowPushback =
        runner.processElementInReadyWindows(multiWindow);
    assertThat(multiWindowPushback, equalTo(multiWindow.explodeWindows()));
    assertThat(underlying.inputElems, emptyIterable());
  }
",non-flaky,5
78235,apache_beam,SimplePushbackSideInputDoFnRunnerTest.processElementSideInputNotReadySomeWindows,"  @Test
  public void processElementSideInputNotReadySomeWindows() {
    when(reader.isReady(Mockito.eq(singletonView), Mockito.eq(GlobalWindow.INSTANCE)))
        .thenReturn(false);
    when(reader.isReady(
            Mockito.eq(singletonView),
            org.mockito.AdditionalMatchers.not(Mockito.eq(GlobalWindow.INSTANCE))))
        .thenReturn(true);

    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =
        createRunner(ImmutableList.of(singletonView));

    IntervalWindow littleWindow = new IntervalWindow(new Instant(-500L), new Instant(0L));
    IntervalWindow bigWindow =
        new IntervalWindow(BoundedWindow.TIMESTAMP_MIN_VALUE, new Instant(250L));
    WindowedValue<Integer> multiWindow =
        WindowedValue.of(
            2,
            new Instant(-2),
            ImmutableList.of(littleWindow, bigWindow, GlobalWindow.INSTANCE),
            PaneInfo.NO_FIRING);
    Iterable<WindowedValue<Integer>> multiWindowPushback =
        runner.processElementInReadyWindows(multiWindow);
    assertThat(
        multiWindowPushback,
        containsInAnyOrder(WindowedValue.timestampedValueInGlobalWindow(2, new Instant(-2L))));
    assertThat(
        underlying.inputElems,
        containsInAnyOrder(
            WindowedValue.of(
                2, new Instant(-2), ImmutableList.of(littleWindow), PaneInfo.NO_FIRING),
            WindowedValue.of(2, new Instant(-2), ImmutableList.of(bigWindow), PaneInfo.NO_FIRING)));
  }
",non-flaky,5
78236,apache_beam,SimplePushbackSideInputDoFnRunnerTest.processElementSideInputReadyAllWindows,"  @Test
  public void processElementSideInputReadyAllWindows() {
    when(reader.isReady(Mockito.eq(singletonView), Mockito.any(BoundedWindow.class)))
        .thenReturn(true);

    ImmutableList<PCollectionView<?>> views = ImmutableList.of(singletonView);
    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner = createRunner(views);

    WindowedValue<Integer> multiWindow =
        WindowedValue.of(
            2,
            new Instant(-2),
            ImmutableList.of(
                new IntervalWindow(new Instant(-500L), new Instant(0L)),
                new IntervalWindow(BoundedWindow.TIMESTAMP_MIN_VALUE, new Instant(250L)),
                GlobalWindow.INSTANCE),
            PaneInfo.ON_TIME_AND_ONLY_FIRING);
    Iterable<WindowedValue<Integer>> multiWindowPushback =
        runner.processElementInReadyWindows(multiWindow);
    assertThat(multiWindowPushback, emptyIterable());
    assertThat(
        underlying.inputElems,
        containsInAnyOrder(ImmutableList.copyOf(multiWindow.explodeWindows()).toArray()));
  }
",non-flaky,5
78237,apache_beam,SimplePushbackSideInputDoFnRunnerTest.processElementNoSideInputs,"  @Test
  public void processElementNoSideInputs() {
    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner = createRunner(ImmutableList.of());

    WindowedValue<Integer> multiWindow =
        WindowedValue.of(
            2,
            new Instant(-2),
            ImmutableList.of(
                new IntervalWindow(new Instant(-500L), new Instant(0L)),
                new IntervalWindow(BoundedWindow.TIMESTAMP_MIN_VALUE, new Instant(250L)),
                GlobalWindow.INSTANCE),
            PaneInfo.ON_TIME_AND_ONLY_FIRING);
    Iterable<WindowedValue<Integer>> multiWindowPushback =
        runner.processElementInReadyWindows(multiWindow);
    assertThat(multiWindowPushback, emptyIterable());
    // Should preserve the compressed representation when there's no side inputs.
    assertThat(underlying.inputElems, containsInAnyOrder(multiWindow));
  }
",non-flaky,5
78238,apache_beam,SimplePushbackSideInputDoFnRunnerTest.testOnTimerCalled,"  @Test
  public void testOnTimerCalled() {
    PushbackSideInputDoFnRunner<Integer, Integer> runner = createRunner(ImmutableList.of());

    String timerId = ""fooTimer"";
    IntervalWindow window = new IntervalWindow(new Instant(4), new Instant(16));
    Instant timestamp = new Instant(72);

    // Mocking is not easily compatible with annotation analysis, so we manually record
    // the method call.
    runner.onTimer(timerId, window, new Instant(timestamp), TimeDomain.EVENT_TIME);

    assertThat(
        underlying.firedTimers,
        contains(
            TimerData.of(
                timerId,
                StateNamespaces.window(IntervalWindow.getCoder(), window),
                timestamp,
                TimeDomain.EVENT_TIME)));
  }
",non-flaky,5
78239,apache_beam,StateInternalsTest.testValue,"  @Test
  public void testValue() throws Exception {
    ValueState<String> value = underTest.state(NAMESPACE_1, STRING_VALUE_ADDR);

    // State instances are cached, but depend on the namespace.
    assertThat(underTest.state(NAMESPACE_1, STRING_VALUE_ADDR), equalTo(value));
    assertThat(underTest.state(NAMESPACE_2, STRING_VALUE_ADDR), not(equalTo(value)));

    assertThat(value.read(), Matchers.nullValue());
    value.write(""hello"");
    assertThat(value.read(), equalTo(""hello""));
    value.write(""world"");
    assertThat(value.read(), equalTo(""world""));

    value.clear();
    assertThat(value.read(), Matchers.nullValue());
    assertThat(underTest.state(NAMESPACE_1, STRING_VALUE_ADDR), equalTo(value));
  }
",non-flaky,5
78240,apache_beam,StateInternalsTest.testBag,"  @Test
  public void testBag() throws Exception {
    BagState<String> value = underTest.state(NAMESPACE_1, STRING_BAG_ADDR);

    // State instances are cached, but depend on the namespace.
    assertThat(value, equalTo(underTest.state(NAMESPACE_1, STRING_BAG_ADDR)));
    assertThat(value, not(equalTo(underTest.state(NAMESPACE_2, STRING_BAG_ADDR))));

    assertThat(value.read(), Matchers.emptyIterable());
    value.add(""hello"");
    assertThat(value.read(), containsInAnyOrder(""hello""));

    value.add(""world"");
    assertThat(value.read(), containsInAnyOrder(""hello"", ""world""));

    value.clear();
    assertThat(value.read(), Matchers.emptyIterable());
    assertThat(underTest.state(NAMESPACE_1, STRING_BAG_ADDR), equalTo(value));
  }
",non-flaky,5
78241,apache_beam,StateInternalsTest.testBagIsEmpty,"  @Test
  public void testBagIsEmpty() throws Exception {
    BagState<String> value = underTest.state(NAMESPACE_1, STRING_BAG_ADDR);

    assertThat(value.isEmpty().read(), Matchers.is(true));
    ReadableState<Boolean> readFuture = value.isEmpty();
    value.add(""hello"");
    assertThat(readFuture.read(), Matchers.is(false));

    value.clear();
    assertThat(readFuture.read(), Matchers.is(true));
  }
",non-flaky,5
78242,apache_beam,StateInternalsTest.testMergeBagIntoSource,"  @Test
  public void testMergeBagIntoSource() throws Exception {
    BagState<String> bag1 = underTest.state(NAMESPACE_1, STRING_BAG_ADDR);
    BagState<String> bag2 = underTest.state(NAMESPACE_2, STRING_BAG_ADDR);

    bag1.add(""Hello"");
    bag2.add(""World"");
    bag1.add(""!"");

    StateMerging.mergeBags(Arrays.asList(bag1, bag2), bag1);

    // Reading the merged bag gets both the contents
    assertThat(bag1.read(), containsInAnyOrder(""Hello"", ""World"", ""!""));
    assertThat(bag2.read(), Matchers.emptyIterable());
  }
",non-flaky,5
78243,apache_beam,StateInternalsTest.testMergeBagIntoNewNamespace,"  @Test
  public void testMergeBagIntoNewNamespace() throws Exception {
    BagState<String> bag1 = underTest.state(NAMESPACE_1, STRING_BAG_ADDR);
    BagState<String> bag2 = underTest.state(NAMESPACE_2, STRING_BAG_ADDR);
    BagState<String> bag3 = underTest.state(NAMESPACE_3, STRING_BAG_ADDR);

    bag1.add(""Hello"");
    bag2.add(""World"");
    bag1.add(""!"");

    StateMerging.mergeBags(Arrays.asList(bag1, bag2, bag3), bag3);

    // Reading the merged bag gets both the contents
    assertThat(bag3.read(), containsInAnyOrder(""Hello"", ""World"", ""!""));
    assertThat(bag1.read(), Matchers.emptyIterable());
    assertThat(bag2.read(), Matchers.emptyIterable());
  }
",non-flaky,5
78244,apache_beam,StateInternalsTest.testSet,"  @Test
  public void testSet() throws Exception {

    SetState<String> value = underTest.state(NAMESPACE_1, STRING_SET_ADDR);

    // State instances are cached, but depend on the namespace.
    assertThat(value, equalTo(underTest.state(NAMESPACE_1, STRING_SET_ADDR)));
    assertThat(value, not(equalTo(underTest.state(NAMESPACE_2, STRING_SET_ADDR))));

    // empty
    assertThat(value.read(), Matchers.emptyIterable());
    assertFalse(value.contains(""A"").read());

    // add
    value.add(""A"");
    value.add(""B"");
    value.add(""A"");
    assertFalse(value.addIfAbsent(""B"").read());
    assertThat(value.read(), containsInAnyOrder(""A"", ""B""));

    // remove
    value.remove(""A"");
    assertThat(value.read(), containsInAnyOrder(""B""));
    value.remove(""C"");
    assertThat(value.read(), containsInAnyOrder(""B""));

    // contains
    assertFalse(value.contains(""A"").read());
    assertTrue(value.contains(""B"").read());
    value.add(""C"");
    value.add(""D"");

    // readLater
    assertThat(value.readLater().read(), containsInAnyOrder(""B"", ""C"", ""D""));
    SetState<String> later = value.readLater();
    assertThat(later.read(), hasItems(""C"", ""D""));
    assertFalse(later.contains(""A"").read());

    // clear
    value.clear();
    assertThat(value.read(), Matchers.emptyIterable());
    assertThat(underTest.state(NAMESPACE_1, STRING_SET_ADDR), equalTo(value));
  }
",non-flaky,5
78245,apache_beam,StateInternalsTest.testSetIsEmpty,"  @Test
  public void testSetIsEmpty() throws Exception {

    SetState<String> value = underTest.state(NAMESPACE_1, STRING_SET_ADDR);

    assertThat(value.isEmpty().read(), Matchers.is(true));
    ReadableState<Boolean> readFuture = value.isEmpty();
    value.add(""hello"");
    assertThat(readFuture.read(), Matchers.is(false));

    value.clear();
    assertThat(readFuture.read(), Matchers.is(true));
  }
",non-flaky,5
78246,apache_beam,StateInternalsTest.testMergeSetIntoSource,"  @Test
  public void testMergeSetIntoSource() throws Exception {

    SetState<String> set1 = underTest.state(NAMESPACE_1, STRING_SET_ADDR);
    SetState<String> set2 = underTest.state(NAMESPACE_2, STRING_SET_ADDR);

    set1.add(""Hello"");
    set2.add(""Hello"");
    set2.add(""World"");
    set1.add(""!"");

    StateMerging.mergeSets(Arrays.asList(set1, set2), set1);

    // Reading the merged set gets both the contents
    assertThat(set1.read(), containsInAnyOrder(""Hello"", ""World"", ""!""));
    assertThat(set2.read(), Matchers.emptyIterable());
  }
",non-flaky,5
78247,apache_beam,StateInternalsTest.testMergeSetIntoNewNamespace,"  @Test
  public void testMergeSetIntoNewNamespace() throws Exception {

    SetState<String> set1 = underTest.state(NAMESPACE_1, STRING_SET_ADDR);
    SetState<String> set2 = underTest.state(NAMESPACE_2, STRING_SET_ADDR);
    SetState<String> set3 = underTest.state(NAMESPACE_3, STRING_SET_ADDR);

    set1.add(""Hello"");
    set2.add(""Hello"");
    set2.add(""World"");
    set1.add(""!"");

    StateMerging.mergeSets(Arrays.asList(set1, set2, set3), set3);

    // Reading the merged set gets both the contents
    assertThat(set3.read(), containsInAnyOrder(""Hello"", ""World"", ""!""));
    assertThat(set1.read(), Matchers.emptyIterable());
    assertThat(set2.read(), Matchers.emptyIterable());
  }
",non-flaky,5
78248,apache_beam,StateInternalsTest.testMap,"  @Test
  public void testMap() throws Exception {

    MapState<String, Integer> value = underTest.state(NAMESPACE_1, STRING_MAP_ADDR);

    // State instances are cached, but depend on the namespace.
    assertThat(value, equalTo(underTest.state(NAMESPACE_1, STRING_MAP_ADDR)));
    assertThat(value, not(equalTo(underTest.state(NAMESPACE_2, STRING_MAP_ADDR))));

    // put
    assertThat(value.entries().read(), Matchers.emptyIterable());
    value.put(""A"", 1);
    value.put(""B"", 2);
    value.put(""A"", 11);
    assertThat(value.putIfAbsent(""B"", 22).read(), equalTo(2));
    assertThat(
        value.entries().read(), containsInAnyOrder(MapEntry.of(""A"", 11), MapEntry.of(""B"", 2)));

    // remove
    value.remove(""A"");
    assertThat(value.entries().read(), containsInAnyOrder(MapEntry.of(""B"", 2)));
    value.remove(""C"");
    assertThat(value.entries().read(), containsInAnyOrder(MapEntry.of(""B"", 2)));

    // get
    assertNull(value.get(""A"").read());
    assertThat(value.get(""B"").read(), equalTo(2));
    value.put(""C"", 3);
    value.put(""D"", 4);
    assertThat(value.get(""C"").read(), equalTo(3));

    // iterate
    value.put(""E"", 5);
    value.remove(""C"");
    assertThat(value.keys().read(), containsInAnyOrder(""B"", ""D"", ""E""));
    assertThat(value.values().read(), containsInAnyOrder(2, 4, 5));
    assertThat(
        value.entries().read(),
        containsInAnyOrder(MapEntry.of(""B"", 2), MapEntry.of(""D"", 4), MapEntry.of(""E"", 5)));

    // readLater
    assertThat(value.get(""B"").readLater().read(), equalTo(2));
    assertNull(value.get(""A"").readLater().read());
    assertThat(
        value.entries().readLater().read(),
        containsInAnyOrder(MapEntry.of(""B"", 2), MapEntry.of(""D"", 4), MapEntry.of(""E"", 5)));

    // clear
    value.clear();
    assertThat(value.entries().read(), Matchers.emptyIterable());
    assertThat(underTest.state(NAMESPACE_1, STRING_MAP_ADDR), equalTo(value));
  }
",non-flaky,5
78249,apache_beam,StateInternalsTest.testCombiningValue,"  @Test
  public void testCombiningValue() throws Exception {

    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);

    // State instances are cached, but depend on the namespace.
    assertEquals(value, underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR));
    assertFalse(value.equals(underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR)));

    assertThat(value.read(), equalTo(0));
    value.add(2);
    assertThat(value.read(), equalTo(2));

    value.add(3);
    assertThat(value.read(), equalTo(5));

    value.clear();
    assertThat(value.read(), equalTo(0));
    assertThat(underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR), equalTo(value));
  }
",non-flaky,5
78250,apache_beam,StateInternalsTest.testCombiningIsEmpty,"  @Test
  public void testCombiningIsEmpty() throws Exception {
    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);

    assertThat(value.isEmpty().read(), Matchers.is(true));
    ReadableState<Boolean> readFuture = value.isEmpty();
    value.add(5);
    assertThat(readFuture.read(), Matchers.is(false));

    value.clear();
    assertThat(readFuture.read(), Matchers.is(true));
  }
",non-flaky,5
78251,apache_beam,StateInternalsTest.testMergeCombiningValueIntoSource,"  @Test
  public void testMergeCombiningValueIntoSource() throws Exception {
    CombiningState<Integer, int[], Integer> value1 = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);
    CombiningState<Integer, int[], Integer> value2 = underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);

    value1.add(5);
    value2.add(10);
    value1.add(6);

    assertThat(value1.read(), equalTo(11));
    assertThat(value2.read(), equalTo(10));

    // Merging clears the old values and updates the result value.
    StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value1);

    assertThat(value1.read(), equalTo(21));
    assertThat(value2.read(), equalTo(0));
  }
",non-flaky,5
78252,apache_beam,StateInternalsTest.testMergeCombiningValueIntoNewNamespace,"  @Test
  public void testMergeCombiningValueIntoNewNamespace() throws Exception {
    CombiningState<Integer, int[], Integer> value1 = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);
    CombiningState<Integer, int[], Integer> value2 = underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);
    CombiningState<Integer, int[], Integer> value3 = underTest.state(NAMESPACE_3, SUM_INTEGER_ADDR);

    value1.add(5);
    value2.add(10);
    value1.add(6);

    StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value3);

    // Merging clears the old values and updates the result value.
    assertThat(value1.read(), equalTo(0));
    assertThat(value2.read(), equalTo(0));
    assertThat(value3.read(), equalTo(21));
  }
",non-flaky,5
78253,apache_beam,StateInternalsTest.testWatermarkEarliestState,"  @Test
  public void testWatermarkEarliestState() throws Exception {
    WatermarkHoldState value = underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR);

    // State instances are cached, but depend on the namespace.
    assertEquals(value, underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR));
    assertFalse(value.equals(underTest.state(NAMESPACE_2, WATERMARK_EARLIEST_ADDR)));

    assertThat(value.read(), Matchers.nullValue());
    value.add(new Instant(2000));
    assertThat(value.read(), equalTo(new Instant(2000)));

    value.add(new Instant(3000));
    assertThat(value.read(), equalTo(new Instant(2000)));

    value.add(new Instant(1000));
    assertThat(value.read(), equalTo(new Instant(1000)));

    value.clear();
    assertThat(value.read(), equalTo(null));
    assertThat(underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR), equalTo(value));
  }
",non-flaky,5
78254,apache_beam,StateInternalsTest.testWatermarkLatestState,"  @Test
  public void testWatermarkLatestState() throws Exception {
    WatermarkHoldState value = underTest.state(NAMESPACE_1, WATERMARK_LATEST_ADDR);

    // State instances are cached, but depend on the namespace.
    assertEquals(value, underTest.state(NAMESPACE_1, WATERMARK_LATEST_ADDR));
    assertFalse(value.equals(underTest.state(NAMESPACE_2, WATERMARK_LATEST_ADDR)));

    assertThat(value.read(), Matchers.nullValue());
    value.add(new Instant(2000));
    assertThat(value.read(), equalTo(new Instant(2000)));

    value.add(new Instant(3000));
    assertThat(value.read(), equalTo(new Instant(3000)));

    value.add(new Instant(1000));
    assertThat(value.read(), equalTo(new Instant(3000)));

    value.clear();
    assertThat(value.read(), equalTo(null));
    assertThat(underTest.state(NAMESPACE_1, WATERMARK_LATEST_ADDR), equalTo(value));
  }
",non-flaky,5
78255,apache_beam,StateInternalsTest.testWatermarkEndOfWindowState,"  @Test
  public void testWatermarkEndOfWindowState() throws Exception {
    WatermarkHoldState value = underTest.state(NAMESPACE_1, WATERMARK_EOW_ADDR);

    // State instances are cached, but depend on the namespace.
    assertEquals(value, underTest.state(NAMESPACE_1, WATERMARK_EOW_ADDR));
    assertFalse(value.equals(underTest.state(NAMESPACE_2, WATERMARK_EOW_ADDR)));

    assertThat(value.read(), Matchers.nullValue());
    value.add(new Instant(2000));
    assertThat(value.read(), equalTo(new Instant(2000)));

    value.clear();
    assertThat(value.read(), equalTo(null));
    assertThat(underTest.state(NAMESPACE_1, WATERMARK_EOW_ADDR), equalTo(value));
  }
",non-flaky,5
78256,apache_beam,StateInternalsTest.testWatermarkStateIsEmpty,"  @Test
  public void testWatermarkStateIsEmpty() throws Exception {
    WatermarkHoldState value = underTest.state(NAMESPACE_1, WATERMARK_EARLIEST_ADDR);

    assertThat(value.isEmpty().read(), Matchers.is(true));
    ReadableState<Boolean> readFuture = value.isEmpty();
    value.add(new Instant(1000));
    assertThat(readFuture.read(), Matchers.is(false));

    value.clear();
    assertThat(readFuture.read(), Matchers.is(true));
  }
",non-flaky,5
78257,apache_beam,StateInternalsTest.testSetReadable,"  @Test
  public void testSetReadable() throws Exception {
    SetState<String> value = underTest.state(NAMESPACE_1, STRING_SET_ADDR);

    // test contains
    ReadableState<Boolean> readable = value.contains(""A"");
    value.add(""A"");
    assertFalse(readable.read());

    // test addIfAbsent
    value.addIfAbsent(""B"");
    assertTrue(value.contains(""B"").read());
  }
",non-flaky,5
78258,apache_beam,StateInternalsTest.testMapReadable,"  @Test
  public void testMapReadable() throws Exception {
    MapState<String, Integer> value = underTest.state(NAMESPACE_1, STRING_MAP_ADDR);

    // test iterable, should just return a iterable view of the values contained in this map.
    // The iterable is backed by the map, so changes to the map are reflected in the iterable.
    ReadableState<Iterable<String>> keys = value.keys();
    ReadableState<Iterable<Integer>> values = value.values();
    ReadableState<Iterable<Map.Entry<String, Integer>>> entries = value.entries();
    value.put(""A"", 1);
    assertFalse(Iterables.isEmpty(keys.read()));
    assertFalse(Iterables.isEmpty(values.read()));
    assertFalse(Iterables.isEmpty(entries.read()));

    // test get
    ReadableState<Integer> get = value.get(""B"");
    value.put(""B"", 2);
    assertNull(get.read());

    // test addIfAbsent
    value.putIfAbsent(""C"", 3);
    assertThat(value.get(""C"").read(), equalTo(3));
  }
",non-flaky,5
78259,apache_beam,StateInternalsTest.testBagWithBadCoderEquality,"  @Test
  public void testBagWithBadCoderEquality() throws Exception {
    // Ensure two instances of the bad coder are distinct; models user who fails to
    // override equals() or inherit from CustomCoder for StructuredCoder
    assertThat(
        new StringCoderWithIdentityEquality(), not(equalTo(new StringCoderWithIdentityEquality())));

    BagState<String> state1 = underTest.state(NAMESPACE_1, STRING_BAG_ADDR1);
    state1.add(""hello"");

    BagState<String> state2 = underTest.state(NAMESPACE_1, STRING_BAG_ADDR2);
    assertThat(state2.read(), containsInAnyOrder(""hello""));
  }
",non-flaky,5
78260,apache_beam,SplittableParDoProcessFnTest.testTrivialProcessFnPropagatesOutputWindowAndTimestamp,"  @Test
  public void testTrivialProcessFnPropagatesOutputWindowAndTimestamp() throws Exception {
    // Tests that ProcessFn correctly propagates the window and timestamp of the element
    // inside the KeyedWorkItem.
    // The underlying DoFn is actually monolithic, so this doesn't test splitting.
    DoFn<Integer, String> fn = new ToStringFn();

    Instant base = Instant.now();

    IntervalWindow w =
        new IntervalWindow(
            base.minus(Duration.standardMinutes(1)), base.plus(Duration.standardMinutes(1)));

    ProcessFnTester<Integer, String, SomeRestriction, Void, SomeRestrictionTracker> tester =
        new ProcessFnTester<>(
            base,
            fn,
            BigEndianIntegerCoder.of(),
            SerializableCoder.of(SomeRestriction.class),
            MAX_OUTPUTS_PER_BUNDLE,
            MAX_BUNDLE_DURATION);
    tester.startElement(
        WindowedValue.of(
            KV.of(42, new SomeRestriction()),
            base,
            Collections.singletonList(w),
            PaneInfo.ON_TIME_AND_ONLY_FIRING));

    assertEquals(
        Arrays.asList(
            TimestampedValue.of(""42a"", base),
            TimestampedValue.of(""42b"", base),
            TimestampedValue.of(""42c"", base)),
        tester.peekOutputElementsInWindow(w));
  }
",non-flaky,5
78261,apache_beam,SplittableParDoProcessFnTest.testUpdatesWatermark,"  @Test
  public void testUpdatesWatermark() throws Exception {
    DoFn<Instant, String> fn = new WatermarkUpdateFn();
    Instant base = Instant.now();

    ProcessFnTester<Instant, String, OffsetRange, Long, OffsetRangeTracker> tester =
        new ProcessFnTester<>(
            base,
            fn,
            InstantCoder.of(),
            SerializableCoder.of(OffsetRange.class),
            3,
            MAX_BUNDLE_DURATION);

    tester.startElement(base, new OffsetRange(0, 8));
    assertThat(tester.takeOutputElements(), hasItems(""0"", ""1"", ""2""));
    assertEquals(base.plus(Duration.standardSeconds(2)), tester.getWatermarkHold());

    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));
    assertThat(tester.takeOutputElements(), hasItems(""3"", ""4"", ""5""));
    assertEquals(base.plus(Duration.standardSeconds(5)), tester.getWatermarkHold());

    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));
    assertThat(tester.takeOutputElements(), hasItems(""6"", ""7""));
    assertEquals(null, tester.getWatermarkHold());
  }
",non-flaky,5
78262,apache_beam,SplittableParDoProcessFnTest.testResumeSetsTimer,"  @Test
  public void testResumeSetsTimer() throws Exception {
    DoFn<Integer, String> fn = new SelfInitiatedResumeFn();
    Instant base = Instant.now();
    ProcessFnTester<Integer, String, SomeRestriction, Void, SomeRestrictionTracker> tester =
        new ProcessFnTester<>(
            base,
            fn,
            BigEndianIntegerCoder.of(),
            SerializableCoder.of(SomeRestriction.class),
            MAX_OUTPUTS_PER_BUNDLE,
            MAX_BUNDLE_DURATION);

    tester.startElement(42, new SomeRestriction());
    assertThat(tester.takeOutputElements(), contains(""42""));

    // Should resume after 5 seconds: advancing by 3 seconds should have no effect.
    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));
    assertTrue(tester.takeOutputElements().isEmpty());

    // 6 seconds should be enough  should invoke the fn again.
    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));
    assertThat(tester.takeOutputElements(), contains(""42""));

    // Should again resume after 5 seconds: advancing by 3 seconds should again have no effect.
    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));
    assertTrue(tester.takeOutputElements().isEmpty());

    // 6 seconds should again be enough.
    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));
    assertThat(tester.takeOutputElements(), contains(""42""));
  }
",non-flaky,5
78263,apache_beam,SplittableParDoProcessFnTest.testResumeCarriesOverState,"  @Test
  public void testResumeCarriesOverState() throws Exception {
    DoFn<Integer, String> fn = new CounterFn(1);
    Instant base = Instant.now();
    ProcessFnTester<Integer, String, OffsetRange, Long, OffsetRangeTracker> tester =
        new ProcessFnTester<>(
            base,
            fn,
            BigEndianIntegerCoder.of(),
            SerializableCoder.of(OffsetRange.class),
            MAX_OUTPUTS_PER_BUNDLE,
            MAX_BUNDLE_DURATION);

    tester.startElement(42, new OffsetRange(0, 3));
    assertThat(tester.takeOutputElements(), contains(""42""));
    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));
    assertThat(tester.takeOutputElements(), contains(""43""));
    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));
    assertThat(tester.takeOutputElements(), contains(""44""));
    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));
    // After outputting all 3 items, should not output anything more.
    assertEquals(0, tester.takeOutputElements().size());
    // Should also not ask to resume.
    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));
  }
",non-flaky,5
78264,apache_beam,SplittableParDoProcessFnTest.testCheckpointsAfterNumOutputs,"  @Test
  public void testCheckpointsAfterNumOutputs() throws Exception {
    int max = 100;
    DoFn<Integer, String> fn = new CounterFn(Integer.MAX_VALUE);
    Instant base = Instant.now();
    int baseIndex = 42;

    ProcessFnTester<Integer, String, OffsetRange, Long, OffsetRangeTracker> tester =
        new ProcessFnTester<>(
            base,
            fn,
            BigEndianIntegerCoder.of(),
            SerializableCoder.of(OffsetRange.class),
            max,
            MAX_BUNDLE_DURATION);

    List<String> elements;

    // Create an fn that attempts to 2x output more than checkpointing allows.
    tester.startElement(baseIndex, new OffsetRange(0, 2 * max + max / 2));
    elements = tester.takeOutputElements();
    assertEquals(max, elements.size());
    // Should output the range [0, max)
    assertThat(elements, hasItem(String.valueOf(baseIndex)));
    assertThat(elements, hasItem(String.valueOf(baseIndex + max - 1)));

    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));
    elements = tester.takeOutputElements();
    assertEquals(max, elements.size());
    // Should output the range [max, 2*max)
    assertThat(elements, hasItem(String.valueOf(baseIndex + max)));
    assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max - 1)));

    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));
    elements = tester.takeOutputElements();
    assertEquals(max / 2, elements.size());
    // Should output the range [2*max, 2*max + max/2)
    assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max)));
    assertThat(elements, hasItem(String.valueOf(baseIndex + 2 * max + max / 2 - 1)));
    assertThat(elements, not(hasItem((String.valueOf(baseIndex + 2 * max + max / 2)))));
  }
",non-flaky,5
78265,apache_beam,SplittableParDoProcessFnTest.testCheckpointsAfterDuration,"  @Test
  public void testCheckpointsAfterDuration() throws Exception {
    // Don't bound number of outputs.
    int max = Integer.MAX_VALUE;
    // But bound bundle duration - the bundle should terminate.
    Duration maxBundleDuration = Duration.standardSeconds(1);
    // Create an fn that attempts to 2x output more than checkpointing allows.
    DoFn<Integer, String> fn = new CounterFn(Integer.MAX_VALUE);
    Instant base = Instant.now();
    int baseIndex = 42;

    ProcessFnTester<Integer, String, OffsetRange, Long, OffsetRangeTracker> tester =
        new ProcessFnTester<>(
            base,
            fn,
            BigEndianIntegerCoder.of(),
            SerializableCoder.of(OffsetRange.class),
            max,
            maxBundleDuration);

    List<String> elements;

    tester.startElement(baseIndex, new OffsetRange(0, Long.MAX_VALUE));
    // Bundle should terminate, and should do at least some processing.
    elements = tester.takeOutputElements();
    assertFalse(elements.isEmpty());
    // Bundle should have run for at least the requested duration.
    assertThat(
        Instant.now().getMillis() - base.getMillis(),
        greaterThanOrEqualTo(maxBundleDuration.getMillis()));
  }
",non-flaky,5
78266,apache_beam,SplittableParDoProcessFnTest.testInvokesLifecycleMethods,"  @Test
  public void testInvokesLifecycleMethods() throws Exception {
    DoFn<Integer, String> fn = new LifecycleVerifyingFn();
    try (ProcessFnTester<Integer, String, SomeRestriction, Void, SomeRestrictionTracker> tester =
        new ProcessFnTester<>(
            Instant.now(),
            fn,
            BigEndianIntegerCoder.of(),
            SerializableCoder.of(SomeRestriction.class),
            MAX_OUTPUTS_PER_BUNDLE,
            MAX_BUNDLE_DURATION)) {
      tester.startElement(42, new SomeRestriction());
    }
  }
",non-flaky,5
78267,apache_beam,LateDataUtilsTest.beforeEndOfGlobalWindowSame,"  @Test
  public void beforeEndOfGlobalWindowSame() {
    FixedWindows windowFn = FixedWindows.of(Duration.standardMinutes(5));
    Duration allowedLateness = Duration.standardMinutes(2);
    WindowingStrategy<?, ?> strategy =
        WindowingStrategy.globalDefault()
            .withWindowFn(windowFn)
            .withAllowedLateness(allowedLateness);

    IntervalWindow window = windowFn.assignWindow(new Instant(10));
    assertThat(
        LateDataUtils.garbageCollectionTime(window, strategy),
        equalTo(window.maxTimestamp().plus(allowedLateness)));
  }
",non-flaky,5
78268,apache_beam,LateDataUtilsTest.garbageCollectionTimeAfterEndOfGlobalWindow,"  @Test
  public void garbageCollectionTimeAfterEndOfGlobalWindow() {
    FixedWindows windowFn = FixedWindows.of(Duration.standardMinutes(5));
    WindowingStrategy<?, ?> strategy = WindowingStrategy.globalDefault().withWindowFn(windowFn);

    IntervalWindow window = windowFn.assignWindow(new Instant(BoundedWindow.TIMESTAMP_MAX_VALUE));
    assertThat(window.maxTimestamp(), equalTo(GlobalWindow.INSTANCE.maxTimestamp()));
    assertThat(
        LateDataUtils.garbageCollectionTime(window, strategy),
        equalTo(GlobalWindow.INSTANCE.maxTimestamp()));
  }
",non-flaky,5
78269,apache_beam,LateDataUtilsTest.garbageCollectionTimeAfterEndOfGlobalWindowWithLateness,"  @Test
  public void garbageCollectionTimeAfterEndOfGlobalWindowWithLateness() {
    FixedWindows windowFn = FixedWindows.of(Duration.standardMinutes(5));
    Duration allowedLateness = Duration.millis(Long.MAX_VALUE);
    WindowingStrategy<?, ?> strategy =
        WindowingStrategy.globalDefault()
            .withWindowFn(windowFn)
            .withAllowedLateness(allowedLateness);

    IntervalWindow window = windowFn.assignWindow(new Instant(-100));
    assertThat(
        window.maxTimestamp().plus(allowedLateness),
        Matchers.greaterThan(GlobalWindow.INSTANCE.maxTimestamp()));
    assertThat(
        LateDataUtils.garbageCollectionTime(window, strategy),
        equalTo(GlobalWindow.INSTANCE.maxTimestamp()));
  }
",non-flaky,5
78270,apache_beam,StateNamespacesTest.testStability,"  @Test
  public void testStability() {
    StateNamespace global = StateNamespaces.global();
    StateNamespace intervalWindow =
        StateNamespaces.window(intervalCoder, intervalWindow(1000, 87392));
    StateNamespace intervalWindowAndTrigger =
        StateNamespaces.windowAndTrigger(intervalCoder, intervalWindow(1000, 87392), 57);
    StateNamespace globalWindow =
        StateNamespaces.window(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE);
    StateNamespace globalWindowAndTrigger =
        StateNamespaces.windowAndTrigger(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE, 12);

    assertEquals(""/"", global.stringKey());
    assertEquals(""/gAAAAAABVWD4ogU/"", intervalWindow.stringKey());
    assertEquals(""/gAAAAAABVWD4ogU/1L/"", intervalWindowAndTrigger.stringKey());
    assertEquals(""//"", globalWindow.stringKey());
    assertEquals(""//C/"", globalWindowAndTrigger.stringKey());
  }
",non-flaky,5
78271,apache_beam,StateNamespacesTest.testIntervalWindowPrefixing,"  @Test
  public void testIntervalWindowPrefixing() {
    StateNamespace window = StateNamespaces.window(intervalCoder, intervalWindow(1000, 87392));
    StateNamespace windowAndTrigger =
        StateNamespaces.windowAndTrigger(intervalCoder, intervalWindow(1000, 87392), 57);
    assertThat(windowAndTrigger.stringKey(), Matchers.startsWith(window.stringKey()));
    assertThat(
        StateNamespaces.global().stringKey(),
        Matchers.not(Matchers.startsWith(window.stringKey())));
  }
",non-flaky,5
78272,apache_beam,StateNamespacesTest.testGlobalWindowPrefixing,"  @Test
  public void testGlobalWindowPrefixing() {
    StateNamespace window =
        StateNamespaces.window(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE);
    StateNamespace windowAndTrigger =
        StateNamespaces.windowAndTrigger(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE, 57);
    assertThat(windowAndTrigger.stringKey(), Matchers.startsWith(window.stringKey()));
    assertThat(
        StateNamespaces.global().stringKey(),
        Matchers.not(Matchers.startsWith(window.stringKey())));
  }
",non-flaky,5
78273,apache_beam,StateNamespacesTest.testFromStringGlobal,"  @Test
  public void testFromStringGlobal() {
    assertStringKeyRoundTrips(intervalCoder, StateNamespaces.global());
  }
",non-flaky,5
78274,apache_beam,StateNamespacesTest.testFromStringIntervalWindow,"  @Test
  public void testFromStringIntervalWindow() {
    assertStringKeyRoundTrips(
        intervalCoder, StateNamespaces.window(intervalCoder, intervalWindow(1000, 8000)));
    assertStringKeyRoundTrips(
        intervalCoder, StateNamespaces.window(intervalCoder, intervalWindow(1000, 8000)));

    assertStringKeyRoundTrips(
        intervalCoder,
        StateNamespaces.windowAndTrigger(intervalCoder, intervalWindow(1000, 8000), 18));
    assertStringKeyRoundTrips(
        intervalCoder,
        StateNamespaces.windowAndTrigger(intervalCoder, intervalWindow(1000, 8000), 19));
    assertStringKeyRoundTrips(
        intervalCoder,
        StateNamespaces.windowAndTrigger(intervalCoder, intervalWindow(2000, 8000), 19));
  }
",non-flaky,5
78275,apache_beam,StateNamespacesTest.testFromStringGlobalWindow,"  @Test
  public void testFromStringGlobalWindow() {
    assertStringKeyRoundTrips(GlobalWindow.Coder.INSTANCE, StateNamespaces.global());
    assertStringKeyRoundTrips(
        GlobalWindow.Coder.INSTANCE,
        StateNamespaces.window(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE));
    assertStringKeyRoundTrips(
        GlobalWindow.Coder.INSTANCE,
        StateNamespaces.windowAndTrigger(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE, 18));
  }
",non-flaky,5
78276,apache_beam,StateTagTest.testValueEquality,"  @Test
  public void testValueEquality() {
    StateTag<?> fooVarInt1 = StateTags.value(""foo"", VarIntCoder.of());
    StateTag<?> fooVarInt2 = StateTags.value(""foo"", VarIntCoder.of());
    StateTag<?> fooBigEndian = StateTags.value(""foo"", BigEndianIntegerCoder.of());
    StateTag<?> barVarInt = StateTags.value(""bar"", VarIntCoder.of());

    assertEquals(fooVarInt1, fooVarInt2);
    assertNotEquals(fooVarInt1, fooBigEndian);
    assertNotEquals(fooVarInt1, barVarInt);
  }
",non-flaky,5
78277,apache_beam,StateTagTest.testBagEquality,"  @Test
  public void testBagEquality() {
    StateTag<?> fooVarInt1 = StateTags.bag(""foo"", VarIntCoder.of());
    StateTag<?> fooVarInt2 = StateTags.bag(""foo"", VarIntCoder.of());
    StateTag<?> fooBigEndian = StateTags.bag(""foo"", BigEndianIntegerCoder.of());
    StateTag<?> barVarInt = StateTags.bag(""bar"", VarIntCoder.of());

    assertEquals(fooVarInt1, fooVarInt2);
    assertNotEquals(fooVarInt1, fooBigEndian);
    assertNotEquals(fooVarInt1, barVarInt);
  }
",non-flaky,5
78278,apache_beam,StateTagTest.testSetEquality,"  @Test
  public void testSetEquality() {
    StateTag<?> fooVarInt1 = StateTags.set(""foo"", VarIntCoder.of());
    StateTag<?> fooVarInt2 = StateTags.set(""foo"", VarIntCoder.of());
    StateTag<?> fooBigEndian = StateTags.set(""foo"", BigEndianIntegerCoder.of());
    StateTag<?> barVarInt = StateTags.set(""bar"", VarIntCoder.of());

    assertEquals(fooVarInt1, fooVarInt2);
    assertNotEquals(fooVarInt1, fooBigEndian);
    assertNotEquals(fooVarInt1, barVarInt);
  }
",non-flaky,5
78279,apache_beam,StateTagTest.testMapEquality,"  @Test
  public void testMapEquality() {
    StateTag<?> fooStringVarInt1 = StateTags.map(""foo"", StringUtf8Coder.of(), VarIntCoder.of());
    StateTag<?> fooStringVarInt2 = StateTags.map(""foo"", StringUtf8Coder.of(), VarIntCoder.of());
    StateTag<?> fooStringBigEndian =
        StateTags.map(""foo"", StringUtf8Coder.of(), BigEndianIntegerCoder.of());
    StateTag<?> fooVarIntBigEndian =
        StateTags.map(""foo"", VarIntCoder.of(), BigEndianIntegerCoder.of());
    StateTag<?> barStringVarInt = StateTags.map(""bar"", StringUtf8Coder.of(), VarIntCoder.of());

    assertEquals(fooStringVarInt1, fooStringVarInt2);
    assertNotEquals(fooStringVarInt1, fooStringBigEndian);
    assertNotEquals(fooStringBigEndian, fooVarIntBigEndian);
    assertNotEquals(fooStringVarInt1, fooVarIntBigEndian);
    assertNotEquals(fooStringVarInt1, barStringVarInt);
  }
",non-flaky,5
78280,apache_beam,StateTagTest.testWatermarkBagEquality,"  @Test
  public void testWatermarkBagEquality() {
    StateTag<?> foo1 = StateTags.watermarkStateInternal(""foo"", TimestampCombiner.EARLIEST);
    StateTag<?> foo2 = StateTags.watermarkStateInternal(""foo"", TimestampCombiner.EARLIEST);
    StateTag<?> bar = StateTags.watermarkStateInternal(""bar"", TimestampCombiner.EARLIEST);

    StateTag<?> bar2 = StateTags.watermarkStateInternal(""bar"", TimestampCombiner.LATEST);

    // Same id, same fn.
    assertEquals(foo1, foo2);
    // Different id, same fn.
    assertNotEquals(foo1, bar);
    // Same id, different fn.
    assertEquals(bar, bar2);
  }
",non-flaky,5
78281,apache_beam,StateTagTest.testCombiningValueEquality,"  @Test
  public void testCombiningValueEquality() {
    Combine.BinaryCombineIntegerFn maxFn = Max.ofIntegers();
    Coder<Integer> input1 = VarIntCoder.of();
    Coder<Integer> input2 = BigEndianIntegerCoder.of();
    Combine.BinaryCombineIntegerFn minFn = Min.ofIntegers();

    StateTag<?> fooCoder1Max1 = StateTags.combiningValueFromInputInternal(""foo"", input1, maxFn);
    StateTag<?> fooCoder1Max2 = StateTags.combiningValueFromInputInternal(""foo"", input1, maxFn);
    StateTag<?> fooCoder1Min = StateTags.combiningValueFromInputInternal(""foo"", input1, minFn);

    StateTag<?> fooCoder2Max = StateTags.combiningValueFromInputInternal(""foo"", input2, maxFn);
    StateTag<?> barCoder1Max = StateTags.combiningValueFromInputInternal(""bar"", input1, maxFn);

    // Same name, coder and combineFn
    assertEquals(fooCoder1Max1, fooCoder1Max2);
    assertEquals(
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1),
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max2));

    // Different combineFn, but we treat them as equal since we only serialize the bits.
    assertEquals(fooCoder1Max1, fooCoder1Min);
    assertEquals(
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1),
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Min));

    // Different input coder coder.
    assertNotEquals(fooCoder1Max1, fooCoder2Max);
    assertNotEquals(
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1),
        StateTags.convertToBagTagInternal((StateTag) fooCoder2Max));

    // These StateTags have different IDs.
    assertNotEquals(fooCoder1Max1, barCoder1Max);
    assertNotEquals(
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1),
        StateTags.convertToBagTagInternal((StateTag) barCoder1Max));
  }
",non-flaky,5
78282,apache_beam,StateTagTest.testCombiningValueWithContextEquality,"  @Test
  public void testCombiningValueWithContextEquality() {
    CoderRegistry registry = CoderRegistry.createDefault();

    Combine.BinaryCombineIntegerFn maxFn = Max.ofIntegers();
    Combine.BinaryCombineIntegerFn minFn = Min.ofIntegers();

    Coder<int[]> accum1 = maxFn.getAccumulatorCoder(registry, VarIntCoder.of());
    Coder<int[]> accum2 = minFn.getAccumulatorCoder(registry, BigEndianIntegerCoder.of());

    StateTag<?> fooCoder1Max1 =
        StateTags.combiningValueWithContext(""foo"", accum1, CombineFnUtil.toFnWithContext(maxFn));
    StateTag<?> fooCoder1Max2 =
        StateTags.combiningValueWithContext(""foo"", accum1, CombineFnUtil.toFnWithContext(maxFn));
    StateTag<?> fooCoder1Min =
        StateTags.combiningValueWithContext(""foo"", accum1, CombineFnUtil.toFnWithContext(minFn));

    StateTag<?> fooCoder2Max =
        StateTags.combiningValueWithContext(""foo"", accum2, CombineFnUtil.toFnWithContext(maxFn));
    StateTag<?> barCoder1Max =
        StateTags.combiningValueWithContext(""bar"", accum1, CombineFnUtil.toFnWithContext(maxFn));

    // Same name, coder and combineFn
    assertEquals(fooCoder1Max1, fooCoder1Max2);
    assertEquals(
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1),
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max2));
    // Different combineFn, but we treat them as equal since we only serialize the bits.
    assertEquals(fooCoder1Max1, fooCoder1Min);
    assertEquals(
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1),
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Min));

    // Different input coder coder.
    assertNotEquals(fooCoder1Max1, fooCoder2Max);
    assertNotEquals(
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1),
        StateTags.convertToBagTagInternal((StateTag) fooCoder2Max));

    // These StateTags have different IDs.
    assertNotEquals(fooCoder1Max1, barCoder1Max);
    assertNotEquals(
        StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1),
        StateTags.convertToBagTagInternal((StateTag) barCoder1Max));
  }
",non-flaky,5
78283,apache_beam,TimerInternalsTest.testTimerDataCoder,"  @Test
  public void testTimerDataCoder() throws Exception {
    CoderProperties.coderDecodeEncodeEqual(
        TimerDataCoder.of(GlobalWindow.Coder.INSTANCE),
        TimerData.of(
            ""arbitrary-id"", StateNamespaces.global(), new Instant(0), TimeDomain.EVENT_TIME));

    Coder<IntervalWindow> windowCoder = IntervalWindow.getCoder();
    CoderProperties.coderDecodeEncodeEqual(
        TimerDataCoder.of(windowCoder),
        TimerData.of(
            ""another-id"",
            StateNamespaces.window(
                windowCoder, new IntervalWindow(new Instant(0), new Instant(100))),
            new Instant(99),
            TimeDomain.PROCESSING_TIME));
  }
",non-flaky,5
78284,apache_beam,TimerInternalsTest.testCoderIsSerializableWithWellKnownCoderType,"  @Test
  public void testCoderIsSerializableWithWellKnownCoderType() {
    CoderProperties.coderSerializable(TimerDataCoder.of(GlobalWindow.Coder.INSTANCE));
  }
",non-flaky,5
78285,apache_beam,TimerInternalsTest.testCompareEqual,"  @Test
  public void testCompareEqual() {
    Instant timestamp = new Instant(100);
    StateNamespace namespace = StateNamespaces.global();
    TimerData timer = TimerData.of(""id"", namespace, timestamp, TimeDomain.EVENT_TIME);

    assertThat(
        timer, comparesEqualTo(TimerData.of(""id"", namespace, timestamp, TimeDomain.EVENT_TIME)));
  }
",non-flaky,5
78286,apache_beam,TimerInternalsTest.testCompareByTimestamp,"  @Test
  public void testCompareByTimestamp() {
    Instant firstTimestamp = new Instant(100);
    Instant secondTimestamp = new Instant(200);
    StateNamespace namespace = StateNamespaces.global();

    TimerData firstTimer = TimerData.of(namespace, firstTimestamp, TimeDomain.EVENT_TIME);
    TimerData secondTimer = TimerData.of(namespace, secondTimestamp, TimeDomain.EVENT_TIME);

    assertThat(firstTimer, lessThan(secondTimer));
  }
",non-flaky,5
78287,apache_beam,TimerInternalsTest.testCompareByDomain,"  @Test
  public void testCompareByDomain() {
    Instant timestamp = new Instant(100);
    StateNamespace namespace = StateNamespaces.global();

    TimerData eventTimer = TimerData.of(namespace, timestamp, TimeDomain.EVENT_TIME);
    TimerData procTimer = TimerData.of(namespace, timestamp, TimeDomain.PROCESSING_TIME);
    TimerData synchronizedProcTimer =
        TimerData.of(namespace, timestamp, TimeDomain.SYNCHRONIZED_PROCESSING_TIME);

    assertThat(eventTimer, lessThan(procTimer));
    assertThat(eventTimer, lessThan(synchronizedProcTimer));
    assertThat(procTimer, lessThan(synchronizedProcTimer));
  }
",non-flaky,5
78288,apache_beam,TimerInternalsTest.testCompareByNamespace,"  @Test
  public void testCompareByNamespace() {
    Instant timestamp = new Instant(100);
    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), timestamp);
    IntervalWindow secondWindow = new IntervalWindow(timestamp, new Instant(200));
    Coder<IntervalWindow> windowCoder = IntervalWindow.getCoder();

    StateNamespace firstWindowNs = StateNamespaces.window(windowCoder, firstWindow);
    StateNamespace secondWindowNs = StateNamespaces.window(windowCoder, secondWindow);

    TimerData secondEventTime = TimerData.of(firstWindowNs, timestamp, TimeDomain.EVENT_TIME);
    TimerData thirdEventTime = TimerData.of(secondWindowNs, timestamp, TimeDomain.EVENT_TIME);

    assertThat(secondEventTime, lessThan(thirdEventTime));
  }
",non-flaky,5
78289,apache_beam,TimerInternalsTest.testCompareByTimerId,"  @Test
  public void testCompareByTimerId() {
    Instant timestamp = new Instant(100);
    StateNamespace namespace = StateNamespaces.global();

    TimerData id0Timer = TimerData.of(""id0"", namespace, timestamp, TimeDomain.EVENT_TIME);
    TimerData id1Timer = TimerData.of(""id1"", namespace, timestamp, TimeDomain.EVENT_TIME);

    assertThat(id0Timer, lessThan(id1Timer));
  }
",non-flaky,5
78290,apache_beam,SideInputHandlerTest.testIsEmpty,"  @Test
  public void testIsEmpty() {
    SideInputHandler sideInputHandler =
        new SideInputHandler(ImmutableList.of(view1), InMemoryStateInternals.<Void>forKey(null));

    assertFalse(sideInputHandler.isEmpty());

    // create an empty handler
    SideInputHandler emptySideInputHandler =
        new SideInputHandler(ImmutableList.of(), InMemoryStateInternals.<Void>forKey(null));

    assertTrue(emptySideInputHandler.isEmpty());
  }
",non-flaky,5
78291,apache_beam,SideInputHandlerTest.testContains,"  @Test
  public void testContains() {
    SideInputHandler sideInputHandler =
        new SideInputHandler(ImmutableList.of(view1), InMemoryStateInternals.<Void>forKey(null));

    assertTrue(sideInputHandler.contains(view1));
    assertFalse(sideInputHandler.contains(view2));
  }
",non-flaky,5
78292,apache_beam,SideInputHandlerTest.testIsReady,"  @Test
  public void testIsReady() {
    SideInputHandler sideInputHandler =
        new SideInputHandler(
            ImmutableList.of(view1, view2), InMemoryStateInternals.<Void>forKey(null));

    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(WINDOW_MSECS_1));

    IntervalWindow secondWindow = new IntervalWindow(new Instant(0), new Instant(WINDOW_MSECS_2));

    // side input should not yet be ready
    assertFalse(sideInputHandler.isReady(view1, firstWindow));

    // add a value for view1
    sideInputHandler.addSideInputValue(
        view1,
        valuesInWindow(
            materializeValuesFor(View.asIterable(), ""Hello""), new Instant(0), firstWindow));

    // now side input should be ready
    assertTrue(sideInputHandler.isReady(view1, firstWindow));

    // second window input should still not be ready
    assertFalse(sideInputHandler.isReady(view1, secondWindow));
  }
",non-flaky,5
78293,apache_beam,SideInputHandlerTest.testNewInputReplacesPreviousInput,"  @Test
  public void testNewInputReplacesPreviousInput() {
    // new input should completely replace old input
    // the creation of the Iterable that has the side input
    // contents happens upstream. this is also where
    // accumulation/discarding is decided.

    SideInputHandler sideInputHandler =
        new SideInputHandler(ImmutableList.of(view1), InMemoryStateInternals.<Void>forKey(null));

    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(WINDOW_MSECS_1));

    // add a first value for view1
    sideInputHandler.addSideInputValue(
        view1,
        valuesInWindow(materializeValuesFor(View.asIterable(), ""Hello""), new Instant(0), window));

    assertThat(sideInputHandler.get(view1, window), contains(""Hello""));

    // subsequent values should replace existing values
    sideInputHandler.addSideInputValue(
        view1,
        valuesInWindow(
            materializeValuesFor(View.asIterable(), ""Ciao"", ""Buongiorno""), new Instant(0), window));

    assertThat(sideInputHandler.get(view1, window), contains(""Ciao"", ""Buongiorno""));
  }
",non-flaky,5
78294,apache_beam,SideInputHandlerTest.testMultipleWindows,"  @Test
  public void testMultipleWindows() {
    SideInputHandler sideInputHandler =
        new SideInputHandler(ImmutableList.of(view1), InMemoryStateInternals.<Void>forKey(null));

    // two windows that we'll later use for adding elements/retrieving side input
    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(WINDOW_MSECS_1));
    IntervalWindow secondWindow =
        new IntervalWindow(new Instant(1000), new Instant(1000 + WINDOW_MSECS_2));

    // add a first value for view1 in the first window
    sideInputHandler.addSideInputValue(
        view1,
        valuesInWindow(
            materializeValuesFor(View.asIterable(), ""Hello""), new Instant(0), firstWindow));

    assertThat(sideInputHandler.get(view1, firstWindow), contains(""Hello""));

    // add something for second window of view1
    sideInputHandler.addSideInputValue(
        view1,
        valuesInWindow(
            materializeValuesFor(View.asIterable(), ""Arrivederci""), new Instant(0), secondWindow));

    assertThat(sideInputHandler.get(view1, secondWindow), contains(""Arrivederci""));

    // contents for first window should be unaffected
    assertThat(sideInputHandler.get(view1, firstWindow), contains(""Hello""));
  }
",non-flaky,5
78295,apache_beam,SideInputHandlerTest.testMultipleSideInputs,"  @Test
  public void testMultipleSideInputs() {
    SideInputHandler sideInputHandler =
        new SideInputHandler(
            ImmutableList.of(view1, view2), InMemoryStateInternals.<Void>forKey(null));

    // two windows that we'll later use for adding elements/retrieving side input
    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(WINDOW_MSECS_1));

    // add value for view1 in the first window
    sideInputHandler.addSideInputValue(
        view1,
        valuesInWindow(
            materializeValuesFor(View.asIterable(), ""Hello""), new Instant(0), firstWindow));

    assertThat(sideInputHandler.get(view1, firstWindow), contains(""Hello""));

    // view2 should not have any data
    assertFalse(sideInputHandler.isReady(view2, firstWindow));

    // also add some data for view2
    sideInputHandler.addSideInputValue(
        view2,
        valuesInWindow(
            materializeValuesFor(View.asIterable(), ""Salut""), new Instant(0), firstWindow));

    assertTrue(sideInputHandler.isReady(view2, firstWindow));
    assertThat(sideInputHandler.get(view2, firstWindow), contains(""Salut""));

    // view1 should not be affected by that
    assertThat(sideInputHandler.get(view1, firstWindow), contains(""Hello""));
  }
",non-flaky,5
78296,apache_beam,LateDataDroppingDoFnRunnerTest.testLateDataFilter,"  @Test
  public void testLateDataFilter() throws Exception {
    MetricsContainerImpl container = new MetricsContainerImpl(""any"");
    MetricsEnvironment.setCurrentContainer(container);
    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(new Instant(15L));

    LateDataFilter lateDataFilter =
        new LateDataFilter(WindowingStrategy.of(WINDOW_FN), mockTimerInternals);

    Iterable<WindowedValue<Integer>> actual =
        lateDataFilter.filter(
            ""a"",
            ImmutableList.of(
                createDatum(13, 13L),
                createDatum(5, 5L), // late element, earlier than 4L.
                createDatum(16, 16L),
                createDatum(18, 18L)));

    Iterable<WindowedValue<Integer>> expected =
        ImmutableList.of(createDatum(13, 13L), createDatum(16, 16L), createDatum(18, 18L));
    assertThat(expected, containsInAnyOrder(Iterables.toArray(actual, WindowedValue.class)));
    long droppedValues =
        container
            .getCounter(
                MetricName.named(
                    LateDataDroppingDoFnRunner.class,
                    LateDataDroppingDoFnRunner.DROPPED_DUE_TO_LATENESS))
            .getCumulative();
    assertEquals(1, droppedValues);
    // Ensure that reiterating returns the same results and doesn't increment the counter again.
    assertThat(expected, containsInAnyOrder(Iterables.toArray(actual, WindowedValue.class)));
    droppedValues =
        container
            .getCounter(
                MetricName.named(
                    LateDataDroppingDoFnRunner.class,
                    LateDataDroppingDoFnRunner.DROPPED_DUE_TO_LATENESS))
            .getCumulative();
    assertEquals(1, droppedValues);
  }
",non-flaky,5
78297,apache_beam,InMemoryStateInternalsTest.testSameInstance,"    @Test
    public void testSameInstance() {
      assertSameInstance(STRING_VALUE_ADDR);
      assertSameInstance(SUM_INTEGER_ADDR);
      assertSameInstance(STRING_BAG_ADDR);
      assertSameInstance(STRING_SET_ADDR);
      assertSameInstance(STRING_MAP_ADDR);
      assertSameInstance(WATERMARK_EARLIEST_ADDR);
    }
",non-flaky,5
78298,apache_beam,OutputAndTimeBoundedSplittableProcessElementInvokerTest.testInvokeProcessElementOutputBounded,"  @Test
  public void testInvokeProcessElementOutputBounded() throws Exception {
    SplittableProcessElementInvoker<Void, String, OffsetRange, OffsetRangeTracker>.Result res =
        runTest(10000, Duration.ZERO, Integer.MAX_VALUE, Duration.ZERO);
    assertFalse(res.getContinuation().shouldResume());
    OffsetRange residualRange = res.getResidualRestriction();
    // Should process the first 100 elements.
    assertEquals(1000, residualRange.getFrom());
    assertEquals(10000, residualRange.getTo());
  }
",non-flaky,5
78299,apache_beam,OutputAndTimeBoundedSplittableProcessElementInvokerTest.testInvokeProcessElementTimeBounded,"  @Test
  public void testInvokeProcessElementTimeBounded() throws Exception {
    SplittableProcessElementInvoker<Void, String, OffsetRange, OffsetRangeTracker>.Result res =
        runTest(10000, Duration.ZERO, Integer.MAX_VALUE, Duration.millis(100));
    assertFalse(res.getContinuation().shouldResume());
    OffsetRange residualRange = res.getResidualRestriction();
    // Should process ideally around 30 elements - but due to timing flakiness, we can't enforce
    // that precisely. Just test that it's not egregiously off.
    assertThat(residualRange.getFrom(), greaterThan(10L));
    assertThat(residualRange.getFrom(), lessThan(100L));
    assertEquals(10000, residualRange.getTo());
  }
",non-flaky,5
78300,apache_beam,OutputAndTimeBoundedSplittableProcessElementInvokerTest.testInvokeProcessElementTimeBoundedWithStartupDelay,"  @Test
  public void testInvokeProcessElementTimeBoundedWithStartupDelay() throws Exception {
    SplittableProcessElementInvoker<Void, String, OffsetRange, OffsetRangeTracker>.Result res =
        runTest(10000, Duration.standardSeconds(3), Integer.MAX_VALUE, Duration.millis(100));
    assertFalse(res.getContinuation().shouldResume());
    OffsetRange residualRange = res.getResidualRestriction();
    // Same as above, but this time it counts from the time of the first tryClaim() call
    assertThat(residualRange.getFrom(), greaterThan(10L));
    assertThat(residualRange.getFrom(), lessThan(100L));
    assertEquals(10000, residualRange.getTo());
  }
",non-flaky,5
78301,apache_beam,OutputAndTimeBoundedSplittableProcessElementInvokerTest.testInvokeProcessElementVoluntaryReturnStop,"  @Test
  public void testInvokeProcessElementVoluntaryReturnStop() throws Exception {
    SplittableProcessElementInvoker<Void, String, OffsetRange, OffsetRangeTracker>.Result res =
        runTest(5, Duration.ZERO, Integer.MAX_VALUE, Duration.millis(100));
    assertFalse(res.getContinuation().shouldResume());
    assertNull(res.getResidualRestriction());
  }
",non-flaky,5
78302,apache_beam,OutputAndTimeBoundedSplittableProcessElementInvokerTest.testInvokeProcessElementVoluntaryReturnResume,"  @Test
  public void testInvokeProcessElementVoluntaryReturnResume() throws Exception {
    SplittableProcessElementInvoker<Void, String, OffsetRange, OffsetRangeTracker>.Result res =
        runTest(10, Duration.ZERO, 5, Duration.millis(100));
    assertTrue(res.getContinuation().shouldResume());
    assertEquals(new OffsetRange(5, 10), res.getResidualRestriction());
  }
",non-flaky,5
78303,apache_beam,OutputAndTimeBoundedSplittableProcessElementInvokerTest.process,"  @Test
  public void testInvokeProcessElementOutputDisallowedBeforeTryClaim() throws Exception {
    DoFn<Void, String> brokenFn =
        new DoFn<Void, String>() {
          @ProcessElement
          public void process(ProcessContext c, OffsetRangeTracker tracker) {
            c.output(""foo"");
          }
",non-flaky,5
78304,apache_beam,OutputAndTimeBoundedSplittableProcessElementInvokerTest.process,"  @Test
  public void testInvokeProcessElementOutputDisallowedAfterFailedTryClaim() throws Exception {
    DoFn<Void, String> brokenFn =
        new DoFn<Void, String>() {
          @ProcessElement
          public void process(ProcessContext c, OffsetRangeTracker tracker) {
            assertFalse(tracker.tryClaim(6L));
            c.output(""foo"");
          }
",non-flaky,5
78305,apache_beam,InMemoryMultimapSideInputViewTest.testStructuralKeyEquality,"  @Test
  public void testStructuralKeyEquality() {
    MultimapView<byte[], Integer> view =
        InMemoryMultimapSideInputView.fromIterable(
            ByteArrayCoder.of(),
            ImmutableList.of(KV.of(new byte[] {0x00}, 0), KV.of(new byte[] {0x01}, 1)));
    assertEquals(view.get(new byte[] {0x00}), ImmutableList.of(0));
    assertEquals(view.get(new byte[] {0x01}), ImmutableList.of(1));
    assertEquals(view.get(new byte[] {0x02}), ImmutableList.of());
  }
",non-flaky,5
78306,apache_beam,InMemoryMultimapSideInputViewTest.testValueGrouping,"  @Test
  public void testValueGrouping() {
    MultimapView<String, String> view =
        InMemoryMultimapSideInputView.fromIterable(
            StringUtf8Coder.of(),
            ImmutableList.of(KV.of(""A"", ""a1""), KV.of(""A"", ""a2""), KV.of(""B"", ""b1"")));
    assertEquals(view.get(""A""), ImmutableList.of(""a1"", ""a2""));
    assertEquals(view.get(""B""), ImmutableList.of(""b1""));
    assertEquals(view.get(""C""), ImmutableList.of());
  }
",non-flaky,5
78307,apache_beam,SimpleDoFnRunnerTest.testProcessElementExceptionsWrappedAsUserCodeException,"  @Test
  public void testProcessElementExceptionsWrappedAsUserCodeException() {
    ThrowingDoFn fn = new ThrowingDoFn();
    DoFnRunner<String, String> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            null,
            null,
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(new GlobalWindows()));

    thrown.expect(UserCodeException.class);
    thrown.expectCause(is(fn.exceptionToThrow));

    runner.processElement(WindowedValue.valueInGlobalWindow(""anyValue""));
  }
",non-flaky,5
78308,apache_beam,SimpleDoFnRunnerTest.testOnTimerExceptionsWrappedAsUserCodeException,"  @Test
  public void testOnTimerExceptionsWrappedAsUserCodeException() {
    ThrowingDoFn fn = new ThrowingDoFn();
    DoFnRunner<String, String> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            null,
            null,
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(new GlobalWindows()));

    thrown.expect(UserCodeException.class);
    thrown.expectCause(is(fn.exceptionToThrow));

    runner.onTimer(
        ThrowingDoFn.TIMER_ID, GlobalWindow.INSTANCE, new Instant(0), TimeDomain.EVENT_TIME);
  }
",non-flaky,5
78309,apache_beam,SimpleDoFnRunnerTest.testTimerSet,"  @Test
  public void testTimerSet() {
    WindowFn<?, ?> windowFn = new GlobalWindows();
    DoFnWithTimers<GlobalWindow> fn = new DoFnWithTimers(windowFn.windowCoder());
    DoFnRunner<String, String> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            null,
            null,
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(new GlobalWindows()));

    // Setting the timer needs the current time, as it is set relative
    Instant currentTime = new Instant(42);
    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(currentTime);

    runner.processElement(WindowedValue.valueInGlobalWindow(""anyValue""));

    verify(mockTimerInternals)
        .setTimer(
            StateNamespaces.window(new GlobalWindows().windowCoder(), GlobalWindow.INSTANCE),
            DoFnWithTimers.TIMER_ID,
            currentTime.plus(DoFnWithTimers.TIMER_OFFSET),
            TimeDomain.EVENT_TIME);
  }
",non-flaky,5
78310,apache_beam,SimpleDoFnRunnerTest.testStartBundleExceptionsWrappedAsUserCodeException,"  @Test
  public void testStartBundleExceptionsWrappedAsUserCodeException() {
    ThrowingDoFn fn = new ThrowingDoFn();
    DoFnRunner<String, String> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            null,
            null,
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(new GlobalWindows()));

    thrown.expect(UserCodeException.class);
    thrown.expectCause(is(fn.exceptionToThrow));

    runner.startBundle();
  }
",non-flaky,5
78311,apache_beam,SimpleDoFnRunnerTest.testFinishBundleExceptionsWrappedAsUserCodeException,"  @Test
  public void testFinishBundleExceptionsWrappedAsUserCodeException() {
    ThrowingDoFn fn = new ThrowingDoFn();
    DoFnRunner<String, String> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            null,
            null,
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(new GlobalWindows()));

    thrown.expect(UserCodeException.class);
    thrown.expectCause(is(fn.exceptionToThrow));

    runner.finishBundle();
  }
",non-flaky,5
78312,apache_beam,SimpleDoFnRunnerTest.testOnTimerCalled,"  @Test
  public void testOnTimerCalled() {
    WindowFn<?, GlobalWindow> windowFn = new GlobalWindows();
    DoFnWithTimers<GlobalWindow> fn = new DoFnWithTimers(windowFn.windowCoder());
    DoFnRunner<String, String> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            null,
            null,
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(windowFn));

    Instant currentTime = new Instant(42);
    Duration offset = Duration.millis(37);

    // Mocking is not easily compatible with annotation analysis, so we manually record
    // the method call.
    runner.onTimer(
        DoFnWithTimers.TIMER_ID,
        GlobalWindow.INSTANCE,
        currentTime.plus(offset),
        TimeDomain.EVENT_TIME);

    assertThat(
        fn.onTimerInvocations,
        contains(
            TimerData.of(
                DoFnWithTimers.TIMER_ID,
                StateNamespaces.window(windowFn.windowCoder(), GlobalWindow.INSTANCE),
                currentTime.plus(offset),
                TimeDomain.EVENT_TIME)));
  }
",non-flaky,5
78313,apache_beam,SimpleDoFnRunnerTest.testBackwardsInTimeNoSkew,"  @Test
  public void testBackwardsInTimeNoSkew() {
    SkewingDoFn fn = new SkewingDoFn(Duration.ZERO);
    DoFnRunner<Duration, Duration> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            new ListOutputManager(),
            new TupleTag<>(),
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(new GlobalWindows()));

    runner.startBundle();
    // An element output at the current timestamp is fine.
    runner.processElement(
        WindowedValue.timestampedValueInGlobalWindow(Duration.ZERO, new Instant(0)));
    thrown.expect(UserCodeException.class);
    thrown.expectCause(isA(IllegalArgumentException.class));
    thrown.expectMessage(""must be no earlier"");
    thrown.expectMessage(
        String.format(""timestamp of the current input (%s)"", new Instant(0).toString()));
    thrown.expectMessage(
        String.format(
            ""the allowed skew (%s)"", PeriodFormat.getDefault().print(Duration.ZERO.toPeriod())));
    // An element output before (current time - skew) is forbidden
    runner.processElement(
        WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), new Instant(0)));
  }
",non-flaky,5
78314,apache_beam,SimpleDoFnRunnerTest.testSkew,"  @Test
  public void testSkew() {
    SkewingDoFn fn = new SkewingDoFn(Duration.standardMinutes(10L));
    DoFnRunner<Duration, Duration> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            new ListOutputManager(),
            new TupleTag<>(),
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(new GlobalWindows()));

    runner.startBundle();
    // Outputting between ""now"" and ""now - allowed skew"" succeeds.
    runner.processElement(
        WindowedValue.timestampedValueInGlobalWindow(Duration.standardMinutes(5L), new Instant(0)));
    thrown.expect(UserCodeException.class);
    thrown.expectCause(isA(IllegalArgumentException.class));
    thrown.expectMessage(""must be no earlier"");
    thrown.expectMessage(
        String.format(""timestamp of the current input (%s)"", new Instant(0).toString()));
    thrown.expectMessage(
        String.format(
            ""the allowed skew (%s)"",
            PeriodFormat.getDefault().print(Duration.standardMinutes(10L).toPeriod())));
    // Outputting before ""now - allowed skew"" fails.
    runner.processElement(
        WindowedValue.timestampedValueInGlobalWindow(Duration.standardHours(1L), new Instant(0)));
  }
",non-flaky,5
78315,apache_beam,SimpleDoFnRunnerTest.testInfiniteSkew,"  @Test
  public void testInfiniteSkew() {
    SkewingDoFn fn = new SkewingDoFn(Duration.millis(Long.MAX_VALUE));
    DoFnRunner<Duration, Duration> runner =
        new SimpleDoFnRunner<>(
            null,
            fn,
            NullSideInputReader.empty(),
            new ListOutputManager(),
            new TupleTag<>(),
            Collections.emptyList(),
            mockStepContext,
            null,
            Collections.emptyMap(),
            WindowingStrategy.of(new GlobalWindows()));

    runner.startBundle();
    runner.processElement(
        WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), new Instant(0)));
    runner.processElement(
        WindowedValue.timestampedValueInGlobalWindow(
            Duration.millis(1L), BoundedWindow.TIMESTAMP_MIN_VALUE.plus(Duration.millis(1))));
    runner.processElement(
        WindowedValue.timestampedValueInGlobalWindow(
            // This is the maximum amount a timestamp in beam can move (from the maximum timestamp
            // to the minimum timestamp).
            Duration.millis(BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis())
                .minus(Duration.millis(BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis())),
            BoundedWindow.TIMESTAMP_MAX_VALUE));
  }
",non-flaky,5
78316,apache_beam,ReduceFnRunnerTest.testProcessingTimeTimerDoesNotGc,"  @Test
  public void testProcessingTimeTimerDoesNotGc() throws Exception {
    WindowingStrategy<?, IntervalWindow> strategy =
        WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(100)))
            .withTimestampCombiner(TimestampCombiner.EARLIEST)
            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES)
            .withAllowedLateness(Duration.ZERO)
            .withTrigger(
                Repeatedly.forever(
                    AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(10))));

    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());

    tester.advanceProcessingTime(new Instant(5000));
    injectElement(tester, 2); // processing timer @ 5000 + 10; EOW timer @ 100
    injectElement(tester, 5);

    tester.advanceProcessingTime(new Instant(10000));

    tester.assertHasOnlyGlobalAndStateFor(new IntervalWindow(new Instant(0), new Instant(100)));

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(
                equalTo(7), 2, 0, 100, PaneInfo.createPane(true, false, Timing.EARLY, 0, 0))));
  }
",non-flaky,5
78317,apache_beam,ReduceFnRunnerTest.testOnElementBufferingDiscarding,"  @Test
  public void testOnElementBufferingDiscarding() throws Exception {
    // Test basic execution of a trigger using a non-combining window set and discarding mode.
    MetricsContainerImpl container = new MetricsContainerImpl(""any"");
    MetricsEnvironment.setCurrentContainer(container);
    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester =
        ReduceFnTester.nonCombining(
            FixedWindows.of(Duration.millis(10)),
            mockTriggerStateMachine,
            AccumulationMode.DISCARDING_FIRED_PANES,
            Duration.millis(100),
            ClosingBehavior.FIRE_IF_NON_EMPTY);

    // Pane of {1, 2}
    injectElement(tester, 1);
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    injectElement(tester, 2);
    assertThat(
        tester.extractOutput(),
        contains(isSingleWindowedValue(containsInAnyOrder(1, 2), 1, 0, 10)));

    // Pane of just 3, and finish
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    triggerShouldFinish(mockTriggerStateMachine);
    injectElement(tester, 3);
    assertThat(
        tester.extractOutput(), contains(isSingleWindowedValue(containsInAnyOrder(3), 3, 0, 10)));
    assertTrue(tester.isMarkedFinished(firstWindow));
    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);

    // This element shouldn't be seen, because the trigger has finished
    injectElement(tester, 4);

    long droppedElements =
        container
            .getCounter(
                MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW))
            .getCumulative();
    assertEquals(1, droppedElements);
  }
",non-flaky,5
78318,apache_beam,ReduceFnRunnerTest.testOnElementBufferingAccumulating,"  @Test
  public void testOnElementBufferingAccumulating() throws Exception {
    // Test basic execution of a trigger using a non-combining window set and accumulating mode.
    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester =
        ReduceFnTester.nonCombining(
            FixedWindows.of(Duration.millis(10)),
            mockTriggerStateMachine,
            AccumulationMode.ACCUMULATING_FIRED_PANES,
            Duration.millis(100),
            ClosingBehavior.FIRE_IF_NON_EMPTY);

    injectElement(tester, 1);

    // Fires {1, 2}
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    injectElement(tester, 2);

    // Fires {1, 2, 3} because we are in accumulating mode
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    triggerShouldFinish(mockTriggerStateMachine);
    injectElement(tester, 3);

    // This element shouldn't be seen, because the trigger has finished
    injectElement(tester, 4);

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(containsInAnyOrder(1, 2), 1, 0, 10),
            isSingleWindowedValue(containsInAnyOrder(1, 2, 3), 3, 0, 10)));
    assertTrue(tester.isMarkedFinished(firstWindow));
    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);
  }
",non-flaky,5
78319,apache_beam,ReduceFnRunnerTest.testSessionEowAndGcTogether,"  @Test
  public void testSessionEowAndGcTogether() throws Exception {
    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester =
        ReduceFnTester.nonCombining(
            Sessions.withGapDuration(Duration.millis(10)),
            DefaultTriggerStateMachine.of(),
            AccumulationMode.ACCUMULATING_FIRED_PANES,
            Duration.millis(50),
            ClosingBehavior.FIRE_ALWAYS);

    tester.setAutoAdvanceOutputWatermark(true);

    tester.advanceInputWatermark(new Instant(0));
    injectElement(tester, 1);
    tester.advanceInputWatermark(new Instant(100));

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(
                contains(1), 1, 1, 11, PaneInfo.createPane(true, true, Timing.ON_TIME))));
  }
",non-flaky,5
78320,apache_beam,ReduceFnRunnerTest.testFixedWindowsEowAndGcTogether,"  @Test
  public void testFixedWindowsEowAndGcTogether() throws Exception {
    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester =
        ReduceFnTester.nonCombining(
            FixedWindows.of(Duration.millis(10)),
            DefaultTriggerStateMachine.of(),
            AccumulationMode.ACCUMULATING_FIRED_PANES,
            Duration.millis(50),
            ClosingBehavior.FIRE_ALWAYS);

    tester.setAutoAdvanceOutputWatermark(true);

    tester.advanceInputWatermark(new Instant(0));
    injectElement(tester, 1);
    tester.advanceInputWatermark(new Instant(100));

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(
                contains(1), 1, 0, 10, PaneInfo.createPane(true, true, Timing.ON_TIME))));
  }
",non-flaky,5
78321,apache_beam,ReduceFnRunnerTest.testFixedWindowsEowAndGcTogetherFireIfNonEmpty,"  @Test
  public void testFixedWindowsEowAndGcTogetherFireIfNonEmpty() throws Exception {
    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester =
        ReduceFnTester.nonCombining(
            FixedWindows.of(Duration.millis(10)),
            DefaultTriggerStateMachine.of(),
            AccumulationMode.ACCUMULATING_FIRED_PANES,
            Duration.millis(50),
            ClosingBehavior.FIRE_IF_NON_EMPTY);

    tester.setAutoAdvanceOutputWatermark(true);

    tester.advanceInputWatermark(new Instant(0));
    injectElement(tester, 1);
    tester.advanceInputWatermark(new Instant(100));

    List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();
    assertThat(
        output,
        contains(
            isSingleWindowedValue(
                contains(1), 1, 0, 10, PaneInfo.createPane(true, true, Timing.ON_TIME))));
  }
",non-flaky,5
78322,apache_beam,ReduceFnRunnerTest.testOnlyOneOnTimePane,"  @Test
  public void testOnlyOneOnTimePane() throws Exception {
    WindowingStrategy<?, IntervalWindow> strategy =
        WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(10)))
            .withTrigger(DefaultTrigger.of())
            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES)
            .withAllowedLateness(Duration.millis(100));

    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());

    tester.advanceInputWatermark(new Instant(0));

    int value1 = 1;
    int value2 = 3;

    // A single element that should be in the ON_TIME output
    tester.injectElements(TimestampedValue.of(value1, new Instant(1)));

    // Should fire ON_TIME
    tester.advanceInputWatermark(new Instant(10));

    // The DefaultTrigger should cause output labeled LATE, even though it does not have to be
    // labeled as such.
    tester.injectElements(TimestampedValue.of(value2, new Instant(3)));

    List<WindowedValue<Integer>> output = tester.extractOutput();
    assertEquals(2, output.size());

    assertThat(output.get(0), isWindowedValue(equalTo(value1)));
    assertThat(output.get(1), isWindowedValue(equalTo(value1 + value2)));

    assertThat(
        output.get(0),
        WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(true, false, Timing.ON_TIME, 0, 0)));
    assertThat(
        output.get(1),
        WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(false, false, Timing.LATE, 1, 1)));
  }
",non-flaky,5
78323,apache_beam,ReduceFnRunnerTest.testOnElementCombiningDiscarding,"  @Test
  public void testOnElementCombiningDiscarding() throws Exception {
    // Test basic execution of a trigger using a non-combining window set and discarding mode.
    WindowingStrategy<?, IntervalWindow> strategy =
        WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(10)))
            .withTimestampCombiner(TimestampCombiner.EARLIEST)
            .withMode(AccumulationMode.DISCARDING_FIRED_PANES)
            .withAllowedLateness(Duration.millis(100));

    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(
            strategy, mockTriggerStateMachine, Sum.ofIntegers(), VarIntCoder.of());

    injectElement(tester, 2);

    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    injectElement(tester, 3);

    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    triggerShouldFinish(mockTriggerStateMachine);
    injectElement(tester, 4);

    // This element shouldn't be seen, because the trigger has finished
    injectElement(tester, 6);

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(equalTo(5), 2, 0, 10),
            isSingleWindowedValue(equalTo(4), 4, 0, 10)));
    assertTrue(tester.isMarkedFinished(firstWindow));
    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);
  }
",non-flaky,5
78324,apache_beam,ReduceFnRunnerTest.testLateProcessingTimeTimer,"  @Test
  public void testLateProcessingTimeTimer() throws Exception {
    WindowingStrategy<?, IntervalWindow> strategy =
        WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(100)))
            .withTimestampCombiner(TimestampCombiner.EARLIEST)
            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES)
            .withAllowedLateness(Duration.ZERO)
            .withTrigger(
                Repeatedly.forever(
                    AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(10))));

    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());

    tester.advanceProcessingTime(new Instant(5000));
    injectElement(tester, 2); // processing timer @ 5000 + 10; EOW timer @ 100
    injectElement(tester, 5);

    // After this advancement, the window is expired and only the GC process
    // should be allowed to touch it
    tester.advanceInputWatermarkNoTimers(new Instant(100));

    // This should not output
    tester.advanceProcessingTime(new Instant(6000));

    assertThat(tester.extractOutput(), emptyIterable());
  }
",non-flaky,5
78325,apache_beam,ReduceFnRunnerTest.testCombiningAccumulatingProcessingTime,"  @Test
  public void testCombiningAccumulatingProcessingTime() throws Exception {
    WindowingStrategy<?, IntervalWindow> strategy =
        WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(100)))
            .withTimestampCombiner(TimestampCombiner.EARLIEST)
            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES)
            .withAllowedLateness(Duration.ZERO)
            .withTrigger(
                Repeatedly.forever(
                    AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(10))));

    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());

    tester.advanceProcessingTime(new Instant(5000));
    injectElement(tester, 2); // processing timer @ 5000 + 10; EOW timer @ 100
    injectElement(tester, 5);

    tester.advanceInputWatermarkNoTimers(new Instant(100));
    tester.advanceProcessingTimeNoTimers(new Instant(5010));

    // Fires the GC/EOW timer at the same time as the processing time timer.
    tester.fireTimers(
        new IntervalWindow(new Instant(0), new Instant(100)),
        TimestampedValue.of(TimeDomain.EVENT_TIME, new Instant(100)),
        TimestampedValue.of(TimeDomain.PROCESSING_TIME, new Instant(5010)));

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(
                equalTo(7), 2, 0, 100, PaneInfo.createPane(true, true, Timing.ON_TIME, 0, 0))));
  }
",non-flaky,5
78326,apache_beam,ReduceFnRunnerTest.element,"  @Test
  public void testFixedWindowEndOfTimeGarbageCollection() throws Exception {
    Duration allowedLateness = Duration.standardDays(365);
    Duration windowSize = Duration.millis(10);
    WindowFn<Object, IntervalWindow> windowFn = FixedWindows.of(windowSize);

    // This timestamp falls into a window where the end of the window is before the end of the
    // global window - the ""end of time"" - yet its expiration time is after.
    final Instant elementTimestamp =
        GlobalWindow.INSTANCE.maxTimestamp().minus(allowedLateness).plus(1);

    IntervalWindow window =
        Iterables.getOnlyElement(
            windowFn.assignWindows(
                windowFn.new AssignContext() {
                  @Override
                  public Object element() {
                    throw new UnsupportedOperationException();
                  }
",non-flaky,5
78327,apache_beam,ReduceFnRunnerTest.testCombiningAccumulatingProcessingTimeSeparateBundles,"  @Test
  public void testCombiningAccumulatingProcessingTimeSeparateBundles() throws Exception {
    WindowingStrategy<?, IntervalWindow> strategy =
        WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(100)))
            .withTimestampCombiner(TimestampCombiner.EARLIEST)
            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES)
            .withAllowedLateness(Duration.ZERO)
            .withTrigger(
                Repeatedly.forever(
                    AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(10))));

    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());

    tester.advanceProcessingTime(new Instant(5000));
    injectElement(tester, 2); // processing timer @ 5000 + 10; EOW timer @ 100
    injectElement(tester, 5);

    tester.advanceInputWatermark(new Instant(100));
    tester.advanceProcessingTime(new Instant(5011));

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(
                equalTo(7), 2, 0, 100, PaneInfo.createPane(true, true, Timing.ON_TIME, 0, 0))));
  }
",non-flaky,5
78328,apache_beam,ReduceFnRunnerTest.testCombiningAccumulatingEventTime,"  @Test
  public void testCombiningAccumulatingEventTime() throws Exception {
    WindowingStrategy<?, IntervalWindow> strategy =
        WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(100)))
            .withTimestampCombiner(TimestampCombiner.EARLIEST)
            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES)
            .withAllowedLateness(Duration.millis(1))
            .withTrigger(Repeatedly.forever(AfterWatermark.pastEndOfWindow()));

    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());

    injectElement(tester, 2); // processing timer @ 5000 + 10; EOW timer @ 100
    injectElement(tester, 5);

    tester.advanceInputWatermark(new Instant(1000));

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(
                equalTo(7), 2, 0, 100, PaneInfo.createPane(true, true, Timing.ON_TIME, 0, 0))));
  }
",non-flaky,5
78329,apache_beam,ReduceFnRunnerTest.testOnElementCombiningAccumulating,"  @Test
  public void testOnElementCombiningAccumulating() throws Exception {
    // Test basic execution of a trigger using a non-combining window set and accumulating mode.
    WindowingStrategy<?, IntervalWindow> strategy =
        WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(10)))
            .withTimestampCombiner(TimestampCombiner.EARLIEST)
            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES)
            .withAllowedLateness(Duration.millis(100));

    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(
            strategy, mockTriggerStateMachine, Sum.ofIntegers(), VarIntCoder.of());

    injectElement(tester, 1);

    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    injectElement(tester, 2);

    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    triggerShouldFinish(mockTriggerStateMachine);
    injectElement(tester, 3);

    // This element shouldn't be seen, because the trigger has finished
    injectElement(tester, 4);

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(equalTo(3), 1, 0, 10),
            isSingleWindowedValue(equalTo(6), 3, 0, 10)));
    assertTrue(tester.isMarkedFinished(firstWindow));
    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);
  }
",non-flaky,5
78330,apache_beam,ReduceFnRunnerTest.testOnElementCombiningWithContext,"  @Test
  public void testOnElementCombiningWithContext() throws Exception {
    // Create values at timestamps 0 .. 8, windowed into fixed windows of 2.
    // Side input windowed into fixed windows of 4:
    // main: [ 0 1 ] [ 2 3 ] [ 4 5 ] [ 6 7 ]
    // side: [     100     ] [     104     ]
    // Combine using a CombineFn ""side input + sum(main inputs)"".
    final int firstWindowSideInput = 100;
    final int secondWindowSideInput = 104;
    final Integer expectedValue = firstWindowSideInput;
    WindowingStrategy<?, IntervalWindow> mainInputWindowingStrategy =
        WindowingStrategy.of(FixedWindows.of(Duration.millis(2)))
            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);

    WindowMappingFn<?> sideInputWindowMappingFn =
        FixedWindows.of(Duration.millis(4)).getDefaultWindowMappingFn();
    when(mockView.getWindowMappingFn()).thenReturn((WindowMappingFn) sideInputWindowMappingFn);

    TestOptions options = PipelineOptionsFactory.as(TestOptions.class);
    options.setValue(expectedValue);

    when(mockSideInputReader.contains(any(PCollectionView.class))).thenReturn(true);
    when(mockSideInputReader.get(any(PCollectionView.class), any(BoundedWindow.class)))
        .then(
            invocation -> {
              IntervalWindow sideInputWindow = (IntervalWindow) invocation.getArguments()[1];
              long startMs = sideInputWindow.start().getMillis();
              long endMs = sideInputWindow.end().getMillis();
              // Window should have been produced by sideInputWindowingStrategy.
              assertThat(startMs, anyOf(equalTo(0L), equalTo(4L)));
              assertThat(endMs - startMs, equalTo(4L));
              // If startMs == 4 (second window), equal to secondWindowSideInput.
              return firstWindowSideInput + (int) startMs;
            });

    SumAndVerifyContextFn combineFn = new SumAndVerifyContextFn(mockView, expectedValue);
    ReduceFnTester<Integer, Integer, IntervalWindow> tester =
        ReduceFnTester.combining(
            mainInputWindowingStrategy,
            mockTriggerStateMachine,
            combineFn,
            VarIntCoder.of(),
            options,
            mockSideInputReader);

    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    for (int i = 0; i < 8; ++i) {
      injectElement(tester, i);
    }

    assertThat(
        tester.extractOutput(),
        contains(
            isSingleWindowedValue(equalTo(0 + firstWindowSideInput), 1, 0, 2),
            isSingleWindowedValue(equalTo(0 + 1 + firstWindowSideInput), 1, 0, 2),
            isSingleWindowedValue(equalTo(2 + firstWindowSideInput), 3, 2, 4),
            isSingleWindowedValue(equalTo(2 + 3 + firstWindowSideInput), 3, 2, 4),
            isSingleWindowedValue(equalTo(4 + secondWindowSideInput), 5, 4, 6),
            isSingleWindowedValue(equalTo(4 + 5 + secondWindowSideInput), 5, 4, 6),
            isSingleWindowedValue(equalTo(6 + secondWindowSideInput), 7, 6, 8),
            isSingleWindowedValue(equalTo(6 + 7 + secondWindowSideInput), 7, 6, 8)));
  }
",non-flaky,5
78331,apache_beam,ReduceFnRunnerTest.testWatermarkHoldAndLateData,"  @Test
  public void testWatermarkHoldAndLateData() throws Exception {
    MetricsContainerImpl container = new MetricsContainerImpl(""any"");
    MetricsEnvironment.setCurrentContainer(container);
    // Test handling of late data. Specifically, ensure the watermark hold is correct.
    Duration allowedLateness = Duration.millis(10);
    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester =
        ReduceFnTester.nonCombining(
            FixedWindows.of(Duration.millis(10)),
            mockTriggerStateMachine,
            AccumulationMode.ACCUMULATING_FIRED_PANES,
            allowedLateness,
            ClosingBehavior.FIRE_IF_NON_EMPTY);

    // Input watermark -> null
    assertEquals(null, tester.getWatermarkHold());
    assertEquals(null, tester.getOutputWatermark());

    // All on time data, verify watermark hold.
    IntervalWindow expectedWindow = new IntervalWindow(new Instant(0), new Instant(10));
    injectElement(tester, 1);
    injectElement(tester, 3);
    assertEquals(new Instant(1), tester.getWatermarkHold());
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    injectElement(tester, 2);
    List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();
    assertThat(
        output,
        contains(
            isSingleWindowedValue(
                containsInAnyOrder(1, 2, 3),
                equalTo(new Instant(1)),
                equalTo((BoundedWindow) expectedWindow))));
    assertThat(
        output.get(0).getPane(), equalTo(PaneInfo.createPane(true, false, Timing.EARLY, 0, -1)));

    // There is no end-of-window hold, but the timer set by the trigger holds the watermark
    assertThat(tester.getWatermarkHold(), nullValue());

    // Nothing dropped.
    long droppedElements =
        container
            .getCounter(
                MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW))
            .getCumulative();
    assertEquals(0, droppedElements);

    // Input watermark -> 4, output watermark should advance that far as well
    tester.advanceInputWatermark(new Instant(4));
    assertEquals(new Instant(4), tester.getOutputWatermark());

    // Some late, some on time. Verify that we only hold to the minimum of on-time.
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(false);
    tester.advanceInputWatermark(new Instant(4));
    injectElement(tester, 2);
    injectElement(tester, 3);

    // Late data has arrived behind the _output_ watermark. The ReduceFnRunner sets a GC hold
    // since this data is not permitted to hold up the output watermark.
    assertThat(
        tester.getWatermarkHold(), equalTo(expectedWindow.maxTimestamp().plus(allowedLateness)));

    // Now data just ahead of the output watermark arrives and sets an earlier ""element"" hold
    injectElement(tester, 5);
    assertEquals(new Instant(5), tester.getWatermarkHold());

    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);
    injectElement(tester, 4);
    output = tester.extractOutput();
    assertThat(
        output,
        contains(
            isSingleWindowedValue(
                containsInAnyOrder(
                    1, 2, 3, // earlier firing
                    2, 3, 4, 5), // new elements
                4, // timestamp
                0, // window start
                10))); // window end
    assertThat(
        output.get(0).getPane(), equalTo(PaneInfo.createPane(false, false, Timing.EARLY, 1, -1)));

    // Since the element hold is cleared, there is no hold remaining
    assertThat(tester.getWatermarkHold(), nullValue());

    // All behind the output watermark -- hold is at GC time (if we imagine the
    // trigger sets a timer for ON_TIME firing, that is actually when they'll be emitted)
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(false);
    tester.advanceInputWatermark(new Instant(8));
    injectElement(tester, 6);
    injectElement(tester, 5);
    assertThat(
        tester.getWatermarkHold(), equalTo(expectedWindow.maxTimestamp().plus(allowedLateness)));

    injectElement(tester, 4);

    // Fire the ON_TIME pane
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);

    // To get an ON_TIME pane, we need the output watermark to be held back a little; this would
    // be done by way of the timers set by the trigger, which are mocked here
    tester.setAutoAdvanceOutputWatermark(false);

    tester.advanceInputWatermark(expectedWindow.maxTimestamp().plus(1));
    tester.fireTimer(expectedWindow, expectedWindow.maxTimestamp(), TimeDomain.EVENT_TIME);

    // Output time is end of the window, because all the new data was late, but the pane
    // is the ON_TIME pane.
    output = tester.extractOutput();
    assertThat(
        output,
        contains(
            isSingleWindowedValue(
                containsInAnyOrder(
                    1, 2, 3, // earlier firing
                    2, 3, 4, 5, // earlier firing
                    4, 5, 6), // new elements
                9, // timestamp
                0, // window start
                10))); // window end
    assertThat(
        output.get(0).getPane(), equalTo(PaneInfo.createPane(false, false, Timing.ON_TIME, 2, 0)));

    tester.setAutoAdvanceOutputWatermark(true);

    // This is ""pending"" at the time the watermark makes it way-late.
    // Because we're about to expire the window, we output it.
    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(false);
    injectElement(tester, 8);
    droppedElements =
        container
            .getCounter(
                MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW))
            .getCumulative();
    assertEquals(0, droppedElements);

    // Exceed the GC limit, triggering the last pane to be fired
    tester.advanceInputWatermark(new Instant(50));
    output = tester.extractOutput();
    // Output time is still end of the window, because the new data (8) was behind
    // the output watermark.
    assertThat(
        output,
        contains(
            isSingleWindowedValue(
                containsInAnyOrder(
                    1, 2, 3, // earlier firing
                    2, 3, 4, 5, // earlier firing
                    4, 5, 6, // earlier firing
                    8), // new element prior to window becoming expired
                9, // timestamp
                0, // window start
                10))); // window end
    assertThat(
        output.get(0).getPane(), equalTo(PaneInfo.createPane(false, true, Timing.LATE, 3, 1)));
    assertEquals(new Instant(50), tester.getOutputWatermark());
    assertEquals(null, tester.getWatermarkHold());

    // Late timers are ignored
    tester.fireTimer(
        new IntervalWindow(new Instant(0), new Instant(10)),
        new Instant(12),
        TimeDomain.EVENT_TIME);

    // And because we're past the end of window + allowed lateness, everything should be cleaned up.
    assertFalse(tester.isMarkedFinished(firstWindow));
    tester.assertHasOnlyGlobalAndFinishedSetsFor();
  }
",non-flaky,5
33,apache_kafka,shouldTogglePrepareForBulkLoadDuringRestoreCalls,"@Test
public void shouldTogglePrepareForBulkLoadDuringRestoreCalls() throws Exception {
    final List<KeyValue<byte[], byte[]>> entries = new ArrayList<>();
    entries.add(new KeyValue<>(""1"".getBytes(""UTF-8""), ""a"".getBytes(""UTF-8"")));
    entries.add(new KeyValue<>(""2"".getBytes(""UTF-8""), ""b"".getBytes(""UTF-8"")));
    entries.add(new KeyValue<>(""3"".getBytes(""UTF-8""), ""c"".getBytes(""UTF-8"")));
    final AtomicReference<Exception> conditionNotMet = new AtomicReference<>();
    final AtomicInteger conditionCheckCount = new AtomicInteger();
    Thread conditionCheckThread = new Thread(new Runnable() {
        @Override
        public void run() {
            assertRocksDBTurnsOnBulkLoading(conditionCheckCount, conditionNotMet);
            assertRockDBTurnsOffBulkLoad(conditionCheckCount, conditionNotMet);
        }
    });
    subject.init(context, subject);
    conditionCheckThread.start();
    context.restore(subject.name(), entries);
    conditionCheckThread.join(2000);
    assertTrue(conditionNotMet.get() == null);
    assertTrue(conditionCheckCount.get() == 2);
}",concurrency,1
206,apache_kafka,testGracefulClose,"@Test
public void testGracefulClose() throws Exception {
    int maxReceiveCountAfterClose = 0;
    for (int i = 6; i <= 100 && maxReceiveCountAfterClose < 5; i++) {
        int receiveCount = 0;
        KafkaChannel channel = createConnectionWithPendingReceives(i);
        selector.poll(1000);
        assertEquals(1, selector.completedReceives().size());
        server.closeConnections();
        while (selector.disconnected().isEmpty()) {
            selector.poll(1);
            receiveCount += selector.completedReceives().size();
            assertTrue(""Too many completed receives in one poll"", selector.completedReceives().size() <= 1);
        }
        assertEquals(channel.id(), selector.disconnected().keySet().iterator().next());
        maxReceiveCountAfterClose = Math.max(maxReceiveCountAfterClose, receiveCount);
    }
    assertTrue(""Too few receives after close: "" + maxReceiveCountAfterClose, maxReceiveCountAfterClose >= 5);
}",async wait,0
309,apache_kafka,testForceMetadataRefreshForPatternSubscriptionDuringRebalance,"@Test
public void testForceMetadataRefreshForPatternSubscriptionDuringRebalance() {
    final String consumerId = ""consumer"";
    subscriptions.subscribe(Pattern.compile("".*""), rebalanceListener);
    client.updateMetadata(TestUtils.metadataUpdateWith(1, singletonMap(topic1, 1)));
    assertEquals(singleton(topic1), subscriptions.subscription());
    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));
    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));
    client.prepareMetadataUpdate(metadataResponse);
    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, ""leader"", Errors.NONE));
    client.prepareResponse(new MockClient.RequestMatcher() {
        @Override
        public boolean matches(AbstractRequest body) {
            SyncGroupRequest sync = (SyncGroupRequest) body;
            return sync.memberId().equals(consumerId) &&
            sync.generationId() == 1 &&
            sync.groupAssignment().isEmpty();
        }
    }, syncGroupResponse(singletonList(t1p), Errors.NONE));
    partitionAssignor.prepare(singletonMap(consumerId, singletonList(t1p)));
    coordinator.poll(time.timer(Long.MAX_VALUE));
    final Set<String> updatedSubscriptionSet = new HashSet<>(Arrays.asList(topic1, topic2));
    assertEquals(updatedSubscriptionSet, subscriptions.subscription());
    metadata.requestUpdate();
    client.poll(Long.MAX_VALUE, time.milliseconds());
    assertFalse(coordinator.rejoinNeededOrPending());
}",concurrency,1
70765,apache_kafka,StartAndStopCounterTest.shouldRecordStarts,"    @Test
    public void shouldRecordStarts() {
        assertEquals(0, counter.starts());
        counter.recordStart();
        assertEquals(1, counter.starts());
        counter.recordStart();
        assertEquals(2, counter.starts());
        assertEquals(2, counter.starts());
    }
",non-flaky,5
70766,apache_kafka,StartAndStopCounterTest.shouldRecordStops,"    @Test
    public void shouldRecordStops() {
        assertEquals(0, counter.stops());
        counter.recordStop();
        assertEquals(1, counter.stops());
        counter.recordStop();
        assertEquals(2, counter.stops());
        assertEquals(2, counter.stops());
    }
",non-flaky,5
70767,apache_kafka,StartAndStopCounterTest.shouldExpectRestarts,"    @Test
    public void shouldExpectRestarts() throws Exception {
        waiters = Executors.newSingleThreadExecutor();

        latch = counter.expectedRestarts(1);
        Future<Boolean> future = asyncAwait(100, TimeUnit.MILLISECONDS);

        clock.sleep(1000);
        counter.recordStop();
        counter.recordStart();
        assertTrue(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
",non-flaky,5
70768,apache_kafka,StartAndStopCounterTest.shouldFailToWaitForRestartThatNeverHappens,"    @Test
    public void shouldFailToWaitForRestartThatNeverHappens() throws Exception {
        waiters = Executors.newSingleThreadExecutor();

        latch = counter.expectedRestarts(1);
        Future<Boolean> future = asyncAwait(100, TimeUnit.MILLISECONDS);

        clock.sleep(1000);
        // Record a stop but NOT a start
        counter.recordStop();
        assertFalse(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
",non-flaky,5
70769,apache_kafka,ConnectWorkerIntegrationTest.testAddAndRemoveWorker,"    @Test
    public void testAddAndRemoveWorker() throws Exception {
        connect = connectBuilder.build();
        // start the clusters
        connect.start();

        int numTasks = 4;
        // create test topic
        connect.kafka().createTopic(""test-topic"", NUM_TOPIC_PARTITIONS);

        // setup up props for the sink connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(numTasks));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Initial group of workers did not start in time."");

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        WorkerHandle extraWorker = connect.addWorker();

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS + 1).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Expanded group of workers did not start in time."");

        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks are not all in running state."");

        Set<WorkerHandle> workers = connect.activeWorkers();
        assertTrue(workers.contains(extraWorker));

        connect.removeWorker(extraWorker);

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false) && !assertWorkersUp(NUM_WORKERS + 1).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Group of workers did not shrink in time."");

        workers = connect.activeWorkers();
        assertFalse(workers.contains(extraWorker));
    }
",non-flaky,5
70770,apache_kafka,ConnectWorkerIntegrationTest.testRestartFailedTask,"    @Test
    public void testRestartFailedTask() throws Exception {
        connect = connectBuilder.build();
        // start the clusters
        connect.start();

        int numTasks = 1;

        // Properties for the source connector. The task should fail at startup due to the bad broker address.
        Map<String, String> connectorProps = new HashMap<>();
        connectorProps.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getName());
        connectorProps.put(TASKS_MAX_CONFIG, Objects.toString(numTasks));
        connectorProps.put(CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX + BOOTSTRAP_SERVERS_CONFIG, ""nobrokerrunningatthisaddress"");

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Initial group of workers did not start in time."");

        // Try to start the connector and its single task.
        connect.configureConnector(CONNECTOR_NAME, connectorProps);

        waitForCondition(() -> assertConnectorTasksFailed(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not fail in time"");

        // Reconfigure the connector without the bad broker address.
        connectorProps.remove(CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX + BOOTSTRAP_SERVERS_CONFIG);
        connect.configureConnector(CONNECTOR_NAME, connectorProps);

        // Restart the failed task
        String taskRestartEndpoint = connect.endpointForResource(
            String.format(""connectors/%s/tasks/0/restart"", CONNECTOR_NAME));
        connect.executePost(taskRestartEndpoint, """", Collections.emptyMap());

        // Ensure the task started successfully this time
        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
            CONNECTOR_SETUP_DURATION_MS, ""Connector tasks are not all in running state."");
    }
",non-flaky,5
70771,apache_kafka,ConnectWorkerIntegrationTest.testBrokerCoordinator,"    @Test
    public void testBrokerCoordinator() throws Exception {
        workerProps.put(DistributedConfig.SCHEDULED_REBALANCE_MAX_DELAY_MS_CONFIG, String.valueOf(5000));
        connect = connectBuilder.workerProps(workerProps).build();
        // start the clusters
        connect.start();
        int numTasks = 4;
        // create test topic
        connect.kafka().createTopic(""test-topic"", NUM_TOPIC_PARTITIONS);

        // setup up props for the sink connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(numTasks));
        props.put(""topic"", ""test-topic"");
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Initial group of workers did not start in time."");

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        connect.kafka().stopOnlyKafka();

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Group of workers did not remain the same after broker shutdown"");

        // Allow for the workers to discover that the coordinator is unavailable, wait is
        // heartbeat timeout * 2 + 4sec
        Thread.sleep(TimeUnit.SECONDS.toMillis(10));

        connect.kafka().startOnlyKafkaOnSamePorts();

        // Allow for the kafka brokers to come back online
        Thread.sleep(TimeUnit.SECONDS.toMillis(10));

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Group of workers did not remain the same within the ""
                        + ""designated time."");

        // Allow for the workers to rebalance and reach a steady state
        Thread.sleep(TimeUnit.SECONDS.toMillis(10));

        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");
    }
",non-flaky,5
70772,apache_kafka,RebalanceSourceConnectorsIntegrationTest.testStartTwoConnectors,"    @Test
    public void testStartTwoConnectors() throws Exception {
        // create test topic
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        // start a source connector
        connect.configureConnector(""another-source"", props);

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        waitForCondition(() -> this.assertConnectorAndTasksRunning(""another-source"", 4).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");
    }
",non-flaky,5
70773,apache_kafka,RebalanceSourceConnectorsIntegrationTest.testReconfigConnector,"    @Test
    public void testReconfigConnector() throws Exception {
        ConnectorHandle connectorHandle = RuntimeHandles.get().connectorHandle(CONNECTOR_NAME);

        // create test topic
        String anotherTopic = ""another-topic"";
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);
        connect.kafka().createTopic(anotherTopic, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        int numRecordsProduced = 100;
        long recordTransferDurationMs = TimeUnit.SECONDS.toMillis(30);

        // consume all records from the source topic or fail, to ensure that they were correctly produced
        int recordNum = connect.kafka().consume(numRecordsProduced, recordTransferDurationMs, TOPIC_NAME).count();
        assertTrue(""Not enough records produced by source connector. Expected at least: "" + numRecordsProduced + "" + but got "" + recordNum,
                recordNum >= numRecordsProduced);

        // expect that we're going to restart the connector and its tasks
        StartAndStopLatch restartLatch = connectorHandle.expectedStarts(1);

        // Reconfigure the source connector by changing the Kafka topic used as output
        props.put(TOPIC_CONFIG, anotherTopic);
        connect.configureConnector(CONNECTOR_NAME, props);

        // Wait for the connector *and tasks* to be restarted
        assertTrue(""Failed to alter connector configuration and see connector and tasks restart ""
                   + ""within "" + CONNECTOR_SETUP_DURATION_MS + ""ms"",
                restartLatch.await(CONNECTOR_SETUP_DURATION_MS, TimeUnit.MILLISECONDS));

        // And wait for the Connect to show the connectors and tasks are running
        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        // consume all records from the source topic or fail, to ensure that they were correctly produced
        recordNum = connect.kafka().consume(numRecordsProduced, recordTransferDurationMs, anotherTopic).count();
        assertTrue(""Not enough records produced by source connector. Expected at least: "" + numRecordsProduced + "" + but got "" + recordNum,
                recordNum >= numRecordsProduced);
    }
",non-flaky,5
70774,apache_kafka,RebalanceSourceConnectorsIntegrationTest.testDeleteConnector,"    @Test
    public void testDeleteConnector() throws Exception {
        // create test topic
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> this.assertWorkersUp(3),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        // start a source connector
        IntStream.range(0, 4).forEachOrdered(
            i -> {
                try {
                    connect.configureConnector(CONNECTOR_NAME + i, props);
                } catch (IOException e) {
                    throw new ConnectException(e);
                }
            });

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(true),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        // delete connector
        connect.deleteConnector(CONNECTOR_NAME + 3);

        waitForCondition(() -> !this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(true),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not stop in time."");

        waitForCondition(this::assertConnectorAndTasksAreUnique,
                WORKER_SETUP_DURATION_MS, ""Connect and tasks are imbalanced between the workers."");
    }
",non-flaky,5
70775,apache_kafka,RebalanceSourceConnectorsIntegrationTest.testAddingWorker,"    @Test
    public void testAddingWorker() throws Exception {
        // create test topic
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> this.assertWorkersUp(3),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        // start a source connector
        IntStream.range(0, 4).forEachOrdered(
            i -> {
                try {
                    connect.configureConnector(CONNECTOR_NAME + i, props);
                } catch (IOException e) {
                    throw new ConnectException(e);
                }
            });

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        connect.addWorker();

        waitForCondition(() -> this.assertWorkersUp(4),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        waitForCondition(this::assertConnectorAndTasksAreUnique,
                WORKER_SETUP_DURATION_MS, ""Connect and tasks are imbalanced between the workers."");
    }
",non-flaky,5
70776,apache_kafka,RebalanceSourceConnectorsIntegrationTest.testRemovingWorker,"    @Test
    public void testRemovingWorker() throws Exception {
        // create test topic
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> this.assertWorkersUp(3),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        // start a source connector
        IntStream.range(0, 4).forEachOrdered(
            i -> {
                try {
                    connect.configureConnector(CONNECTOR_NAME + i, props);
                } catch (IOException e) {
                    throw new ConnectException(e);
                }
            });

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        connect.removeWorker();

        waitForCondition(() -> this.assertWorkersUp(2),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        waitForCondition(this::assertConnectorAndTasksAreUnique,
                WORKER_SETUP_DURATION_MS, ""Connect and tasks are imbalanced between the workers."");
    }
",non-flaky,5
70777,apache_kafka,SessionedProtocolIntegrationTest.ensureInternalEndpointIsSecured,"    @Test
    public void ensureInternalEndpointIsSecured() throws Throwable {
        final String connectorTasksEndpoint = connect.endpointForResource(String.format(
            ""connectors/%s/tasks"",
            CONNECTOR_NAME
        ));
        final Map<String, String> emptyHeaders = new HashMap<>();
        final Map<String, String> invalidSignatureHeaders = new HashMap<>();
        invalidSignatureHeaders.put(SIGNATURE_HEADER, ""S2Fma2Flc3F1ZQ=="");
        invalidSignatureHeaders.put(SIGNATURE_ALGORITHM_HEADER, ""HmacSHA256"");

        // We haven't created the connector yet, but this should still return a 400 instead of a 404
        // if the endpoint is secured
        log.info(
            ""Making a POST request to the {} endpoint with no connector started and no signature header; "" 
                + ""expecting 400 error response"",
            connectorTasksEndpoint
        );
        assertEquals(
            BAD_REQUEST.getStatusCode(),
            connect.executePost(connectorTasksEndpoint, ""[]"", emptyHeaders)
        );

        // Try again, but with an invalid signature
        log.info(
            ""Making a POST request to the {} endpoint with no connector started and an invalid signature header; ""
                + ""expecting 403 error response"",
            connectorTasksEndpoint
        );
        assertEquals(
            FORBIDDEN.getStatusCode(),
            connect.executePost(connectorTasksEndpoint, ""[]"", invalidSignatureHeaders)
        );

        // Create the connector now
        // setup up props for the sink connector
        Map<String, String> connectorProps = new HashMap<>();
        connectorProps.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
        connectorProps.put(TASKS_MAX_CONFIG, String.valueOf(1));
        connectorProps.put(TOPICS_CONFIG, ""test-topic"");
        connectorProps.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        connectorProps.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // start a sink connector
        log.info(""Starting the {} connector"", CONNECTOR_NAME);
        StartAndStopLatch startLatch = connectorHandle.expectedStarts(1);
        connect.configureConnector(CONNECTOR_NAME, connectorProps);
        startLatch.await(CONNECTOR_SETUP_DURATION_MS, TimeUnit.MILLISECONDS);


        // Verify the exact same behavior, after starting the connector

        // We haven't created the connector yet, but this should still return a 400 instead of a 404
        // if the endpoint is secured
        log.info(
            ""Making a POST request to the {} endpoint with the connector started and no signature header; ""
                + ""expecting 400 error response"",
            connectorTasksEndpoint
        );
        assertEquals(
            BAD_REQUEST.getStatusCode(),
            connect.executePost(connectorTasksEndpoint, ""[]"", emptyHeaders)
        );

        // Try again, but with an invalid signature
        log.info(
            ""Making a POST request to the {} endpoint with the connector started and an invalid signature header; ""
                + ""expecting 403 error response"",
            connectorTasksEndpoint
        );
        assertEquals(
            FORBIDDEN.getStatusCode(),
            connect.executePost(connectorTasksEndpoint, ""[]"", invalidSignatureHeaders)
        );
    }
",non-flaky,5
70778,apache_kafka,ErrorHandlingIntegrationTest.testSkipRetryAndDLQWithHeaders,"    @Test
    public void testSkipRetryAndDLQWithHeaders() throws Exception {
        // create test topic
        connect.kafka().createTopic(""test-topic"");

        // setup connector config
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(TOPICS_CONFIG, ""test-topic"");
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(TRANSFORMS_CONFIG, ""failing_transform"");
        props.put(""transforms.failing_transform.type"", FaultyPassthrough.class.getName());

        // log all errors, along with message metadata
        props.put(ERRORS_LOG_ENABLE_CONFIG, ""true"");
        props.put(ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, ""true"");

        // produce bad messages into dead letter queue
        props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);
        props.put(DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, ""true"");
        props.put(DLQ_TOPIC_REPLICATION_FACTOR_CONFIG, ""1"");

        // tolerate all erros
        props.put(ERRORS_TOLERANCE_CONFIG, ""all"");

        // retry for up to one second
        props.put(ERRORS_RETRY_TIMEOUT_CONFIG, ""1000"");

        // set expected records to successfully reach the task
        connectorHandle.taskHandle(TASK_ID).expectedRecords(EXPECTED_CORRECT_RECORDS);

        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(this::checkForPartitionAssignment,
                CONNECTOR_SETUP_DURATION_MS,
                ""Connector task was not assigned a partition."");

        // produce some strings into test topic
        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {
            connect.kafka().produce(""test-topic"", ""key-"" + i, ""value-"" + i);
        }

        // consume all records from test topic
        log.info(""Consuming records from test topic"");
        int i = 0;
        for (ConsumerRecord<byte[], byte[]> rec : connect.kafka().consume(NUM_RECORDS_PRODUCED, CONSUME_MAX_DURATION_MS, ""test-topic"")) {
            String k = new String(rec.key());
            String v = new String(rec.value());
            log.debug(""Consumed record (key='{}', value='{}') from topic {}"", k, v, rec.topic());
            assertEquals(""Unexpected key"", k, ""key-"" + i);
            assertEquals(""Unexpected value"", v, ""value-"" + i);
            i++;
        }

        // wait for records to reach the task
        connectorHandle.taskHandle(TASK_ID).awaitRecords(CONSUME_MAX_DURATION_MS);

        // consume failed records from dead letter queue topic
        log.info(""Consuming records from test topic"");
        ConsumerRecords<byte[], byte[]> messages = connect.kafka().consume(EXPECTED_INCORRECT_RECORDS, CONSUME_MAX_DURATION_MS, DLQ_TOPIC);
        for (ConsumerRecord<byte[], byte[]> recs : messages) {
            log.debug(""Consumed record (key={}, value={}) from dead letter queue topic {}"",
                    new String(recs.key()), new String(recs.value()), DLQ_TOPIC);
            assertTrue(recs.headers().toArray().length > 0);
            assertValue(""test-topic"", recs.headers(), ERROR_HEADER_ORIG_TOPIC);
            assertValue(RetriableException.class.getName(), recs.headers(), ERROR_HEADER_EXCEPTION);
            assertValue(""Error when value='value-7'"", recs.headers(), ERROR_HEADER_EXCEPTION_MESSAGE);
        }

        connect.deleteConnector(CONNECTOR_NAME);
    }
",non-flaky,5
70779,apache_kafka,ConnectorClientPolicyIntegrationTest.testCreateWithOverridesForNonePolicy,"    @Test
    public void testCreateWithOverridesForNonePolicy() throws Exception {
        Map<String, String> props = basicConnectorConfig();
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + SaslConfigs.SASL_JAAS_CONFIG, ""sasl"");
        assertFailCreateConnector(""None"", props);
    }
",non-flaky,5
70780,apache_kafka,ConnectorClientPolicyIntegrationTest.testCreateWithNotAllowedOverridesForPrincipalPolicy,"    @Test
    public void testCreateWithNotAllowedOverridesForPrincipalPolicy() throws Exception {
        Map<String, String> props = basicConnectorConfig();
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + SaslConfigs.SASL_JAAS_CONFIG, ""sasl"");
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""latest"");
        assertFailCreateConnector(""Principal"", props);
    }
",non-flaky,5
70781,apache_kafka,ConnectorClientPolicyIntegrationTest.testCreateWithAllowedOverridesForPrincipalPolicy,"    @Test
    public void testCreateWithAllowedOverridesForPrincipalPolicy() throws Exception {
        Map<String, String> props = basicConnectorConfig();
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, ""PLAIN"");
        assertPassCreateConnector(""Principal"", props);
    }
",non-flaky,5
70782,apache_kafka,ConnectorClientPolicyIntegrationTest.testCreateWithAllowedOverridesForAllPolicy,"    @Test
    public void testCreateWithAllowedOverridesForAllPolicy() throws Exception {
        // setup up props for the sink connector
        Map<String, String> props = basicConnectorConfig();
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + CommonClientConfigs.CLIENT_ID_CONFIG, ""test"");
        assertPassCreateConnector(""All"", props);
    }
",non-flaky,5
70783,apache_kafka,StartAndStopLatchTest.shouldReturnFalseWhenAwaitingForStartToNeverComplete,"    @Test
    public void shouldReturnFalseWhenAwaitingForStartToNeverComplete() throws Throwable {
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);
        future = asyncAwait(100);
        clock.sleep(10);
        assertFalse(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
",non-flaky,5
70784,apache_kafka,StartAndStopLatchTest.shouldReturnFalseWhenAwaitingForStopToNeverComplete,"    @Test
    public void shouldReturnFalseWhenAwaitingForStopToNeverComplete() throws Throwable {
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);
        future = asyncAwait(100);
        latch.recordStart();
        clock.sleep(10);
        assertFalse(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
",non-flaky,5
70785,apache_kafka,StartAndStopLatchTest.shouldReturnTrueWhenAwaitingForStartAndStopToComplete,"    @Test
    public void shouldReturnTrueWhenAwaitingForStartAndStopToComplete() throws Throwable {
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);
        future = asyncAwait(100);
        latch.recordStart();
        latch.recordStop();
        clock.sleep(10);
        assertTrue(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
",non-flaky,5
70786,apache_kafka,StartAndStopLatchTest.shouldReturnFalseWhenAwaitingForDependentLatchToComplete,"    @Test
    public void shouldReturnFalseWhenAwaitingForDependentLatchToComplete() throws Throwable {
        StartAndStopLatch depLatch = new StartAndStopLatch(1, 1, this::complete, null, clock);
        dependents = Collections.singletonList(depLatch);
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);

        future = asyncAwait(100);
        latch.recordStart();
        latch.recordStop();
        clock.sleep(10);
        assertFalse(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
",non-flaky,5
70787,apache_kafka,StartAndStopLatchTest.shouldReturnTrueWhenAwaitingForStartAndStopAndDependentLatch,"    @Test
    public void shouldReturnTrueWhenAwaitingForStartAndStopAndDependentLatch() throws Throwable {
        StartAndStopLatch depLatch = new StartAndStopLatch(1, 1, this::complete, null, clock);
        dependents = Collections.singletonList(depLatch);
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);

        future = asyncAwait(100);
        latch.recordStart();
        latch.recordStop();
        depLatch.recordStart();
        depLatch.recordStop();
        clock.sleep(10);
        assertTrue(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
",non-flaky,5
70788,apache_kafka,ExampleConnectIntegrationTest.testSinkConnector,"    @Test
    public void testSinkConnector() throws Exception {
        // create test topic
        connect.kafka().createTopic(""test-topic"", NUM_TOPIC_PARTITIONS);

        // setup up props for the sink connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(TOPICS_CONFIG, ""test-topic"");
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // expect all records to be consumed by the connector
        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);

        // expect all records to be consumed by the connector
        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);

        // start a sink connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(this::checkForPartitionAssignment,
                CONNECTOR_SETUP_DURATION_MS,
                ""Connector tasks were not assigned a partition each."");

        // produce some messages into source topic partitions
        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {
            connect.kafka().produce(""test-topic"", i % NUM_TOPIC_PARTITIONS, ""key"", ""simple-message-value-"" + i);
        }

        // consume all records from the source topic or fail, to ensure that they were correctly produced.
        assertEquals(""Unexpected number of records consumed"", NUM_RECORDS_PRODUCED,
                connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, ""test-topic"").count());

        // wait for the connector tasks to consume all records.
        connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);

        // wait for the connector tasks to commit all records.
        connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);

        // delete connector
        connect.deleteConnector(CONNECTOR_NAME);
    }
",non-flaky,5
70789,apache_kafka,ExampleConnectIntegrationTest.testSourceConnector,"    @Test
    public void testSourceConnector() throws Exception {
        // create test topic
        connect.kafka().createTopic(""test-topic"", NUM_TOPIC_PARTITIONS);

        // setup up props for the sink connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""topic"", ""test-topic"");
        props.put(""throughput"", String.valueOf(500));
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // expect all records to be produced by the connector
        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);

        // expect all records to be produced by the connector
        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        // wait for the connector tasks to produce enough records
        connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);

        // wait for the connector tasks to commit enough records
        connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);

        // consume all records from the source topic or fail, to ensure that they were correctly produced
        int recordNum = connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, ""test-topic"").count();
        assertTrue(""Not enough records produced by source connector. Expected at least: "" + NUM_RECORDS_PRODUCED + "" + but got "" + recordNum,
                recordNum >= NUM_RECORDS_PRODUCED);

        // delete connector
        connect.deleteConnector(CONNECTOR_NAME);
    }
",non-flaky,5
70790,apache_kafka,RestExtensionIntegrationTest.testRestExtensionApi,"    @Test
    public void testRestExtensionApi() throws IOException, InterruptedException {
        // setup Connect worker properties
        Map<String, String> workerProps = new HashMap<>();
        workerProps.put(REST_EXTENSION_CLASSES_CONFIG, IntegrationTestRestExtension.class.getName());

        // build a Connect cluster backed by Kafka and Zk
        connect = new EmbeddedConnectCluster.Builder()
            .name(""connect-cluster"")
            .numWorkers(1)
            .numBrokers(1)
            .workerProps(workerProps)
            .build();

        // start the clusters
        connect.start();

        WorkerHandle worker = connect.workers().stream()
            .findFirst()
            .orElseThrow(() -> new AssertionError(""At least one worker handle should be available""));

        waitForCondition(
            this::extensionIsRegistered,
            REST_EXTENSION_REGISTRATION_TIMEOUT_MS,
            ""REST extension was never registered""
        );

        ConnectorHandle connectorHandle = RuntimeHandles.get().connectorHandle(""test-conn"");
        try {
            // setup up props for the connector
            Map<String, String> connectorProps = new HashMap<>();
            connectorProps.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
            connectorProps.put(TASKS_MAX_CONFIG, String.valueOf(1));
            connectorProps.put(TOPICS_CONFIG, ""test-topic"");

            // start a connector
            connectorHandle.taskHandle(connectorHandle.name() + ""-0"");
            StartAndStopLatch connectorStartLatch = connectorHandle.expectedStarts(1);
            connect.configureConnector(connectorHandle.name(), connectorProps);
            connectorStartLatch.await(CONNECTOR_HEALTH_AND_CONFIG_TIMEOUT_MS, TimeUnit.MILLISECONDS);

            String workerId = String.format(""%s:%d"", worker.url().getHost(), worker.url().getPort());
            ConnectorHealth expectedHealth = new ConnectorHealth(
                connectorHandle.name(),
                new ConnectorState(
                    ""RUNNING"",
                    workerId,
                    null
                ),
                Collections.singletonMap(
                    0,
                    new TaskState(0, ""RUNNING"", workerId, null)
                ),
                ConnectorType.SINK
            );

            connectorProps.put(NAME_CONFIG, connectorHandle.name());

            // Test the REST extension API; specifically, that the connector's health and configuration
            // are available to the REST extension we registered and that they contain expected values
            waitForCondition(
                () -> verifyConnectorHealthAndConfig(connectorHandle.name(), expectedHealth, connectorProps),
                CONNECTOR_HEALTH_AND_CONFIG_TIMEOUT_MS,
                ""Connector health and/or config was never accessible by the REST extension""
            );
        } finally {
            RuntimeHandles.get().deleteConnector(connectorHandle.name());
        }
    }
",non-flaky,5
70791,apache_kafka,DelegatingClassLoaderTest.testWhiteListedManifestResources,"    @Test
    public void testWhiteListedManifestResources() {
        assertTrue(
            DelegatingClassLoader.serviceLoaderManifestForPlugin(""META-INF/services/org.apache.kafka.connect.rest.ConnectRestExtension""));
        assertTrue(
            DelegatingClassLoader.serviceLoaderManifestForPlugin(""META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider""));
    }
",non-flaky,5
70792,apache_kafka,DelegatingClassLoaderTest.testOtherResources,"    @Test
    public void testOtherResources() {
        assertFalse(
            DelegatingClassLoader.serviceLoaderManifestForPlugin(""META-INF/services/org.apache.kafka.connect.transforms.Transformation""));
        assertFalse(DelegatingClassLoader.serviceLoaderManifestForPlugin(""resource/version.properties""));
    }
",non-flaky,5
70793,apache_kafka,DelegatingClassLoaderTest.testLoadingUnloadedPluginClass,"    @Test(expected = ClassNotFoundException.class)
    public void testLoadingUnloadedPluginClass() throws ClassNotFoundException {
        TestPlugins.assertAvailable();
        DelegatingClassLoader classLoader = new DelegatingClassLoader(Collections.emptyList());
        classLoader.initLoaders();
        for (String pluginClassName : TestPlugins.pluginClasses()) {
            classLoader.loadClass(pluginClassName);
        }
    }
",non-flaky,5
70794,apache_kafka,DelegatingClassLoaderTest.testLoadingPluginClass,"    @Test
    public void testLoadingPluginClass() throws ClassNotFoundException {
        TestPlugins.assertAvailable();
        DelegatingClassLoader classLoader = new DelegatingClassLoader(TestPlugins.pluginPath());
        classLoader.initLoaders();
        for (String pluginClassName : TestPlugins.pluginClasses()) {
            assertNotNull(classLoader.loadClass(pluginClassName));
            assertNotNull(classLoader.pluginClassLoader(pluginClassName));
        }
    }
",non-flaky,5
70795,apache_kafka,PluginUtilsTest.testJavaLibraryClasses,"    @Test
    public void testJavaLibraryClasses() {
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.lang.Object""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.lang.String""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.util.HashMap$Entry""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.io.Serializable""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""javax.rmi.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""javax.management.loading.ClassLoaderRepository"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.omg.CORBA.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.omg.CORBA.Object""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.w3c.dom.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.w3c.dom.traversal.TreeWalker""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.xml.sax.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.xml.sax.EntityResolver""));
    }
",non-flaky,5
70796,apache_kafka,PluginUtilsTest.testThirdPartyClasses,"    @Test
    public void testThirdPartyClasses() {
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.slf4j.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.slf4j.LoggerFactory""));
    }
",non-flaky,5
70797,apache_kafka,PluginUtilsTest.testConnectFrameworkClasses,"    @Test
    public void testConnectFrameworkClasses() {
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.common.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.AbstractConfig"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.ConfigDef$Type"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.serialization.Deserializer"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.Connector"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.source.SourceConnector"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.sink.SinkConnector"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.connector.Task""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.source.SourceTask"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.sink.SinkTask""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.transforms.Transformation"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.storage.Converter"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.storage.OffsetBackingStore"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.clients.producer.ProducerConfig"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.clients.consumer.ConsumerConfig"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.clients.admin.KafkaAdminClient"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.rest.ConnectRestExtension"")
        );
    }
",non-flaky,5
70798,apache_kafka,PluginUtilsTest.testAllowedConnectFrameworkClasses,"    @Test
    public void testAllowedConnectFrameworkClasses() {
        assertTrue(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.transforms.""));
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.transforms.ExtractField"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.transforms.ExtractField$Key"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.json.""));
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.json.JsonConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.json.JsonConverter$21"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.file.""));
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.file.FileStreamSourceTask"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.file.FileStreamSinkConnector"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.mirror.MirrorSourceTask"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.mirror.MirrorSourceConnector"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.converters.""));
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.ByteArrayConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.DoubleConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.FloatConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.IntegerConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.LongConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.ShortConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.storage.StringConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.storage.SimpleHeaderConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
            ""org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension""
        ));
    }
",non-flaky,5
70799,apache_kafka,PluginUtilsTest.testClientConfigProvider,"    @Test
    public void testClientConfigProvider() {
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.provider.ConfigProvider"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.provider.FileConfigProvider"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.provider.FutureConfigProvider"")
        );
    }
",non-flaky,5
70800,apache_kafka,PluginUtilsTest.testConnectorClientConfigOverridePolicy,"    @Test
    public void testConnectorClientConfigOverridePolicy() {
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.ConnectorClientConfigOverridePolicy"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.AbstractConnectorClientConfigOverridePolicy"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy"")
        );
    }
",non-flaky,5
70801,apache_kafka,PluginUtilsTest.testEmptyPluginUrls,"    @Test
    public void testEmptyPluginUrls() throws Exception {
        assertEquals(Collections.<Path>emptyList(), PluginUtils.pluginUrls(pluginPath));
    }
",non-flaky,5
70802,apache_kafka,PluginUtilsTest.testEmptyStructurePluginUrls,"    @Test
    public void testEmptyStructurePluginUrls() throws Exception {
        createBasicDirectoryLayout();
        assertEquals(Collections.<Path>emptyList(), PluginUtils.pluginUrls(pluginPath));
    }
",non-flaky,5
70803,apache_kafka,PluginUtilsTest.testPluginUrlsWithJars,"    @Test
    public void testPluginUrlsWithJars() throws Exception {
        createBasicDirectoryLayout();

        List<Path> expectedUrls = createBasicExpectedUrls();

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
",non-flaky,5
70804,apache_kafka,PluginUtilsTest.testOrderOfPluginUrlsWithJars,"    @Test
    public void testOrderOfPluginUrlsWithJars() throws Exception {
        createBasicDirectoryLayout();
        // Here this method is just used to create the files. The result is not used.
        createBasicExpectedUrls();

        List<Path> actual = PluginUtils.pluginUrls(pluginPath);
        // 'simple-transform.jar' is created first. In many cases, without sorting within the
        // PluginUtils, this jar will be placed before 'another-transform.jar'. However this is
        // not guaranteed because a DirectoryStream does not maintain a certain order in its
        // results. Besides this test case, sorted order in every call to assertUrls below.
        int i = Arrays.toString(actual.toArray()).indexOf(""another-transform.jar"");
        int j = Arrays.toString(actual.toArray()).indexOf(""simple-transform.jar"");
        assertTrue(i < j);
    }
",non-flaky,5
70805,apache_kafka,PluginUtilsTest.testPluginUrlsWithZips,"    @Test
    public void testPluginUrlsWithZips() throws Exception {
        createBasicDirectoryLayout();

        List<Path> expectedUrls = new ArrayList<>();
        expectedUrls.add(Files.createFile(pluginPath.resolve(""connectorA/my-sink.zip"")));
        expectedUrls.add(Files.createFile(pluginPath.resolve(""connectorB/a-source.zip"")));
        expectedUrls.add(Files.createFile(pluginPath.resolve(""transformC/simple-transform.zip"")));
        expectedUrls.add(Files.createFile(
                pluginPath.resolve(""transformC/deps/another-transform.zip""))
        );

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
",non-flaky,5
70806,apache_kafka,PluginUtilsTest.testPluginUrlsWithClasses,"    @Test
    public void testPluginUrlsWithClasses() throws Exception {
        Files.createDirectories(pluginPath.resolve(""org/apache/kafka/converters""));
        Files.createDirectories(pluginPath.resolve(""com/mycompany/transforms""));
        Files.createDirectories(pluginPath.resolve(""edu/research/connectors""));
        Files.createFile(pluginPath.resolve(""org/apache/kafka/converters/README.txt""));
        Files.createFile(pluginPath.resolve(""org/apache/kafka/converters/AlienFormat.class""));
        Files.createDirectories(pluginPath.resolve(""com/mycompany/transforms/Blackhole.class""));
        Files.createDirectories(pluginPath.resolve(""edu/research/connectors/HalSink.class""));

        List<Path> expectedUrls = new ArrayList<>();
        expectedUrls.add(pluginPath);

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
",non-flaky,5
70807,apache_kafka,PluginUtilsTest.testPluginUrlsWithAbsoluteSymlink,"    @Test
    public void testPluginUrlsWithAbsoluteSymlink() throws Exception {
        createBasicDirectoryLayout();

        Path anotherPath = rootDir.newFolder(""moreplugins"").toPath().toRealPath();
        Files.createDirectories(anotherPath.resolve(""connectorB-deps""));
        Files.createSymbolicLink(
                pluginPath.resolve(""connectorB/deps/symlink""),
                anotherPath.resolve(""connectorB-deps"")
        );

        List<Path> expectedUrls = createBasicExpectedUrls();
        expectedUrls.add(Files.createFile(anotherPath.resolve(""connectorB-deps/converter.jar"")));

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
",non-flaky,5
70808,apache_kafka,PluginUtilsTest.testPluginUrlsWithRelativeSymlinkBackwards,"    @Test
    public void testPluginUrlsWithRelativeSymlinkBackwards() throws Exception {
        createBasicDirectoryLayout();

        Path anotherPath = rootDir.newFolder(""moreplugins"").toPath().toRealPath();
        Files.createDirectories(anotherPath.resolve(""connectorB-deps""));
        Files.createSymbolicLink(
                pluginPath.resolve(""connectorB/deps/symlink""),
                Paths.get(""../../../moreplugins/connectorB-deps"")
        );

        List<Path> expectedUrls = createBasicExpectedUrls();
        expectedUrls.add(Files.createFile(anotherPath.resolve(""connectorB-deps/converter.jar"")));

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
",non-flaky,5
70809,apache_kafka,PluginUtilsTest.testPluginUrlsWithRelativeSymlinkForwards,"    @Test
    public void testPluginUrlsWithRelativeSymlinkForwards() throws Exception {
        // Since this test case defines a relative symlink within an already included path, the main
        // assertion of this test is absence of exceptions and correct resolution of paths.
        createBasicDirectoryLayout();
        Files.createDirectories(pluginPath.resolve(""connectorB/deps/more""));
        Files.createSymbolicLink(
                pluginPath.resolve(""connectorB/deps/symlink""),
                Paths.get(""more"")
        );

        List<Path> expectedUrls = createBasicExpectedUrls();
        expectedUrls.add(
                Files.createFile(pluginPath.resolve(""connectorB/deps/more/converter.jar""))
        );

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
",non-flaky,5
70810,apache_kafka,PluginsTest.shouldInstantiateAndConfigureConverters,"    @Test
    public void shouldInstantiateAndConfigureConverters() {
        instantiateAndConfigureConverter(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);
        // Validate extra configs got passed through to overridden converters
        assertEquals(""true"", converter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));
        assertEquals(""foo1"", converter.configs.get(""extra.config""));

        instantiateAndConfigureConverter(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);
        // Validate extra configs got passed through to overridden converters
        assertEquals(""true"", converter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));
        assertEquals(""foo2"", converter.configs.get(""extra.config""));
    }
",non-flaky,5
70811,apache_kafka,PluginsTest.shouldInstantiateAndConfigureInternalConverters,"    @Test
    public void shouldInstantiateAndConfigureInternalConverters() {
        instantiateAndConfigureInternalConverter(WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);
        // Validate schemas.enable is defaulted to false for internal converter
        assertEquals(false, internalConverter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));
        // Validate internal converter properties can still be set
        assertEquals(""bar1"", internalConverter.configs.get(""extra.config""));

        instantiateAndConfigureInternalConverter(WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);
        // Validate schemas.enable is defaulted to false for internal converter
        assertEquals(false, internalConverter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));
        // Validate internal converter properties can still be set
        assertEquals(""bar2"", internalConverter.configs.get(""extra.config""));
    }
",non-flaky,5
70812,apache_kafka,PluginsTest.shouldInstantiateAndConfigureExplicitlySetHeaderConverterWithCurrentClassLoader,"    @Test
    public void shouldInstantiateAndConfigureExplicitlySetHeaderConverterWithCurrentClassLoader() {
        assertNotNull(props.get(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG));
        HeaderConverter headerConverter = plugins.newHeaderConverter(config,
                                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
                                                                     ClassLoaderUsage.CURRENT_CLASSLOADER);
        assertNotNull(headerConverter);
        assertTrue(headerConverter instanceof TestHeaderConverter);
        this.headerConverter = (TestHeaderConverter) headerConverter;

        // Validate extra configs got passed through to overridden converters
        assertConverterType(ConverterType.HEADER, this.headerConverter.configs);
        assertEquals(""baz"", this.headerConverter.configs.get(""extra.config""));

        headerConverter = plugins.newHeaderConverter(config,
                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
                                                     ClassLoaderUsage.PLUGINS);
        assertNotNull(headerConverter);
        assertTrue(headerConverter instanceof TestHeaderConverter);
        this.headerConverter = (TestHeaderConverter) headerConverter;

        // Validate extra configs got passed through to overridden converters
        assertConverterType(ConverterType.HEADER, this.headerConverter.configs);
        assertEquals(""baz"", this.headerConverter.configs.get(""extra.config""));
    }
",non-flaky,5
70813,apache_kafka,PluginsTest.shouldInstantiateAndConfigureConnectRestExtension,"    @Test
    public void shouldInstantiateAndConfigureConnectRestExtension() {
        props.put(WorkerConfig.REST_EXTENSION_CLASSES_CONFIG,
                  TestConnectRestExtension.class.getName());
        createConfig();

        List<ConnectRestExtension> connectRestExtensions =
            plugins.newPlugins(config.getList(WorkerConfig.REST_EXTENSION_CLASSES_CONFIG),
                               config,
                               ConnectRestExtension.class);
        assertNotNull(connectRestExtensions);
        assertEquals(""One Rest Extension expected"", 1, connectRestExtensions.size());
        assertNotNull(connectRestExtensions.get(0));
        assertTrue(""Should be instance of TestConnectRestExtension"",
                   connectRestExtensions.get(0) instanceof TestConnectRestExtension);
        assertNotNull(((TestConnectRestExtension) connectRestExtensions.get(0)).configs);
        assertEquals(config.originals(),
                     ((TestConnectRestExtension) connectRestExtensions.get(0)).configs);
    }
",non-flaky,5
70814,apache_kafka,PluginsTest.shouldInstantiateAndConfigureDefaultHeaderConverter,"    @Test
    public void shouldInstantiateAndConfigureDefaultHeaderConverter() {
        props.remove(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG);
        createConfig();

        // Because it's not explicitly set on the supplied configuration, the logic to use the current classloader for the connector
        // will exit immediately, and so this method always returns null
        HeaderConverter headerConverter = plugins.newHeaderConverter(config,
                                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
                                                                     ClassLoaderUsage.CURRENT_CLASSLOADER);
        assertNull(headerConverter);
        // But we should always find it (or the worker's default) when using the plugins classloader ...
        headerConverter = plugins.newHeaderConverter(config,
                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
                                                     ClassLoaderUsage.PLUGINS);
        assertNotNull(headerConverter);
        assertTrue(headerConverter instanceof SimpleHeaderConverter);
    }
",non-flaky,5
70815,apache_kafka,PluginsTest.shouldThrowIfPluginThrows,"    @Test(expected = ConnectException.class)
    public void shouldThrowIfPluginThrows() {
        TestPlugins.assertAvailable();

        plugins.newPlugin(
            TestPlugins.ALWAYS_THROW_EXCEPTION,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );
    }
",non-flaky,5
70816,apache_kafka,PluginsTest.shouldShareStaticValuesBetweenSamePlugin,"    @Test
    public void shouldShareStaticValuesBetweenSamePlugin() {
        // Plugins are not isolated from other instances of their own class.
        TestPlugins.assertAvailable();
        Converter firstPlugin = plugins.newPlugin(
            TestPlugins.ALIASED_STATIC_FIELD,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );

        assertInstanceOf(SamplingTestPlugin.class, firstPlugin, ""Cannot collect samples"");

        Converter secondPlugin = plugins.newPlugin(
            TestPlugins.ALIASED_STATIC_FIELD,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );

        assertInstanceOf(SamplingTestPlugin.class, secondPlugin, ""Cannot collect samples"");
        assertSame(
            ((SamplingTestPlugin) firstPlugin).otherSamples(),
            ((SamplingTestPlugin) secondPlugin).otherSamples()
        );
    }
",non-flaky,5
70817,apache_kafka,PluginsTest.newPluginShouldServiceLoadWithPluginClassLoader,"    @Test
    public void newPluginShouldServiceLoadWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        Converter plugin = plugins.newPlugin(
            TestPlugins.SERVICE_LOADER,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        // Assert that the service loaded subclass is found in both environments
        assertTrue(samples.containsKey(""ServiceLoadedSubclass.static""));
        assertTrue(samples.containsKey(""ServiceLoadedSubclass.dynamic""));
        assertPluginClassLoaderAlwaysActive(samples);
    }
",non-flaky,5
70818,apache_kafka,PluginsTest.newPluginShouldInstantiateWithPluginClassLoader,"    @Test
    public void newPluginShouldInstantiateWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        Converter plugin = plugins.newPlugin(
            TestPlugins.ALIASED_STATIC_FIELD,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertPluginClassLoaderAlwaysActive(samples);
    }
",non-flaky,5
70819,apache_kafka,PluginsTest.shouldFailToFindConverterInCurrentClassloader,"    @Test(expected = ConfigException.class)
    public void shouldFailToFindConverterInCurrentClassloader() {
        TestPlugins.assertAvailable();
        props.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, TestPlugins.SAMPLING_CONVERTER);
        createConfig();
    }
",non-flaky,5
70820,apache_kafka,PluginsTest.newConverterShouldConfigureWithPluginClassLoader,"    @Test
    public void newConverterShouldConfigureWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        props.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, TestPlugins.SAMPLING_CONVERTER);
        ClassLoader classLoader = plugins.delegatingLoader().pluginClassLoader(TestPlugins.SAMPLING_CONVERTER);
        ClassLoader savedLoader = Plugins.compareAndSwapLoaders(classLoader);
        createConfig();
        Plugins.compareAndSwapLoaders(savedLoader);

        Converter plugin = plugins.newConverter(
            config,
            WorkerConfig.KEY_CONVERTER_CLASS_CONFIG,
            ClassLoaderUsage.PLUGINS
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertTrue(samples.containsKey(""configure""));
        assertPluginClassLoaderAlwaysActive(samples);
    }
",non-flaky,5
70821,apache_kafka,PluginsTest.newConfigProviderShouldConfigureWithPluginClassLoader,"    @Test
    public void newConfigProviderShouldConfigureWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        String providerPrefix = ""some.provider"";
        props.put(providerPrefix + "".class"", TestPlugins.SAMPLING_CONFIG_PROVIDER);

        PluginClassLoader classLoader = plugins.delegatingLoader().pluginClassLoader(TestPlugins.SAMPLING_CONFIG_PROVIDER);
        assertNotNull(classLoader);
        ClassLoader savedLoader = Plugins.compareAndSwapLoaders(classLoader);
        createConfig();
        Plugins.compareAndSwapLoaders(savedLoader);

        ConfigProvider plugin = plugins.newConfigProvider(
            config,
            providerPrefix,
            ClassLoaderUsage.PLUGINS
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertTrue(samples.containsKey(""configure""));
        assertPluginClassLoaderAlwaysActive(samples);
    }
",non-flaky,5
70822,apache_kafka,PluginsTest.newHeaderConverterShouldConfigureWithPluginClassLoader,"    @Test
    public void newHeaderConverterShouldConfigureWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        props.put(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, TestPlugins.SAMPLING_HEADER_CONVERTER);
        ClassLoader classLoader = plugins.delegatingLoader().pluginClassLoader(TestPlugins.SAMPLING_HEADER_CONVERTER);
        ClassLoader savedLoader = Plugins.compareAndSwapLoaders(classLoader);
        createConfig();
        Plugins.compareAndSwapLoaders(savedLoader);

        HeaderConverter plugin = plugins.newHeaderConverter(
            config,
            WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
            ClassLoaderUsage.PLUGINS
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertTrue(samples.containsKey(""configure"")); // HeaderConverter::configure was called
        assertPluginClassLoaderAlwaysActive(samples);
    }
",non-flaky,5
70823,apache_kafka,PluginsTest.newPluginsShouldConfigureWithPluginClassLoader,"    @Test
    public void newPluginsShouldConfigureWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        List<Configurable> configurables = plugins.newPlugins(
            Collections.singletonList(TestPlugins.SAMPLING_CONFIGURABLE),
            config,
            Configurable.class
        );
        assertEquals(1, configurables.size());
        Configurable plugin = configurables.get(0);

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertTrue(samples.containsKey(""configure"")); // Configurable::configure was called
        assertPluginClassLoaderAlwaysActive(samples);
    }
",non-flaky,5
70824,apache_kafka,PluginDescTest.testRegularPluginDesc,"    @Test
    public void testRegularPluginDesc() {
        PluginDesc<Connector> connectorDesc = new PluginDesc<>(
                Connector.class,
                regularVersion,
                pluginLoader
        );

        assertPluginDesc(connectorDesc, Connector.class, regularVersion, pluginLoader.location());

        PluginDesc<Converter> converterDesc = new PluginDesc<>(
                Converter.class,
                snaphotVersion,
                pluginLoader
        );

        assertPluginDesc(converterDesc, Converter.class, snaphotVersion, pluginLoader.location());

        PluginDesc<Transformation> transformDesc = new PluginDesc<>(
                Transformation.class,
                noVersion,
                pluginLoader
        );

        assertPluginDesc(transformDesc, Transformation.class, noVersion, pluginLoader.location());
    }
",non-flaky,5
70825,apache_kafka,PluginDescTest.testPluginDescWithSystemClassLoader,"    @Test
    public void testPluginDescWithSystemClassLoader() {
        String location = ""classpath"";
        PluginDesc<SinkConnector> connectorDesc = new PluginDesc<>(
                SinkConnector.class,
                regularVersion,
                systemLoader
        );

        assertPluginDesc(connectorDesc, SinkConnector.class, regularVersion, location);

        PluginDesc<Converter> converterDesc = new PluginDesc<>(
                Converter.class,
                snaphotVersion,
                systemLoader
        );

        assertPluginDesc(converterDesc, Converter.class, snaphotVersion, location);

        PluginDesc<Transformation> transformDesc = new PluginDesc<>(
                Transformation.class,
                noVersion,
                systemLoader
        );

        assertPluginDesc(transformDesc, Transformation.class, noVersion, location);
    }
",non-flaky,5
70826,apache_kafka,PluginDescTest.testPluginDescWithNullVersion,"    @Test
    public void testPluginDescWithNullVersion() {
        String nullVersion = ""null"";
        PluginDesc<SourceConnector> connectorDesc = new PluginDesc<>(
                SourceConnector.class,
                null,
                pluginLoader
        );

        assertPluginDesc(
                connectorDesc,
                SourceConnector.class,
                nullVersion,
                pluginLoader.location()
        );

        String location = ""classpath"";
        PluginDesc<Converter> converterDesc = new PluginDesc<>(
                Converter.class,
                null,
                systemLoader
        );

        assertPluginDesc(converterDesc, Converter.class, nullVersion, location);
    }
",non-flaky,5
70827,apache_kafka,PluginDescTest.testPluginDescEquality,"    @Test
    public void testPluginDescEquality() {
        PluginDesc<Connector> connectorDescPluginPath = new PluginDesc<>(
                Connector.class,
                snaphotVersion,
                pluginLoader
        );

        PluginDesc<Connector> connectorDescClasspath = new PluginDesc<>(
                Connector.class,
                snaphotVersion,
                systemLoader
        );

        assertEquals(connectorDescPluginPath, connectorDescClasspath);
        assertEquals(connectorDescPluginPath.hashCode(), connectorDescClasspath.hashCode());

        PluginDesc<Converter> converterDescPluginPath = new PluginDesc<>(
                Converter.class,
                noVersion,
                pluginLoader
        );

        PluginDesc<Converter> converterDescClasspath = new PluginDesc<>(
                Converter.class,
                noVersion,
                systemLoader
        );

        assertEquals(converterDescPluginPath, converterDescClasspath);
        assertEquals(converterDescPluginPath.hashCode(), converterDescClasspath.hashCode());

        PluginDesc<Transformation> transformDescPluginPath = new PluginDesc<>(
                Transformation.class,
                null,
                pluginLoader
        );

        PluginDesc<Transformation> transformDescClasspath = new PluginDesc<>(
                Transformation.class,
                noVersion,
                pluginLoader
        );

        assertNotEquals(transformDescPluginPath, transformDescClasspath);
    }
",non-flaky,5
70828,apache_kafka,PluginDescTest.testPluginDescComparison,"    @Test
    public void testPluginDescComparison() {
        PluginDesc<Connector> connectorDescPluginPath = new PluginDesc<>(
                Connector.class,
                regularVersion,
                pluginLoader
        );

        PluginDesc<Connector> connectorDescClasspath = new PluginDesc<>(
                Connector.class,
                newerVersion,
                systemLoader
        );

        assertNewer(connectorDescPluginPath, connectorDescClasspath);

        PluginDesc<Converter> converterDescPluginPath = new PluginDesc<>(
                Converter.class,
                noVersion,
                pluginLoader
        );

        PluginDesc<Converter> converterDescClasspath = new PluginDesc<>(
                Converter.class,
                snaphotVersion,
                systemLoader
        );

        assertNewer(converterDescPluginPath, converterDescClasspath);

        PluginDesc<Transformation> transformDescPluginPath = new PluginDesc<>(
                Transformation.class,
                null,
                pluginLoader
        );

        PluginDesc<Transformation> transformDescClasspath = new PluginDesc<>(
                Transformation.class,
                regularVersion,
                systemLoader
        );

        assertNewer(transformDescPluginPath, transformDescClasspath);
    }
",non-flaky,5
70829,apache_kafka,WorkerConfigTest.testAdminListenersConfigAllowedValues,"    @Test
    public void testAdminListenersConfigAllowedValues() {
        Map<String, String> props = baseProps();

        // no value set for ""admin.listeners""
        WorkerConfig config = new WorkerConfig(WorkerConfig.baseConfigDef(), props);
        assertNull(""Default value should be null."", config.getList(WorkerConfig.ADMIN_LISTENERS_CONFIG));

        props.put(WorkerConfig.ADMIN_LISTENERS_CONFIG, """");
        config = new WorkerConfig(WorkerConfig.baseConfigDef(), props);
        assertTrue(config.getList(WorkerConfig.ADMIN_LISTENERS_CONFIG).isEmpty());

        props.put(WorkerConfig.ADMIN_LISTENERS_CONFIG, ""http://a.b:9999, https://a.b:7812"");
        config = new WorkerConfig(WorkerConfig.baseConfigDef(), props);
        assertEquals(config.getList(WorkerConfig.ADMIN_LISTENERS_CONFIG), Arrays.asList(""http://a.b:9999"", ""https://a.b:7812""));

        new WorkerConfig(WorkerConfig.baseConfigDef(), props);
    }
",non-flaky,5
70830,apache_kafka,WorkerConfigTest.testAdminListenersNotAllowingEmptyStrings,"    @Test(expected = ConfigException.class)
    public void testAdminListenersNotAllowingEmptyStrings() {
        Map<String, String> props = baseProps();
        props.put(WorkerConfig.ADMIN_LISTENERS_CONFIG, ""http://a.b:9999,"");
        new WorkerConfig(WorkerConfig.baseConfigDef(), props);
    }
",non-flaky,5
70831,apache_kafka,WorkerConfigTest.testAdminListenersNotAllowingBlankStrings,"    @Test(expected = ConfigException.class)
    public void testAdminListenersNotAllowingBlankStrings() {
        Map<String, String> props = baseProps();
        props.put(WorkerConfig.ADMIN_LISTENERS_CONFIG, ""http://a.b:9999, ,https://a.b:9999"");
        new WorkerConfig(WorkerConfig.baseConfigDef(), props);
    }
",non-flaky,5
70832,apache_kafka,WorkerSourceTaskTest.answer,"    @Test
    public void testStartPaused() throws Exception {
        final CountDownLatch pauseLatch = new CountDownLatch(1);

        createWorkerTask(TargetState.PAUSED);

        statusListener.onPause(taskId);
        EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {
            @Override
            public Void answer() throws Throwable {
                pauseLatch.countDown();
                return null;
            }
",non-flaky,5
70833,apache_kafka,WorkerSourceTaskTest.testPause,"    @Test
    public void testPause() throws Exception {
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        AtomicInteger count = new AtomicInteger(0);
        CountDownLatch pollLatch = expectPolls(10, count);
        // In this test, we don't flush, so nothing goes any further than the offset writer

        statusListener.onPause(taskId);
        EasyMock.expectLastCall();

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(true);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);
        assertTrue(awaitLatch(pollLatch));

        workerTask.transitionTo(TargetState.PAUSED);

        int priorCount = count.get();
        Thread.sleep(100);

        // since the transition is observed asynchronously, the count could be off by one loop iteration
        assertTrue(count.get() - priorCount <= 1);

        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();

        PowerMock.verifyAll();
    }
",non-flaky,5
70834,apache_kafka,WorkerSourceTaskTest.testPollsInBackground,"    @Test
    public void testPollsInBackground() throws Exception {
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        final CountDownLatch pollLatch = expectPolls(10);
        // In this test, we don't flush, so nothing goes any further than the offset writer

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(true);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);

        assertTrue(awaitLatch(pollLatch));
        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();
        assertPollMetrics(10);

        PowerMock.verifyAll();
    }
",non-flaky,5
70835,apache_kafka,WorkerSourceTaskTest.answer,"    @Test
    public void testFailureInPoll() throws Exception {
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        final CountDownLatch pollLatch = new CountDownLatch(1);
        final RuntimeException exception = new RuntimeException();
        EasyMock.expect(sourceTask.poll()).andAnswer(new IAnswer<List<SourceRecord>>() {
            @Override
            public List<SourceRecord> answer() throws Throwable {
                pollLatch.countDown();
                throw exception;
            }
",non-flaky,5
70836,apache_kafka,WorkerSourceTaskTest.testPollReturnsNoRecords,"    @Test
    public void testPollReturnsNoRecords() throws Exception {
        // Test that the task handles an empty list of records
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        // We'll wait for some data, then trigger a flush
        final CountDownLatch pollLatch = expectEmptyPolls(1, new AtomicInteger());
        expectOffsetFlush(true);

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(true);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);

        assertTrue(awaitLatch(pollLatch));
        assertTrue(workerTask.commitOffsets());
        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();
        assertPollMetrics(0);

        PowerMock.verifyAll();
    }
",non-flaky,5
70837,apache_kafka,WorkerSourceTaskTest.testCommit,"    @Test
    public void testCommit() throws Exception {
        // Test that the task commits properly when prompted
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        // We'll wait for some data, then trigger a flush
        final CountDownLatch pollLatch = expectPolls(1);
        expectOffsetFlush(true);

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(true);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);

        assertTrue(awaitLatch(pollLatch));
        assertTrue(workerTask.commitOffsets());
        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();
        assertPollMetrics(1);

        PowerMock.verifyAll();
    }
",non-flaky,5
70838,apache_kafka,WorkerSourceTaskTest.testCommitFailure,"    @Test
    public void testCommitFailure() throws Exception {
        // Test that the task commits properly when prompted
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        // We'll wait for some data, then trigger a flush
        final CountDownLatch pollLatch = expectPolls(1);
        expectOffsetFlush(true);

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(false);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);

        assertTrue(awaitLatch(pollLatch));
        assertTrue(workerTask.commitOffsets());
        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();
        assertPollMetrics(1);

        PowerMock.verifyAll();
    }
",non-flaky,5
70839,apache_kafka,WorkerSourceTaskTest.testSendRecordsConvertsData,"    @Test
    public void testSendRecordsConvertsData() throws Exception {
        createWorkerTask();

        List<SourceRecord> records = new ArrayList<>();
        // Can just use the same record for key and value
        records.add(new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD));

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(SERIALIZED_KEY, sent.getValue().key());
        assertEquals(SERIALIZED_RECORD, sent.getValue().value());

        PowerMock.verifyAll();
    }
",non-flaky,5
70840,apache_kafka,WorkerSourceTaskTest.testSendRecordsPropagatesTimestamp,"    @Test
    public void testSendRecordsPropagatesTimestamp() throws Exception {
        final Long timestamp = System.currentTimeMillis();

        createWorkerTask();

        List<SourceRecord> records = Collections.singletonList(
                new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)
        );

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(timestamp, sent.getValue().timestamp());

        PowerMock.verifyAll();
    }
",non-flaky,5
70841,apache_kafka,WorkerSourceTaskTest.testSendRecordsCorruptTimestamp,"    @Test(expected = InvalidRecordException.class)
    public void testSendRecordsCorruptTimestamp() throws Exception {
        final Long timestamp = -3L;
        createWorkerTask();

        List<SourceRecord> records = Collections.singletonList(
                new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)
        );

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(null, sent.getValue().timestamp());

        PowerMock.verifyAll();
    }
",non-flaky,5
70842,apache_kafka,WorkerSourceTaskTest.testSendRecordsNoTimestamp,"    @Test
    public void testSendRecordsNoTimestamp() throws Exception {
        final Long timestamp = -1L;
        createWorkerTask();

        List<SourceRecord> records = Collections.singletonList(
                new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)
        );

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(null, sent.getValue().timestamp());

        PowerMock.verifyAll();
    }
",non-flaky,5
70843,apache_kafka,WorkerSourceTaskTest.testSendRecordsRetries,"    @Test
    public void testSendRecordsRetries() throws Exception {
        createWorkerTask();

        // Differentiate only by Kafka partition so we can reuse conversion expectations
        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, ""topic"", 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, ""topic"", 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record3 = new SourceRecord(PARTITION, OFFSET, ""topic"", 3, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);

        // First round
        expectSendRecordOnce(false);
        // Any Producer retriable exception should work here
        expectSendRecordSyncFailure(new org.apache.kafka.common.errors.TimeoutException(""retriable sync failure""));

        // Second round
        expectSendRecordOnce(true);
        expectSendRecordOnce(false);

        PowerMock.replayAll();

        // Try to send 3, make first pass, second fail. Should save last two
        Whitebox.setInternalState(workerTask, ""toSend"", Arrays.asList(record1, record2, record3));
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(true, Whitebox.getInternalState(workerTask, ""lastSendFailed""));
        assertEquals(Arrays.asList(record2, record3), Whitebox.getInternalState(workerTask, ""toSend""));

        // Next they all succeed
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(false, Whitebox.getInternalState(workerTask, ""lastSendFailed""));
        assertNull(Whitebox.getInternalState(workerTask, ""toSend""));

        PowerMock.verifyAll();
    }
",non-flaky,5
70844,apache_kafka,WorkerSourceTaskTest.testSendRecordsProducerCallbackFail,"    @Test(expected = ConnectException.class)
    public void testSendRecordsProducerCallbackFail() throws Exception {
        createWorkerTask();

        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, ""topic"", 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, ""topic"", 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);

        expectSendRecordProducerCallbackFail();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", Arrays.asList(record1, record2));
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
    }
",non-flaky,5
70845,apache_kafka,WorkerSourceTaskTest.testSendRecordsTaskCommitRecordFail,"    @Test
    public void testSendRecordsTaskCommitRecordFail() throws Exception {
        createWorkerTask();

        // Differentiate only by Kafka partition so we can reuse conversion expectations
        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, ""topic"", 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, ""topic"", 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record3 = new SourceRecord(PARTITION, OFFSET, ""topic"", 3, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);

        // Source task commit record failure will not cause the task to abort
        expectSendRecordOnce(false);
        expectSendRecordTaskCommitRecordFail(false, false);
        expectSendRecordOnce(false);

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", Arrays.asList(record1, record2, record3));
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(false, Whitebox.getInternalState(workerTask, ""lastSendFailed""));
        assertNull(Whitebox.getInternalState(workerTask, ""toSend""));

        PowerMock.verifyAll();
    }
",non-flaky,5
70846,apache_kafka,WorkerSourceTaskTest.answer,"    @Test
    public void testSlowTaskStart() throws Exception {
        final CountDownLatch startupLatch = new CountDownLatch(1);
        final CountDownLatch finishStartupLatch = new CountDownLatch(1);

        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall().andAnswer(new IAnswer<Object>() {
            @Override
            public Object answer() throws Throwable {
                startupLatch.countDown();
                assertTrue(awaitLatch(finishStartupLatch));
                return null;
            }
",non-flaky,5
70847,apache_kafka,WorkerSourceTaskTest.testCancel,"    @Test
    public void testCancel() {
        createWorkerTask();

        offsetReader.close();
        PowerMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.cancel();

        PowerMock.verifyAll();
    }
",non-flaky,5
70848,apache_kafka,WorkerSourceTaskTest.testMetricsGroup,"    @Test
    public void testMetricsGroup() {
        SourceTaskMetricsGroup group = new SourceTaskMetricsGroup(taskId, metrics);
        SourceTaskMetricsGroup group1 = new SourceTaskMetricsGroup(taskId1, metrics);
        for (int i = 0; i != 10; ++i) {
            group.recordPoll(100, 1000 + i * 100);
            group.recordWrite(10);
        }
        for (int i = 0; i != 20; ++i) {
            group1.recordPoll(100, 1000 + i * 100);
            group1.recordWrite(10);
        }
        assertEquals(1900.0, metrics.currentMetricValueAsDouble(group.metricGroup(), ""poll-batch-max-time-ms""), 0.001d);
        assertEquals(1450.0, metrics.currentMetricValueAsDouble(group.metricGroup(), ""poll-batch-avg-time-ms""), 0.001d);
        assertEquals(33.333, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-poll-rate""), 0.001d);
        assertEquals(1000, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-poll-total""), 0.001d);
        assertEquals(3.3333, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-write-rate""), 0.001d);
        assertEquals(100, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-write-total""), 0.001d);
        assertEquals(900.0, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-active-count""), 0.001d);

        // Close the group
        group.close();

        for (MetricName metricName : group.metricGroup().metrics().metrics().keySet()) {
            // Metrics for this group should no longer exist
            assertFalse(group.metricGroup().groupId().includes(metricName));
        }
        // Sensors for this group should no longer exist
        assertNull(group.metricGroup().metrics().getSensor(""sink-record-read""));
        assertNull(group.metricGroup().metrics().getSensor(""sink-record-send""));
        assertNull(group.metricGroup().metrics().getSensor(""sink-record-active-count""));
        assertNull(group.metricGroup().metrics().getSensor(""partition-count""));
        assertNull(group.metricGroup().metrics().getSensor(""offset-seq-number""));
        assertNull(group.metricGroup().metrics().getSensor(""offset-commit-completion""));
        assertNull(group.metricGroup().metrics().getSensor(""offset-commit-completion-skip""));
        assertNull(group.metricGroup().metrics().getSensor(""put-batch-time""));

        assertEquals(2900.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""poll-batch-max-time-ms""), 0.001d);
        assertEquals(1950.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""poll-batch-avg-time-ms""), 0.001d);
        assertEquals(66.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-poll-rate""), 0.001d);
        assertEquals(2000, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-poll-total""), 0.001d);
        assertEquals(6.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-write-rate""), 0.001d);
        assertEquals(200, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-write-total""), 0.001d);
        assertEquals(1800.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-active-count""), 0.001d);
    }
",non-flaky,5
70849,apache_kafka,WorkerSourceTaskTest.testHeaders,"    @Test
    public void testHeaders() throws Exception {
        Headers headers = new RecordHeaders();
        headers.add(""header_key"", ""header_value"".getBytes());

        org.apache.kafka.connect.header.Headers connectHeaders = new ConnectHeaders();
        connectHeaders.add(""header_key"", new SchemaAndValue(Schema.STRING_SCHEMA, ""header_value""));

        createWorkerTask();

        List<SourceRecord> records = new ArrayList<>();
        records.add(new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, null, connectHeaders));

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecord(true, false, true, true, true, headers);

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(SERIALIZED_KEY, sent.getValue().key());
        assertEquals(SERIALIZED_RECORD, sent.getValue().value());
        assertEquals(headers, sent.getValue().headers());

        PowerMock.verifyAll();
    }
",non-flaky,5
70850,apache_kafka,WorkerSourceTaskTest.testHeadersWithCustomConverter,"    @Test
    public void testHeadersWithCustomConverter() throws Exception {
        StringConverter stringConverter = new StringConverter();
        TestConverterWithHeaders testConverter = new TestConverterWithHeaders();

        createWorkerTask(TargetState.STARTED, stringConverter, testConverter, stringConverter);

        List<SourceRecord> records = new ArrayList<>();

        String stringA = ""ÃrvÃ­ztÅ±rÅ tÃ¼kÃ¶rfÃºrÃ³gÃ©p"";
        org.apache.kafka.connect.header.Headers headersA = new ConnectHeaders();
        String encodingA = ""latin2"";
        headersA.addString(""encoding"", encodingA);

        records.add(new SourceRecord(PARTITION, OFFSET, ""topic"", null, Schema.STRING_SCHEMA, ""a"", Schema.STRING_SCHEMA, stringA, null, headersA));

        String stringB = ""Ð¢ÐµÑÑÐ¾Ð²Ð¾Ðµ ÑÐ¾Ð¾Ð±ÑÐµÐ½Ð¸Ðµ"";
        org.apache.kafka.connect.header.Headers headersB = new ConnectHeaders();
        String encodingB = ""koi8_r"";
        headersB.addString(""encoding"", encodingB);

        records.add(new SourceRecord(PARTITION, OFFSET, ""topic"", null, Schema.STRING_SCHEMA, ""b"", Schema.STRING_SCHEMA, stringB, null, headersB));

        Capture<ProducerRecord<byte[], byte[]>> sentRecordA = expectSendRecord(false, false, true, true, false, null);
        Capture<ProducerRecord<byte[], byte[]>> sentRecordB = expectSendRecord(false, false, true, true, false, null);

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");

        assertEquals(ByteBuffer.wrap(""a"".getBytes()), ByteBuffer.wrap(sentRecordA.getValue().key()));
        assertEquals(
            ByteBuffer.wrap(stringA.getBytes(encodingA)),
            ByteBuffer.wrap(sentRecordA.getValue().value())
        );
        assertEquals(encodingA, new String(sentRecordA.getValue().headers().lastHeader(""encoding"").value()));

        assertEquals(ByteBuffer.wrap(""b"".getBytes()), ByteBuffer.wrap(sentRecordB.getValue().key()));
        assertEquals(
            ByteBuffer.wrap(stringB.getBytes(encodingB)),
            ByteBuffer.wrap(sentRecordB.getValue().value())
        );
        assertEquals(encodingB, new String(sentRecordB.getValue().headers().lastHeader(""encoding"").value()));

        PowerMock.verifyAll();
    }
",non-flaky,5
70851,apache_kafka,TransformationConfigTest.testEmbeddedConfigCast,"    @Test
    public void testEmbeddedConfigCast() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", Cast.Value.class.getName());
        connProps.put(""transforms.example.spec"", ""int8"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70852,apache_kafka,TransformationConfigTest.testEmbeddedConfigExtractField,"    @Test
    public void testEmbeddedConfigExtractField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", ExtractField.Value.class.getName());
        connProps.put(""transforms.example.field"", ""field"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70853,apache_kafka,TransformationConfigTest.testEmbeddedConfigFlatten,"    @Test
    public void testEmbeddedConfigFlatten() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", Flatten.Value.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70854,apache_kafka,TransformationConfigTest.testEmbeddedConfigHoistField,"    @Test
    public void testEmbeddedConfigHoistField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", HoistField.Value.class.getName());
        connProps.put(""transforms.example.field"", ""field"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70855,apache_kafka,TransformationConfigTest.testEmbeddedConfigInsertField,"    @Test
    public void testEmbeddedConfigInsertField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", InsertField.Value.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70856,apache_kafka,TransformationConfigTest.testEmbeddedConfigMaskField,"    @Test
    public void testEmbeddedConfigMaskField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", MaskField.Value.class.getName());
        connProps.put(""transforms.example.fields"", ""field"");


        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70857,apache_kafka,TransformationConfigTest.testEmbeddedConfigRegexRouter,"    @Test
    public void testEmbeddedConfigRegexRouter() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", RegexRouter.class.getName());
        connProps.put(""transforms.example.regex"", ""(.*)"");
        connProps.put(""transforms.example.replacement"", ""prefix-$1"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70858,apache_kafka,TransformationConfigTest.testEmbeddedConfigReplaceField,"    @Test
    public void testEmbeddedConfigReplaceField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", ReplaceField.Value.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70859,apache_kafka,TransformationConfigTest.testEmbeddedConfigSetSchemaMetadata,"    @Test
    public void testEmbeddedConfigSetSchemaMetadata() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", SetSchemaMetadata.Value.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70860,apache_kafka,TransformationConfigTest.testEmbeddedConfigTimestampConverter,"    @Test
    public void testEmbeddedConfigTimestampConverter() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", TimestampConverter.Value.class.getName());
        connProps.put(""transforms.example.target.type"", ""unix"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70861,apache_kafka,TransformationConfigTest.testEmbeddedConfigTimestampRouter,"    @Test
    public void testEmbeddedConfigTimestampRouter() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", TimestampRouter.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70862,apache_kafka,TransformationConfigTest.testEmbeddedConfigValueToKey,"    @Test
    public void testEmbeddedConfigValueToKey() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", ValueToKey.class.getName());
        connProps.put(""transforms.example.fields"", ""field"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
",non-flaky,5
70863,apache_kafka,StateTrackerTest.currentStateIsNullWhenNotInitialized,"    @Test
    public void currentStateIsNullWhenNotInitialized() {
        assertNull(tracker.currentState());
    }
",non-flaky,5
70864,apache_kafka,StateTrackerTest.currentState,"    @Test
    public void currentState() {
        for (State state : State.values()) {
            tracker.changeState(state, time.milliseconds());
            assertEquals(state, tracker.currentState());
        }
    }
",non-flaky,5
349,GoogleCloudPlatform_google-cloud-eclipse,CreateAppEngineStandardWtpProjectTest.testNoTestClassesInDeploymentAssembly,"@Test
public void testNoTestClassesInDeploymentAssembly()
throws InvocationTargetException, CoreException {
    CreateAppEngineWtpProject creator = new CreateAppEngineStandardWtpProject(config, adaptable);
    creator.execute(monitor);
    ProjectUtils.waitForProjects(project);
    assertNoTestClassesInDeploymentAssembly();
}
private void assertNoTestClassesInDeploymentAssembly() throws CoreException {
    StructureEdit core = StructureEdit.getStructureEditForRead(project);
    WorkbenchComponent component = core.getComponent();
    assertNotNull(component);
    boolean seenMainSourcePath = false;
    List<ComponentResource> resources = component.getResources();
    for (ComponentResource resource : resources) {
        assertFalse(containsSegment(resource.getSourcePath(), ""test""));
        if (resource.getSourcePath().equals(new Path(""/src/main/java""))
        && resource.getRuntimePath().equals(new Path(""/WEB-INF/classes""))) {
            seenMainSourcePath = true;
        }
    }
    assertTrue(seenMainSourcePath);
}",async wait,0
366,GoogleCloudPlatform_google-cloud-eclipse,XmlValidatorTest.testValidate_badXml,"@Test
public void testValidate_badXml() throws IOException, CoreException {
    XmlValidator validator = new XmlValidator();
    validator.setHelper(new AppEngineWebXmlValidator());
    IFile file = createBogusProjectFile();
    byte[] badXml = BAD_XML.getBytes(StandardCharsets.UTF_8);
    validator.validate(file, badXml);
    IMarker[] emptyMarkers =
    ProjectUtils.waitUntilNoMarkersFound(file, PROBLEM, true, DEPTH_ZERO);
    ArrayAssertions.assertIsEmpty(emptyMarkers);
}",async wait,0
175737,GoogleCloudPlatform_google-cloud-eclipse,MultipleConnectionsTest.testDefaultSettings,"	@Test
	public void testDefaultSettings() throws CoreException {
		connector = new SocketListenMultiConnector();
		Map<String, Connector.Argument> defaults = connector.getDefaultArguments();
		assertTrue(defaults.containsKey(""connectionLimit""));
		assertEquals(1, ((Connector.IntegerArgument) defaults.get(""connectionLimit"")).intValue());
	}
",non-flaky,5
175738,GoogleCloudPlatform_google-cloud-eclipse,MultipleConnectionsTest.testDefaultBehaviour,"	@Test
	public void testDefaultBehaviour() throws CoreException, InterruptedException {
		connector = new SocketListenMultiConnector();
		Map<String, String> arguments = new HashMap<>();
		arguments.put(""port"", Integer.toString(port));
		connector.connect(arguments, new NullProgressMonitor(), launch);
		Thread.sleep(200);

		assertTrue(""first connect should succeed"", connect());
		assertFalse(""second connect should fail"", connect());
	}
",non-flaky,5
175739,GoogleCloudPlatform_google-cloud-eclipse,MultipleConnectionsTest.testSingleConnectionBehaviour,"	@Test
	public void testSingleConnectionBehaviour() throws CoreException, InterruptedException {
		connector = new SocketListenMultiConnector();
		Map<String, String> arguments = new HashMap<>();
		arguments.put(""port"", Integer.toString(port));
		arguments.put(""connectionLimit"", ""1"");
		connector.connect(arguments, new NullProgressMonitor(), launch);
		Thread.sleep(200);

		assertTrue(""first connect should succeed"", connect());
		assertFalse(""second connect should fail"", connect());
	}
",non-flaky,5
175740,GoogleCloudPlatform_google-cloud-eclipse,MultipleConnectionsTest.testTwoConnectionsBehaviour,"	@Test
	public void testTwoConnectionsBehaviour() throws CoreException, InterruptedException {
		connector = new SocketListenMultiConnector();
		Map<String, String> arguments = new HashMap<>();
		arguments.put(""port"", Integer.toString(port));
		arguments.put(""connectionLimit"", ""2"");
		connector.connect(arguments, new NullProgressMonitor(), launch);
		Thread.sleep(200);

		assertTrue(""first connect should succeed"", connect());
		assertTrue(""second connect should succeed"", connect());
	}
",non-flaky,5
175741,GoogleCloudPlatform_google-cloud-eclipse,MultipleConnectionsTest.testUnlimitedConnectionsBehaviour,"	@Test
	public void testUnlimitedConnectionsBehaviour() throws CoreException, InterruptedException {
		connector = new SocketListenMultiConnector();
		Map<String, String> arguments = new HashMap<>();
		arguments.put(""port"", Integer.toString(port));
		arguments.put(""connectionLimit"", ""0"");
		connector.connect(arguments, new NullProgressMonitor(), launch);
		Thread.sleep(200);

		for (int i = 0; i < 10; i++) {
			assertTrue(""connection "" + i + "" should succeed"", connect());
		}
	}
",non-flaky,5
175742,GoogleCloudPlatform_google-cloud-eclipse,MessagesTest.testCloudSdkNotConfigured,"  @Test
  public void testCloudSdkNotConfigured() {
    Assert.assertEquals(""Deploy failed."", Messages.getString(""deploy.failed.error.message""));
  }
",non-flaky,5
175743,GoogleCloudPlatform_google-cloud-eclipse,MessagesTest.testSpecifyVersionTooltip,"  @Test
  public void testSpecifyVersionTooltip() {
    Assert.assertEquals(
        ""If checked, stops the previously running version when ""
        + ""deploying a new version that receives all traffic."",
        Messages.getString(""tooltip.stop.previous.version""));
  }
",non-flaky,5
175744,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testAutoSelectSingleAccount,"  @Test
  public void testAutoSelectSingleAccount() {
    when(loginService.getAccounts()).thenReturn(oneAccountSet);
    deployPanel = createPanel(true /* requireValues */);
    assertThat(deployPanel.getSelectedCredential(), is(credential));

    // verify not in error
    IStatus status = getAccountSelectorValidationStatus();
    assertTrue(""account selector is in error: "" + status.getMessage(), status.isOK());

    assertThat(""auto-selected value should be propagated back to model"",
        deployPanel.model.getAccountEmail(), is(account1.getEmail()));
  }
",non-flaky,5
175745,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testAutoSelectSingleAccount_loadGcpProjects,"  @Test
  public void testAutoSelectSingleAccount_loadGcpProjects()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(oneAccountSet);
    initializeProjectRepository();
    deployPanel = createPanel(true /* requireValues */);
    assertNotNull(deployPanel.latestGcpProjectQueryJob);
    deployPanel.latestGcpProjectQueryJob.join();

    Table projectTable = getProjectSelector().getViewer().getTable();
    assertThat(projectTable.getItemCount(), is(2));
  }
",non-flaky,5
175746,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testValidationMessageWhenNotSignedIn,"  @Test
  public void testValidationMessageWhenNotSignedIn() {
    deployPanel = createPanel(true /* requireValues */);
    IStatus status = getAccountSelectorValidationStatus();
    assertThat(status.getMessage(), is(""Sign in to Google.""));
  }
",non-flaky,5
175747,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testValidationMessageWhenSignedIn,"  @Test
  public void testValidationMessageWhenSignedIn() {
    // Return two accounts because the account selector will auto-select if there exists only one.
    when(loginService.getAccounts()).thenReturn(twoAccountSet);

    deployPanel = createPanel(true /* requireValues */);
    IStatus status = getAccountSelectorValidationStatus();
    assertThat(status.getMessage(), is(""Select an account.""));
  }
",non-flaky,5
175748,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testUncheckStopPreviousVersionButtonWhenDisabled,"  @Test
  public void testUncheckStopPreviousVersionButtonWhenDisabled() {
    deployPanel = createPanel(true /* requireValues */);

    Button promoteButton = getButtonWithText(""Promote the deployed version to receive all traffic"");
    Button stopButton = getButtonWithText(""Stop previous version"");
    SWTBotCheckBox promote = new SWTBotCheckBox(promoteButton);
    SWTBotCheckBox stop = new SWTBotCheckBox(stopButton);

    // Initially, everything is checked and enabled.
    assertTrue(promoteButton.getSelection());
    assertTrue(stopButton.getSelection());
    assertTrue(stopButton.getEnabled());

    promote.click();
    assertFalse(promoteButton.getSelection());
    assertFalse(stopButton.getSelection());
    assertFalse(stopButton.getEnabled());

    promote.click();
    assertTrue(promoteButton.getSelection());
    assertTrue(stopButton.getSelection());
    assertTrue(stopButton.getEnabled());

    stop.click();
    assertTrue(promoteButton.getSelection());
    assertFalse(stopButton.getSelection());
    assertTrue(stopButton.getEnabled());

    promote.click();
    assertFalse(promoteButton.getSelection());
    assertFalse(stopButton.getSelection());
    assertFalse(stopButton.getEnabled());

    promote.click();
    assertTrue(promoteButton.getSelection());
    assertFalse(stopButton.getSelection());
    assertTrue(stopButton.getEnabled());
  }
",non-flaky,5
175749,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testProjectSavedInPreferencesSelected,"  @Test
  public void testProjectSavedInPreferencesSelected()
      throws ProjectRepositoryException, InterruptedException, BackingStoreException {
    IEclipsePreferences node =
        new ProjectScope(project).getNode(DeployPreferences.PREFERENCE_STORE_QUALIFIER);
    try {
      node.put(""project.id"", ""projectId1"");
      node.put(""account.email"", EMAIL_1);
      initializeProjectRepository();
      when(loginService.getAccounts()).thenReturn(twoAccountSet);
      deployPanel = createPanel(true /* requireValues */);
      deployPanel.latestGcpProjectQueryJob.join();

      ProjectSelector projectSelector = getProjectSelector();
      IStructuredSelection selection = projectSelector.getViewer().getStructuredSelection();
      assertThat(selection.size(), is(1));
      assertThat(((GcpProject) selection.getFirstElement()).getId(), is(""projectId1""));
    } finally {
      node.clear();
    }
  }
",non-flaky,5
175750,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testProjectNotSelectedIsAnErrorWhenRequireValuesIsTrue,"  @Test
  public void testProjectNotSelectedIsAnErrorWhenRequireValuesIsTrue() {
    deployPanel = createPanel(true /* requireValues */);
    assertThat(getProjectSelectionValidator().getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175751,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testProjectNotSelectedIsNotAnErrorWhenRequireValuesIsFalse,"  @Test
  public void testProjectNotSelectedIsNotAnErrorWhenRequireValuesIsFalse() {
    deployPanel = createPanel(false /* requireValues */);
    assertThat(getProjectSelectionValidator().getSeverity(), is(IStatus.INFO));
  }
",non-flaky,5
175752,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testProjectsExistThenNoProjectNotFoundError,"  @Test
  public void testProjectsExistThenNoProjectNotFoundError()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(oneAccountSet);
    initializeProjectRepository();
    deployPanel = createPanel(false /* requireValues */);
    selectAccount(account1);
    deployPanel.latestGcpProjectQueryJob.join();
    assertThat(getProjectSelectionValidator().getSeverity(), is(IStatus.OK));
  }
",non-flaky,5
175753,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testRefreshProjectsForSelectedCredential,"  @Test
  public void testRefreshProjectsForSelectedCredential()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(twoAccountSet);
    initializeProjectRepository();

    deployPanel = createPanel(false /* requireValues */);
    Table projectTable = getProjectSelector().getViewer().getTable();
    assertNull(deployPanel.latestGcpProjectQueryJob);
    assertThat(projectTable.getItemCount(), is(0));

    selectAccount(account1);
    assertNotNull(deployPanel.latestGcpProjectQueryJob);
    deployPanel.latestGcpProjectQueryJob.join();
    assertThat(projectTable.getItemCount(), is(2));
    assertThat(((GcpProject) projectTable.getItem(0).getData()).getId(), is(""projectId1""));
    assertThat(((GcpProject) projectTable.getItem(1).getData()).getId(), is(""projectId2""));
  }
",non-flaky,5
175754,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testRefreshProjectsForSelectedCredential_switchAccounts,"  @Test
  public void testRefreshProjectsForSelectedCredential_switchAccounts()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(twoAccountSet);
    initializeProjectRepository();

    deployPanel = createPanel(false /* requireValues */);
    Table projectTable = getProjectSelector().getViewer().getTable();
    assertNull(deployPanel.latestGcpProjectQueryJob);
    assertThat(projectTable.getItemCount(), is(0));

    selectAccount(account1);
    Job jobForAccount1 = deployPanel.latestGcpProjectQueryJob;
    jobForAccount1.join();
    assertThat(projectTable.getItemCount(), is(2));

    selectAccount(account2);
    assertNotEquals(jobForAccount1, deployPanel.latestGcpProjectQueryJob);
    deployPanel.latestGcpProjectQueryJob.join();
    assertThat(projectTable.getItemCount(), is(1));
    assertThat(((GcpProject) projectTable.getItem(0).getData()).getId(), is(""projectId2""));
  }
",non-flaky,5
175755,GoogleCloudPlatform_google-cloud-eclipse,AppEngineDeployPreferencesPanelTest.testNoProjectSelectedWhenSwitchingAccounts,"  @Test
  public void testNoProjectSelectedWhenSwitchingAccounts()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(twoAccountSet);
    initializeProjectRepository();

    deployPanel = createPanel(false /* requireValues */);
    selectAccount(account1);
    deployPanel.latestGcpProjectQueryJob.join();

    Table projectTable = getProjectSelector().getViewer().getTable();
    assertThat(projectTable.getItemCount(), is(2));
    projectTable.setSelection(0);
    assertThat(projectTable.getSelectionCount(), is(1));

    selectAccount(account2);
    deployPanel.latestGcpProjectQueryJob.join();

    assertThat(projectTable.getItemCount(), is(1));
    assertThat(projectTable.getSelectionCount(), is(0));
  }
",non-flaky,5
175756,GoogleCloudPlatform_google-cloud-eclipse,PluginXmlTest.testLimitedVisibility,"  @Test
  public void testLimitedVisibility() {
    NodeList pages = getDocument().getElementsByTagName(""page"");
    Assert.assertEquals(2, pages.getLength());
    NodeList enabledWhen = getDocument().getElementsByTagName(""enabledWhen"");
    Assert.assertEquals(4, enabledWhen.getLength());
    NodeList tests = getDocument().getElementsByTagName(""test"");
    Assert.assertEquals(4, tests.getLength());
    NodeList adapts = getDocument().getElementsByTagName(""adapt"");
    Assert.assertEquals(4, adapts.getLength());

    for (int i = 0; i < enabledWhen.getLength(); i++) {
      Element element = (Element) enabledWhen.item(i);
      Node parent = element.getParentNode();
      assertThat(parent.getNodeName(), either(is(""page"")).or(is(""handler"")));
    }

    Element standardAdapt = (Element) adapts.item(0);
    verifyAdapt(standardAdapt, AppEngineStandardFacet.ID);
    Element flexAdapt = (Element) adapts.item(1);
    verifyAdapt(flexAdapt, AppEngineFlexFacet.ID);
  }
",non-flaky,5
175757,GoogleCloudPlatform_google-cloud-eclipse,GcpProjectQueryJobTest.testNullCredential,"  @Test(expected = NullPointerException.class)
  public void testNullCredential() {
    new GcpProjectQueryJob(null /* credential */, projectRepository, projectSelector,
        dataBindingContext, isLatestQueryJob);
  }
",non-flaky,5
175758,GoogleCloudPlatform_google-cloud-eclipse,GcpProjectQueryJobTest.testRun_setsProjects,"  @Test
  public void testRun_setsProjects() throws InterruptedException, ProjectRepositoryException {
    queryJob.schedule();
    queryJob.join();

    verify(projectRepository).getProjects(credential);
    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector).isDisposed();
    verify(projectSelector).setProjects(projects);
  }
",non-flaky,5
175759,GoogleCloudPlatform_google-cloud-eclipse,GcpProjectQueryJobTest.testRun_abandonIfDisposed,"  @Test
  public void testRun_abandonIfDisposed() throws InterruptedException, ProjectRepositoryException {
    when(projectSelector.isDisposed()).thenReturn(true);

    queryJob.schedule();
    queryJob.join();

    verify(projectRepository).getProjects(credential);
    verify(projectSelector, never()).setProjects(projects);
  }
",non-flaky,5
175760,GoogleCloudPlatform_google-cloud-eclipse,GcpProjectQueryJobTest.testRun_abandonIfNotLatestJob,"  @Test
  public void testRun_abandonIfNotLatestJob()
      throws InterruptedException, ProjectRepositoryException {
    when(isLatestQueryJob.apply(queryJob)).thenReturn(false);

    queryJob.schedule();
    queryJob.join();

    verify(projectRepository).getProjects(credential);
    verify(projectSelector, never()).setProjects(projects);
  }
",non-flaky,5
175761,GoogleCloudPlatform_google-cloud-eclipse,GcpProjectQueryJobTest.testRun_abandonStaleJob,"  @Test
  public void testRun_abandonStaleJob() throws InterruptedException, ProjectRepositoryException {
    // Prepare another concurrent query job.
    Credential staleCredential = mock(Credential.class);

    List<GcpProject> anotherProjectList = mock(List.class);
    ProjectRepository projectRepository2 = mock(ProjectRepository.class);
    when(projectRepository2.getProjects(staleCredential)).thenReturn(anotherProjectList);

    Predicate<Job> notLatest = mock(Predicate.class);
    Job staleJob = new GcpProjectQueryJob(staleCredential, projectRepository2,
        projectSelector, dataBindingContext, notLatest);

    // This second job is stale, i.e., it was fired, but user has selected another credential.
    when(notLatest.apply(staleJob)).thenReturn(false);

    queryJob.schedule();
    queryJob.join();
    // Make the stale job complete even after ""queryJob"" finishes.
    staleJob.schedule();
    staleJob.join();

    verify(projectRepository).getProjects(credential);
    verify(projectRepository2).getProjects(staleCredential);

    verify(projectSelector).setProjects(projects);
    verify(projectSelector, never()).setProjects(anotherProjectList);
  }
",non-flaky,5
175762,GoogleCloudPlatform_google-cloud-eclipse,BlankDeployPreferencesPanelTest.testGetHelpContextId,"  @Test
  public void testGetHelpContextId() {
    assertNull(new BlankDeployPreferencesPanel(shellTestResource.getShell()).getHelpContextId());
  }
",non-flaky,5
175763,GoogleCloudPlatform_google-cloud-eclipse,DeployPropertyPageTest.testCorrectPanelIsShownForFacetedProject,"  @Test
  public void testCorrectPanelIsShownForFacetedProject() {
    DeployPropertyPage page = new DeployPropertyPage(loginService, googleApiFactory);
    Shell parent = shellTestResource.getShell();
    page.setElement(getProject());
    page.createControl(parent);
    page.setVisible(true);
    Composite preferencePageComposite = (Composite) parent.getChildren()[0];
    for (Control control : preferencePageComposite.getChildren()) {
      if (control instanceof Composite) {
        Composite maybeDeployPageComposite = (Composite) control;
        Layout layout = maybeDeployPageComposite.getLayout();
        if (layout instanceof StackLayout) {
          StackLayout stackLayout = (StackLayout) layout;
          assertThat(stackLayout.topControl, instanceOf(getPanelClass()));
          return;
        }
      }
    }
    fail(""Did not find the deploy preferences panel"");
  }
",non-flaky,5
175764,GoogleCloudPlatform_google-cloud-eclipse,StandardDeployPreferencesPanelTest.testGetHelpContextId,"  @Test
  public void testGetHelpContextId() {
    IProject project = mock(IProject.class);
    when(project.getName()).thenReturn("""");
    StandardDeployPreferencesPanel panel = new StandardDeployPreferencesPanel(
        shellResource.getShell(), project, mock(IGoogleLoginService.class), mock(Runnable.class),
        false, mock(ProjectRepository.class));

    assertEquals(
        ""com.google.cloud.tools.eclipse.appengine.deploy.ui.DeployAppEngineStandardProjectContext"",
        panel.getHelpContextId());
  }
",non-flaky,5
175765,GoogleCloudPlatform_google-cloud-eclipse,FlexDeployPreferencesPanelTest.testGetHelpContextId,"  @Test
  public void testGetHelpContextId() {
    FlexDeployPreferencesPanel panel = createPanel(true /* requireValues */);

    assertEquals(
        ""com.google.cloud.tools.eclipse.appengine.deploy.ui.DeployAppEngineFlexProjectContext"",
        panel.getHelpContextId());
  }
",non-flaky,5
175766,GoogleCloudPlatform_google-cloud-eclipse,FlexDeployPreferencesPanelTest.testDefaultAppYamlPathSet,"  @Test
  public void testDefaultAppYamlPathSet() {
    FlexDeployPreferencesPanel panel = createPanel(true /* requireValues */);

    Text appYamlField = findAppYamlField(panel);
    assertEquals(""src/main/appengine/app.yaml"", appYamlField.getText());
    assertTrue(getAppYamlPathValidationStatus(panel).isOK());
  }
",non-flaky,5
175767,GoogleCloudPlatform_google-cloud-eclipse,FlexDeployPreferencesPanelTest.testAppYamlPathValidation_nonExistingAppYaml,"  @Test
  public void testAppYamlPathValidation_nonExistingAppYaml() {
    FlexDeployPreferencesPanel panel = createPanel(true /* requireValues */);

    Text appYamlField = findAppYamlField(panel);
    appYamlField.setText(""non/existing/app.yaml"");
    assertFalse(getAppYamlPathValidationStatus(panel).isOK());
  }
",non-flaky,5
175768,GoogleCloudPlatform_google-cloud-eclipse,FlexDeployPreferencesPanelTest.testAppYamlPathValidation_noValidationIfRequireValuesIsFalse,"  @Test
  public void testAppYamlPathValidation_noValidationIfRequireValuesIsFalse() {
    FlexDeployPreferencesPanel panel = createPanel(false /* requireValues */);

    Text appYamlField = findAppYamlField(panel);
    appYamlField.setText(""non/existing/app.yaml"");
    assertNull(getAppYamlPathValidationStatus(panel));
  }
",non-flaky,5
175769,GoogleCloudPlatform_google-cloud-eclipse,FlexDeployPreferencesPanelTest.testAppYamlPathValidation_absolutePathWorks,"  @Test
  public void testAppYamlPathValidation_absolutePathWorks() {
    FlexDeployPreferencesPanel panel = createPanel(true /* requireValues */);
    Text appYamlField = findAppYamlField(panel);

    IPath absolutePath = project.getLocation().append(""src/main/appengine/app.yaml"");
    assertTrue(absolutePath.isAbsolute());

    appYamlField.setText(absolutePath.toString());
    assertTrue(getAppYamlPathValidationStatus(panel).isOK());
  }
",non-flaky,5
175770,GoogleCloudPlatform_google-cloud-eclipse,FlexDeployPreferencesDialogTest.testFlexPricingLabel,"  @Test
  public void testFlexPricingLabel() {
    dialog.setBlockOnOpen(false);
    dialog.open();
    Composite dialogArea = (Composite) dialog.createDialogArea(shellResource.getShell());

    assertNotNull(findGcpPricingLink(dialogArea));
  }
",non-flaky,5
175771,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testContructor_nonAbsoluteBasePath,"  @Test
  public void testContructor_nonAbsoluteBasePath() {
    try {
      when(appYamlPath.getValue()).thenReturn(""app.yaml"");
      new AppYamlValidator(new Path(""non/absolute/base/path""), appYamlPath);
      fail();
    } catch (IllegalArgumentException ex) {
      assertEquals(""basePath is not absolute."", ex.getMessage());
    }
  }
",non-flaky,5
175772,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidate_relativePathAndNoAppYaml,"  @Test
  public void testValidate_relativePathAndNoAppYaml() {
    when(appYamlPath.getValue()).thenReturn(""relative/path/app.yaml"");

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""app.yaml does not exist."", result.getMessage());
  }
",non-flaky,5
175773,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidate_absolutePathAndNoAppYaml,"  @Test
  public void testValidate_absolutePathAndNoAppYaml() {
    String absolutePath = basePath + ""/sub/directory/app.yaml"";
    when(appYamlPath.getValue()).thenReturn(absolutePath);

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""app.yaml does not exist."", result.getMessage());
  }
",non-flaky,5
175774,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidate_relativePathAndInvalidFileName,"  @Test
  public void testValidate_relativePathAndInvalidFileName() {
    when(appYamlPath.getValue()).thenReturn(""relative/path/my-app.yaml"");

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""File name is not app.yaml: ""
        + new Path(basePath + ""/relative/path/my-app.yaml"").toOSString(),
        result.getMessage());
  }
",non-flaky,5
175775,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidate_absolutePathInvalidFileName,"  @Test
  public void testValidate_absolutePathInvalidFileName() {
    String absolutePath = basePath + ""/sub/directory/my-app.yaml"";
    when(appYamlPath.getValue()).thenReturn(absolutePath);

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""File name is not app.yaml: ""
        + new Path(basePath + ""/sub/directory/my-app.yaml"").toOSString(),
        result.getMessage());
  }
",non-flaky,5
175776,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidate_relativePathNotFile,"  @Test
  public void testValidate_relativePathNotFile() {
    createAppYamlAsDirectory(basePath);
    when(appYamlPath.getValue()).thenReturn(""app.yaml"");

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""Not a file: "" + new Path(basePath + ""/app.yaml"").toOSString(),
        result.getMessage());
  }
",non-flaky,5
175777,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidate_absolutePathNotFile,"  @Test
  public void testValidate_absolutePathNotFile() {
    createAppYamlAsDirectory(basePath);

    String absolutePath = basePath + ""/app.yaml"";
    when(appYamlPath.getValue()).thenReturn(absolutePath);

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""Not a file: "" + new Path(basePath + ""/app.yaml"").toOSString(),
        result.getMessage());
  }
",non-flaky,5
175778,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidate_relativePathWithAppYaml,"  @Test
  public void testValidate_relativePathWithAppYaml() throws IOException {
    createAppYamlFile(basePath + ""/some/directory"", ""runtime: java"");

    when(appYamlPath.getValue()).thenReturn(""some/directory/app.yaml"");
    IStatus result = pathValidator.validate();
    assertTrue(result.isOK());
  }
",non-flaky,5
175779,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidate_absolutePathWithAppYaml,"  @Test
  public void testValidate_absolutePathWithAppYaml() throws IOException {
    File absolutePath = tempFolder.newFolder(""another"", ""folder"");
    File appYaml = createAppYamlFile(absolutePath.toString(), ""runtime: java"");

    when(appYamlPath.getValue()).thenReturn(appYaml.toString());
    IStatus result = pathValidator.validate();
    assertTrue(result.isOK());
  }
",non-flaky,5
175780,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidateRuntime_javaRuntime,"  @Test
  public void testValidateRuntime_javaRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""runtime: java"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertTrue(result.isOK());
  }
",non-flaky,5
175781,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidateRuntime_malformedAppYaml,"  @Test
  public void testValidateRuntime_malformedAppYaml() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), "": m a l f o r m e d !"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""Malformed app.yaml."", result.getMessage());
  }
",non-flaky,5
175782,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidateRuntime_noRuntime,"  @Test
  public void testValidateRuntime_noRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""env: flex"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""\""runtime: null\"" in app.yaml is not \""java\""."", result.getMessage());
  }
",non-flaky,5
175783,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidateRuntime_nullRuntime,"  @Test
  public void testValidateRuntime_nullRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""runtime:"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""\""runtime: null\"" in app.yaml is not \""java\""."", result.getMessage());
  }
",non-flaky,5
175784,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidateRuntime_notJavaRuntime,"  @Test
  public void testValidateRuntime_notJavaRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""runtime: python"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""\""runtime: python\"" in app.yaml is not \""java\""."", result.getMessage());
  }
",non-flaky,5
175785,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidateRuntime_customRuntime,"  @Test
  public void testValidateRuntime_customRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""runtime: custom"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""\""runtime: custom\"" is not yet supported by Cloud Tools for Eclipse."",
        result.getMessage());
  }
",non-flaky,5
175786,GoogleCloudPlatform_google-cloud-eclipse,AppYamlValidatorTest.testValidateRuntime_ioException,"  @Test
  public void testValidateRuntime_ioException() {
    File nonExisting = new File(""/non/existing/file"");
    IStatus result = AppYamlValidator.validateRuntime(nonExisting);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertTrue(result.getMessage().startsWith(""Cannot read app.yaml:""));
  }
",non-flaky,5
175787,GoogleCloudPlatform_google-cloud-eclipse,AppEngineApplicationQueryJobTest.testRun_projectHasNoApplication,"  @Test
  public void testRun_projectHasNoApplication()
      throws ProjectRepositoryException, InterruptedException {
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);
    assertNull(project.getAppEngine());

    queryJob.schedule();
    queryJob.join();

    verify(projectRepository).getAppEngineApplication(credential, ""projectId"");
    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector).isDisposed();
    verify(projectSelection).isEmpty();
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);

    assertEquals(AppEngine.NO_APPENGINE_APPLICATION, project.getAppEngine());
  }
",non-flaky,5
175788,GoogleCloudPlatform_google-cloud-eclipse,AppEngineApplicationQueryJobTest.testRun_projectHasApplication,"  @Test
  public void testRun_projectHasApplication()
      throws ProjectRepositoryException, InterruptedException {
    AppEngine appEngine = AppEngine.withId(""unique-id"");
    when(projectRepository.getAppEngineApplication(credential, ""projectId"")).thenReturn(appEngine);

    queryJob.schedule();
    queryJob.join();

    verify(isLatestQueryJob, never()).apply(queryJob);
    verify(projectSelector, never()).isDisposed();
    verify(projectSelector, never()).setStatusLink(anyString(), anyString());

    assertTrue(appEngine == project.getAppEngine());
  }
",non-flaky,5
175789,GoogleCloudPlatform_google-cloud-eclipse,AppEngineApplicationQueryJobTest.testRun_queryError,"  @Test
  public void testRun_queryError() throws ProjectRepositoryException, InterruptedException {
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenThrow(new ProjectRepositoryException(""testException""));

    queryJob.schedule();
    queryJob.join();

    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector).isDisposed();
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_EXCEPTION, null);

    assertNull(project.getAppEngine());
  }
",non-flaky,5
175790,GoogleCloudPlatform_google-cloud-eclipse,AppEngineApplicationQueryJobTest.testRun_abandonIfDisposed,"  @Test
  public void testRun_abandonIfDisposed() throws InterruptedException, ProjectRepositoryException {
    when(projectSelector.isDisposed()).thenReturn(true);
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    queryJob.schedule();
    queryJob.join();

    verify(projectSelector).isDisposed();
    verify(projectSelector, never()).setStatusLink(anyString(), anyString());
  }
",non-flaky,5
175791,GoogleCloudPlatform_google-cloud-eclipse,AppEngineApplicationQueryJobTest.testRun_abandonIfNotLatestJob,"  @Test
  public void testRun_abandonIfNotLatestJob()
      throws InterruptedException, ProjectRepositoryException {
    when(isLatestQueryJob.apply(queryJob)).thenReturn(false);
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    queryJob.schedule();
    queryJob.join();

    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector, never()).setStatusLink(anyString(), anyString());
  }
",non-flaky,5
175792,GoogleCloudPlatform_google-cloud-eclipse,AppEngineApplicationQueryJobTest.testRun_abandonIfProjectSelectorHasNoSelection,"  @Test
  public void testRun_abandonIfProjectSelectorHasNoSelection()
      throws ProjectRepositoryException, InterruptedException {
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);
    when(projectSelection.isEmpty()).thenReturn(true);

    queryJob.schedule();
    queryJob.join();

    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector, never()).setStatusLink(anyString(), anyString());
  }
",non-flaky,5
175793,GoogleCloudPlatform_google-cloud-eclipse,AppEngineApplicationQueryJobTest.testRun_abandonStaleJob,"  @Test
  public void testRun_abandonStaleJob() throws InterruptedException, ProjectRepositoryException {
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    // Prepare another concurrent query job.
    Credential staleCredential = mock(Credential.class);

    GcpProject staleProject = new GcpProject(""name"", ""staleProjectId"");
    ProjectRepository projectRepository2 = mock(ProjectRepository.class);
    when(projectRepository2.getAppEngineApplication(staleCredential, ""staleProjectId""))
        .thenThrow(new ProjectRepositoryException(""testException""));

    Predicate<Job> notLatest = mock(Predicate.class);
    Job staleJob = new AppEngineApplicationQueryJob(staleProject, staleCredential,
        projectRepository2, projectSelector, EXPECTED_LINK, notLatest);

    // This second job is stale, i.e., it was fired, but user has selected another credential.
    when(notLatest.apply(staleJob)).thenReturn(false);

    queryJob.schedule();
    queryJob.join();
    // Make the stale job complete even after ""queryJob"" finishes.
    staleJob.schedule();
    staleJob.join();

    verify(projectRepository).getAppEngineApplication(credential, ""projectId"");
    verify(projectRepository2).getAppEngineApplication(staleCredential, ""staleProjectId"");

    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);
    verify(projectSelector, never()).setStatusLink(EXPECTED_MESSAGE_WHEN_EXCEPTION, null);
  }
",non-flaky,5
175794,GoogleCloudPlatform_google-cloud-eclipse,RelativeFileFieldSetterTest.testConstructor_nonAbsoluteBasePath,"  @Test
  public void testConstructor_nonAbsoluteBasePath() {
    try {
      new RelativeFileFieldSetter(field, new Path(""non/absolute/base/path""), dialog);
      fail();
    } catch (IllegalArgumentException ex) {}
  }
",non-flaky,5
175795,GoogleCloudPlatform_google-cloud-eclipse,RelativeFileFieldSetterTest.testFileDialogCanceled,"  @Test
  public void testFileDialogCanceled() {
    when(field.getText()).thenReturn("""");
    when(dialog.open()).thenReturn(null /* means canceled */);

    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    verify(field, never()).setText(anyString());
  }
",non-flaky,5
175796,GoogleCloudPlatform_google-cloud-eclipse,RelativeFileFieldSetterTest.testSetField,"  @Test
  public void testSetField() {
    when(field.getText()).thenReturn("""");
    when(dialog.open()).thenReturn(basePath + ""/sub/directory/app.yaml"");

    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    verify(field).setText(""sub/directory/app.yaml"");
  }
",non-flaky,5
175797,GoogleCloudPlatform_google-cloud-eclipse,RelativeFileFieldSetterTest.testSetField_userSuppliesPathOutsideBase,"  @Test
  public void testSetField_userSuppliesPathOutsideBase() {
    when(field.getText()).thenReturn("""");
    when(dialog.open()).thenReturn(""/path/outside/base/app.yaml"");

    new RelativeFileFieldSetter(field, new Path(""/base/path""), dialog).widgetSelected(event);
    verify(field).setText(""../../path/outside/base/app.yaml"");
  }
",non-flaky,5
175798,GoogleCloudPlatform_google-cloud-eclipse,RelativeFileFieldSetterTest.testFileDialogFilterSet_relativePathInField,"  @Test
  public void testFileDialogFilterSet_relativePathInField() {
    when(field.getText()).thenReturn(""src/main/appengine/app.yaml"");
    when(dialog.open()).thenReturn(null);

    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    // ""basePath"" is the first physically existing directory.
    verify(dialog).setFilterPath(basePath.toString());

    basePath.append(""src"").toFile().mkdir();
    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    verify(dialog).setFilterPath(basePath + ""/src"");
  }
",non-flaky,5
175799,GoogleCloudPlatform_google-cloud-eclipse,RelativeFileFieldSetterTest.testFileDialogFilterSet_absolutePathInField,"  @Test
  public void testFileDialogFilterSet_absolutePathInField() {
    when(field.getText()).thenReturn(basePath + ""/deploy/temp/app.yaml"");
    when(dialog.open()).thenReturn(null);

    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    // ""basePath"" is the first physically existing directory.
    verify(dialog).setFilterPath(basePath.toString());

    basePath.append(""deploy"").toFile().mkdir();
    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    verify(dialog).setFilterPath(basePath + ""/deploy"");
  }
",non-flaky,5
175800,GoogleCloudPlatform_google-cloud-eclipse,ProjectSelectorSelectionChangedListenerTest.testSelectionChanged_emptySelection,"  @Test
  public void testSelectionChanged_emptySelection() {
    when(event.getSelection()).thenReturn(new StructuredSelection());
    listener.selectionChanged(event);
    verify(projectSelector).clearStatusLink();
  }
",non-flaky,5
175801,GoogleCloudPlatform_google-cloud-eclipse,ProjectSelectorSelectionChangedListenerTest.testSelectionChanged_repositoryException,"  @Test
  public void testSelectionChanged_repositoryException()
      throws ProjectRepositoryException, InterruptedException {
    initSelectionAndAccountSelector();
    when(projectRepository.getAppEngineApplication(any(Credential.class), anyString()))
        .thenThrow(new ProjectRepositoryException(""testException""));

    listener.selectionChanged(event);
    listener.latestQueryJob.join();
    verify(projectSelector).clearStatusLink();  // Should clear initially.
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_EXCEPTION, null /* tooltip */);
  }
",non-flaky,5
175802,GoogleCloudPlatform_google-cloud-eclipse,ProjectSelectorSelectionChangedListenerTest.testSelectionChanged_noAppEngineApplication,"  @Test
  public void testSelectionChanged_noAppEngineApplication()
      throws ProjectRepositoryException, InterruptedException {
    initSelectionAndAccountSelector();
    when(projectRepository.getAppEngineApplication(any(Credential.class), anyString()))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    listener.selectionChanged(event);
    listener.latestQueryJob.join();
    verify(projectSelector).clearStatusLink();  // Should clear initially.
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);
  }
",non-flaky,5
175803,GoogleCloudPlatform_google-cloud-eclipse,ProjectSelectorSelectionChangedListenerTest.testSelectionChanged_hasAppEngineApplication,"  @Test
  public void testSelectionChanged_hasAppEngineApplication()
      throws ProjectRepositoryException, InterruptedException {
    initSelectionAndAccountSelector();
    when(projectRepository.getAppEngineApplication(any(Credential.class), anyString()))
        .thenReturn(AppEngine.withId(""id""));

    listener.selectionChanged(event);
    listener.latestQueryJob.join();
    verify(projectSelector).clearStatusLink();
  }
",non-flaky,5
175804,GoogleCloudPlatform_google-cloud-eclipse,ProjectSelectorSelectionChangedListenerTest.testSelectionChanged_doNotRunQueryJobIfCached,"  @Test
  public void testSelectionChanged_doNotRunQueryJobIfCached() throws ProjectRepositoryException {
    GcpProject gcpProject = new GcpProject(""projectName"", ""projectId"");
    initSelectionAndAccountSelector(gcpProject);
    gcpProject.setAppEngine(AppEngine.withId(""id""));

    listener.selectionChanged(event);
    assertNull(listener.latestQueryJob);
    verify(projectRepository, never()).getAppEngineApplication(any(Credential.class), anyString());
    verify(projectSelector).clearStatusLink();
  }
",non-flaky,5
175805,GoogleCloudPlatform_google-cloud-eclipse,ProjectSelectorSelectionChangedListenerTest.testSelectionChanged_whenCachedResultIsNoAppEngineApplication,"  @Test
  public void testSelectionChanged_whenCachedResultIsNoAppEngineApplication()
      throws ProjectRepositoryException {
    GcpProject gcpProject = new GcpProject(""projectName"", ""projectId"");
    initSelectionAndAccountSelector(gcpProject);
    gcpProject.setAppEngine(AppEngine.NO_APPENGINE_APPLICATION);

    listener.selectionChanged(event);
    assertNull(listener.latestQueryJob);
    verify(projectRepository, never()).getAppEngineApplication(any(Credential.class), anyString());
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);
  }
",non-flaky,5
175806,GoogleCloudPlatform_google-cloud-eclipse,ProjectSelectorSelectionChangedListenerTest.testSelectionChanged_changeSelectedProject,"  @Test
  public void testSelectionChanged_changeSelectedProject()
      throws ProjectRepositoryException, InterruptedException {
    when(projectRepository.getAppEngineApplication(any(Credential.class), eq(""oldProjectId"")))
        .thenThrow(new ProjectRepositoryException(""testException""));
    when(projectRepository.getAppEngineApplication(any(Credential.class), eq(""projectId"")))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    initSelectionAndAccountSelector(new GcpProject(""oldProjectName"", ""oldProjectId""));
    listener.selectionChanged(event);

    Job oldJob = listener.latestQueryJob;
    assertNotNull(oldJob);
    oldJob.join();

    initSelectionAndAccountSelector();
    listener.selectionChanged(event);

    Job newJob = listener.latestQueryJob;
    assertNotNull(newJob);
    assertNotEquals(oldJob, newJob);
    newJob.join();

    verify(projectRepository).getAppEngineApplication(any(Credential.class), eq(""oldProjectId""));
    verify(projectRepository).getAppEngineApplication(any(Credential.class), eq(""projectId""));
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);
  }
",non-flaky,5
175807,GoogleCloudPlatform_google-cloud-eclipse,MessagesTest.testUrlOpenErrorDialogTitle,"  @Test
  public void testUrlOpenErrorDialogTitle() {
    assertEquals(""Error"", Messages.getString(""openurllistener.error.title""));
  }
",non-flaky,5
175808,GoogleCloudPlatform_google-cloud-eclipse,MessagesTest.testUrlOpenErrorDialogMessage,"  @Test
  public void testUrlOpenErrorDialogMessage() {
    assertEquals(""Could not open URL"", Messages.getString(""openurllistener.error.message""));
  }
",non-flaky,5
175809,GoogleCloudPlatform_google-cloud-eclipse,MessagesTest.testInvalidUrlErrorMessage,"  @Test
  public void testInvalidUrlErrorMessage() {
    assertEquals(""Invalid URL: http://www.example.com"", 
        Messages.getString(""invalid.url"", ""http://www.example.com""));
  }
",non-flaky,5
175810,GoogleCloudPlatform_google-cloud-eclipse,SharedImagesTest.testCreateRefreshIcon,"  @Test
  public void testCreateRefreshIcon() {
    assertNotNull(SharedImages.REFRESH_IMAGE_DESCRIPTOR.createImage(shell.getDisplay()));
  }
",non-flaky,5
175811,GoogleCloudPlatform_google-cloud-eclipse,PluginXmlTest.testExtensionPoint,"  @Test
  public void testExtensionPoint() {
    NodeList extensions = getDocument().getElementsByTagName(""extension"");
    assertEquals(1, extensions.getLength());
    Element extension = (Element) extensions.item(0);
    assertEquals(""org.eclipse.ui.commands"", extension.getAttribute(""point""));

    NodeList commandDefinitions = extension.getElementsByTagName(""command"");
    assertEquals(1, commandDefinitions.getLength());
    Element configExtension = (Element) commandDefinitions.item(0);
    assertEquals(OpenDropDownMenuHandler.class.getName(),
        configExtension.getAttribute(""defaultHandler""));
    assertEquals(""com.google.cloud.tools.eclipse.ui.util.showPopup"",
        configExtension.getAttribute(""id""));
  }
",non-flaky,5
175812,GoogleCloudPlatform_google-cloud-eclipse,FontUtilTest.testConvertFontToBold,"  @Test
  public void testConvertFontToBold() {
    Label label = new Label(shellTestResource.getShell(), SWT.NONE);
    for (FontData fontData : label.getFont().getFontData()) {
      assertThat(fontData.getStyle(), is(not(SWT.BOLD)));
    }
    FontUtil.convertFontToBold(label);
    for (FontData fontData : label.getFont().getFontData()) {
      assertThat(fontData.getStyle(), is(SWT.BOLD));
    }
  }
",non-flaky,5
175813,GoogleCloudPlatform_google-cloud-eclipse,FontUtilTest.testConvertFontToItalic,"  @Test
  public void testConvertFontToItalic() {
    Label label = new Label(shellTestResource.getShell(), SWT.NONE);
    for (FontData fontData : label.getFont().getFontData()) {
      assertThat(fontData.getStyle(), is(not(SWT.ITALIC)));
    }
    FontUtil.convertFontToItalic(label);
    for (FontData fontData : label.getFont().getFontData()) {
      assertThat(fontData.getStyle(), is(SWT.ITALIC));
    }
  }
",non-flaky,5
175814,GoogleCloudPlatform_google-cloud-eclipse,OpenUriSelectionListenerTest.testWidgetSelected_InvalidURI,"  @Test
  public void testWidgetSelected_InvalidURI() {
    SelectionEvent selectionEvent = getEvent(INVALID_URI);

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(URISyntaxException.class));
  }
",non-flaky,5
175815,GoogleCloudPlatform_google-cloud-eclipse,OpenUriSelectionListenerTest.testWidgetDefaultSelected_InvalidURI,"  @Test
  public void testWidgetDefaultSelected_InvalidURI() {
    SelectionEvent selectionEvent = getEvent(INVALID_URI);

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetDefaultSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(URISyntaxException.class));
  }
",non-flaky,5
175816,GoogleCloudPlatform_google-cloud-eclipse,OpenUriSelectionListenerTest.testWidgetSelected_MalformedURL,"  @Test
  public void testWidgetSelected_MalformedURL() {
    SelectionEvent selectionEvent = getEvent(MALFORMED_URL);

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(MalformedURLException.class));
  }
",non-flaky,5
175817,GoogleCloudPlatform_google-cloud-eclipse,OpenUriSelectionListenerTest.testWidgetDefaultSelected_MalformedURL,"  @Test
  public void testWidgetDefaultSelected_MalformedURL() {
    SelectionEvent selectionEvent = getEvent(MALFORMED_URL);

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetDefaultSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(MalformedURLException.class));
  }
",non-flaky,5
175818,GoogleCloudPlatform_google-cloud-eclipse,OpenUriSelectionListenerTest.testWidgetSelected_errorInvokingBrowser,"  @Test
  public void testWidgetSelected_errorInvokingBrowser() throws PartInitException {
    SelectionEvent selectionEvent = getEvent(VALID_URI);
    doThrow(new PartInitException(""fake exception"")).when(browser).openURL(any(URL.class));

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(PartInitException.class));
  }
",non-flaky,5
175819,GoogleCloudPlatform_google-cloud-eclipse,OpenUriSelectionListenerTest.testWidgetDefaultSelected_errorInvokingBrowser,"  @Test
  public void testWidgetDefaultSelected_errorInvokingBrowser() throws PartInitException {
    SelectionEvent selectionEvent = getEvent(VALID_URI);
    doThrow(new PartInitException(""fake exception"")).when(browser).openURL(any(URL.class));

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport)
      .widgetDefaultSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(PartInitException.class));
  }
",non-flaky,5
175820,GoogleCloudPlatform_google-cloud-eclipse,OpenUriSelectionListenerTest.testWidgetSelected_successful,"  @Test
  public void testWidgetSelected_successful() throws PartInitException, MalformedURLException {
    SelectionEvent selectionEvent = getEvent(VALID_URI);
    when(queryParameterProvider.getParameters()).thenReturn(Collections.singletonMap(URL_PARAM_PROJECT, PROJECT_ID));

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetSelected(selectionEvent);
    verify(errorHandler, never()).handle(any(Exception.class), any(URI.class));
    verify(browser).openURL(new URL(VALID_URI + ""?project="" + PROJECT_ID));
  }
",non-flaky,5
175821,GoogleCloudPlatform_google-cloud-eclipse,OpenUriSelectionListenerTest.testWidgetDefaultSelected_successful,"  @Test
  public void testWidgetDefaultSelected_successful() throws PartInitException, MalformedURLException {
    SelectionEvent selectionEvent = getEvent(VALID_URI);
    when(queryParameterProvider.getParameters()).thenReturn(Collections.singletonMap(URL_PARAM_PROJECT, PROJECT_ID));

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport)
      .widgetDefaultSelected(selectionEvent);
    verify(errorHandler, never()).handle(any(Exception.class), any(URI.class));
    verify(browser).openURL(new URL(VALID_URI + ""?project="" + PROJECT_ID));
  }
",non-flaky,5
175822,GoogleCloudPlatform_google-cloud-eclipse,BooleanConverterTest.testNegate,"  @Test
  public void testNegate() {
    assertTrue((Boolean) BooleanConverter.negate().convert(Boolean.FALSE));
    assertFalse((Boolean) BooleanConverter.negate().convert(Boolean.TRUE));
  }
",non-flaky,5
175823,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_nonStringInput,"  @Test
  public void testValidation_nonStringInput() {
    IStatus status = validator.validate(new Object());
    assertThat(status.getSeverity(), is(IStatus.ERROR));
    assertThat(status.getMessage(), is(""Invalid bucket name""));
  }
",non-flaky,5
175824,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_emptyString,"  @Test
  public void testValidation_emptyString() {
    assertThat(validator.validate("""").getSeverity(), is(IStatus.OK));
  }
",non-flaky,5
175825,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_upperCaseLetter,"  @Test
  public void testValidation_upperCaseLetter() {
    IStatus status = validator.validate(""THISWOULDBEVALIDIFLOWERCASE"");
    assertThat(status.getSeverity(), is(IStatus.ERROR));
    assertThat(status.getMessage(), is(""Invalid bucket name: THISWOULDBEVALIDIFLOWERCASE""));
  }
",non-flaky,5
175826,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_startWithDot,"  @Test
  public void testValidation_startWithDot() {
    assertThat(validator.validate("".bucket"").getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175827,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_endWithDot,"  @Test
  public void testValidation_endWithDot() {
    assertThat(validator.validate(""bucket."").getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175828,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_startWithHyphen,"  @Test
  public void testValidation_startWithHyphen() {
    assertThat(validator.validate(""-bucket"").getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175829,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_endWithHyphen,"  @Test
  public void testValidation_endWithHyphen() {
    assertThat(validator.validate(""bucket-"").getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175830,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_startWithUnderscore,"  @Test
  public void testValidation_startWithUnderscore() {
    assertThat(validator.validate(""_bucket"").getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175831,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_endWithUnderscore,"  @Test
  public void testValidation_endWithUnderscore() {
    assertThat(validator.validate(""bucket_"").getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175832,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_maxLengthWithoutDot,"  @Test
  public void testValidation_maxLengthWithoutDot() {
    assertThat(validator.validate(LENGTH_63).getSeverity(), is(IStatus.OK));
  }
",non-flaky,5
175833,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_tooLongNameWithoutDot,"  @Test
  public void testValidation_tooLongNameWithoutDot() {
    assertThat(validator.validate(LENGTH_63 + ""4"").getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175834,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_validNameWithDot,"  @Test
  public void testValidation_validNameWithDot() {
    assertThat(validator.validate(LENGTH_64_WITH_DOT).getSeverity(), is(IStatus.OK));
  }
",non-flaky,5
175835,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_tooLongNameWithDot,"  @Test
  public void testValidation_tooLongNameWithDot() {
    assertThat(validator.validate(LENGTH_222 + ""9"").getSeverity(), is(IStatus.ERROR));
  }
",non-flaky,5
175836,GoogleCloudPlatform_google-cloud-eclipse,BucketNameValidatorTest.testValidation_maxLengthWithDot,"  @Test
  public void testValidation_maxLengthWithDot() {
    assertThat(validator.validate(LENGTH_222).getSeverity(), is(IStatus.OK));
  }
",non-flaky,5
77,apache_kylin,CoordinatorTest.testReassignFailOnStopAndSync,"@Test
public void testReassignFailOnStopAndSync() throws IOException {
    ReceiverAdminClient receiverAdminClient = mockReceiverClientFailOnStopAndSync();
    coordinator = new Coordinator(metadataStore, receiverAdminClient);
    Map<Integer, List<Partition>> preAssignMap = metadataStore.getAssignmentsByCube(cubeName).getAssignments();
    Map<Integer, List<Partition>> newAssignMap = new HashMap<>();
    newAssignMap.put(1, Lists.newArrayList(p1, p2, p3));
    newAssignMap.put(2, Lists.newArrayList(p4, p5));
    newAssignMap.put(3, Lists.newArrayList(p6));
    CubeAssignment preAssigment = new CubeAssignment(cube.getName(), preAssignMap);
    CubeAssignment newAssigment = new CubeAssignment(cube.getName(), newAssignMap);
    try {
        coordinator.doReassign(cube, preAssigment, newAssigment);
    } catch (ClusterStateException rune) {
        assertSame(ROLLBACK_FAILED, rune.getClusterState());
        assertSame(STOP_AND_SNYC, rune.getTransactionStep());
        System.out.println(rune.getMessage());
        throw rune;
    }
}",unordered collections,3
116,apache_kylin,CoordinatorTest.testReassignFailOnStartNew,"@Test
public void testReassignFailOnStartNew() throws IOException {
    ReceiverAdminClient receiverAdminClient = mockReceiverClientFailOnStartNewComsumer();
    coordinator = new Coordinator(metadataStore, receiverAdminClient);
    Map<Integer, List<Partition>> preAssignMap = metadataStore.getAssignmentsByCube(cubeName).getAssignments();
    Map<Integer, List<Partition>> newAssignMap = new HashMap<>();
    newAssignMap.put(1, Lists.newArrayList(p1, p2, p3));
    newAssignMap.put(2, Lists.newArrayList(p4, p5));
    newAssignMap.put(3, Lists.newArrayList(p6));
    CubeAssignment preAssigment = new CubeAssignment(cube.getName(), preAssignMap);
    CubeAssignment newAssigment = new CubeAssignment(cube.getName(), newAssignMap);
    try {
        coordinator.doReassign(cube, preAssigment, newAssigment);
    } catch (ClusterStateException rune) {
        assertSame(ROLLBACK_FAILED, rune.getClusterState());
        assertSame(START_NEW, rune.getTransactionStep());
        System.out.println(rune.getMessage());
        throw rune;
    }
}",unordered collections,3
91494,apache_kylin,DriverTest.testVersion,"    @Test
    public void testVersion() {
        Driver driver = new DummyDriver();
        DriverVersion version = driver.getDriverVersion();
        Assert.assertNotEquals(""unknown version"", version.productVersion);
    }
",non-flaky,5
91495,apache_kylin,DriverTest.testStatementWithMockData,"    @Test
    public void testStatementWithMockData() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/test_db"", null);

        ResultSet tables = conn.getMetaData().getTables(null, null, null, null);
        while (tables.next()) {
            for (int i = 0; i < 4; i++) {
                assertEquals(""dummy"", tables.getString(i + 1));
            }
            for (int i = 4; i < 10; i++) {
                assertEquals(null, tables.getString(i + 1));
            }
        }

        Statement state = conn.createStatement();
        ResultSet resultSet = state.executeQuery(""select * from test_table"");

        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(12, metadata.getColumnType(1));
        assertEquals(""varchar"", metadata.getColumnTypeName(1));
        assertEquals(1, metadata.isNullable(1));

        while (resultSet.next()) {
            assertEquals(""foo"", resultSet.getString(1));
            assertEquals(""bar"", resultSet.getString(2));
            assertEquals(""tool"", resultSet.getString(3));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91496,apache_kylin,DriverTest.testStatementWithQuestionMask,"    @Test
    public void testStatementWithQuestionMask() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/test_db"", null);
        Statement state = conn.createStatement();
        ResultSet resultSet = state.executeQuery(""select * from test_table where url not in ('http://a.b.com/?a=b')"");
        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(12, metadata.getColumnType(1));
        assertEquals(""varchar"", metadata.getColumnTypeName(1));
        assertEquals(1, metadata.isNullable(1));

        while (resultSet.next()) {
            assertEquals(""foo"", resultSet.getString(1));
            assertEquals(""bar"", resultSet.getString(2));
            assertEquals(""tool"", resultSet.getString(3));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91497,apache_kylin,DriverTest.testDateAndTimeStampWithMockData,"    @Test
    public void testDateAndTimeStampWithMockData() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/test_db"", null);
        PreparedStatement state = conn.prepareStatement(""select * from test_table where id=?"");
        state.setInt(1, 10);
        ResultSet resultSet = state.executeQuery();

        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(""date"", metadata.getColumnTypeName(4));
        assertEquals(""timestamp"", metadata.getColumnTypeName(5));

        while (resultSet.next()) {
            assertEquals(""2019-04-27"", resultSet.getString(4));
            assertEquals(""2019-04-27 17:30:03"", resultSet.getString(5));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91498,apache_kylin,DriverTest.testMultipathOfDomainForConnection,"    @Test
    public void testMultipathOfDomainForConnection() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/kylin/test_db/"", null);
        Statement state = conn.createStatement();
        ResultSet resultSet = state.executeQuery(""select * from test_table where url not in ('http://a.b.com/?a=b') limit 1"");
        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(12, metadata.getColumnType(1));
        assertEquals(""varchar"", metadata.getColumnTypeName(1));
        assertEquals(1, metadata.isNullable(1));

        while (resultSet.next()) {
            assertEquals(""foo"", resultSet.getString(1));
            assertEquals(""bar"", resultSet.getString(2));
            assertEquals(""tool"", resultSet.getString(3));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91499,apache_kylin,DriverTest.testPreparedStatementWithMockData,"    @Test
    public void testPreparedStatementWithMockData() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/test_db"", null);
        PreparedStatement state = conn.prepareStatement(""select * from test_table where id=?"");
        state.setInt(1, 10);
        ResultSet resultSet = state.executeQuery();

        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(12, metadata.getColumnType(1));
        assertEquals(""varchar"", metadata.getColumnTypeName(1));
        assertEquals(1, metadata.isNullable(1));

        while (resultSet.next()) {
            assertEquals(""foo"", resultSet.getString(1));
            assertEquals(""bar"", resultSet.getString(2));
            assertEquals(""tool"", resultSet.getString(3));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91500,apache_kylin,DriverTest.testWithCubeData,"    @Test
    public void testWithCubeData() throws Exception {
        Driver driver = new Driver();
        Properties info = new Properties();
        info.put(""user"", ""ADMIN"");
        info.put(""password"", ""KYLIN"");
        Connection conn = driver.connect(""jdbc:kylin://localhost:7070/default"", info);

        ResultSet catalogs = conn.getMetaData().getCatalogs();
        System.out.println(""CATALOGS"");
        printResultSetMetaData(catalogs);
        printResultSet(catalogs);

        ResultSet schemas = conn.getMetaData().getSchemas();
        System.out.println(""SCHEMAS"");
        printResultSetMetaData(schemas);
        printResultSet(schemas);

        ResultSet tables = conn.getMetaData().getTables(null, null, null, null);
        System.out.println(""TABLES"");
        printResultSetMetaData(tables);
        printResultSet(tables);

        for (int j = 0; j < 3; j++) {
            Statement state = conn.createStatement();
            ResultSet resultSet = state.executeQuery(""select * from test_kylin_fact"");

            printResultSetMetaData(resultSet);
            printResultSet(resultSet);

            resultSet.close();
        }

        catalogs.close();
        schemas.close();
        tables.close();
        conn.close();
    }
",non-flaky,5
91501,apache_kylin,DriverTest.testPreparedStatementWithCubeData,"    @Test
    public void testPreparedStatementWithCubeData() throws SQLException {
        Driver driver = new Driver();
        Properties info = new Properties();
        info.put(""user"", ""ADMIN"");
        info.put(""password"", ""KYLIN"");
        Connection conn = driver.connect(""jdbc:kylin://localhost:7070/default"", info);

        PreparedStatement state = conn
                .prepareStatement(""select cal_dt, count(*) from test_kylin_fact where seller_id=? group by cal_dt"");
        state.setLong(1, 10000001);
        ResultSet resultSet = state.executeQuery();

        printResultSetMetaData(resultSet);
        printResultSet(resultSet);

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91502,apache_kylin,DriverTest.testSSLFromURL,"    @Test
    public void testSSLFromURL() throws SQLException {
        Driver driver = new DummyDriver();
        Connection conn = driver.connect(""jdbc:kylin:ssl=True;//test_url/test_db"", null);
        assertEquals(""test_url"", ((KylinConnection) conn).getBaseUrl());
        assertEquals(""test_db"", ((KylinConnection) conn).getProject());
        assertTrue(Boolean.parseBoolean((String) ((KylinConnection) conn).getConnectionProperties().get(""ssl"")));
        conn.close();
    }
",non-flaky,5
91503,apache_kylin,DriverTest.testCalciteProps,"    @Test
    public void testCalciteProps() throws SQLException {
        Driver driver = new DummyDriver();
        Properties props = new Properties();
        props.setProperty(""kylin.query.calcite.extras-props.caseSensitive"", ""true"");
        props.setProperty(""kylin.query.calcite.extras-props.unquotedCasing"", ""TO_LOWER"");
        props.setProperty(""kylin.query.calcite.extras-props.quoting"", ""BRACKET"");
        KylinConnection conn = (KylinConnection) driver.connect(""jdbc:kylin:test_url/test_db"", props);
        Properties connProps = conn.getConnectionProperties();
        assertEquals(""true"", connProps.getProperty(""kylin.query.calcite.extras-props.caseSensitive""));
        assertEquals(""TO_LOWER"", connProps.getProperty(""kylin.query.calcite.extras-props.unquotedCasing""));
        assertEquals(""BRACKET"", connProps.getProperty(""kylin.query.calcite.extras-props.quoting""));

        // parameters in url is prior to props parameter
        KylinConnection conn2 = (KylinConnection) driver.connect(""jdbc:kylin:kylin.query.calcite.extras-props.caseSensitive=false;"" +
                ""kylin.query.calcite.extras-props.unquotedCasing=UNCHANGED;"" +
                ""kylin.query.calcite.extras-props.quoting=BACK_TICK;"" +
                ""test_url/test_db"", props);
        Properties connProps2 = conn2.getConnectionProperties();
        assertEquals(""false"", connProps2.getProperty(""kylin.query.calcite.extras-props.caseSensitive""));
        assertEquals(""UNCHANGED"", connProps2.getProperty(""kylin.query.calcite.extras-props.unquotedCasing""));
        assertEquals(""BACK_TICK"", connProps2.getProperty(""kylin.query.calcite.extras-props.quoting""));
        conn.close();
        conn2.close();
    }
",non-flaky,5
91504,apache_kylin,KylinClientTest.connect,"    @Test
    public void connect() throws IOException {
        HttpResponse response = mock(HttpResponse.class);
        when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(response);
        when(response.getStatusLine()).thenReturn(new BasicStatusLine(HTTP_1_1, 200, ""OK""));
        client.connect();
    }
",non-flaky,5
91505,apache_kylin,KylinClientTest.retrieveMetaData,"    @Test
    public void retrieveMetaData() throws IOException {
        HttpResponse response = TestUtil.mockHttpResponseWithFile(200, ""OK"", ""tables_and_columns.json"");
        when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(response);

        KylinMeta.KMetaProject metaData = client.retrieveMetaData(connInfo.getProject());

        assertEquals(connInfo.getProject(), metaData.projectName);
        assertTrue(!metaData.catalogs.isEmpty());
        KylinMeta.KMetaCatalog catalog = metaData.catalogs.get(0);
        assertEquals(""defaultCatalog"", catalog.getName());
        assertEquals(1, catalog.schemas.size());
        KylinMeta.KMetaSchema schema = catalog.schemas.get(0);
        assertEquals(""DEFAULT"", schema.getName());
        assertEquals(5, schema.tables.size());
    }
",non-flaky,5
91506,apache_kylin,KylinClientTest.retrieveMetaDataWithWrongProject,"    @Test(expected = AssertionError.class)
    public void retrieveMetaDataWithWrongProject() throws IOException {
        client.retrieveMetaData(""defualt2"");
    }
",non-flaky,5
91507,apache_kylin,KylinClientTest.executeQuery,"    @Test
    public void executeQuery() throws IOException {
        HttpResponse response = TestUtil.mockHttpResponseWithFile(200, ""OK"", ""query.json"");
        when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(response);
        IRemoteClient.QueryResult queryResult = client.executeQuery(""SELECT 1 as val"", Collections.emptyList(), new HashMap<String, String>());
        assertEquals(1, queryResult.columnMeta.size());
        Iterable<Object> iterable = queryResult.iterable;
        ArrayList<Object> list = Lists.newArrayList(iterable);
        assertEquals(1, list.size());
    }
",non-flaky,5
91508,apache_kylin,KylinClientTest.testWrapObjectThrowsIllegalArgumentExceptionUsingDateType,"  @Test(expected = IllegalArgumentException.class)
  public void testWrapObjectThrowsIllegalArgumentExceptionUsingDateType() {
      KylinClient.wrapObject(""OQ? PYC6BWm`kOE"", Types.DATE);
  }
",non-flaky,5
91509,apache_kylin,KylinClientTest.testWrapObjectUsingNull,"  @Test
  public void testWrapObjectUsingNull() {
      assertNull(KylinClient.wrapObject(null, 1));
  }
",non-flaky,5
91510,apache_kylin,KylinClientTest.testConvertBooleanType,"  @Test
  public void testConvertBooleanType() {
      assertEquals(""java.lang.Boolean"", KylinClient.convertType(Types.BOOLEAN).getName());
  }
",non-flaky,5
91511,apache_kylin,SQLResonseStubTest.testReadValuePartRecognizedField,"    @Test
    public void testReadValuePartRecognizedField() throws IOException {
        final String payload = ""{ \""columnMetas\"":[ { \""isNullable\"":1, \""displaySize\"":0, \""schemaName\"":null, \""catelogName\"":null, \""tableName\"":null, \""precision\"":0, \""scale\"":0, \""columnType\"":91, \""columnTypeName\"":\""DATE\"", \""readOnly\"":true, \""writable\"":false, \""caseSensitive\"":true, \""searchable\"":false, \""currency\"":false, \""signed\"":true, \""autoIncrement\"":false, \""definitelyWritable\"":false },"" + ""{ \""isNullable\"":1, \""displaySize\"":10, \""label\"":\""LEAF_CATEG_ID\"", \""name\"":\""LEAF_CATEG_ID\"", ""
                + ""\""schemaName\"":null, \""catelogName\"":null, \""tableName\"":null, \""precision\"":10, \""scale\"":0, \""columnType\"":4, \""columnTypeName\"":\""INTEGER\"", \""readOnly\"":true, \""writable\"":false, \""caseSensitive\"":true, \""searchable\"":false, \""currency\"":false, \""signed\"":true, \""autoIncrement\"":false, \""definitelyWritable\"":false } ], \""results\"":[ [ \""2013-08-07\"", \""32996\"", \""15\"", \""15\"", \""Auction\"", \""10000000\"", \""49.048952730908745\"", \""49.048952730908745\"", \""49.048952730908745\"", \""1\"" ], [ \""2013-08-07\"", \""43398\"", \""0\"", \""14\"", \""ABIN\"", \""10000633\"", \""85.78317064220418\"", \""85.78317064220418\"", \""85.78317064220418\"", \""1\"" ] ], \""cube\"":\""test_kylin_cube_with_slr_desc\"", \""affectedRowCount\"":0, \""isException\"":false, \""exceptionMessage\"":null, \""duration\"":3451, \""partial\"":false }"";
        final SQLResponseStub stub = new ObjectMapper().readValue(payload, SQLResponseStub.class);
        assertEquals(""test_kylin_cube_with_slr_desc"", stub.getCube());
        assertEquals(3451, stub.getDuration());
        assertFalse(stub.getColumnMetas().isEmpty());
        assertEquals(91, stub.getColumnMetas().get(0).getColumnType());
        assertNull(stub.getColumnMetas().get(0).getLabel());
        assertFalse(stub.getResults().isEmpty());
        assertNull(stub.getExceptionMessage());
    }
",non-flaky,5
91512,apache_kylin,SQLResonseStubTest.testReadValueWithUnrecognizedField,"    @Test
    public void testReadValueWithUnrecognizedField() throws IOException {
        final String payload = ""{ \""columnMetas\"":[ { \""Unrecognized\"":0, \""isNullable\"":1, \""displaySize\"":0, "" + ""\""label\"":\""CAL_DT\"", \""name\"":\""CAL_DT\"", \""schemaName\"":null, \""catelogName\"":null, "" + ""\""tableName\"":null, \""precision\"":0, \""scale\"":0, \""columnType\"":91, \""columnTypeName\"":\""DATE\"", "" + ""\""readOnly\"":true, \""writable\"":false, \""caseSensitive\"":true, \""searchable\"":false, \""currency\"":false, \""signed\"":true, \""autoIncrement\"":false, \""definitelyWritable\"":false },""
                + "" { \""isNullable\"":1, \""displaySize\"":10, \""label\"":\""LEAF_CATEG_ID\"", \""name\"":\""LEAF_CATEG_ID\"", \""schemaName\"":null, \""catelogName\"":null, \""tableName\"":null, \""precision\"":10, \""scale\"":0, \""columnType\"":4, \""columnTypeName\"":\""INTEGER\"", \""readOnly\"":true, \""writable\"":false, \""caseSensitive\"":true, \""searchable\"":false, \""currency\"":false, \""signed\"":true, \""autoIncrement\"":false, \""definitelyWritable\"":false } ], \""results\"":[ [ \""2013-08-07\"", \""32996\"", \""15\"", \""15\"", \""Auction\"", \""10000000\"", \""49.048952730908745\"", \""49.048952730908745\"", \""49.048952730908745\"", \""1\"" ], [ \""2013-08-07\"", \""43398\"", \""0\"", \""14\"", \""ABIN\"", \""10000633\"", \""85.78317064220418\"", \""85.78317064220418\"", \""85.78317064220418\"", \""1\"" ] ], \""cube\"":\""test_kylin_cube_with_slr_desc\"", \""affectedRowCount\"":0, \""isException\"":false, \""exceptionMessage\"":null, \""duration\"":3451, \""partial\"":false, \""hitCache\"":false }"";
        final SQLResponseStub stub = new ObjectMapper().readValue(payload, SQLResponseStub.class);
        assertEquals(""test_kylin_cube_with_slr_desc"", stub.getCube());
        assertEquals(3451, stub.getDuration());
        assertFalse(stub.getColumnMetas().isEmpty());
        assertEquals(91, stub.getColumnMetas().get(0).getColumnType());
        assertEquals(""CAL_DT"", stub.getColumnMetas().get(0).getLabel());
        assertFalse(stub.getResults().isEmpty());
        assertNull(stub.getExceptionMessage());
    }
",non-flaky,5
91513,apache_kylin,KylinConnectionTest.testPrepareStatementWithMockKylinClient,"    @Test
    public void testPrepareStatementWithMockKylinClient() throws SQLException, IOException {
        String sql = ""select 1 as val"";
        // mock client
        when(client.executeQuery(anyString(), Mockito.<List<Object>>any(), Mockito.<Map<String, String>>any())).thenReturn(getMockResult());

        try (KylinConnection conn = getConnectionWithMockClient()) {
            PreparedStatement preparedStatement = conn.prepareStatement(sql);
            try (ResultSet resultSet = preparedStatement.executeQuery()) {
                verify(client).executeQuery(eq(sql), Mockito.<List<Object>>any(), Mockito.<Map<String, String>>any());

                assertTrue(resultSet.next());
                ResultSetMetaData metaData = resultSet.getMetaData();
                assertEquals(""VAL"", metaData.getColumnName(1));
                assertEquals(1, resultSet.getInt(""VAL""));
            }
        }
    }
",non-flaky,5
91514,apache_kylin,KylinConnectionTest.testPrepareStatementWithMockHttp,"    @Test
    public void testPrepareStatementWithMockHttp() throws IOException, SQLException {
        String sql = ""select 1 as val"";
        try (KylinConnection connection = getConnectionWithMockHttp()) {

            // mock http
            HttpResponse response = TestUtil.mockHttpResponseWithFile(200, ""OK"", ""query.json"");
            when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(response);

            try (ResultSet resultSet = connection.prepareStatement(sql).executeQuery()) {
                assertTrue(resultSet.next());
                ResultSetMetaData metaData = resultSet.getMetaData();
                assertEquals(""VAL"", metaData.getColumnName(1));
                assertEquals(1, resultSet.getInt(""VAL""));
            }
        }
    }
",non-flaky,5
91515,apache_kylin,KylinConnectionTest.matches,"    @Test
    public void testJdbcClientCalcitePropsInUrl() throws Exception {
        String sql = ""select 1 as val"";

        // mock client
        when(client.executeQuery(anyString(), Mockito.<List<Object>>any(), Mockito.<Map<String, String>>any())).thenReturn(getMockResult());
        Map<String, String> toggles = new HashMap<>();
        Properties info = new Properties();
        info.setProperty(""caseSensitive"", ""false"");
        info.setProperty(""unquotedCasing"", ""UNCHANGED"");
        try (KylinConnection conn = getConnectionWithMockClient(""jdbc:kylin:test_url/test_db"", info)) {
            PreparedStatement preparedStatement = conn.prepareStatement(sql);
            try (ResultSet resultSet = preparedStatement.executeQuery()) {
                verify(client).executeQuery(eq(sql), Mockito.<List<Object>>any(), argThat(new ArgumentMatcher<Map<String, String>>() {
                    @Override
                    public boolean matches(Map<String, String> argument) {
                        String propsStr = argument.get(""JDBC_CLIENT_CALCITE_PROPS"");
                        assertNotNull(propsStr);
                        Properties props = new Properties();
                        try {
                            props.load(new StringReader(propsStr));
                        } catch (IOException e) {
                            throw new RuntimeException(e);
                        }
                        assertEquals(""false"", props.getProperty(""caseSensitive""));
                        assertEquals(""UNCHANGED"", props.getProperty(""unquotedCasing""));
                        return true;
                    }
",non-flaky,5
91516,apache_kylin,KafkaInputRecordReaderTest.testNextKeyValue,"    @Test
    public void testNextKeyValue()  throws Throwable  {
        KafkaInputRecordReader kafkaInputRecordReader = new KafkaInputRecordReader();
        assertFalse(kafkaInputRecordReader.nextKeyValue());
        assertFalse(kafkaInputRecordReader.nextKeyValue());
    }
",non-flaky,5
91517,apache_kylin,KafkaInputRecordReaderTest.testGetProgress,"    @Test
    public void testGetProgress()  throws Throwable  {
        KafkaInputRecordReader kafkaInputRecordReader = new KafkaInputRecordReader();
        assertEquals(1.0F, kafkaInputRecordReader.getProgress(), 0.01F);
    }
",non-flaky,5
91518,apache_kylin,KafkaInputRecordReaderTest.testGetCurrentKey,"    @Test
    public void testGetCurrentKey()  throws Throwable  {
        KafkaInputRecordReader kafkaInputRecordReader = new KafkaInputRecordReader();
        kafkaInputRecordReader.nextKeyValue();
        assertEquals(0L, kafkaInputRecordReader.getCurrentKey().get());
    }
",non-flaky,5
91519,apache_kylin,KafkaInputRecordReaderTest.testGetCurrentValue,"    @Test
    public void testGetCurrentValue()  throws Throwable  {
        KafkaInputRecordReader kafkaInputRecordReader = new KafkaInputRecordReader();
        kafkaInputRecordReader.nextKeyValue();
        assertEquals(0, kafkaInputRecordReader.getCurrentValue().getBytes().length);
    }
",non-flaky,5
91520,apache_kylin,TimedJsonStreamParserTest.testNormalValue,"    @Test
    public void testNormalValue() throws Exception {
        userNeedColNames = new String[] { ""createdAt"", ""id"", ""isTruncated"", ""text"" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        assertEquals(""Jul 20, 2016 9:59:17 AM"", result.get(0));
        assertEquals(""755703618762862600"", result.get(1));
        assertEquals(""false"", result.get(2));
        assertEquals(""dejamos"", result.get(3));
    }
",non-flaky,5
91521,apache_kylin,TimedJsonStreamParserTest.testEmbeddedValue,"    @Test
    public void testEmbeddedValue() throws Exception {
        userNeedColNames = new String[] { ""user_id"", ""user_description"", ""user_isProtected"",
                ""user_is_Default_Profile_Image"" };
        userNeedColNamesComment = new String[] { """", """", """",
                ""user"" + TimedJsonStreamParser.EMBEDDED_PROPERTY_SEPARATOR + ""is_Default_Profile_Image"" };
        List<TblColRef> allCol = mockupTblColRefListWithComment(userNeedColNamesComment);
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        assertEquals(""4853763947"", result.get(0));
        assertEquals(""Noticias"", result.get(1));
        assertEquals(""false"", result.get(2));
        assertEquals(""false"", result.get(3));
    }
",non-flaky,5
91522,apache_kylin,TimedJsonStreamParserTest.testEmbeddedValueFaultTolerant,"    @Test
    public void testEmbeddedValueFaultTolerant() throws Exception {
        userNeedColNames = new String[] { ""user_id"", ""nonexisted_description"" };
        userNeedColNamesComment = new String[] { """", """" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        assertEquals(""4853763947"", result.get(0));
        assertEquals(StringUtils.EMPTY, result.get(1));
    }
",non-flaky,5
91523,apache_kylin,TimedJsonStreamParserTest.testArrayValue,"    @Test
    public void testArrayValue() throws Exception {
        userNeedColNames = new String[] { ""userMentionEntities"", ""mediaEntities"" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        HashMap<String, Object> map = (HashMap<String, Object>) msg;
        Object array = map.get(""mediaEntities"");
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        System.out.println(result);

    }
",non-flaky,5
91524,apache_kylin,TimedJsonStreamParserTest.testMapValue,"    @Test
    public void testMapValue() throws Exception {
        userNeedColNames = new String[] { ""user"" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();

    }
",non-flaky,5
91525,apache_kylin,TimedJsonStreamParserTest.testNullKey,"    @Test
    public void testNullKey() throws Exception {
        userNeedColNames = new String[] { ""null"", """" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        assertEquals(StringUtils.EMPTY, result.get(0));
        assertEquals(StringUtils.EMPTY, result.get(1));
    }
",non-flaky,5
91526,apache_kylin,KafkaConsumerPropertiesTest.testLoadKafkaProperties,"    @Test
    public void testLoadKafkaProperties() {
        KafkaConsumerProperties kafkaConsumerProperties = KafkaConsumerProperties.getInstanceFromEnv();
        assertFalse(kafkaConsumerProperties.extractKafkaConfigToProperties().containsKey(""acks""));
        assertTrue(kafkaConsumerProperties.extractKafkaConfigToProperties().containsKey(""session.timeout.ms""));
        assertEquals(""10000"", kafkaConsumerProperties.extractKafkaConfigToProperties().getProperty(""session.timeout.ms""));
        assertTrue(kafkaConsumerProperties.extractKafkaConfigToProperties().containsKey(""client.id""));
        assertEquals(""kylin"", kafkaConsumerProperties.extractKafkaConfigToProperties().getProperty(""client.id""));
    }
",non-flaky,5
91527,apache_kylin,KafkaConsumerPropertiesTest.testLoadKafkaPropertiesAsHadoopJobConf,"    @Test
    public void testLoadKafkaPropertiesAsHadoopJobConf() throws IOException, ParserConfigurationException, SAXException {
        KafkaConsumerProperties kafkaConsumerProperties = KafkaConsumerProperties.getInstanceFromEnv();
        Configuration conf = new Configuration(false);
        conf.addResource(new FileInputStream(new File(kafkaConsumerProperties.getKafkaConsumerHadoopJobConf())), KafkaConsumerProperties.KAFKA_CONSUMER_FILE);
        assertEquals(""10000"", conf.get(""session.timeout.ms""));

        Properties prop = KafkaConsumerProperties.extractKafkaConfigToProperties(conf);
        assertEquals(""10000"", prop.getProperty(""session.timeout.ms""));
    }
",non-flaky,5
91528,apache_kylin,SqlUtilTest.testJdbcTypetoKylinDataType,"    @Test
    public void testJdbcTypetoKylinDataType() {
        this.getClass().getClassLoader().toString();
        assertEquals(""double"", SqlUtil.jdbcTypeToKylinDataType(Types.FLOAT));
        assertEquals(""varchar"", SqlUtil.jdbcTypeToKylinDataType(Types.NVARCHAR));
        assertEquals(""any"", SqlUtil.jdbcTypeToKylinDataType(Types.ARRAY));
        assertEquals(""integer"", SqlUtil.jdbcTypeToKylinDataType((4)));
        assertEquals(""smallint"", SqlUtil.jdbcTypeToKylinDataType((5)));
        assertEquals(""tinyint"", SqlUtil.jdbcTypeToKylinDataType((-6)));
        assertEquals(""char"", SqlUtil.jdbcTypeToKylinDataType((1)));
        assertEquals(""decimal"", SqlUtil.jdbcTypeToKylinDataType((2)));
        assertEquals(""varchar"", SqlUtil.jdbcTypeToKylinDataType((-1)));
        assertEquals(""byte"", SqlUtil.jdbcTypeToKylinDataType((-2)));
        assertEquals(""any"", SqlUtil.jdbcTypeToKylinDataType((-1720774701)));
        assertEquals(""boolean"", SqlUtil.jdbcTypeToKylinDataType((-7)));
        assertEquals(""timestamp"", SqlUtil.jdbcTypeToKylinDataType((93)));
        assertEquals(""time"", SqlUtil.jdbcTypeToKylinDataType((92)));
        assertEquals(""date"", SqlUtil.jdbcTypeToKylinDataType((91)));
        assertEquals(""bigint"", SqlUtil.jdbcTypeToKylinDataType((-5)));
    }
",non-flaky,5
91529,apache_kylin,SqlUtilTest.testIsPrecisionApplicable,"    @Test
    public void testIsPrecisionApplicable() {
        assertFalse(SqlUtil.isPrecisionApplicable(""boolean""));
        assertTrue(SqlUtil.isPrecisionApplicable(""varchar""));
    }
",non-flaky,5
91530,apache_kylin,SqlUtilTest.testIsScaleApplicable,"    @Test
    public void testIsScaleApplicable() {
        assertFalse(SqlUtil.isScaleApplicable(""varchar""));
        assertTrue(SqlUtil.isScaleApplicable(""decimal""));
    }
",non-flaky,5
91531,apache_kylin,DefaultJdbcMetadataTest.testListDatabases,"    @Test
    public void testListDatabases() throws SQLException {
        ResultSet rs = mock(ResultSet.class);
        when(rs.next()).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs.getString(""TABLE_SCHEM"")).thenReturn(""schema1"").thenReturn(""schema2"");
        when(rs.getString(""TABLE_CATALOG"")).thenReturn(""catalog1"").thenReturn(""catalog2"");

        when(connection.getMetaData()).thenReturn(dbmd);
        when(dbmd.getSchemas()).thenReturn(rs);

        List<String> dbs = jdbcMetadata.listDatabases();

        Assert.assertEquals(2, dbs.size());
        Assert.assertEquals(""schema1"", dbs.get(0));
    }
",non-flaky,5
91532,apache_kylin,DefaultJdbcMetadataTest.testListTables,"    @Test
    public void testListTables() throws SQLException {
        ResultSet rs = mock(ResultSet.class);
        when(rs.next()).thenReturn(true).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs.getString(""TABLE_NAME"")).thenReturn(""KYLIN_SALES"").thenReturn(""CAT_DT"").thenReturn(""KYLIN_CAT"");

        String schema = ""testschema"";
        when(connection.getMetaData()).thenReturn(dbmd);
        when(dbmd.getTables(null, schema, null, null)).thenReturn(rs);

        List<String> tables = jdbcMetadata.listTables(schema);

        Assert.assertEquals(3, tables.size());
        Assert.assertEquals(""CAT_DT"", tables.get(1));
    }
",non-flaky,5
91533,apache_kylin,DefaultJdbcMetadataTest.testGetTable,"    @Test
    public void testGetTable() throws SQLException {
        String schema = ""testSchema"";
        String table = ""testTable"";
        ResultSet rs = mock(ResultSet.class);
        when(dbmd.getTables(null, schema, table, null)).thenReturn(rs);

        ResultSet result = jdbcMetadata.getTable(dbmd, schema, table);

        verify(dbmd, times(1)).getTables(null, schema, table, null);
        Assert.assertEquals(rs, result);
    }
",non-flaky,5
91534,apache_kylin,DefaultJdbcMetadataTest.testListColumns,"    @Test
    public void testListColumns() throws SQLException {
        String schema = ""testSchema"";
        String table = ""testTable"";
        ResultSet rs = mock(ResultSet.class);
        when(dbmd.getColumns(null, schema, table, null)).thenReturn(rs);

        ResultSet result = jdbcMetadata.listColumns(dbmd, schema, table);

        verify(dbmd, times(1)).getColumns(null, schema, table, null);
        Assert.assertEquals(rs, result);
    }
",non-flaky,5
91535,apache_kylin,SQLServerJdbcMetadataTest.testListDatabases,"    @Test
    public void testListDatabases() throws SQLException {
        ResultSet rs = mock(ResultSet.class);
        when(rs.next()).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs.getString(""TABLE_SCHEM"")).thenReturn(""schema1"").thenReturn(""schema2"");
        when(rs.getString(""TABLE_CAT"")).thenReturn(""catalog1"").thenReturn(""testdb"");

        when(connection.getCatalog()).thenReturn(""testdb"");
        when(connection.getMetaData()).thenReturn(dbmd);
        when(dbmd.getTables(""testdb"", null, null, null)).thenReturn(rs);

        List<String> dbs = jdbcMetadata.listDatabases();

        Assert.assertEquals(1, dbs.size());
        Assert.assertEquals(""schema2"", dbs.get(0));
    }
",non-flaky,5
91536,apache_kylin,SQLServerJdbcMetadataTest.testListDatabasesWithoutSpecificDB,"    @Test(expected = IllegalArgumentException.class)
    public void testListDatabasesWithoutSpecificDB() throws SQLException {
        when(connection.getCatalog()).thenReturn("""");
        jdbcMetadata.listDatabases();
    }
",non-flaky,5
91537,apache_kylin,MySQLJdbcMetadataTest.testListDatabases,"    @Test
    public void testListDatabases() throws SQLException {
        when(connection.getCatalog()).thenReturn(""catalog1"");

        List<String> dbs = jdbcMetadata.listDatabases();

        Assert.assertEquals(1, dbs.size());
        Assert.assertEquals(""catalog1"", dbs.get(0));
    }
",non-flaky,5
91538,apache_kylin,MySQLJdbcMetadataTest.testListTables,"    @Test
    public void testListTables() throws SQLException {
        ResultSet rs = mock(ResultSet.class);
        when(rs.next()).thenReturn(true).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs.getString(""TABLE_NAME"")).thenReturn(""KYLIN_SALES"").thenReturn(""CAT_DT"").thenReturn(""KYLIN_CAT"");

        String catalog = ""testCatalog"";
        when(connection.getMetaData()).thenReturn(dbmd);
        when(dbmd.getTables(catalog, null, null, null)).thenReturn(rs);

        List<String> tables = jdbcMetadata.listTables(catalog);

        Assert.assertEquals(3, tables.size());
        Assert.assertEquals(""CAT_DT"", tables.get(1));
    }
",non-flaky,5
91539,apache_kylin,MySQLJdbcMetadataTest.testGetTable,"    @Test
    public void testGetTable() throws SQLException {
        String catalog = ""testSchema"";
        String table = ""testTable"";
        ResultSet rs = mock(ResultSet.class);
        when(dbmd.getTables(catalog, null, table, null)).thenReturn(rs);

        ResultSet result = jdbcMetadata.getTable(dbmd, catalog, table);

        verify(dbmd, times(1)).getTables(catalog, null, table, null);
        Assert.assertEquals(rs, result);
    }
",non-flaky,5
91540,apache_kylin,MySQLJdbcMetadataTest.testListColumns,"    @Test
    public void testListColumns() throws SQLException {
        String catalog = ""testSchema"";
        String table = ""testTable"";
        ResultSet rs = mock(ResultSet.class);
        when(dbmd.getColumns(catalog, null, table, null)).thenReturn(rs);

        ResultSet result = jdbcMetadata.listColumns(dbmd, catalog, table);

        verify(dbmd, times(1)).getColumns(catalog, null, table, null);
        Assert.assertEquals(rs, result);
    }
",non-flaky,5
91541,apache_kylin,JdbcExplorerTest.testListDatabases,"    @Test
    public void testListDatabases() throws SQLException {
        List<String> databases = new ArrayList<>();
        databases.add(""DB1"");
        databases.add(""DB2"");
        when(jdbcMetadata.listDatabases()).thenReturn(databases);

        List<String> result = jdbcExplorer.listDatabases();

        verify(jdbcMetadata, times(1)).listDatabases();
        Assert.assertEquals(databases, result);
    }
",non-flaky,5
91542,apache_kylin,JdbcExplorerTest.testListTables,"    @Test
    public void testListTables() throws SQLException {
        List<String> tables = new ArrayList<>();
        tables.add(""T1"");
        tables.add(""T2"");
        String databaseName = ""testDb"";
        when(jdbcMetadata.listTables(databaseName)).thenReturn(tables);

        List<String> result = jdbcExplorer.listTables(databaseName);
        verify(jdbcMetadata, times(1)).listTables(databaseName);
        Assert.assertEquals(tables, result);
    }
",non-flaky,5
91543,apache_kylin,JdbcExplorerTest.testLoadTableMetadata,"    @Test
    public void testLoadTableMetadata() throws SQLException {
        String tableName = ""tb1"";
        String databaseName = ""testdb"";
        ResultSet rs1 = mock(ResultSet.class);
        when(rs1.next()).thenReturn(true).thenReturn(false);
        when(rs1.getString(""TABLE_TYPE"")).thenReturn(""TABLE"");

        ResultSet rs2 = mock(ResultSet.class);
        when(rs2.next()).thenReturn(true).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs2.getString(""COLUMN_NAME"")).thenReturn(""COL1"").thenReturn(""COL2"").thenReturn(""COL3"");
        when(rs2.getInt(""DATA_TYPE"")).thenReturn(Types.VARCHAR).thenReturn(Types.INTEGER).thenReturn(Types.DECIMAL);
        when(rs2.getInt(""COLUMN_SIZE"")).thenReturn(128).thenReturn(10).thenReturn(19);
        when(rs2.getInt(""DECIMAL_DIGITS"")).thenReturn(0).thenReturn(0).thenReturn(4);
        when(rs2.getInt(""ORDINAL_POSITION"")).thenReturn(1).thenReturn(3).thenReturn(2);
        when(rs2.getString(""REMARKS"")).thenReturn(""comment1"").thenReturn(""comment2"").thenReturn(""comment3"");

        when(jdbcMetadata.getTable(dbmd, databaseName, tableName)).thenReturn(rs1);
        when(jdbcMetadata.listColumns(dbmd, databaseName, tableName)).thenReturn(rs2);

        Pair<TableDesc, TableExtDesc> result = jdbcExplorer.loadTableMetadata(databaseName, tableName, ""proj"");
        TableDesc tableDesc = result.getFirst();
        ColumnDesc columnDesc = tableDesc.getColumns()[1];

        Assert.assertEquals(databaseName.toUpperCase(Locale.ROOT), tableDesc.getDatabase());
        Assert.assertEquals(3, tableDesc.getColumnCount());
        Assert.assertEquals(""TABLE"", tableDesc.getTableType());
        Assert.assertEquals(""COL2"", columnDesc.getName());
        Assert.assertEquals(""integer"", columnDesc.getTypeName());
        Assert.assertEquals(""comment2"", columnDesc.getComment());
        Assert.assertEquals(databaseName.toUpperCase(Locale.ROOT) + ""."" + tableName.toUpperCase(Locale.ROOT),
                result.getSecond().getIdentity());
    }
",non-flaky,5
91544,apache_kylin,JdbcExplorerTest.testListDatabases,"    @Test
    public void testListDatabases() throws Exception {
        List<String> dbList = explorer.listDatabases();
        Assert.assertTrue(dbList.size() >= 3);
        Assert.assertTrue(dbList.contains(""EDW""));
        Assert.assertTrue(dbList.contains(""DEFAULT""));
    }
",non-flaky,5
91545,apache_kylin,JdbcExplorerTest.testListTables,"    @Test
    public void testListTables() throws Exception {
        List<String> tblList = explorer.listTables(""DEFAULT"");
        Assert.assertTrue(tblList.size() >= 3);
        Assert.assertTrue(tblList.contains(""TEST_KYLIN_FACT""));
        Assert.assertTrue(tblList.contains(""TEST_ACCOUNT""));
        Assert.assertTrue(tblList.contains(""TEST_COUNTRY""));
    }
",non-flaky,5
91546,apache_kylin,JdbcExplorerTest.testValidateSql,"    @Test
    public void testValidateSql() throws Exception {
        explorer.validateSQL(""select 1"");
        validateSQLInvalidEx.expect(Exception.class);
        explorer.validateSQL(""select"");
    }
",non-flaky,5
91547,apache_kylin,JdbcExplorerTest.testGetRelatedKylinResources,"    @Test
    public void testGetRelatedKylinResources() {
        Assert.assertTrue(explorer.getRelatedKylinResources(null).isEmpty());
    }
",non-flaky,5
91548,apache_kylin,JdbcExplorerTest.testLoadTableMetadata,"    @Test
    public void testLoadTableMetadata() throws Exception {
        Pair<TableDesc, TableExtDesc> pair = explorer.loadTableMetadata(""DEFAULT"", ""TEST_KYLIN_FACT"", ""DEFAULT"");
        Assert.assertNotNull(pair.getFirst());
        Assert.assertNotNull(pair.getSecond());

        TableDesc tblDesc = pair.getFirst();
        TableExtDesc tblExtDesc = pair.getSecond();
        Assert.assertEquals(""TEST_KYLIN_FACT"", tblDesc.getName());
        Assert.assertEquals(""TABLE"", tblDesc.getTableType());
        Assert.assertEquals(""DEFAULT.TEST_KYLIN_FACT"", tblDesc.getIdentity());
        Assert.assertEquals(""DEFAULT"", tblDesc.getDatabase());
        Assert.assertEquals(""DEFAULT"", tblDesc.getProject());
        Assert.assertEquals(tblDesc.getIdentity(), tblExtDesc.getIdentity());
        Assert.assertEquals(tblDesc.getProject(), tblExtDesc.getProject());

        ColumnDesc[] columnDescs = tblDesc.getColumns();
        Assert.assertEquals(tblDesc.getColumnCount(), columnDescs.length);
        Assert.assertNotNull(columnDescs[0].getName());
        Assert.assertNotNull(columnDescs[0].getDatatype());
        Assert.assertNotNull(columnDescs[0].getType());
        Assert.assertNotNull(columnDescs[0].getId());
    }
",non-flaky,5
91549,apache_kylin,JdbcExplorerTest.testEvalQueryMetadata,"    @Test
    public void testEvalQueryMetadata() {
        ColumnDesc[] columnDescs = explorer
                .evalQueryMetadata(""select cal_dt, count(*) as cnt from DEFAULT.test_kylin_fact group by cal_dt"");
        Assert.assertNotNull(columnDescs);
        Assert.assertEquals(2, columnDescs.length);
        Assert.assertEquals(""date"", columnDescs[0].getDatatype());
        Assert.assertEquals(""CAL_DT"", columnDescs[0].getName());
        Assert.assertEquals(""bigint"", columnDescs[1].getDatatype());
        Assert.assertEquals(""CNT"", columnDescs[1].getName());
    }
",non-flaky,5
91550,apache_kylin,JdbcTableTest.testBasics,"    @Test
    public void testBasics() throws Exception {
        TableMetadataManager tblManager = TableMetadataManager.getInstance(getTestConfig());
        TableDesc tblDesc = tblManager.getTableDesc(""test_kylin_fact"", ""default"");
        IReadableTable table = SourceManager.getSource(new JdbcSourceTest.JdbcSourceAware())
                .createReadableTable(tblDesc, null);

        // test TableReader
        try (IReadableTable.TableReader reader = table.getReader()) {
            Assert.assertTrue(reader instanceof JdbcTableReader);
            Assert.assertTrue(table instanceof JdbcTable);

            Assert.assertTrue(reader.next());
            String[] row = reader.getRow();
            Assert.assertNotNull(row);
            Assert.assertEquals(tblDesc.getColumnCount(), row.length);
        }

        // test basics
        Assert.assertTrue(table.exists());

        IReadableTable.TableSignature sign = table.getSignature();
        Assert.assertNotNull(sign);
        Assert.assertEquals(String.format(Locale.ROOT, ""%s.%s"", tblDesc.getDatabase(), tblDesc.getName()), sign.getPath());
        Assert.assertTrue(sign.getLastModifiedTime() > 0);
    }
",non-flaky,5
91551,apache_kylin,JdbcHiveMRInputTest.testGenSqoopCmd_Partition,"    @Test
    public void testGenSqoopCmd_Partition() throws IOException {
        ISource source = SourceManager.getSource(new JdbcSourceAware());
        IMRInput input = source.adaptToBuildEngine(IMRInput.class);
        Assert.assertNotNull(input);

        CubeManager cubeManager = CubeManager.getInstance(getTestConfig());
        CubeDesc cubeDesc = CubeDescManager.getInstance(getTestConfig()).getCubeDesc(""ci_inner_join_cube"");
        CubeSegment seg = cubeManager.appendSegment(cubeManager.getCube(cubeDesc.getName()),
                new SegmentRange.TSRange(System.currentTimeMillis() - 100L, System.currentTimeMillis() + 100L));
        CubeJoinedFlatTableDesc flatDesc = new CubeJoinedFlatTableDesc(seg);
        JdbcHiveMRInput.JdbcMRBatchCubingInputSide inputSide = (JdbcHiveMRInput.JdbcMRBatchCubingInputSide) input
                .getBatchCubingInputSide(flatDesc);

        AbstractExecutable executable = new MockInputSide(flatDesc, inputSide).createSqoopToFlatHiveStep(""/tmp"",
                cubeDesc.getName());
        Assert.assertNotNull(executable);

        String cmd = executable.getParam(""cmd"");
        Assert.assertTrue(cmd.contains(""org.h2.Driver""));
        Assert.assertTrue(cmd.contains(
                ""--boundary-query \""SELECT MIN(\\\""TEST_KYLIN_FACT\\\"".\\\""LEAF_CATEG_ID\\\""), MAX(\\\""TEST_KYLIN_FACT\\\"".\\\""LEAF_CATEG_ID\\\"")"" + System.lineSeparator()
                        + ""FROM \\\""DEFAULT\\\"".\\\""TEST_KYLIN_FACT\\\"" AS \\\""TEST_KYLIN_FACT\\\""""));
        source.close();
    }
",non-flaky,5
91552,apache_kylin,JdbcHiveMRInputTest.testGenSqoopCmd_NoPartition,"    @Test
    public void testGenSqoopCmd_NoPartition() throws IOException {
        ISource source = SourceManager.getSource(new JdbcSourceAware());
        IMRInput input = source.adaptToBuildEngine(IMRInput.class);
        Assert.assertNotNull(input);

        CubeManager cubeManager = CubeManager.getInstance(getTestConfig());
        CubeDesc cubeDesc = CubeDescManager.getInstance(getTestConfig()).getCubeDesc(""ci_left_join_cube"");
        CubeSegment seg = cubeManager.appendSegment(cubeManager.getCube(cubeDesc.getName()),
                new SegmentRange.TSRange(0L, Long.MAX_VALUE));
        CubeJoinedFlatTableDesc flatDesc = new CubeJoinedFlatTableDesc(seg);
        JdbcHiveMRInput.JdbcMRBatchCubingInputSide inputSide = (JdbcHiveMRInput.JdbcMRBatchCubingInputSide) input
                .getBatchCubingInputSide(flatDesc);

        AbstractExecutable executable = new MockInputSide(flatDesc, inputSide).createSqoopToFlatHiveStep(""/tmp"",
                cubeDesc.getName());
        Assert.assertNotNull(executable);
        String cmd = executable.getParam(""cmd"");
        Assert.assertTrue(cmd.contains(""org.h2.Driver""));
        Assert.assertTrue(
                cmd.contains(""--boundary-query \""SELECT MIN(\\\""TEST_KYLIN_FACT\\\"".\\\""CAL_DT\\\""), MAX(\\\""TEST_KYLIN_FACT\\\"".\\\""CAL_DT\\\"")"" + System.lineSeparator()
                        + ""FROM \\\""DEFAULT\\\"".\\\""TEST_KYLIN_FACT\\\"" AS \\\""TEST_KYLIN_FACT\\\""\""""));
        source.close();
    }
",non-flaky,5
91553,apache_kylin,JdbcHiveMRInputTest.testGenSqoopCmd_WithLookupShardBy,"    @Test
    public void testGenSqoopCmd_WithLookupShardBy() throws IOException {
        ISource source = SourceManager.getSource(new JdbcSourceAware());
        IMRInput input = source.adaptToBuildEngine(IMRInput.class);
        Assert.assertNotNull(input);

        CubeManager cubeManager = CubeManager.getInstance(getTestConfig());
        CubeDesc cubeDesc = CubeDescManager.getInstance(getTestConfig()).getCubeDesc(""ut_jdbc_shard"");
        CubeSegment seg = cubeManager.appendSegment(cubeManager.getCube(cubeDesc.getName()),
                new SegmentRange.TSRange(System.currentTimeMillis() - 100L, System.currentTimeMillis() + 100L));
        CubeJoinedFlatTableDesc flatDesc = new CubeJoinedFlatTableDesc(seg);
        JdbcHiveMRInput.JdbcMRBatchCubingInputSide inputSide = (JdbcHiveMRInput.JdbcMRBatchCubingInputSide) input
                .getBatchCubingInputSide(flatDesc);

        AbstractExecutable executable = new MockInputSide(flatDesc, inputSide).createSqoopToFlatHiveStep(""/tmp"",
                cubeDesc.getName());
        Assert.assertNotNull(executable);

        String cmd = executable.getParam(""cmd"");
        Assert.assertTrue(cmd.contains(""org.h2.Driver""));
        Assert.assertTrue(cmd.contains(
                ""--boundary-query \""SELECT MIN(\\\""TEST_CATEGORY_GROUPINGS\\\"".\\\""META_CATEG_NAME\\\""), MAX(\\\""TEST_CATEGORY_GROUPINGS\\\"".\\\""META_CATEG_NAME\\\"")"" + System.lineSeparator()
                        + ""FROM \\\""DEFAULT\\\"".\\\""TEST_CATEGORY_GROUPINGS\\\"" AS \\\""TEST_CATEGORY_GROUPINGS\\\""\""""));

        source.close();
    }
",non-flaky,5
91554,apache_kylin,JdbcSourceTest.testBasics,"    @Test
    public void testBasics() throws IOException {
        ISource source = SourceManager.getSource(new JdbcSourceAware());
        ISourceMetadataExplorer explorer = source.getSourceMetadataExplorer();
        ISampleDataDeployer deployer = source.getSampleDataDeployer();

        Assert.assertTrue(source instanceof JdbcSource);
        Assert.assertTrue(explorer instanceof JdbcExplorer);
        Assert.assertTrue(deployer instanceof JdbcExplorer);

        IMRInput input = source.adaptToBuildEngine(IMRInput.class);
        Assert.assertNotNull(input);

        Class adaptTo = Object.class;
        expectedCannotAdaptEx.expect(RuntimeException.class);
        expectedCannotAdaptEx.expectMessage(""Cannot adapt to "" + adaptTo);
        source.adaptToBuildEngine(adaptTo);

        TableMetadataManager tblManager = TableMetadataManager.getInstance(getTestConfig());
        IReadableTable table = source.createReadableTable(tblManager.getTableDesc(""test_kylin_fact"", ""default""), null);
        Assert.assertTrue(table instanceof JdbcTable);

        source.close();
    }
",non-flaky,5
91555,apache_kylin,JdbcHiveInputBaseTest.testFetchValue,"    @Test
    public void testFetchValue() {
        Map<String, String> map = new HashMap<>();
        String guess = JdbcHiveInputBase.fetchValue(""DB_1"", ""TB_2"", ""COL_3"", map);

        // not found, return input value
        assertEquals(""DB_1.TB_2.COL_3"", guess);
        map.put(""DB_1.TB_2.COL_3"", ""Db_1.Tb_2.Col_3"");

        guess = JdbcHiveInputBase.fetchValue(""DB_1"", ""TB_2"", ""COL_3"", map);
        // found, return cached value
        assertEquals(""Db_1.Tb_2.Col_3"", guess);
    }
",non-flaky,5
91556,apache_kylin,JdbcHiveInputBaseTest.testQuoteIdentifier,"    @Test
    public void testQuoteIdentifier() {
        String guess = JdbcHiveInputBase.quoteIdentifier(""Tbl1.Col1"", SourceDialect.MYSQL);
        assertEquals(""`Tbl1`.`Col1`"", guess);
        guess = JdbcHiveInputBase.quoteIdentifier(""Tbl1.Col1"", SourceDialect.SQL_SERVER);
        assertEquals(""[Tbl1].[Col1]"", guess);
    }
",non-flaky,5
91557,apache_kylin,ResourceToolTest.testCopy,"    @Test
    public void testCopy() throws IOException {
        KylinConfig dstConfig = KylinConfig.createInstanceFromUri(dstPath);
        ResourceStore srcStore = ResourceStore.getStore(KylinConfig.getInstanceFromEnv());
        ResourceStore dstStore = ResourceStore.getStore(dstConfig);

        //metadata under source path and destination path are not equal before copy
        Assert.assertNotEquals(srcStore.listResources(""/""), dstStore.listResources(""/""));

        new ResourceTool().copy(KylinConfig.getInstanceFromEnv(), dstConfig, ""/"");

        //After copy, two paths have same metadata
        NavigableSet<String> dstFiles = dstStore.listResourcesRecursively(""/"");
        NavigableSet<String> srcFiles = srcStore.listResourcesRecursively(""/"");
        Assert.assertTrue(srcFiles.containsAll(EXEC_FILES));
        Assert.assertFalse(dstFiles.containsAll(EXEC_FILES));
        srcFiles.removeAll(EXEC_FILES);
        Assert.assertEquals(srcFiles, dstFiles);
    }
",non-flaky,5
91558,apache_kylin,HDFSResourceStoreTest.testListResourcesImpl,"    @Test
    public void testListResourcesImpl() throws Exception {
        String path = ""../examples/test_metadata/"";
        String cp = new File(path).getCanonicalFile().getPath();
        FileSystem fs = HadoopUtil.getFileSystem(cp);
        HDFSResourceStore store = new HDFSResourceStore(KylinConfig.getInstanceFromEnv(),
                StorageURL.valueOf(""hdfs@hdfs""));
        Field field = store.getClass().getDeclaredField(""fs"");
        field.setAccessible(true);
        field.set(store, fs);

        File f1 = new File(cp + ""/resource/resource/e1.json"");
        File f2 = new File(cp + ""/resource/resource/e2.json"");
        if (!f1.getParentFile().exists()) {
            if (!f1.getParentFile().mkdirs()) {
                throw new RuntimeException(""Can not create dir."");
            }
        }
        if (!(f1.createNewFile() && f2.createNewFile())) {
            throw new RuntimeException(""Can not create file."");
        }

        Path p = new Path(cp);
        TreeSet<String> resources = store.getAllFilePath(new Path(p, ""resource""), ""/resource/"");
        TreeSet<String> expected = new TreeSet<>();
        expected.add(""/resource/resource/e1.json"");
        expected.add(""/resource/resource/e2.json"");
        Assert.assertEquals(expected, resources);
    }
",non-flaky,5
91559,apache_kylin,AutoDeleteDirectoryTest.testBasic,"    @Test
    public void testBasic() throws IOException {
        File tempFile = null;
        try (AutoDeleteDirectory autoTempFile = new AutoDeleteDirectory(""test"", """")) {
            Assert.assertTrue(autoTempFile.getFile().isDirectory());
            Assert.assertEquals(0, autoTempFile.getFile().listFiles().length);
            tempFile = autoTempFile.getFile();
        }
        Assert.assertTrue(!tempFile.exists());
    }
",non-flaky,5
91560,apache_kylin,LocalFileResourceStoreTest.testFileStore,"    @Test
    public void testFileStore() throws Exception {
        KylinConfig config = KylinConfig.getInstanceFromEnv();
        ResourceStoreTest.testAStore(config.getMetadataUrl().toString(), config);
    }
",non-flaky,5
91561,apache_kylin,LocalFileResourceStoreTest.testRollback,"    @Test
    public void testRollback() throws Exception {
        ResourceStore store = ResourceStore.getStore(KylinConfig.getInstanceFromEnv());
        byte[] bytes = new byte[] { 0, 1, 2 };
        RawResource raw;
        Checkpoint cp;

        cp = store.checkpoint();
        try {
            store.putResource(""/res1"", new StringEntity(""data1""), 1000, StringEntity.serializer);
        } finally {
            cp.close();
        }
        StringEntity str = store.getResource(""/res1"", StringEntity.serializer);
        assertEquals(""data1"", str.toString());

        cp = store.checkpoint();
        try {
            ByteArrayInputStream is = new ByteArrayInputStream(bytes);
            store.putResource(""/res2"", is, 2000);
            is.close();
            
            store.putResource(""/res1"", str, 2000, StringEntity.serializer);
            store.deleteResource(""/res1"");

            assertEquals(null, store.getResource(""/res1""));
            assertEquals(2000, (raw = store.getResource(""/res2"")).lastModified());
            raw.content().close();
            
            cp.rollback();
            
            assertEquals(null, store.getResource(""/res2""));
            assertEquals(1000, (raw = store.getResource(""/res1"")).lastModified());
            raw.content().close();
        } finally {
            cp.close();
        }
    }
",non-flaky,5
91562,apache_kylin,KylinVersionTest.testNormal,"    @Test
    public void testNormal() {
        KylinVersion ver1 = new KylinVersion(""2.1.0"");
        Assert.assertEquals(2, ver1.major);
        Assert.assertEquals(1, ver1.minor);
        Assert.assertEquals(0, ver1.revision);
    }
",non-flaky,5
91563,apache_kylin,KylinVersionTest.testNoRevision,"    @Test
    public void testNoRevision() {
        KylinVersion ver1 = new KylinVersion(""2.1"");
        Assert.assertEquals(2, ver1.major);
        Assert.assertEquals(1, ver1.minor);
        Assert.assertEquals(0, ver1.revision);
    }
",non-flaky,5
91564,apache_kylin,KylinVersionTest.testToString,"    @Test
    public void testToString() {
        KylinVersion ver1 = new KylinVersion(""2.1.7.321"");
        Assert.assertEquals(2, ver1.major);
        Assert.assertEquals(1, ver1.minor);
        Assert.assertEquals(7, ver1.revision);
        Assert.assertEquals(321, ver1.internal);
        Assert.assertEquals(""2.1.7.321"", ver1.toString());
    }
",non-flaky,5
91565,apache_kylin,KylinVersionTest.testCompare,"    @Test
    public void testCompare() {
        Assert.assertEquals(true, KylinVersion.isBefore200(""1.9.9""));
        Assert.assertEquals(false, KylinVersion.isBefore200(""2.0.0""));
        Assert.assertEquals(true, new KylinVersion(""2.1.0"").compareTo(new KylinVersion(""2.1.0.123"")) < 0);
    }
",non-flaky,5
91566,apache_kylin,RestClientTest.basicTests,"    @Test
    public void basicTests() throws IOException {
        RestClient a = new RestClient(""prod01:80"");
        //a.wipeCache(""metadata"", ""a"", ""a"");
        //String aa = a.getKylinProperties();
        //System.out.println(aa);
        RestClient b = new RestClient(""sandbox.hortonworks.com:7070"");
        //b.wipeCache(""metadata"", ""a"", ""a"");
        //String bb = b.getKylinProperties();
        //System.out.println(bb);

    }
",non-flaky,5
91567,apache_kylin,RandomUtilTest.testRandomUUID,"    @Test
    public void testRandomUUID() {
        Assert.assertEquals(RandomUtil.randomUUID().toString().length(), UUID.randomUUID().toString().length());
        Assert.assertNotEquals(RandomUtil.randomUUID().toString(), RandomUtil.randomUUID().toString());
    }
",non-flaky,5
91568,apache_kylin,MailServiceTest.testSendEmail,"    @Test
    public void testSendEmail() throws IOException {

        KylinConfig config = KylinConfig.getInstanceFromEnv();

        MailService mailservice = new MailService(config);
        boolean sent = sendTestEmail(mailservice);
        assert sent;

        System.setProperty(""kylin.job.notification-enabled"", ""false"");
        // set kylin.job.notification-enabled=false, and run again, this time should be no mail delivered
        mailservice = new MailService(config);
        sent = sendTestEmail(mailservice);
        assert !sent;

    }
",non-flaky,5
91569,apache_kylin,IdentityUtilTest.basicTest,"    @Test
    public void basicTest() {
        String s1 = new String(""hi"");
        String s2 = new String(""hi"");

        List<String> c1 = Lists.newArrayList(s1);
        List<String> c2 = Lists.newArrayList(s2);
        List<String> c3 = Lists.newArrayList(s2);

        Assert.assertFalse(IdentityUtils.collectionReferenceEquals(c1, c2));
        Assert.assertTrue(IdentityUtils.collectionReferenceEquals(c3, c2));
    }
",non-flaky,5
91570,apache_kylin,DateFormatTest.testIsSupportedDateFormat,"    @Test
    public void testIsSupportedDateFormat() {
        assertTrue(DateFormat.isSupportedDateFormat(""2010-01-01""));
        assertTrue(DateFormat.isSupportedDateFormat(""20100101""));
        assertTrue(DateFormat.isSupportedDateFormat(""2010-01-01 01:01:01""));
        assertTrue(DateFormat.isSupportedDateFormat(""2010-01-01 01:00:00.000""));

        assertFalse(DateFormat.isSupportedDateFormat(""2010-01""));
        assertFalse(DateFormat.isSupportedDateFormat(""2010/01/01""));
        assertFalse(DateFormat.isSupportedDateFormat(""2010-1-1""));
        assertFalse(DateFormat.isSupportedDateFormat(""abc""));
    }
",non-flaky,5
91571,apache_kylin,StringSplitterTest.testSplitReturningNonEmptyArray,"  @Test
  public void testSplitReturningNonEmptyArray() {
      String[] stringArray = StringSplitter.split(""Fc8!v~f?aQL"", ""Fc8!v~f?aQL"");

      assertEquals(2, stringArray.length);
      assertEquals("""", stringArray[0]);
      assertEquals("""", stringArray[1]);
  }
",non-flaky,5
91572,apache_kylin,StringSplitterTest.testSplitWithNonEmptyString,"  @Test
  public void testSplitWithNonEmptyString() {
      String[] stringArray = StringSplitter.split(""]sZ}gR\""cws,8p#|m"", ""Fc8!v~f?aQL"");
",non-flaky,5
91573,apache_kylin,ClassUtilTest.testFindContainingJar,"    @Test
    public void testFindContainingJar() throws ClassNotFoundException {
        Assert.assertTrue(ClassUtil.findContainingJar(Class.forName(""org.apache.commons.beanutils.BeanUtils"")).contains(""commons-beanutils""));
        Assert.assertTrue(ClassUtil.findContainingJar(Class.forName(""org.apache.commons.beanutils.BeanUtils""), ""core"").contains(""commons-beanutils-core""));
    }
",non-flaky,5
91574,apache_kylin,EncryptUtilTest.testAESEncrypt,"    @Test
    public void testAESEncrypt(){
        String input = ""hello world"";
        String result = EncryptUtil.encrypt(input);
        Assert.assertEquals(""4stv/RRleOtvie/8SLHmXA=="", result);
    }
",non-flaky,5
91575,apache_kylin,CaseInsensitiveStringCollectionTest.testCaseInsensitiveMap,"    @Test
    public void testCaseInsensitiveMap() {
        CaseInsensitiveStringMap<String> m1 = new CaseInsensitiveStringMap<>();
        m1.put(""a"", ""a"");
        Map<String, String> m2 = new HashMap<>();
        m2.put(""a"", ""a"");
        Assert.assertEquals(m2, m1);
        Assert.assertTrue(m1.containsKey(""A""));
        Assert.assertFalse(m1.containsValue(""A""));
    }
",non-flaky,5
91576,apache_kylin,CaseInsensitiveStringCollectionTest.testCaseInsensitiveSet,"    @Test
    public void testCaseInsensitiveSet() {
        CaseInsensitiveStringSet s1 = new CaseInsensitiveStringSet();
        s1.add(""a"");
        Set<String> s2 = new HashSet<>();
        s2.add(""a"");
        Assert.assertEquals(s2, s1);
        Assert.assertTrue(s1.contains(""A""));
    }
",non-flaky,5
91577,apache_kylin,JacksonTest.foo,"    @Test
    public void foo() throws IOException {
        HashMap a = new HashMap<String, String>();
        a.put(""1"", ""1"");
        a.put(""3"", ""3"");
        a.put(""2"", ""2"");


        JacksonBean bean = new JacksonBean();
        bean.setA(""valuea"");
        bean.setConfiguration(a);

        String s = JsonUtil.writeValueAsString(bean);
        System.out.println(s);

        JacksonBean desBean = (JacksonBean) JsonUtil.readValue(""{\""a\"":\""valuea\"",\""b\"":0,\""configuration\"":{\""2\"":\""2\"",\""3\"":\""3\"",\""1\"":\""1\""}}"", JacksonBean.class);
        
        String x2 = JsonUtil.writeValueAsString(desBean);
        System.out.println(x2);
        
        System.out.println(desBean);
    }
",non-flaky,5
91578,apache_kylin,BasicTest.testxx,"    @Test
    public void testxx() throws InterruptedException {
        System.out.println(
                ""((?<![\\p{L}_0-9\\.\\\""])(\\\""[\\p{L}_0-9]+\\\""\\.)?(\\\""[\\p{L}_0-9]+\\\"")(?![\\p{L}_0-9\\.\\\""]))"");
        System.out.println(0x8fL);
        byte[] space = new byte[100];
        ByteBuffer buffer = ByteBuffer.wrap(space, 10, 20);
        buffer.put((byte) 1);
    }
",non-flaky,5
91579,apache_kylin,BasicTest.testyy,"    @Test
    public void testyy() throws InterruptedException {
        long wallClock = System.currentTimeMillis();

        HashMap<Integer, byte[]> map = Maps.newHashMap();
        for (int i = 0; i < 10000000; i++) {
            byte[] a = new byte[100];
            map.put(i, a);
        }

        System.out.println(""Time Consumed: "" + (System.currentTimeMillis() - wallClock));
    }
",non-flaky,5
91580,apache_kylin,BasicTest.run,"    @Test
    public void test0() throws Exception {

        ExecutorService executorService = Executors.newCachedThreadPool();
        List<Future<?>> futures = Lists.newArrayList();

        futures.add(executorService.submit(new Runnable() {
            @Override
            public void run() {
                throw new RuntimeException(""hi"");
            }
",non-flaky,5
91581,apache_kylin,BasicTest.test1,"    @Test
    public void test1() throws Exception {

        System.out.println(org.apache.kylin.common.util.DateFormat.formatToTimeStr(1433833611000L));
        System.out.println(org.apache.kylin.common.util.DateFormat.formatToTimeStr(1433250517000L));
        System.out.println(org.apache.kylin.common.util.DateFormat.stringToMillis(""2015-06-01 00:00:00""));
        System.out.println(org.apache.kylin.common.util.DateFormat.stringToMillis(""2015-05-15 17:00:00""));
        Assert.assertEquals(1568960682251L,
                org.apache.kylin.common.util.DateFormat.stringToMillis(""2019-09-20T14:24:42.251+08:00""));

        String bb = ""\\x00\\x00\\x00\\x00\\x01\\x3F\\xD0\\x2D\\58\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00"";//2013/07/12 07:59:37
        String cc = ""\\x00\\x00\\x00\\x00\\x01\\x41\\xBE\\x8F\\xD8\\x00\\x00\\x00\\x00\\x00\\x00\\x00"";//2013/10/16 08:00:00
        String dd = ""\\x00\\x00\\x00\\x00\\x01\\x41\\xBE\\x8F\\xD8\\x07\\x00\\x18\\x00\\x00\\x00"";

        byte[] bytes = BytesUtil.fromReadableText(dd);
        long ttt = BytesUtil.readLong(bytes, 2, 8);
        System.out.println(time(ttt));

        System.out.println(""\\"");
        System.out.println(""n"");

        System.out.println(""The start key is set to "" + null);
        long current = System.currentTimeMillis();
        System.out.println(time(current));

        Calendar a = Calendar.getInstance(TimeZone.getDefault(), Locale.ROOT);
        Calendar b = Calendar.getInstance(TimeZone.getDefault(), Locale.ROOT);
        Calendar c = Calendar.getInstance(TimeZone.getDefault(), Locale.ROOT);
        b.clear();
        c.clear();

        System.out.println(time(b.getTimeInMillis()));
        System.out.println(time(c.getTimeInMillis()));

        a.setTimeInMillis(current);
        b.set(a.get(Calendar.YEAR), a.get(Calendar.MONTH), a.get(Calendar.DAY_OF_MONTH), a.get(Calendar.HOUR_OF_DAY),
                a.get(Calendar.MINUTE));
        c.set(a.get(Calendar.YEAR), a.get(Calendar.MONTH), a.get(Calendar.DAY_OF_MONTH), a.get(Calendar.HOUR_OF_DAY),
                0);

        System.out.println(time(b.getTimeInMillis()));
        System.out.println(time(c.getTimeInMillis()));

    }
",non-flaky,5
91582,apache_kylin,BasicTest.test3,"    @Test
    public void test3() throws Exception {
        FastDateFormat formatter = org.apache.kylin.common.util.DateFormat.getDateFormat(""MM dd, yyyy hh:mm:ss a"");

        String timeStr = ""07 20, 2016 09:59:17 AM"";

        System.out.println(formatter.parse(timeStr).getTime());
    }
",non-flaky,5
91583,apache_kylin,BasicTest.testStringSplit,"    @Test
    public void testStringSplit() throws Exception {

        String[] origin = new String[] { ""ab,c"", ""cd|e"" };

        // test with sequence file default delimiter
        String delimiter = ""\01""; //""\u001F""; ""\t"";
        String concated = StringUtils.join(Arrays.asList(origin), delimiter);
        System.out.println(concated);

        String[] newValues = concated.split(delimiter);
        Assert.assertEquals(origin, newValues);

        newValues = concated.split(""\\"" + delimiter);
        Assert.assertEquals(origin, newValues);
    }
",non-flaky,5
91584,apache_kylin,RandomSamplerTest.test,"    @Test
    public void test() {
        RandomSampler<String> s = new RandomSampler<String>();
        List<String> data = new ArrayList<String>();
        for (int i = 0; i < 1000; i++) {
            data.add(String.valueOf(i));
        }

        List<String> result = s.sample(data, 50);
        System.out.println(result);
        assertEquals(50, result.size());
    }
",non-flaky,5
91585,apache_kylin,BytesUtilTest.test,"    @Test
    public void test() {
        ByteBuffer buffer = ByteBuffer.allocate(10000);
        int[] x = new int[] { 1, 2, 3 };
        BytesUtil.writeIntArray(x, buffer);
        buffer.flip();

        byte[] buf = new byte[buffer.limit()];
        System.arraycopy(buffer.array(), 0, buf, 0, buffer.limit());

        ByteBuffer newBuffer = ByteBuffer.wrap(buf);
        int[] y = BytesUtil.readIntArray(newBuffer);
        assertEquals(y[2], 3);
    }
",non-flaky,5
91586,apache_kylin,BytesUtilTest.testBooleanArray,"    @Test
    public void testBooleanArray() {
        ByteBuffer buffer = ByteBuffer.allocate(10000);
        boolean[] x = new boolean[] { true, false, true };
        BytesUtil.writeBooleanArray(x, buffer);
        buffer.flip();
        boolean[] y = BytesUtil.readBooleanArray(buffer);
        assertEquals(y[2], true);
        assertEquals(y[1], false);
    }
",non-flaky,5
91587,apache_kylin,BytesUtilTest.testWriteReadUnsignedInt,"    @Test
    public void testWriteReadUnsignedInt() {
        testWriteReadUnsignedInt(735033, 3);
        testWriteReadUnsignedInt(73503300, 4);
    }
",non-flaky,5
91588,apache_kylin,BytesUtilTest.testReadable,"    @Test
    public void testReadable() {
        String x = ""\\x00\\x00\\x00\\x00\\x00\\x01\\xFC\\xA8"";
        byte[] bytes = BytesUtil.fromReadableText(x);
        String y = BytesUtil.toHex(bytes);
        assertEquals(x, y);
    }
",non-flaky,5
91589,apache_kylin,PartialSorterTest.compare,"    @Test
    public void basicTest() {
        List<Integer> a = Lists.newArrayList();
        a.add(100);
        a.add(2);
        a.add(92);
        a.add(1);
        a.add(0);
        PartialSorter.partialSort(a, Lists.newArrayList(1, 3, 4), new Comparator<Integer>() {
            @Override
            public int compare(Integer o1, Integer o2) {
                return o1.compareTo(o2);
            }
",non-flaky,5
91590,apache_kylin,SourceConfigurationUtilTest.testHiveConf,"    @Test
    public void testHiveConf() {
        Properties properties = SourceConfigurationUtil.loadHiveJDBCProperties();
        assertTrue(properties.containsKey(""hiveconf:hive.auto.convert.join.noconditionaltask.size""));
    }
",non-flaky,5
91591,apache_kylin,SourceConfigurationUtilTest.testSqoopConf,"    @Test
    public void testSqoopConf() {
        Map<String, String> configMap = SourceConfigurationUtil.loadSqoopConfiguration();
        assertFalse(configMap.isEmpty());
        assertEquals(""1"", configMap.get(""dfs.replication""));
    }
",non-flaky,5
91592,apache_kylin,HiveCmdBuilderTest.testHiveCLI,"    @Test
    public void testHiveCLI() {
        System.setProperty(""kylin.source.hive.client"", ""cli"");

        Map<String, String> hiveProps = new HashMap<>();
        hiveProps.put(""hive.execution.engine"", ""mr"");
        Map<String, String> hivePropsOverwrite = new HashMap<>();
        hivePropsOverwrite.put(""hive.execution.engine"", ""tez"");
        HiveCmdBuilder hiveCmdBuilder = new HiveCmdBuilder(""test HiveCLI"");
        hiveCmdBuilder.addStatement(""USE default;"");
        hiveCmdBuilder.addStatement(""DROP TABLE `test`;"");
        hiveCmdBuilder.addStatement(""SHOW\n TABLES;"");
        hiveCmdBuilder.setHiveConfProps(hiveProps);
        hiveCmdBuilder.overwriteHiveProps(hivePropsOverwrite);
        assertEquals(
                ""hive -e \""set mapred.job.name='test HiveCLI';\nUSE default;\nDROP TABLE \\`test\\`;\nSHOW\n TABLES;\n\"" --hiveconf hive.execution.engine=tez"",
                hiveCmdBuilder.build());
    }
",non-flaky,5
91593,apache_kylin,HiveCmdBuilderTest.testBeeline,"    @Test
    public void testBeeline() throws IOException {
        String lineSeparator = java.security.AccessController
                .doPrivileged(new sun.security.action.GetPropertyAction(""line.separator""));
        System.setProperty(""kylin.source.hive.client"", ""beeline"");
        System.setProperty(""kylin.source.hive.beeline-shell"", ""/spark-client/bin/beeline"");
        System.setProperty(""kylin.source.hive.beeline-params"", ""-u jdbc_url"");

        HiveCmdBuilder hiveCmdBuilder = new HiveCmdBuilder();
        hiveCmdBuilder.addStatement(""USE default;"");
        hiveCmdBuilder.addStatement(""DROP TABLE `test`;"");
        hiveCmdBuilder.addStatement(""SHOW TABLES;"");

        String cmd = hiveCmdBuilder.build();
        String hqlFile = cmd.substring(cmd.lastIndexOf(""-f "") + 3).trim();
        hqlFile = hqlFile.substring(0, hqlFile.length() - "";exit $ret_code"".length());
        String createFileCmd = cmd.substring(0, cmd.indexOf(""EOL\n"", cmd.indexOf(""EOL\n"") + 1) + 3);
        CliCommandExecutor cliCommandExecutor = new CliCommandExecutor();
        Pair<Integer, String> execute = cliCommandExecutor.execute(createFileCmd);
        String hqlStatement = FileUtils.readFileToString(new File(hqlFile), Charset.defaultCharset());
        assertEquals(
                ""USE default;"" + lineSeparator + ""DROP TABLE `test`;"" + lineSeparator + ""SHOW TABLES;"" + lineSeparator,
                hqlStatement);
        assertBeelineCmd(cmd);
        FileUtils.forceDelete(new File(hqlFile));
    }
",non-flaky,5
133,ONSdigital_rm-collection-exercise-service,SampleSummaryServiceTest.testActivateSamples,"@Test
public void testActivateSamples() throws Exception {
    UUID collectionExerciseId = UUID.randomUUID();
    UUID surveyId = UUID.randomUUID();
    UUID sampleSummaryId = UUID.randomUUID();
    SampleLink sampleLink = new SampleLink();
    sampleLink.setSampleSummaryId(sampleSummaryId);
    sampleLink.setCollectionExerciseId(collectionExerciseId);
    List<SampleLink> sampleLinks = new ArrayList<>();
    sampleLinks.add(sampleLink);
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setId(collectionExerciseId);
    collectionExercise.setSurveyId(surveyId);
    Event event = new Event();
    event.setTimestamp(new Timestamp(System.currentTimeMillis()));
    when(collectionExerciseRepository.findOneById(collectionExerciseId)).thenReturn(collectionExercise);
    when(sampleLinkRepository.findByCollectionExerciseId(collectionExerciseId)).thenReturn(sampleLinks);
    when(eventRepository.findOneByCollectionExerciseAndTag(collectionExercise, go_live.name())).thenReturn(event);
    sampleSummaryService.activateSamples(collectionExerciseId);
    sampleSummaryService.sampleSummaryValidated(true, collectionExerciseId);
    sampleSummaryService.sampleSummaryDistributed(true, collectionExerciseId);
    verify(collectionExerciseRepository, times(3)).findOneById(collectionExerciseId);
    verify(sampleSummaryActivationPublisher, times(1)).sendSampleSummaryActivation(collectionExerciseId, sampleSummaryId, surveyId);
    verify(collectionExerciseService, times(1)).transitionCollectionExercise(collectionExercise, EXECUTE);
    verify(collectionExerciseService, times(1)).transitionCollectionExercise(collectionExercise, VALIDATE);
    verify(collectionExerciseService, times(1)).transitionCollectionExercise(collectionExercise, EXECUTION_COMPLETE);
    verify(collectionExerciseService, times(1)).transitionCollectionExercise(collectionExercise, GO_LIVE);
}",time,2
98369,ONSdigital_rm-collection-exercise-service,MultipleMandatoryEventsValidatorTest.testValidMpsEventCreation,"  @Test
  public void testValidMpsEventCreation() throws CTPException {
    List<Event> events = getExistingEvents();
    Instant timestamp = Instant.now().plus(2, ChronoUnit.DAYS);
    Event newMpsEvent = new Event();
    newMpsEvent.setTag((EventService.Tag.mps.toString()));
    newMpsEvent.setTimestamp(Timestamp.from(timestamp));
    validator.validate(events, newMpsEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98370,ONSdigital_rm-collection-exercise-service,MultipleMandatoryEventsValidatorTest.testValidGoLiveEventCreation,"  @Test
  public void testValidGoLiveEventCreation() throws CTPException {
    List<Event> events = getExistingEvents();
    Instant timestamp = Instant.now().plus(4, ChronoUnit.DAYS);
    Event newGoLiveEvent = new Event();
    newGoLiveEvent.setTag((EventService.Tag.go_live.toString()));
    newGoLiveEvent.setTimestamp(Timestamp.from(timestamp));
    validator.validate(
        events, newGoLiveEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98371,ONSdigital_rm-collection-exercise-service,MultipleMandatoryEventsValidatorTest.testValidReturnByEventCreation,"  @Test
  public void testValidReturnByEventCreation() throws CTPException {
    List<Event> events = getExistingEvents();
    Instant timestamp = Instant.now().plus(6, ChronoUnit.DAYS);
    Event newReturnByEvent = new Event();
    newReturnByEvent.setTag((EventService.Tag.return_by.toString()));
    newReturnByEvent.setTimestamp(Timestamp.from(timestamp));
    validator.validate(
        events, newReturnByEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98372,ONSdigital_rm-collection-exercise-service,MultipleMandatoryEventsValidatorTest.testValidExerciseEndEventCreation,"  @Test
  public void testValidExerciseEndEventCreation() throws CTPException {
    List<Event> events = getExistingEvents();
    Instant timestamp = Instant.now().plus(8, ChronoUnit.DAYS);
    Event newExerciseEndEvent = new Event();
    newExerciseEndEvent.setTag((EventService.Tag.exercise_end.toString()));
    newExerciseEndEvent.setTimestamp(Timestamp.from(timestamp));
    validator.validate(
        events, newExerciseEndEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98373,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.isEventValidator,"  @Test
  public void isEventValidator() {
    assertThat(reminderValidator, instanceOf(EventValidator.class));
  }
",non-flaky,5
98374,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.returnTrueAndDoNothingIfNotReminder,"  @Test
  public void returnTrueAndDoNothingIfNotReminder() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now()));
    final List<Event> events = new ArrayList<>();
    reminderValidator.validate(events, mpsEvent, CollectionExerciseState.CREATED);

    verify(eventDateOrderChecker, never()).isEventDatesInOrder(anyList());
  }
",non-flaky,5
98375,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testCanUpdateReminderWhenReadyForLive,"  @Test
  public void testCanUpdateReminderWhenReadyForLive() throws CTPException {
    final Event reminderEvent = new Event();
    reminderEvent.setTag(Tag.reminder.toString());
    reminderEvent.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();
    reminderValidator.validate(events, reminderEvent, CollectionExerciseState.READY_FOR_LIVE);
  }
",non-flaky,5
98376,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testCanUpdateReminderWhenLive,"  @Test
  public void testCanUpdateReminderWhenLive() throws CTPException {
    final Event reminderEvent = new Event();
    reminderEvent.setTag(Tag.reminder.toString());
    reminderEvent.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();

    reminderValidator.validate(events, reminderEvent, CollectionExerciseState.LIVE);
  }
",non-flaky,5
98377,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testCantUpdateReminderThatHasPastAndCollectionExerciseInLockedState,"  @Test
  public void testCantUpdateReminderThatHasPastAndCollectionExerciseInLockedState() {
    final Event reminder = new Event();
    reminder.setTag((Tag.reminder.toString()));
    reminder.setTimestamp(Timestamp.from(Instant.now().minus(2, ChronoUnit.DAYS)));

    final Event newReminder = new Event();
    newReminder.setTag((Tag.reminder.toString()));
    newReminder.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(reminder);
    CTPException actualException = null;
    try {
      reminderValidator.validate(events, newReminder, CollectionExerciseState.LIVE);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(""Reminder cannot be set in the past"", actualException.getMessage());
  }
",non-flaky,5
98378,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testCanUpdateReminderThatHasPastAndCollectionExerciseNotInLockedState,"  @Test
  public void testCanUpdateReminderThatHasPastAndCollectionExerciseNotInLockedState()
      throws CTPException {
    final Event reminder = new Event();
    reminder.setTag((Tag.reminder.toString()));
    reminder.setTimestamp(Timestamp.from(Instant.now().minus(2, ChronoUnit.DAYS)));

    final Event newReminder = new Event();
    newReminder.setTag((Tag.reminder.toString()));
    newReminder.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(reminder);

    reminderValidator.validate(events, newReminder, CollectionExerciseState.SCHEDULED);
  }
",non-flaky,5
98379,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testValidReminderEventCreation,"  @Test
  public void testValidReminderEventCreation() throws CTPException {
    final Event goLive = new Event();
    goLive.setTag((Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now()));

    final Event reminder = new Event();
    reminder.setTag((Tag.reminder.toString()));
    reminder.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event exerciseEnd = new Event();
    exerciseEnd.setTag((Tag.exercise_end.toString()));
    exerciseEnd.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, exerciseEnd);

    reminderValidator.validate(events, reminder, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98380,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testReminderAfterExerciseEndInvalid,"  @Test
  public void testReminderAfterExerciseEndInvalid() {
    final Event reminderEvent = new Event();
    reminderEvent.setTag(Tag.reminder3.toString());
    reminderEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final Event exerciseEnd = new Event();
    exerciseEnd.setTag((Tag.exercise_end.toString()));
    exerciseEnd.setTimestamp(Timestamp.from(Instant.now().plus(3, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(exerciseEnd);
    CTPException actualException = null;
    try {
      reminderValidator.validate(events, reminderEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Reminder must take place during collection exercise period"", actualException.getMessage());
  }
",non-flaky,5
98381,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testReminderBeforeGoliveInvalid,"  @Test
  public void testReminderBeforeGoliveInvalid() {
    final Event goLive = new Event();
    goLive.setTag((Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final List<Event> events = Collections.singletonList(goLive);

    final Event reminderEvent = new Event();
    reminderEvent.setTag(Tag.reminder.toString());
    reminderEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      reminderValidator.validate(events, reminderEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Reminder must take place during collection exercise period"", actualException.getMessage());
  }
",non-flaky,5
98382,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testReminder2WrongOrderEventCreation,"  @Test
  public void testReminder2WrongOrderEventCreation() {
    final Event reminder = new Event();
    reminder.setTag((Tag.reminder.toString()));
    reminder.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final Event reminder2 = new Event();
    reminder2.setTag((Tag.reminder2.toString()));
    reminder2.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));
    final List<Event> events = new ArrayList<>();
    events.add(reminder);
    CTPException actualException = null;
    try {
      reminderValidator.validate(events, reminder2, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98383,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testReminder3WrongOrderEventCreation,"  @Test
  public void testReminder3WrongOrderEventCreation() {
    final Event reminder2 = new Event();
    reminder2.setTag((Tag.reminder2.toString()));
    reminder2.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final Event reminder3 = new Event();
    reminder3.setTag((Tag.reminder3.toString()));
    reminder3.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));
    final List<Event> events = new ArrayList<>();
    events.add(reminder2);
    CTPException actualException = null;
    try {
      reminderValidator.validate(events, reminder3, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98384,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.isEventValidator,"  @Test
  public void isEventValidator() {
    assertThat(mandatoryValidator, instanceOf(EventValidator.class));
  }
",non-flaky,5
98385,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.returnTrueAndDoNothingIfNotReminder,"  @Test
  public void returnTrueAndDoNothingIfNotReminder() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.reminder.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now()));
    final List<Event> events = new ArrayList<>();
    mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.CREATED);

    verify(eventDateOrderChecker, never()).isEventDatesInOrder(anyList());
  }
",non-flaky,5
98386,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidMpsEventCreation,"  @Test
  public void testValidMpsEventCreation() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final List<Event> events = new ArrayList<>();
    mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98387,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidGoLiveEventCreation,"  @Test
  public void testValidGoLiveEventCreation() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event goLiveEvent = new Event();
    goLiveEvent.setTag((Tag.go_live.toString()));
    goLiveEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(mpsEvent);

    mandatoryValidator.validate(events, goLiveEvent, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98388,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidReturnByEventCreation,"  @Test
  public void testValidReturnByEventCreation() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event goLiveEvent = new Event();
    goLiveEvent.setTag((Tag.go_live.toString()));
    goLiveEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(mpsEvent, goLiveEvent);

    final Event returnByEvent = new Event();
    returnByEvent.setTag((Tag.return_by.toString()));
    returnByEvent.setTimestamp(Timestamp.from(Instant.now().plus(6, ChronoUnit.DAYS)));

    mandatoryValidator.validate(events, returnByEvent, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98389,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidGoLiveEventCreation,"  @Test
  public void testInvalidGoLiveEventCreation() {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(10, ChronoUnit.DAYS)));

    final Event goLive = new Event();
    goLive.setTag((Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(mpsEvent);

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, goLive, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98390,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidReturnByEventCreation,"  @Test
  public void testInvalidReturnByEventCreation() {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event goLiveEvent = new Event();
    goLiveEvent.setTag((Tag.go_live.toString()));
    goLiveEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(mpsEvent, goLiveEvent);

    final Event returnByEvent = new Event();
    returnByEvent.setTag((Tag.return_by.toString()));
    returnByEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, returnByEvent, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98391,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsLive,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsLive() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.LIVE);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98392,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsValidated,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsValidated() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.VALIDATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98393,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsExecutionStarted,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsExecutionStarted() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.EXECUTION_STARTED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98394,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsReadyForLive,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsReadyForLive() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.READY_FOR_LIVE);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98395,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsExecuted,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsExecuted() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.EXECUTED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98396,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidReturnByEventUpdate,"  @Test
  public void testValidReturnByEventUpdate() throws CTPException {
    final List<Event> events = createMandatoryEvents();

    final Event returnByEvent = new Event();
    returnByEvent.setTag(Tag.return_by.toString());
    returnByEvent.setTimestamp(Timestamp.from(Instant.now().plus(5, ChronoUnit.DAYS)));

    mandatoryValidator.validate(events, returnByEvent, CollectionExerciseState.SCHEDULED);
  }
",non-flaky,5
98397,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidExerciseEndEventUpdate,"  @Test
  public void testValidExerciseEndEventUpdate() throws CTPException {
    final List<Event> events = createMandatoryEvents();

    final Event exerciseEndEvent = new Event();
    exerciseEndEvent.setTag(Tag.exercise_end.toString());
    exerciseEndEvent.setTimestamp(Timestamp.from(Instant.now().plus(10, ChronoUnit.DAYS)));

    mandatoryValidator.validate(events, exerciseEndEvent, CollectionExerciseState.SCHEDULED);
  }
",non-flaky,5
98398,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidMpsEventUpdate,"  @Test
  public void testInvalidMpsEventUpdate() {
    final List<Event> events = createMandatoryEvents();

    final Event mpsEvent = new Event();
    mpsEvent.setTag(Tag.mps.toString());
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(6, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98399,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidGoLiveEventUpdate,"  @Test
  public void testInvalidGoLiveEventUpdate() {
    final List<Event> events = createMandatoryEvents();

    final Event goLiveEvent = new Event();
    goLiveEvent.setTag(Tag.go_live.toString());
    goLiveEvent.setTimestamp(Timestamp.from(Instant.now().plus(8, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, goLiveEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98400,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidReturnByEventUpdate,"  @Test
  public void testInvalidReturnByEventUpdate() {
    final List<Event> events = createMandatoryEvents();

    final Event returnByEvent = new Event();
    returnByEvent.setTag(Tag.return_by.toString());
    returnByEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, returnByEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98401,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidExerciseEndEventUpdate,"  @Test
  public void testInvalidExerciseEndEventUpdate() {
    final List<Event> events = createMandatoryEvents();

    final Event exerciseEndEvent = new Event();
    exerciseEndEvent.setTag(Tag.exercise_end.toString());
    exerciseEndEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, exerciseEndEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98402,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.isEventValidator,"  @Test
  public void isEventValidator() {
    assertThat(referencePeriodValidator, instanceOf(EventValidator.class));
  }
",non-flaky,5
98403,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.returnTrueAndDoNothingIfNotReferencePeriodEvent,"  @Test
  public void returnTrueAndDoNothingIfNotReferencePeriodEvent() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now()));
    final List<Event> events = new ArrayList<>();
    referencePeriodValidator.validate(events, mpsEvent, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98404,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.canUpdateReferencePeriodWhenCollectionExerciseReadyForLive,"  @Test
  public void canUpdateReferencePeriodWhenCollectionExerciseReadyForLive() throws CTPException {
    final Event referencePeriodStart = new Event();
    referencePeriodStart.setTag(Tag.ref_period_end.toString());
    referencePeriodStart.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();

    referencePeriodValidator.validate(
        events, referencePeriodStart, CollectionExerciseState.READY_FOR_LIVE);
  }
",non-flaky,5
98405,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.canUpdateReferencePeriodWhenCollectionExerciseLive,"  @Test
  public void canUpdateReferencePeriodWhenCollectionExerciseLive() throws CTPException {
    final Event referencePeriodStart = new Event();
    referencePeriodStart.setTag(Tag.ref_period_start.toString());
    referencePeriodStart.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();

    referencePeriodValidator.validate(events, referencePeriodStart, CollectionExerciseState.LIVE);
  }
",non-flaky,5
98406,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.testReferenceStartCanBeSetInThePast,"  @Test
  public void testReferenceStartCanBeSetInThePast() throws CTPException {
    final Event refStart = new Event();
    refStart.setTag((Tag.ref_period_start.toString()));
    refStart.setTimestamp(Timestamp.from(Instant.now().minus(1, ChronoUnit.DAYS)));

    final List<Event> events = new ArrayList<>();
    referencePeriodValidator.validate(events, refStart, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98407,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.testReferenceEndCanBeSetInThePast,"  @Test
  public void testReferenceEndCanBeSetInThePast() throws CTPException {
    final Event refEnd = new Event();
    refEnd.setTag((Tag.ref_period_end.toString()));
    refEnd.setTimestamp(Timestamp.from(Instant.now().minus(1, ChronoUnit.DAYS)));

    final List<Event> events = new ArrayList<>();
    referencePeriodValidator.validate(events, refEnd, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98408,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.testReferenceEndBeforeReferenceStartIsInvalid,"  @Test
  public void testReferenceEndBeforeReferenceStartIsInvalid() {
    final Event refEnd = new Event();
    refEnd.setTag((Tag.ref_period_end.toString()));
    refEnd.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));
    final Event refStart = new Event();
    refStart.setTag((Tag.ref_period_start.toString()));
    refStart.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final List<Event> events = new ArrayList<>();
    events.add(refEnd);
    CTPException actualException = null;
    try {
      referencePeriodValidator.validate(events, refStart, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Reference period end date must be after start date"", actualException.getMessage());
  }
",non-flaky,5
98409,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.isEventValidator,"  @Test
  public void isEventValidator() {
    assertThat(nudgeEmailValidator, instanceOf(EventValidator.class));
  }
",non-flaky,5
98410,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.returnTrueAndDoNothingIfNotNudge,"  @Test
  public void returnTrueAndDoNothingIfNotNudge() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((EventService.Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now()));
    final List<Event> events = new ArrayList<>();
    nudgeEmailValidator.validate(
        events, mpsEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);

    verify(eventDateOrderChecker, never()).isEventDatesInOrder(anyList());
  }
",non-flaky,5
98411,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testCanUpdateNudgeWhenReadyForLive,"  @Test
  public void testCanUpdateNudgeWhenReadyForLive() throws CTPException {
    final Event nudgeEvent = new Event();
    nudgeEvent.setTag(EventService.Tag.nudge_email_0.toString());
    nudgeEvent.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();
    nudgeEmailValidator.validate(
        events, nudgeEvent, CollectionExerciseDTO.CollectionExerciseState.READY_FOR_LIVE);
  }
",non-flaky,5
98412,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testCanUpdateNudgeWhenLive,"  @Test
  public void testCanUpdateNudgeWhenLive() throws CTPException {
    final Event nudgeEvent = new Event();
    nudgeEvent.setTag(EventService.Tag.nudge_email_0.toString());
    nudgeEvent.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();

    nudgeEmailValidator.validate(
        events, nudgeEvent, CollectionExerciseDTO.CollectionExerciseState.LIVE);
  }
",non-flaky,5
98413,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testCantUpdateNudgeThatHasPastAndCollectionExerciseInLockedState,"  @Test
  public void testCantUpdateNudgeThatHasPastAndCollectionExerciseInLockedState() {
    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(Timestamp.from(Instant.now().minus(2, ChronoUnit.DAYS)));

    final Event newNudge = new Event();
    newNudge.setTag((EventService.Tag.nudge_email_0.toString()));
    newNudge.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(nudge);
    CTPException actualException = null;
    try {
      nudgeEmailValidator.validate(
          events, newNudge, CollectionExerciseDTO.CollectionExerciseState.LIVE);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(""Nudge email cannot be set in the past"", actualException.getMessage());
  }
",non-flaky,5
98414,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testCanUpdateNudgeThatHasPastAndCollectionExerciseNotInLockedState,"  @Test
  public void testCanUpdateNudgeThatHasPastAndCollectionExerciseNotInLockedState()
      throws CTPException {
    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(Timestamp.from(Instant.now().minus(2, ChronoUnit.DAYS)));

    final Event newNudge = new Event();
    newNudge.setTag((EventService.Tag.nudge_email_0.toString()));
    newNudge.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(nudge);

    nudgeEmailValidator.validate(
        events, newNudge, CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
  }
",non-flaky,5
98415,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testValidNudgeEventCreation,"  @Test
  public void testValidNudgeEventCreation() throws CTPException {
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now()));

    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, returnBy);

    nudgeEmailValidator.validate(
        events, nudge, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98416,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testNudgeAfterReturnByEndInvalid,"  @Test
  public void testNudgeAfterReturnByEndInvalid() {
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now()));

    final Event nudgeEvent = new Event();
    nudgeEvent.setTag(EventService.Tag.nudge_email_0.toString());
    nudgeEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(Instant.now().plus(3, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, returnBy);
    CTPException actualException = null;

    SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss.SSS"");
    sdf.setTimeZone(TimeZone.getTimeZone(""Europe/London""));
    Date goLiveDate = new Date(goLive.getTimestamp().getTime());
    Date returnByDate = new Date(returnBy.getTimestamp().getTime());

    try {
      nudgeEmailValidator.validate(
          events, nudgeEvent, CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    String expectedMessage =
        ""Nudge email must be set after the Go Live date (""
            + sdf.format(goLiveDate)
            + "") ""
            + ""and before Return by date (""
            + sdf.format(returnByDate)
            + "")"";
    assertEquals(expectedMessage, actualException.getMessage());
  }
",non-flaky,5
98417,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testNudgeBeforeGoliveInvalid,"  @Test
  public void testNudgeBeforeGoliveInvalid() {
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(Instant.now().plus(3, ChronoUnit.DAYS)));
    final List<Event> events = Arrays.asList(goLive, returnBy);

    final Event nudgeEvent = new Event();
    nudgeEvent.setTag(EventService.Tag.nudge_email_0.toString());
    nudgeEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));

    SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss.SSS"");
    sdf.setTimeZone(TimeZone.getTimeZone(""Europe/London""));
    Date goLiveDate = new Date(goLive.getTimestamp().getTime());
    Date returnByDate = new Date(returnBy.getTimestamp().getTime());

    CTPException actualException = null;
    try {
      nudgeEmailValidator.validate(
          events, nudgeEvent, CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    String expectedMessage =
        ""Nudge email must be set after the Go Live date (""
            + sdf.format(goLiveDate)
            + "") ""
            + ""and before Return by date (""
            + sdf.format(returnByDate)
            + "")"";
    assertEquals(expectedMessage, actualException.getMessage());
  }
",non-flaky,5
98418,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testNudgeWithSameDateAndTimeEventCreation,"  @Test
  public void testNudgeWithSameDateAndTimeEventCreation() {
    final Instant now = Instant.now();
    final Long nudgeTime = now.plus(2, ChronoUnit.DAYS).toEpochMilli();
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(now));

    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(new Timestamp(nudgeTime));

    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(now.plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, returnBy, nudge);

    final Event submittedEvent = new Event();
    submittedEvent.setTag((EventService.Tag.nudge_email_1.toString()));
    submittedEvent.setTimestamp(new Timestamp(nudgeTime));

    CTPException actualException = null;
    try {
      nudgeEmailValidator.validate(
          events, submittedEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""A nudge email has already been scheduled for this date and time. Choose a different date or time."",
        actualException.getMessage());
  }
",non-flaky,5
98419,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testNudgeWithSameDateAndTimeEventCreationNotValidForTheSameEvent,"  @Test
  public void testNudgeWithSameDateAndTimeEventCreationNotValidForTheSameEvent() {
    final Instant now = Instant.now();
    final Long nudgeTime = now.plus(2, ChronoUnit.DAYS).toEpochMilli();
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(now));

    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(new Timestamp(nudgeTime));

    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(now.plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, returnBy, nudge);

    final Event submittedEvent = new Event();
    submittedEvent.setTag((EventService.Tag.nudge_email_0.toString()));
    submittedEvent.setTimestamp(new Timestamp(nudgeTime));

    CTPException actualException = null;
    try {
      nudgeEmailValidator.validate(
          events, submittedEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNull(actualException);
  }
",non-flaky,5
98420,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testCreateCollectionExercise,"  @Test
  public void testCreateCollectionExercise() throws Exception {
    // Given
    CollectionExercise collectionExercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    when(collexRepo.saveAndFlush(any())).thenReturn(collectionExercise);

    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    CollectionExerciseDTO toCreate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);

    // When
    this.collectionExerciseService.createCollectionExercise(toCreate, survey);

    // Then
    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());
    CollectionExercise collex = captor.getValue();
    assertEquals(toCreate.getUserDescription(), collex.getUserDescription());
    assertEquals(toCreate.getExerciseRef(), collex.getExerciseRef());
    assertEquals(toCreate.getSurveyId(), collex.getSurveyId().toString());
    assertNotNull(collex.getCreated());
  }
",non-flaky,5
98421,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUpdateCollectionExercise,"  @Test
  public void testUpdateCollectionExercise() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    UUID surveyId = UUID.fromString(survey.getId());
    existing.setSurveyId(surveyId);
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);
    when(surveyService.findSurvey(surveyId)).thenReturn(survey);

    this.collectionExerciseService.updateCollectionExercise(existing.getId(), toUpdate);

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);

    verify(collexRepo).saveAndFlush(captor.capture());
    CollectionExercise collex = captor.getValue();
    assertEquals(UUID.fromString(toUpdate.getSurveyId()), collex.getSurveyId());
    assertEquals(toUpdate.getExerciseRef(), collex.getExerciseRef());
    assertEquals(toUpdate.getUserDescription(), collex.getUserDescription());
    assertNotNull(collex.getUpdated());
  }
",non-flaky,5
98422,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUpdateCollectionExerciseInvalidSurvey,"  @Test
  public void testUpdateCollectionExerciseInvalidSurvey() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    existing.setSurveyId(UUID.randomUUID());
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    try {
      this.collectionExerciseService.updateCollectionExercise(existing.getId(), toUpdate);
      fail(""Update collection exercise with null survey succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.BAD_REQUEST, e.getFault());
    }
  }
",non-flaky,5
98423,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUpdateCollectionExerciseNonUnique,"  @Test
  public void testUpdateCollectionExerciseNonUnique() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    existing.setSurveyId(UUID.randomUUID());
    // Set up the mock to return the one we are attempting to update
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    UUID uuid = UUID.fromString(""0f66744b-bfdb-458a-b495-1eb605462003"");
    CollectionExercise otherExisting = new CollectionExercise();
    otherExisting.setId(uuid);
    // Set up the mock to return a different one with the same exercise ref and survey id
    when(collexRepo.findByExerciseRefAndSurveyId(
            toUpdate.getExerciseRef(), UUID.fromString(toUpdate.getSurveyId())))
        .thenReturn(Collections.singletonList(otherExisting));

    try {
      this.collectionExerciseService.updateCollectionExercise(existing.getId(), toUpdate);

      fail(""Update to collection exercise breaking uniqueness constraint succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.RESOURCE_VERSION_CONFLICT, e.getFault());
    }
  }
",non-flaky,5
98424,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUpdateCollectionExerciseDoesNotExist,"  @Test
  public void testUpdateCollectionExerciseDoesNotExist() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    UUID updateUuid = UUID.randomUUID();

    try {
      this.collectionExerciseService.updateCollectionExercise(updateUuid, toUpdate);
      fail(""Update of non-existent collection exercise succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.RESOURCE_NOT_FOUND, e.getFault());
    }
  }
",non-flaky,5
98425,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testDeleteCollectionExercise,"  @Test
  public void testDeleteCollectionExercise() throws Exception {
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    this.collectionExerciseService.deleteCollectionExercise(existing.getId());

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    assertEquals(true, captor.getValue().getDeleted());
  }
",non-flaky,5
98426,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUndeleteCollectionExercise,"  @Test
  public void testUndeleteCollectionExercise() throws Exception {
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    this.collectionExerciseService.undeleteCollectionExercise(existing.getId());

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    assertEquals(false, captor.getValue().getDeleted());
  }
",non-flaky,5
98427,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseNotExists,"  @Test
  public void testPatchCollectionExerciseNotExists() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    UUID updateUuid = UUID.randomUUID();

    try {
      this.collectionExerciseService.patchCollectionExercise(updateUuid, toUpdate);

      fail(""Attempt to patch non-existent collection exercise succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.RESOURCE_NOT_FOUND, e.getFault());
    }
  }
",non-flaky,5
98428,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseExerciseRef,"  @Test
  public void testPatchCollectionExerciseExerciseRef() throws Exception {
    CollectionExercise existing = setupCollectionExercise();
    CollectionExerciseDTO collex = new CollectionExerciseDTO();
    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    UUID surveyId = UUID.fromString(survey.getId());
    String exerciseRef = ""209966"";
    collex.setExerciseRef(exerciseRef);
    collex.setSurveyId(surveyId.toString());
    when(surveyService.findSurvey(surveyId)).thenReturn(survey);
    this.collectionExerciseService.patchCollectionExercise(existing.getId(), collex);

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    CollectionExercise ce = captor.getValue();
    assertEquals(exerciseRef, ce.getExerciseRef());
    assertNotNull(ce.getUpdated());
  }
",non-flaky,5
98429,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseName,"  @Test
  public void testPatchCollectionExerciseName() throws Exception {
    CollectionExercise existing = setupCollectionExercise();
    CollectionExerciseDTO collex = new CollectionExerciseDTO();
    String name = ""Not BRES"";
    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    when(surveyService.findSurvey(any())).thenReturn(survey);
    this.collectionExerciseService.patchCollectionExercise(existing.getId(), collex);

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    CollectionExercise ce = captor.getValue();
    assertNotNull(ce.getUpdated());
  }
",non-flaky,5
98430,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseUserDescription,"  @Test
  public void testPatchCollectionExerciseUserDescription() throws Exception {
    CollectionExercise existing = setupCollectionExercise();
    CollectionExerciseDTO collex = new CollectionExerciseDTO();
    String userDescription = ""Really odd description"";
    collex.setUserDescription(userDescription);
    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    when(surveyService.findSurvey(any())).thenReturn(survey);
    this.collectionExerciseService.patchCollectionExercise(existing.getId(), collex);

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    CollectionExercise ce = captor.getValue();
    assertEquals(userDescription, ce.getUserDescription());
    assertNotNull(ce.getUpdated());
  }
",non-flaky,5
98431,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseNonUnique,"  @Test
  public void testPatchCollectionExerciseNonUnique() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    existing.setSurveyId(UUID.randomUUID());
    // Set up the mock to return the one we are attempting to update
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    UUID uuid = UUID.fromString(""0f66744b-bfdb-458a-b495-1eb605462003"");
    CollectionExercise otherExisting = new CollectionExercise();
    otherExisting.setId(uuid);
    // Set up the mock to return a different one with the same exercise ref and survey id
    when(collexRepo.findByExerciseRefAndSurveyId(
            toUpdate.getExerciseRef(), UUID.fromString(toUpdate.getSurveyId())))
        .thenReturn(Collections.singletonList(otherExisting));

    try {
      this.collectionExerciseService.patchCollectionExercise(existing.getId(), toUpdate);

      fail(""Update to collection exercise breaking uniqueness constraint succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.RESOURCE_VERSION_CONFLICT, e.getFault());
    }
  }
",non-flaky,5
98432,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testTransitionToReadyToReviewWhenScheduledWithCIsAndSample,"  @Test
  public void testTransitionToReadyToReviewWhenScheduledWithCIsAndSample() throws Exception {
    // Given
    CollectionExercise exercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    SampleLink testSampleLink = new SampleLink();
    testSampleLink.setSampleSummaryId(UUID.randomUUID());
    given(sampleLinkRepository.findByCollectionExerciseId(exercise.getId()))
        .willReturn(Collections.singletonList(testSampleLink));

    SampleSummaryDTO sampleSummary = new SampleSummaryDTO();
    sampleSummary.setState(SampleSummaryDTO.SampleState.ACTIVE);
    given(sampleSvcClient.getSampleSummary(testSampleLink.getSampleSummaryId()))
        .willReturn(sampleSummary);

    String searchStringJson =
        new JSONObject(Collections.singletonMap(""COLLECTION_EXERCISE"", exercise.getId().toString()))
            .toString();
    given(collectionInstrument.countCollectionInstruments(searchStringJson)).willReturn(1);

    // When
    collectionExerciseService.transitionScheduleCollectionExerciseToReadyToReview(exercise);

    // Then
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.READY_FOR_REVIEW);
    verify(collexRepo).saveAndFlush(exercise);
  }
",non-flaky,5
98433,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testDoNotTransitionToReadyToReviewWhenScheduledWithCIsAndNoSample,"  @Test
  public void testDoNotTransitionToReadyToReviewWhenScheduledWithCIsAndNoSample() throws Exception {
    // Given
    CollectionExercise exercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    given(sampleLinkRepository.findByCollectionExerciseId(exercise.getId()))
        .willReturn(Collections.emptyList());
    String searchStringJson =
        new JSONObject(Collections.singletonMap(""COLLECTION_EXERCISE"", exercise.getId().toString()))
            .toString();
    given(collectionInstrument.countCollectionInstruments(searchStringJson)).willReturn(1);

    // When
    collectionExerciseService.transitionScheduleCollectionExerciseToReadyToReview(exercise);

    // Then
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.READY_FOR_REVIEW);
    verify(collexRepo, times(0)).saveAndFlush(exercise);
  }
",non-flaky,5
98434,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testDoNotTransitionToReadyToReviewWhenScheduledWithNoCIsAndSample,"  @Test
  public void testDoNotTransitionToReadyToReviewWhenScheduledWithNoCIsAndSample() throws Exception {
    // Given
    CollectionExercise exercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    given(sampleLinkRepository.findByCollectionExerciseId(exercise.getId()))
        .willReturn(Collections.emptyList());
    String searchStringJson =
        new JSONObject(Collections.singletonMap(""COLLECTION_EXERCISE"", exercise.getId().toString()))
            .toString();
    given(collectionInstrument.countCollectionInstruments(searchStringJson)).willReturn(0);

    // When
    collectionExerciseService.transitionScheduleCollectionExerciseToReadyToReview(exercise);

    // Then
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.READY_FOR_REVIEW);
    verify(collexRepo, times(0)).saveAndFlush(exercise);
  }
",non-flaky,5
98435,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testDoNotTransitionToReadyToReviewWhenCIsCountFailsAndReturnsNull,"  @Test
  public void testDoNotTransitionToReadyToReviewWhenCIsCountFailsAndReturnsNull() throws Exception {
    // Given
    CollectionExercise exercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    given(sampleLinkRepository.findByCollectionExerciseId(exercise.getId()))
        .willReturn(Collections.emptyList());
    String searchStringJson =
        new JSONObject(Collections.singletonMap(""COLLECTION_EXERCISE"", exercise.getId().toString()))
            .toString();
    given(collectionInstrument.countCollectionInstruments(searchStringJson)).willReturn(null);

    // When
    collectionExerciseService.transitionScheduleCollectionExerciseToReadyToReview(exercise);

    // Then
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.READY_FOR_REVIEW);
    verify(collexRepo, times(0)).saveAndFlush(exercise);
  }
",non-flaky,5
98436,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testCreateLink,"  @Test
  public void testCreateLink() {
    UUID sampleSummaryUuid = UUID.randomUUID(), collexUuid = UUID.randomUUID();

    when(this.sampleLinkRepository.saveAndFlush(any(SampleLink.class))).then(returnsFirstArg());

    SampleLink sampleLink =
        this.collectionExerciseService.createLink(sampleSummaryUuid, collexUuid);

    assertEquals(sampleSummaryUuid, sampleLink.getSampleSummaryId());
    assertEquals(collexUuid, sampleLink.getCollectionExerciseId());

    verify(sampleLinkRepository, times(1)).saveAndFlush(any());
  }
",non-flaky,5
98437,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testCreateLinkShouldAttemptToTransitionToReadyToReview,"  @Test
  public void testCreateLinkShouldAttemptToTransitionToReadyToReview() throws CTPException {
    // Given
    UUID sampleSummaryUuid = UUID.randomUUID();
    UUID collexUuid = UUID.randomUUID();
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setId(collexUuid);
    collectionExercise.setState(CollectionExerciseDTO.CollectionExerciseState.CREATED);
    given(collexRepo.findOneById(collexUuid)).willReturn(collectionExercise);
    given(collectionInstrument.countCollectionInstruments(any())).willReturn(1);
    SampleSummaryDTO sampleSummary = new SampleSummaryDTO();
    SampleLink sampleLink = new SampleLink();
    sampleLink.setSampleSummaryId(sampleSummaryUuid);
    given(sampleLinkRepository.findByCollectionExerciseId(collexUuid))
        .willReturn(Collections.singletonList(sampleLink));
    sampleSummary.setState(SampleSummaryDTO.SampleState.ACTIVE);
    given(sampleSvcClient.getSampleSummary(sampleSummaryUuid)).willReturn(sampleSummary);

    // When
    this.collectionExerciseService.linkSampleSummaryToCollectionExercise(
        collexUuid, Collections.singletonList(sampleSummaryUuid));

    // Then
    verify(stateManager)
        .transition(
            CollectionExerciseDTO.CollectionExerciseState.CREATED,
            CollectionExerciseDTO.CollectionExerciseEvent.CI_SAMPLE_ADDED);
  }
",non-flaky,5
98438,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testFindCollectionExercisesForSurveys,"  @Test
  public void testFindCollectionExercisesForSurveys() throws Exception {
    final UUID SURVEY_ID_1 = UUID.fromString(""31ec898e-f370-429a-bca4-eab1045aff4e"");

    List<UUID> surveys = Arrays.asList(SURVEY_ID_1);

    List<CollectionExercise> existing = FixtureHelper.loadClassFixtures(CollectionExercise[].class);

    given(collexRepo.findBySurveyIdInOrderBySurveyId(surveys)).willReturn(existing);

    HashMap<UUID, List<CollectionExercise>> result =
        this.collectionExerciseService.findCollectionExercisesForSurveys(surveys);

    assertEquals(result.get(SURVEY_ID_1).size(), 2);
  }
",non-flaky,5
98439,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testFindCollectionExercisesForSurveysByState,"  @Test
  public void testFindCollectionExercisesForSurveysByState() throws Exception {
    final UUID SURVEY_ID_1 = UUID.fromString(""31ec898e-f370-429a-bca4-eab1045aff4e"");

    List<UUID> surveys = Arrays.asList(SURVEY_ID_1);

    List<CollectionExercise> existing = FixtureHelper.loadClassFixtures(CollectionExercise[].class);

    given(
            collexRepo.findBySurveyIdInAndStateOrderBySurveyId(
                surveys, CollectionExerciseDTO.CollectionExerciseState.LIVE))
        .willReturn(existing);

    HashMap<UUID, List<CollectionExercise>> result =
        this.collectionExerciseService.findCollectionExercisesForSurveysByState(
            surveys, CollectionExerciseDTO.CollectionExerciseState.LIVE);

    assertEquals(result.get(SURVEY_ID_1).size(), 2);
  }
",non-flaky,5
98440,ONSdigital_rm-collection-exercise-service,SampleServiceTest.testAcceptSampleUnitAlreadyExists,"  @Test
  public void testAcceptSampleUnitAlreadyExists() throws CTPException {
    CollectionExercise collex = new CollectionExercise();
    collex.setId(COLLEX_ID);
    collex.setSampleSize(99);
    collex.setState(CollectionExerciseState.EXECUTION_STARTED);

    SampleUnit sampleUnit = new SampleUnit();
    sampleUnit.setCollectionExerciseId(COLLEX_ID.toString());
    sampleUnit.setFormType(""X"");
    sampleUnit.setId(SAMPLE_ID.toString());
    sampleUnit.setSampleUnitType(""B"");
    sampleUnit.setSampleUnitRef(""REF123"");
    when(collectRepo.findOneById(any())).thenReturn(collex);
    when(sampleUnitRepo.existsBySampleUnitRefAndSampleUnitTypeAndSampleUnitGroupCollectionExercise(
            any(), any(), any()))
        .thenReturn(true);

    underTest.acceptSampleUnit(sampleUnit);

    verify(collectionExerciseTransitionState, never()).transition(any(), any());
    verify(sampleUnitGroupRepo, never()).saveAndFlush(any());
    verify(sampleUnitRepo, never()).saveAndFlush(any());
    verify(collectRepo, never()).saveAndFlush(any());
  }
",non-flaky,5
98441,ONSdigital_rm-collection-exercise-service,SampleServiceTest.requestSampleUnitsHappyPath,"  @Test
  public void requestSampleUnitsHappyPath() throws CTPException {
    UUID collexId = UUID.randomUUID();
    UUID sampleSummaryId = UUID.randomUUID();
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setId(collexId);
    SampleLink sampleLink = new SampleLink();
    sampleLink.setSampleSummaryId(sampleSummaryId);
    List<SampleLink> sampleLinks = Collections.singletonList(sampleLink);
    SampleUnitsRequestDTO sampleUnitsRequestDTO = new SampleUnitsRequestDTO();
    sampleUnitsRequestDTO.setSampleUnitsTotal(666);

    // Given
    when(collectRepo.findOneById(eq(collexId))).thenReturn(collectionExercise);
    when(sampleLinkRepo.findByCollectionExerciseId(any())).thenReturn(sampleLinks);
    when(sampleSvcClient.getSampleUnitCount(any())).thenReturn(sampleUnitsRequestDTO);
    when(sampleSvcClient.requestSampleUnits(any())).thenReturn(sampleUnitsRequestDTO);

    // When
    underTest.requestSampleUnits(collexId);

    // Then
    verify(collexSampleUnitReceiptPreparer).prepareCollexToAcceptSampleUnits(eq(collexId), eq(666));
    verify(partySvcClient).linkSampleSummaryId(any(), any());
  }
",non-flaky,5
98442,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExcerciseDoesNotExistWhenEventIsCreatedThenExceptionIsThrown,"  @Test
  public void givenCollectionExcerciseDoesNotExistWhenEventIsCreatedThenExceptionIsThrown() {
    EventDTO eventDto = new EventDTO();
    UUID collexUuid = UUID.randomUUID();
    eventDto.setCollectionExerciseId(collexUuid);

    try {
      eventService.createEvent(eventDto);

      fail(""Created event with non-existent collection exercise"");
    } catch (CTPException e) {
      // Expected 404
      assertThat(e.getFault(), is(Fault.RESOURCE_NOT_FOUND));
    }
  }
",non-flaky,5
98443,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventAlreadyExistsWhenEventIsCreatedThenExceptionIsThrown,"  @Test
  public void givenEventAlreadyExistsWhenEventIsCreatedThenExceptionIsThrown() throws CTPException {
    String tag = Tag.mps.name();
    EventDTO eventDto = new EventDTO();
    CollectionExercise collex = new CollectionExercise();
    UUID collexUuid = UUID.randomUUID();
    eventDto.setCollectionExerciseId(collexUuid);
    eventDto.setTag(tag);
    collex.setId(collexUuid);

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(collex);
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, tag)).thenReturn(new Event());

    try {
      eventService.createEvent(eventDto);

      fail(""Created event with non-existent collection exercise"");
    } catch (CTPException e) {
      // Expected 409
      assertThat(e.getFault(), is(Fault.RESOURCE_VERSION_CONFLICT));
    }
  }
",non-flaky,5
98444,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenNoEventsWhenScheduledIsCheckedThenFalse,"  @Test
  public void givenNoEventsWhenScheduledIsCheckedThenFalse() throws CTPException {
    UUID collexUuid = UUID.randomUUID();
    when(eventRepository.findByCollectionExerciseId(collexUuid)).thenReturn(new ArrayList<>());

    boolean scheduled = this.eventService.isScheduled(collexUuid);

    assertFalse(scheduled);
  }
",non-flaky,5
98445,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExcerciseDoesNotExistWhenEventIsUpdatedThenExceptionIsThrown,"  @Test
  public void givenCollectionExcerciseDoesNotExistWhenEventIsUpdatedThenExceptionIsThrown() {
    final UUID collexUuid = UUID.randomUUID();

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(null);

    try {
      eventService.updateEvent(collexUuid, Tag.mps.name(), new Date());

      Assert.fail(""Updated event with non-existent collection exercise"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.BAD_REQUEST));
    }
  }
",non-flaky,5
98446,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExcerciseDoesNotExistWhenEventIsDeletedThenExceptionIsThrown,"  @Test
  public void givenCollectionExcerciseDoesNotExistWhenEventIsDeletedThenExceptionIsThrown() {
    final UUID collexUuid = UUID.randomUUID();

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(null);

    try {
      eventService.deleteEvent(collexUuid, Tag.mps.name());

      Assert.fail(""Deleted event with non-existent collection exercise"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.BAD_REQUEST));
    }
  }
",non-flaky,5
98447,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventDoesNotExistWhenEventIsUpdatedThenExceptionIsThrown,"  @Test
  public void givenEventDoesNotExistWhenEventIsUpdatedThenExceptionIsThrown() {
    final UUID collexUuid = UUID.randomUUID();

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(collexUuid);

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(collex);
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.mps.name()))
        .thenReturn(null);

    try {
      eventService.updateEvent(collexUuid, Tag.mps.name(), new Date());

      Assert.fail(""Updated non-existent event"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.RESOURCE_NOT_FOUND));
    }
  }
",non-flaky,5
98448,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventDoesNotExistWhenEventIsDeletedThenExceptionIsThrown,"  @Test
  public void givenEventDoesNotExistWhenEventIsDeletedThenExceptionIsThrown() {
    final UUID collexUuid = UUID.randomUUID();

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(collexUuid);

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(collex);
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.mps.name()))
        .thenReturn(null);

    try {
      eventService.deleteEvent(collexUuid, Tag.mps.name());

      Assert.fail(""Deleted non-existent event"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.RESOURCE_NOT_FOUND));
    }
  }
",non-flaky,5
98449,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventsForCollectionExerciseValidateWhenEventIsUpdatedItIsSaved,"  @Test
  public void givenEventsForCollectionExerciseValidateWhenEventIsUpdatedItIsSaved()
      throws CTPException {

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(COLLEX_UUID);
    collex.setExercisePK(EXERCISE_PK);
    final CollectionExerciseState collectionExerciseState = CollectionExerciseState.SCHEDULED;
    collex.setState(collectionExerciseState);

    when(collectionExerciseService.findCollectionExercise(COLLEX_UUID)).thenReturn(collex);
    final Event existingEvent = new Event();
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.mps.name()))
        .thenReturn(existingEvent);

    final List<Event> existingEvents = new ArrayList<>();

    when(eventRepository.findByCollectionExercise(collex)).thenReturn(existingEvents);
    eventValidators.add(eventValidator);

    eventService.updateEvent(COLLEX_UUID, Tag.mps.name(), new Date());

    verify(eventRepository, atLeastOnce()).save(eq(existingEvent));
  }
",non-flaky,5
98450,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventsForCollectionExerciseValidateWhenEventIsDeletedItIsSaved,"  @Test
  public void givenEventsForCollectionExerciseValidateWhenEventIsDeletedItIsSaved()
      throws CTPException {

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(COLLEX_UUID);
    collex.setExercisePK(EXERCISE_PK);
    final CollectionExerciseState collectionExerciseState = CollectionExerciseState.SCHEDULED;
    collex.setState(collectionExerciseState);

    when(collectionExerciseService.findCollectionExercise(COLLEX_UUID)).thenReturn(collex);
    final Event existingEvent = new Event();
    existingEvent.setTag(Tag.nudge_email_4.toString());
    existingEvent.setId(UUID.randomUUID());
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.nudge_email_4.name()))
        .thenReturn(existingEvent);

    final List<Event> existingEvents = new ArrayList<>();

    eventValidators.add(eventValidator);

    eventService.deleteEvent(COLLEX_UUID, Tag.nudge_email_4.name());

    verify(eventRepository, atLeastOnce()).delete(eq(existingEvent));
  }
",non-flaky,5
98451,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenReminderEmailIsDeletedItGetsPropagatedToActionSVC,"  @Test
  public void givenReminderEmailIsDeletedItGetsPropagatedToActionSVC() throws CTPException {

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(COLLEX_UUID);
    collex.setExercisePK(EXERCISE_PK);
    final CollectionExerciseState collectionExerciseState = CollectionExerciseState.SCHEDULED;
    collex.setState(collectionExerciseState);

    when(collectionExerciseService.findCollectionExercise(COLLEX_UUID)).thenReturn(collex);
    final Event existingEvent = new Event();
    existingEvent.setTag(Tag.reminder.toString());
    existingEvent.setId(UUID.randomUUID());
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.reminder.name()))
        .thenReturn(existingEvent);

    final List<Event> existingEvents = new ArrayList<>();

    eventValidators.add(eventValidator);

    eventService.deleteEvent(COLLEX_UUID, Tag.reminder.name());

    verify(eventRepository, atLeastOnce()).delete(eq(existingEvent));
  }
",non-flaky,5
98452,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenSomeEventsWhenScheduledIsCheckedThenFalse,"  @Test
  public void givenSomeEventsWhenScheduledIsCheckedThenFalse() throws CTPException {
    UUID collexUuid = UUID.randomUUID();
    List<Event> events = createEventList(Tag.mps, Tag.exercise_end);
    when(eventRepository.findByCollectionExerciseId(collexUuid)).thenReturn(events);

    boolean scheduled = this.eventService.isScheduled(collexUuid);

    assertFalse(scheduled);
  }
",non-flaky,5
98453,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenAllEventsWhenScheduledIsCheckedThenTrue,"  @Test
  public void givenAllEventsWhenScheduledIsCheckedThenTrue() throws CTPException {
    UUID collexUuid = UUID.randomUUID();
    List<Event> events = createEventList(Tag.values());
    when(eventRepository.findByCollectionExerciseId(collexUuid)).thenReturn(events);

    boolean scheduled = this.eventService.isScheduled(collexUuid);

    assertTrue(scheduled);
  }
",non-flaky,5
98454,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExerciseDoesNotExistWhenCreatingAnExceptionThrowError,"  @Test
  public void givenCollectionExerciseDoesNotExistWhenCreatingAnExceptionThrowError() {
    final String tag = Tag.mps.name();
    final EventDTO eventDto = new EventDTO();
    final UUID collexUuid = UUID.randomUUID();
    eventDto.setCollectionExerciseId(collexUuid);
    eventDto.setTag(tag);
    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(null);
    try {
      eventService.createEvent(eventDto);
      fail(""Created event with non-existent collection exercise"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.RESOURCE_NOT_FOUND));
    }
  }
",non-flaky,5
98455,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExerciseEventsAreInInvalidStateThrowException,"  @Test
  public void givenCollectionExerciseEventsAreInInvalidStateThrowException() {
    final String tag = Tag.mps.name();
    final EventDTO eventDto = new EventDTO();
    final CollectionExercise collex = new CollectionExercise();
    final UUID collexUuid = UUID.randomUUID();
    eventDto.setCollectionExerciseId(collexUuid);
    eventDto.setTag(tag);
    eventDto.setTimestamp(new Timestamp(Instant.now().toEpochMilli()));
    collex.setId(collexUuid);
    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(collex);
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.mps.name()))
        .thenReturn(null);
    final List<Event> existingEvents = new ArrayList<>();
    final Event event = new Event();
    existingEvents.add(event);
    when(eventRepository.findByCollectionExercise(collex)).thenReturn(existingEvents);
    eventValidators.add(eventValidator);
    try {
      eventService.createEvent(eventDto);
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.BAD_REQUEST));
    }
  }
",non-flaky,5
98456,ONSdigital_rm-collection-exercise-service,EventServiceTest.testStatusIsSetToScheduledNewEventCreated,"  @Test
  public void testStatusIsSetToScheduledNewEventCreated() {
    final CollectionExercise collex = new CollectionExercise();
    String tag = Tag.mps.name();

    when(collectionExerciseService.findCollectionExercise(COLLEX_UUID)).thenReturn(collex);
    when(eventRepository.save(any(Event.class))).then(returnsFirstArg());

    EventDTO eventDto = new EventDTO();
    eventDto.setCollectionExerciseId(COLLEX_UUID);
    eventDto.setTag(tag);
    eventDto.setTimestamp(new Timestamp(new Date().getTime()));

    try {
      Event event = eventService.createEvent(eventDto);
      assertThat(event.getStatus(), is(EventDTO.Status.SCHEDULED));
    } catch (CTPException e) {
      fail();
    }
  }
",non-flaky,5
98457,ONSdigital_rm-collection-exercise-service,EventServiceTest.testProcessEventsNoScheduledEvents,"  @Test
  public void testProcessEventsNoScheduledEvents() {
    // Given
    List<Event> emptyList = Collections.emptyList();
    when(eventRepository.findByStatus(EventDTO.Status.SCHEDULED)).thenReturn(emptyList);

    // When
    eventService.processEvents();

    // Then
    verify(eventRepository, atMost(1)).findByStatus(EventDTO.Status.SCHEDULED);
    verify(actionSvcClient, never()).processEvent(any(), any());
  }
",non-flaky,5
98458,ONSdigital_rm-collection-exercise-service,EventServiceTest.testProcessEventsOnlyEventInFuture,"  @Test
  public void testProcessEventsOnlyEventInFuture() {
    // Given
    List<Event> list = new ArrayList<>();
    Event event = createEvent(Tag.mps, ""31/12/2999"");
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setState(CollectionExerciseState.LIVE);
    event.setCollectionExercise(collectionExercise);
    list.add(event);

    when(eventRepository.findByStatus(EventDTO.Status.SCHEDULED)).thenReturn(list);

    // When
    eventService.processEvents();

    // Then
    verify(eventRepository, atMost(1)).findByStatus(EventDTO.Status.SCHEDULED);
    verify(actionSvcClient, never()).processEvent(any(), any());
  }
",non-flaky,5
98459,ONSdigital_rm-collection-exercise-service,EventServiceTest.testProcessEventsTransitionGoLive,"  @Test
  public void testProcessEventsTransitionGoLive() {
    // Given
    List<Event> list = new ArrayList<>();
    Event event = createEvent(Tag.go_live);
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setState(CollectionExerciseState.LIVE);
    event.setCollectionExercise(collectionExercise);
    list.add(event);

    when(eventRepository.findByStatus(EventDTO.Status.SCHEDULED)).thenReturn(list);

    // When
    eventService.processEvents();

    // Then
    verify(eventRepository, atMost(1)).findByStatus(EventDTO.Status.SCHEDULED);
    verify(actionSvcClient, atMost(1)).processEvent(any(), any());
    try {
      verify(collectionExerciseService, atMost(1))
          .transitionCollectionExercise(
              any(CollectionExercise.class),
              any(CollectionExerciseDTO.CollectionExerciseEvent.class));
    } catch (CTPException e) {
      fail();
    }
  }
",non-flaky,5
98460,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveMpsAsIsActionable,"  @Test
  public void testTagShouldHaveMpsAsIsActionable() {
    assertThat(Tag.mps.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98461,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveGoLiveAsIsAnActionableTag,"  @Test
  public void testTagShouldHaveGoLiveAsIsAnActionableTag() {
    assertThat(Tag.go_live.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98462,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveExerciseEndAsNotIsAnActionableTag,"  @Test
  public void testTagShouldHaveExerciseEndAsNotIsAnActionableTag() {
    assertThat(Tag.exercise_end.isActionable(), Matchers.is(false));
  }
",non-flaky,5
98463,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveReminderAsAnActionableTag,"  @Test
  public void testTagShouldHaveReminderAsAnActionableTag() {
    assertThat(Tag.reminder.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98464,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveReminder2AsAnActionableTag,"  @Test
  public void testTagShouldHaveReminder2AsAnActionableTag() {
    assertThat(Tag.reminder2.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98465,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveReminder3AsAnActionableTag,"  @Test
  public void testTagShouldHaveReminder3AsAnActionableTag() {
    assertThat(Tag.reminder3.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98466,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveNudge0AsAnActionableTag,"  @Test
  public void testTagShouldHaveNudge0AsAnActionableTag() {
    assertThat(Tag.nudge_email_0.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98467,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveNudge1AsAnActionableTag,"  @Test
  public void testTagShouldHaveNudge1AsAnActionableTag() {
    assertThat(Tag.nudge_email_1.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98468,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveNudge2AsAnActionableTag,"  @Test
  public void testTagShouldHaveNudge2AsAnActionableTag() {
    assertThat(Tag.nudge_email_2.isActionable(), Matchers.is(true));
  }
",non-flaky,5
144,spring-cloud_spring-cloud-config,GiteePropertyPathNotificationExtractorTests.giteeSample,"@Test
public void giteeSample() throws Exception {
    Map<String, Object> value = new ObjectMapper().readValue(new ClassPathResource(""pathsamples/gitee.json"").getInputStream(), new TypeReference<Map<String, Object>>() {});
    this.headers.set(""x-git-oschina-event"", ""Push Hook"");
    PropertyPathNotification extracted = this.extractor.extract(this.headers, value);
    assertThat(extracted).isNotNull();
    assertThat(extracted.getPaths()[0]).isEqualTo(""d.txt"");
}",unordered collections,3
104091,spring-cloud_spring-cloud-config,ServerNativeApplicationTests.contextLoads,"	@Test
	public void contextLoads() {
		// The remote config was bad so there is no bootstrap
		assertThat(this.environment.getPropertySources().contains(""bootstrap"")).isFalse();
	}
",non-flaky,5
104092,spring-cloud_spring-cloud-config,ApplicationBootstrapTests.contextLoads,"	@Test
	public void contextLoads() {
		Map res = new TestRestTemplate().getForObject(
				""http://localhost:"" + this.port + BASE_PATH + ""/env/info.foo"", Map.class);
		assertThat(res).containsKey(""propertySources"");
		Map<String, Object> property = (Map<String, Object>) res.get(""property"");
		assertThat(property).containsEntry(""value"", ""bar"");
	}
",non-flaky,5
104093,spring-cloud_spring-cloud-config,ApplicationTests.contextLoads,"	@Test
	public void contextLoads() {
		Map res = new TestRestTemplate().getForObject(
				""http://localhost:"" + this.port + BASE_PATH + ""/env/info.foo"", Map.class);
		assertThat(res).containsKey(""propertySources"");
		Map<String, Object> property = (Map<String, Object>) res.get(""property"");
		assertThat(property).containsEntry(""value"", ""bar"");
	}
",non-flaky,5
104094,spring-cloud_spring-cloud-config,ApplicationFailFastTests.contextFails,"	@Test
	public void contextFails() {
		try {
			new SpringApplicationBuilder().sources(Application.class).run(
					""--server.port=0"", ""--spring.cloud.config.enabled=true"",
					""--spring.cloud.config.fail-fast=true"",
					""--spring.cloud.config.uri=http://server-host-doesnt-exist:1234"");
			fail(""failFast option did not produce an exception"");
		}
		catch (Exception e) {
			assertThat(e.getMessage().contains(""fail fast""))
					.as(""Exception not caused by fail fast"").isTrue();
		}
	}
",non-flaky,5
104095,spring-cloud_spring-cloud-config,ConfigClientWatchTests.stateChangedWorks,"	@Test
	public void stateChangedWorks() {
		ConfigClientWatch watch = new ConfigClientWatch(null);
		assertThat(watch.stateChanged(null, ""1"")).isTrue();
		assertThat(watch.stateChanged(""1"", ""2"")).isTrue();
		assertThat(watch.stateChanged(""1"", null)).isTrue();
		assertThat(watch.stateChanged(""1"", ""1"")).isFalse();
		watch.close();
	}
",non-flaky,5
104096,spring-cloud_spring-cloud-config,ConfigServiceBootstrapConfigurationTest.overrideConfigServicePropertySourceLocatorWhenBeanIsProvided,"	@Test
	public void overrideConfigServicePropertySourceLocatorWhenBeanIsProvided() {
		TestPropertyValues.of(""spring.cloud.config.enabled=true"").applyTo(this.context);
		this.context.register(ConfigServicePropertySourceLocatorOverrideConfig.class);
		this.context.register(ConfigServiceBootstrapConfiguration.class);
		this.context.refresh();

		ConfigServicePropertySourceLocator locator = this.context
				.getBean(ConfigServicePropertySourceLocator.class);

		Field restTemplateField = ReflectionUtils
				.findField(ConfigServicePropertySourceLocator.class, ""restTemplate"");
		restTemplateField.setAccessible(true);

		RestTemplate restTemplate = (RestTemplate) ReflectionUtils
				.getField(restTemplateField, locator);

		assertThat(restTemplate).isNotNull();
	}
",non-flaky,5
104097,spring-cloud_spring-cloud-config,ConfigClientAutoConfigurationTests.sunnyDay,"	@Test
	public void sunnyDay() {
		AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(
				ConfigClientAutoConfiguration.class);
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		context.close();
	}
",non-flaky,5
104098,spring-cloud_spring-cloud-config,ConfigClientAutoConfigurationTests.withParent,"	@Test
	public void withParent() {
		ConfigurableApplicationContext context = new SpringApplicationBuilder(
				ConfigClientAutoConfiguration.class).child(Object.class)
						.web(WebApplicationType.NONE).run();
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		context.close();
	}
",non-flaky,5
104099,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationNoSpringRetryTests.shouldFailWithExceptionGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldFailWithExceptionGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		expectNoInstancesOfConfigServerException();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=true"");
	}
",non-flaky,5
104100,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationNoSpringRetryTests.shouldFailWithMessageGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldFailWithMessageGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=false"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasDefaultConfiguration();
		verifyDiscoveryClientCalledOnce();
	}
",non-flaky,5
104101,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationNoSpringRetryTests.shouldSucceedGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldSucceedGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasConfigurationFromEureka();
		verifyDiscoveryClientCalledOnce();
	}
",non-flaky,5
104102,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.offByDefault,"	@Test
	public void offByDefault() throws Exception {
		this.context = new AnnotationConfigApplicationContext(
				DiscoveryClientConfigServiceBootstrapConfiguration.class);

		assertThat(this.context.getBeanNamesForType(DiscoveryClient.class).length)
				.isEqualTo(0);
		assertThat(this.context.getBeanNamesForType(
				DiscoveryClientConfigServiceBootstrapConfiguration.class).length)
						.isEqualTo(0);
	}
",non-flaky,5
104103,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.onWhenRequested,"	@Test
	public void onWhenRequested() throws Exception {
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
",non-flaky,5
104104,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.onWhenHeartbeat,"	@Test
	public void onWhenHeartbeat() throws Exception {
		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		givenDiscoveryClientReturnsInfo();
		verifyDiscoveryClientCalledOnce();

		this.context.publishEvent(new HeartbeatEvent(this.context, ""new""));

		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
",non-flaky,5
104105,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.secureWhenRequested,"	@Test
	public void secureWhenRequested() throws Exception {
		this.info = new DefaultServiceInstance(""app"", ""foo"", 443, true);
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasConfiguration(""https://foo:443/"");
	}
",non-flaky,5
104106,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.multipleInstancesReturnedFromDiscovery,"	@Test
	public void multipleInstancesReturnedFromDiscovery() {
		ServiceInstance info1 = new DefaultServiceInstance(""app"", ""localhost"", 8888,
				true);
		ServiceInstance info2 = new DefaultServiceInstance(""app"", ""localhost1"", 8888,
				false);
		givenDiscoveryClientReturnsInfoForMultipleInstances(info1, info2);

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasMultipleUris(""https://localhost:8888/"",
				""http://localhost1:8888/"");

	}
",non-flaky,5
104107,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.setsPasssword,"	@Test
	public void setsPasssword() throws Exception {
		this.info.getMetadata().put(""password"", ""bar"");
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		ConfigClientProperties locator = this.context
				.getBean(ConfigClientProperties.class);
		Credentials credentials = locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://foo:8877/"");
		assertThat(credentials.getPassword()).isEqualTo(""bar"");
		assertThat(credentials.getUsername()).isEqualTo(""user"");
	}
",non-flaky,5
104108,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.setsPath,"	@Test
	public void setsPath() throws Exception {
		this.info.getMetadata().put(""configPath"", ""/bar"");
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectConfigClientPropertiesHasConfiguration(""http://foo:8877/bar"");
	}
",non-flaky,5
104109,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldFailGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldFailGetConfigServerInstanceFromDiscoveryClient() throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
",non-flaky,5
104110,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldRetryAndSucceedGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldRetryAndSucceedGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsInfoOnThirdTry();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledThreeTimes();

		this.context.publishEvent(new HeartbeatEvent(this.context, ""new""));

		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
",non-flaky,5
104111,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldNotRetryIfNotFailFastPropertySet,"	@Test
	public void shouldNotRetryIfNotFailFastPropertySet() throws Exception {
		givenDiscoveryClientReturnsInfoOnThirdTry();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
",non-flaky,5
104112,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldRetryAndFailWithExceptionGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldRetryAndFailWithExceptionGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		expectNoInstancesOfConfigServerException();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=true"");
	}
",non-flaky,5
104113,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldRetryAndFailWithMessageGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldRetryAndFailWithMessageGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=false"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
",non-flaky,5
104114,spring-cloud_spring-cloud-config,ConfigServerHealthIndicatorTests.testDefaultStatus,"	@Test
	public void testDefaultStatus() {
		// UNKNOWN is better than DOWN since it doesn't stop the app from working
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UNKNOWN);
	}
",non-flaky,5
104115,spring-cloud_spring-cloud-config,ConfigServerHealthIndicatorTests.testExceptionStatus,"	@Test
	public void testExceptionStatus() {
		doThrow(new IllegalStateException()).when(this.locator)
				.locate(any(Environment.class));
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.DOWN);
		verify(this.locator, times(1)).locate(any(Environment.class));
	}
",non-flaky,5
104116,spring-cloud_spring-cloud-config,ConfigServerHealthIndicatorTests.testServerUp,"	@Test
	public void testServerUp() {
		PropertySource<?> source = new MapPropertySource(""foo"",
				Collections.<String, Object>emptyMap());
		doReturn(source).when(this.locator).locate(any(Environment.class));
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);
		verify(this.locator, times(1)).locate(any(Environment.class));
	}
",non-flaky,5
104117,spring-cloud_spring-cloud-config,ConfigServerHealthIndicatorTests.healthIsCached,"	@Test
	public void healthIsCached() {
		PropertySource<?> source = new MapPropertySource(""foo"",
				Collections.<String, Object>emptyMap());
		doReturn(source).when(this.locator).locate(any(Environment.class));

		// not cached
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);

		// cached
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);

		verify(this.locator, times(1)).locate(any(Environment.class));
	}
",non-flaky,5
104118,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.sunnyDay,"	@Test
	public void sunnyDay() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithoutLabel(new ResponseEntity<>(body, HttpStatus.OK));
		this.locator.setRestTemplate(this.restTemplate);

		ArgumentCaptor<HttpEntity> argumentCaptor = ArgumentCaptor
				.forClass(HttpEntity.class);

		assertThat(this.locator.locate(this.environment)).isNotNull();

		Mockito.verify(this.restTemplate).exchange(anyString(), any(HttpMethod.class),
				argumentCaptor.capture(), any(Class.class), anyString(), anyString());

		HttpEntity httpEntity = argumentCaptor.getValue();
		assertThat(httpEntity.getHeaders().getAccept())
				.containsExactly(MediaType.APPLICATION_JSON);
	}
",non-flaky,5
104119,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.sunnyDayWithLabel,"	@Test
	public void sunnyDayWithLabel() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithLabel(new ResponseEntity<>(body, HttpStatus.OK), ""v1.0.0"");
		this.locator.setRestTemplate(this.restTemplate);
		TestPropertyValues.of(""spring.cloud.config.label:v1.0.0"")
				.applyTo(this.environment);
		assertThat(this.locator.locate(this.environment)).isNotNull();
	}
",non-flaky,5
104120,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.sunnyDayWithLabelThatContainsASlash,"	@Test
	public void sunnyDayWithLabelThatContainsASlash() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithLabel(new ResponseEntity<>(body, HttpStatus.OK),
				""release(_)v1.0.0"");
		this.locator.setRestTemplate(this.restTemplate);
		TestPropertyValues.of(""spring.cloud.config.label:release/v1.0.0"")
				.applyTo(this.environment);
		assertThat(this.locator.locate(this.environment)).isNotNull();
	}
",non-flaky,5
104121,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.sunnyDayWithNoSuchLabel,"	@Test
	public void sunnyDayWithNoSuchLabel() {
		mockRequestResponseWithLabel(
				new ResponseEntity<Void>((Void) null, HttpStatus.NOT_FOUND),
				""nosuchlabel"");
		this.locator.setRestTemplate(this.restTemplate);
		assertThat(this.locator.locate(this.environment)).isNull();
	}
",non-flaky,5
104122,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.failsQuietly,"	@Test
	public void failsQuietly() {
		mockRequestResponseWithoutLabel(
				new ResponseEntity<>(""Wah!"", HttpStatus.INTERNAL_SERVER_ERROR));
		this.locator.setRestTemplate(this.restTemplate);
		assertThat(this.locator.locate(this.environment)).isNull();
	}
",non-flaky,5
104123,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.failFast,"	@Test
	public void failFast() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		ClientHttpResponse response = Mockito.mock(ClientHttpResponse.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		RestTemplate restTemplate = new RestTemplate(requestFactory);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		Mockito.when(request.getHeaders()).thenReturn(new HttpHeaders());
		Mockito.when(request.execute()).thenReturn(response);
		HttpHeaders headers = new HttpHeaders();
		headers.setContentType(MediaType.APPLICATION_JSON);
		Mockito.when(response.getHeaders()).thenReturn(headers);
		Mockito.when(response.getStatusCode())
				.thenReturn(HttpStatus.INTERNAL_SERVER_ERROR);
		Mockito.when(response.getBody())
				.thenReturn(new ByteArrayInputStream(""{}"".getBytes()));
		this.locator.setRestTemplate(restTemplate);
		this.expected
				.expectCause(IsInstanceOf.instanceOf(IllegalArgumentException.class));
		this.expected.expectMessage(""fail fast property is set"");
		this.locator.locate(this.environment);
	}
",non-flaky,5
104124,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.failFastWhenNotFound,"	@Test
	public void failFastWhenNotFound() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		ClientHttpResponse response = Mockito.mock(ClientHttpResponse.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		RestTemplate restTemplate = new RestTemplate(requestFactory);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		Mockito.when(request.getHeaders()).thenReturn(new HttpHeaders());
		Mockito.when(request.execute()).thenReturn(response);
		HttpHeaders headers = new HttpHeaders();
		headers.setContentType(MediaType.APPLICATION_JSON);
		Mockito.when(response.getHeaders()).thenReturn(headers);
		Mockito.when(response.getStatusCode()).thenReturn(HttpStatus.NOT_FOUND);
		Mockito.when(response.getBody())
				.thenReturn(new ByteArrayInputStream("""".getBytes()));
		this.locator.setRestTemplate(restTemplate);
		this.expected
				.expectCause(IsInstanceOf.instanceOf(IllegalArgumentException.class));
		this.expected.expectMessage(""fail fast property is set"");
		this.locator.locate(this.environment);
	}
",non-flaky,5
104125,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.failFastWhenBothPasswordAndAuthorizationPropertiesSet,"	@Test
	public void failFastWhenBothPasswordAndAuthorizationPropertiesSet() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		defaults.setUsername(""username"");
		defaults.setPassword(""password"");
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(
				""Could not locate PropertySource and the fail fast property is set, failing"");
		this.locator.locate(this.environment);
	}
",non-flaky,5
104126,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.interceptorShouldAddHeadersWhenHeadersPropertySet,"	@Test
	public void interceptorShouldAddHeadersWhenHeadersPropertySet() throws Exception {
		MockClientHttpRequest request = new MockClientHttpRequest();
		ClientHttpRequestExecution execution = Mockito
				.mock(ClientHttpRequestExecution.class);
		byte[] body = new byte[] {};
		Map<String, String> headers = new HashMap<>();
		headers.put(""X-Example-Version"", ""2.1"");
		new ConfigServicePropertySourceLocator.GenericRequestHeaderInterceptor(headers)
				.intercept(request, body, execution);
		Mockito.verify(execution).execute(request, body);
		assertThat(request.getHeaders().getFirst(""X-Example-Version"")).isEqualTo(""2.1"");
	}
",non-flaky,5
104127,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldAddAuthorizationHeaderWhenPasswordSet,"	@Test
	public void shouldAddAuthorizationHeaderWhenPasswordSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = ""pass"";
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
		assertThat(headers).hasSize(1);
	}
",non-flaky,5
104128,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldAddAuthorizationHeaderWhenAuthorizationSet,"	@Test
	public void shouldAddAuthorizationHeaderWhenAuthorizationSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = null;
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
		assertThat(headers).hasSize(1);
	}
",non-flaky,5
104129,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldThrowExceptionWhenPasswordAndAuthorizationBothSet,"	@Test
	public void shouldThrowExceptionWhenPasswordAndAuthorizationBothSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = ""pass"";
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""You must set either 'password' or 'authorization'"");
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
	}
",non-flaky,5
104130,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldThrowExceptionWhenNegativeReadTimeoutSet,"	@Test
	public void shouldThrowExceptionWhenNegativeReadTimeoutSet() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setRequestReadTimeout(-1);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Invalid Value for Read Timeout set."");
		ReflectionTestUtils.invokeMethod(this.locator, ""getSecureRestTemplate"", defaults);
	}
",non-flaky,5
104131,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldThrowExceptionWhenNegativeConnectTimeoutSet,"	@Test
	public void shouldThrowExceptionWhenNegativeConnectTimeoutSet() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setRequestConnectTimeout(-1);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Invalid Value for Connect Timeout set."");
		ReflectionTestUtils.invokeMethod(this.locator, ""getSecureRestTemplate"", defaults);
	}
",non-flaky,5
104132,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.checkInterceptorHasNoAuthorizationHeaderPresent,"	@Test
	public void checkInterceptorHasNoAuthorizationHeaderPresent() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		defaults.getHeaders().put(""key"", ""value"");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		RestTemplate restTemplate = ReflectionTestUtils.invokeMethod(this.locator,
				""getSecureRestTemplate"", defaults);
		Iterator<ClientHttpRequestInterceptor> iterator = restTemplate.getInterceptors()
				.iterator();
		while (iterator.hasNext()) {
			GenericRequestHeaderInterceptor genericRequestHeaderInterceptor = (GenericRequestHeaderInterceptor) iterator
					.next();
			assertThat(genericRequestHeaderInterceptor.getHeaders().get(AUTHORIZATION))
					.isEqualTo(null);
		}
	}
",non-flaky,5
104133,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.vanilla,"	@Test
	public void vanilla() {
		this.locator.setUri(new String[] { ""http://localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""user"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104134,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.uriCreds,"	@Test
	public void uriCreds() {
		this.locator.setUri(new String[] { ""http://foo:bar@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foo"");
		assertThat(credentials.getPassword()).isEqualTo(""bar"");
	}
",non-flaky,5
104135,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.explicitPassword,"	@Test
	public void explicitPassword() {
		this.locator.setUri(new String[] { ""http://foo:bar@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foo"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104136,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfNoColonPresentInUriCreds,"	@Test
	public void testIfNoColonPresentInUriCreds() {
		this.locator.setUri(new String[] { ""http://foobar@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foobar"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104137,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfColonPresentAtTheEndInUriCreds,"	@Test
	public void testIfColonPresentAtTheEndInUriCreds() {
		this.locator.setUri(new String[] { ""http://foobar:@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foobar"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104138,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfColonPresentAtTheStartInUriCreds,"	@Test
	public void testIfColonPresentAtTheStartInUriCreds() {
		this.locator.setUri(new String[] { ""http://:foobar@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo("""");
		assertThat(credentials.getPassword()).isEqualTo(""foobar"");
	}
",non-flaky,5
104139,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfColonPresentAtTheStartAndEndInUriCreds,"	@Test
	public void testIfColonPresentAtTheStartAndEndInUriCreds() {
		this.locator.setUri(new String[] { ""http://:foobar:@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo("""");
		assertThat(credentials.getPassword()).isEqualTo(""foobar:"");
	}
",non-flaky,5
104140,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfSpacePresentAsUriCreds,"	@Test
	public void testIfSpacePresentAsUriCreds() {
		this.locator.setUri(new String[] { ""http://  @localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""  "");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104141,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.changeNameInOverride,"	@Test
	public void changeNameInOverride() {
		this.locator.setName(""one"");
		ConfigurableEnvironment environment = new StandardEnvironment();
		TestPropertyValues.of(""spring.application.name:two"").applyTo(environment);
		ConfigClientProperties override = this.locator.override(environment);
		assertThat(override.getName()).isEqualTo(""two"");
	}
",non-flaky,5
104142,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testThatExplicitUsernamePasswordTakePrecedence,"	@Test
	public void testThatExplicitUsernamePasswordTakePrecedence() {
		ConfigClientProperties properties = new ConfigClientProperties(
				new MockEnvironment());

		properties.setUri(
				new String[] { ""https://userInfoName:userInfoPW@localhost:8888/"" });
		properties.setUsername(""explicitName"");
		properties.setPassword(""explicitPW"");
		Credentials credentials = properties.getCredentials(0);
		assertThat(credentials.getPassword()).isEqualTo(""explicitPW"");
		assertThat(credentials.getUsername()).isEqualTo(""explicitName"");
	}
",non-flaky,5
104143,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.checkIfExceptionThrownForNegativeIndex,"	@Test
	public void checkIfExceptionThrownForNegativeIndex() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(-1);
	}
",non-flaky,5
104144,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.checkIfExceptionThrownForPositiveInvalidIndex,"	@Test
	public void checkIfExceptionThrownForPositiveInvalidIndex() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(3);
	}
",non-flaky,5
104145,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.checkIfExceptionThrownForIndexEqualToLength,"	@Test
	public void checkIfExceptionThrownForIndexEqualToLength() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(2);
	}
",non-flaky,5
104146,spring-cloud_spring-cloud-config,ConfigServerBootstrapConfigurationTests.withHealthIndicator,"	@Test
	public void withHealthIndicator() {
		ConfigurableApplicationContext context = new SpringApplicationBuilder(
				PropertySourceBootstrapConfiguration.class,
				ConfigServiceBootstrapConfiguration.class)
						.child(ConfigClientAutoConfiguration.class)
						.web(WebApplicationType.NONE).run();
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigServerHealthIndicator.class).length).isEqualTo(1);
		context.close();
	}
",non-flaky,5
104147,spring-cloud_spring-cloud-config,EncryptionControllerTests.cannotDecryptWithoutKey,"	@Test(expected = EncryptionTooWeakException.class)
	public void cannotDecryptWithoutKey() {
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
",non-flaky,5
104148,spring-cloud_spring-cloud-config,EncryptionControllerTests.cannotDecryptWithNoopEncryptor,"	@Test(expected = EncryptionTooWeakException.class)
	public void cannotDecryptWithNoopEncryptor() {
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
",non-flaky,5
104149,spring-cloud_spring-cloud-config,EncryptionControllerTests.shouldThrowExceptionOnDecryptInvalidData,"	@Test(expected = InvalidCipherException.class)
	public void shouldThrowExceptionOnDecryptInvalidData() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
",non-flaky,5
104150,spring-cloud_spring-cloud-config,EncryptionControllerTests.shouldThrowExceptionOnDecryptWrongKey,"	@Test(expected = InvalidCipherException.class)
	public void shouldThrowExceptionOnDecryptWrongKey() {
		RsaSecretEncryptor encryptor = new RsaSecretEncryptor();
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		this.controller.decrypt(encryptor.encrypt(""foo""), MediaType.TEXT_PLAIN);
	}
",non-flaky,5
104151,spring-cloud_spring-cloud-config,EncryptionControllerTests.sunnyDayRsaKey,"	@Test
	public void sunnyDayRsaKey() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		String cipher = this.controller.encrypt(""foo"", MediaType.TEXT_PLAIN);
		assertThat(this.controller.decrypt(cipher, MediaType.TEXT_PLAIN))
				.isEqualTo(""foo"");
	}
",non-flaky,5
104152,spring-cloud_spring-cloud-config,EncryptionControllerTests.publicKey,"	@Test
	public void publicKey() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		String key = this.controller.getPublicKey();
		assertThat(key.startsWith(""ssh-rsa"")).as(""Wrong key format: "" + key).isTrue();
	}
",non-flaky,5
104153,spring-cloud_spring-cloud-config,EncryptionControllerTests.appAndProfile,"	@Test
	public void appAndProfile() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""app"", ""default"", ""foo bar"",
				MediaType.TEXT_PLAIN);
		String decrypt = this.controller.decrypt(""app"", ""default"", cipher,
				MediaType.TEXT_PLAIN);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
",non-flaky,5
104154,spring-cloud_spring-cloud-config,EncryptionControllerTests.formDataIn,"	@Test
	public void formDataIn() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""foo bar="",
				MediaType.APPLICATION_FORM_URLENCODED);
		String decrypt = this.controller.decrypt(cipher + ""="",
				MediaType.APPLICATION_FORM_URLENCODED);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
",non-flaky,5
104155,spring-cloud_spring-cloud-config,EncryptionControllerTests.formDataInWithPrefix,"	@Test
	public void formDataInWithPrefix() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""{key:test}foo bar="",
				MediaType.APPLICATION_FORM_URLENCODED);
		String decrypt = this.controller.decrypt(cipher + ""="",
				MediaType.APPLICATION_FORM_URLENCODED);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
",non-flaky,5
104156,spring-cloud_spring-cloud-config,EncryptionControllerTests.prefixStrippedBeforeEncrypt,"	@Test
	public void prefixStrippedBeforeEncrypt() {
		TextEncryptor encryptor = mock(TextEncryptor.class);
		when(encryptor.encrypt(anyString())).thenReturn(""myEncryptedValue"");

		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(encryptor));
		this.controller.encrypt(""{key:test}foo"", MediaType.TEXT_PLAIN);

		ArgumentCaptor<String> captor = ArgumentCaptor.forClass(String.class);
		verify(encryptor, atLeastOnce()).encrypt(captor.capture());
		assertThat(captor.getValue()).doesNotContain(""{key:test}"")
				.as(""Prefix must be stripped prior to encrypt"");
	}
",non-flaky,5
104157,spring-cloud_spring-cloud-config,EncryptionControllerTests.encryptDecyptTextWithCurlyBrace,"	@Test
	public void encryptDecyptTextWithCurlyBrace() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));

		String plain = ""textwith}brace"";
",non-flaky,5
104158,spring-cloud_spring-cloud-config,EncryptionControllerTests.locate,"	@Test
	public void addEnvironment() {
		TextEncryptorLocator locator = new TextEncryptorLocator() {

			private RsaSecretEncryptor encryptor = new RsaSecretEncryptor();

			@Override
			public TextEncryptor locate(Map<String, String> keys) {
				return this.encryptor;
			}
",non-flaky,5
104159,spring-cloud_spring-cloud-config,CipherEnvironmentEncryptorTests.shouldDecryptEnvironment,"	@Test
	public void shouldDecryptEnvironment() {
		// given
		String secret = randomUUID().toString();

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"", Collections.<Object, Object>singletonMap(
				environment.getName(), ""{cipher}"" + this.textEncryptor.encrypt(secret))));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(secret);
	}
",non-flaky,5
104160,spring-cloud_spring-cloud-config,CipherEnvironmentEncryptorTests.shouldDecryptEnvironmentWithKey,"	@Test
	public void shouldDecryptEnvironmentWithKey() {
		// given
		String secret = randomUUID().toString();

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"",
				Collections.<Object, Object>singletonMap(environment.getName(),
						""{cipher}{key:test}"" + this.textEncryptor.encrypt(secret))));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(secret);
	}
",non-flaky,5
104161,spring-cloud_spring-cloud-config,CipherEnvironmentEncryptorTests.shouldBeAbleToUseNullAsPropertyValue,"	@Test
	public void shouldBeAbleToUseNullAsPropertyValue() {

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"",
				Collections.<Object, Object>singletonMap(environment.getName(), null)));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(null);
	}
",non-flaky,5
104162,spring-cloud_spring-cloud-config,EncryptionIntegrationTests.symmetricEncryptionEnabled,"		@Test
		public void symmetricEncryptionEnabled() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
",non-flaky,5
104163,spring-cloud_spring-cloud-config,EncryptionIntegrationTests.symmetricEncryptionBootstrapConfig,"		@Test
		public void symmetricEncryptionBootstrapConfig() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
",non-flaky,5
104164,spring-cloud_spring-cloud-config,EncryptionIntegrationTests.keystoreBootstrapConfig,"		@Test
		public void keystoreBootstrapConfig() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
",non-flaky,5
104165,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testAddPrefix,"	@Test
	public void testAddPrefix() {
		assertThat(this.helper.addPrefix(Collections.singletonMap(""bar"", ""spam""), ""foo""))
				.isEqualTo(""{bar:spam}foo"");
	}
",non-flaky,5
104166,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testAddNoPrefix,"	@Test
	public void testAddNoPrefix() {
		assertThat(this.helper.addPrefix(Collections.<String, String>emptyMap(), ""foo""))
				.isEqualTo(""foo"");
	}
",non-flaky,5
104167,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testStripNoPrefix,"	@Test
	public void testStripNoPrefix() {
		assertThat(this.helper.stripPrefix(""foo"")).isEqualTo(""foo"");
	}
",non-flaky,5
104168,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testStripPrefix,"	@Test
	public void testStripPrefix() {
		assertThat(this.helper.stripPrefix(""{key:foo}foo"")).isEqualTo(""foo"");
	}
",non-flaky,5
104169,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testStripPrefixWithEscape,"	@Test
	public void testStripPrefixWithEscape() {
		assertThat(this.helper.stripPrefix(""{plain}{key:foo}foo""))
				.isEqualTo(""{key:foo}foo"");
	}
",non-flaky,5
104170,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testKeysDefaults,"	@Test
	public void testKeysDefaults() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"", ""spam"");
		assertThat(keys.get(""name"")).isEqualTo(""foo"");
		assertThat(keys.get(""profiles"")).isEqualTo(""bar"");
	}
",non-flaky,5
104171,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testKeysWithPrefix,"	@Test
	public void testKeysWithPrefix() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"",
				""{key:mykey}foo"");
		assertThat(keys.size()).isEqualTo(3);
		assertThat(keys.get(""key"")).isEqualTo(""mykey"");
	}
",non-flaky,5
104172,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testKeysWithPrefixAndEscape,"	@Test
	public void testKeysWithPrefixAndEscape() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"",
				""{key:mykey}{plain}{foo:bar}foo"");
		assertThat(keys.size()).isEqualTo(3);
		assertThat(keys.get(""key"")).isEqualTo(""mykey"");
	}
",non-flaky,5
104173,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testTextWithCurlyBracesNoPrefix,"	@Test
	public void testTextWithCurlyBracesNoPrefix() {
		assertThat(this.helper.stripPrefix(""textwith}brac{es""))
				.isEqualTo(""textwith}brac{es"");
	}
",non-flaky,5
104174,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testTextWithCurlyBracesPrefix,"	@Test
	public void testTextWithCurlyBracesPrefix() {
		assertThat(
				this.helper.stripPrefix(""{key:foo}{name:bar}textwith}brac{es{and}prefix""))
						.isEqualTo(""textwith}brac{es{and}prefix"");
	}
",non-flaky,5
104175,spring-cloud_spring-cloud-config,EncryptionControllerMultiTextEncryptorTests.shouldEncryptUsingApplicationAndProfiles,"	@Test
	public void shouldEncryptUsingApplicationAndProfiles() {

		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(Encryptors.text(""application"", ""11"")));

		// when
		String encrypted = this.controller.encrypt(this.application, this.profiles,
				this.data, TEXT_PLAIN);

		// then
		assertThat(this.controller.decrypt(this.application, this.profiles, encrypted,
				TEXT_PLAIN)).isEqualTo(this.data);
	}
",non-flaky,5
104176,spring-cloud_spring-cloud-config,EncryptionControllerMultiTextEncryptorTests.shouldNotEncryptUsingNoOp,"	@Test(expected = EncryptionTooWeakException.class)
	public void shouldNotEncryptUsingNoOp() {
		// given
		String application = ""unknown"";

		// when
		this.controller.encrypt(application, this.profiles, this.data, TEXT_PLAIN);

		// then exception is thrown
	}
",non-flaky,5
104177,spring-cloud_spring-cloud-config,EncryptionControllerMultiTextEncryptorTests.shouldNotDecryptUsingNoOp,"	@Test(expected = EncryptionTooWeakException.class)
	public void shouldNotDecryptUsingNoOp() {
		// given
		String application = ""unknown"";

		// when
		this.controller.decrypt(application, this.profiles, this.data, TEXT_PLAIN);

		// then exception is thrown
	}
",non-flaky,5
104178,spring-cloud_spring-cloud-config,KeyStoreTextEncryptorLocatorTests.testDefaults,"	@Test
	public void testDefaults() {
		TextEncryptor encryptor = this.locator
				.locate(Collections.<String, String>emptyMap());
		assertThat(encryptor.decrypt(encryptor.encrypt(""foo""))).isEqualTo(""foo"");
	}
",non-flaky,5
104179,spring-cloud_spring-cloud-config,KeyStoreTextEncryptorLocatorTests.locate,"	@Test
	public void testDifferentKeyDefaultSecret() {
		this.locator.setSecretLocator(new SecretLocator() {

			@Override
			public char[] locate(String secret) {
				assertThat(secret).isEqualTo(""changeme"");
				// The actual secret for ""mykey"" is the same as the keystore password
				return ""letmein"".toCharArray();
			}
",non-flaky,5
104180,spring-cloud_spring-cloud-config,KeyStoreTextEncryptorLocatorTests.testDifferentKeyAndSecret,"	@Test
	public void testDifferentKeyAndSecret() {
		Map<String, String> map = new HashMap<String, String>();
		map.put(""key"", ""mytestkey"");
		map.put(""secret"", ""changeme"");
		TextEncryptor encryptor = this.locator.locate(map);
		assertThat(encryptor.decrypt(encryptor.encrypt(""foo""))).isEqualTo(""foo"");
	}
",non-flaky,5
104181,spring-cloud_spring-cloud-config,EnvironmentPropertySourceTest.testEscapedPlaceholdersRemoved,"	@Test
	public void testEscapedPlaceholdersRemoved() {
		assertThat(resolvePlaceholders(this.env, ""\\${abc}"")).isEqualTo(""${abc}"");
		// JSON generated from jackson will be double escaped
		assertThat(resolvePlaceholders(this.env, ""\\\\${abc}"")).isEqualTo(""${abc}"");
	}
",non-flaky,5
104182,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testCanHandle,"	@Test
	public void testCanHandle() {
		assertThat(GitSkipSslValidationCredentialsProvider
				.canHandle(""https://github.com/org/repo"")).as(
						""GitSkipSslValidationCredentialsProvider only handles HTTPS uris"")
						.isTrue();
		assertThat(GitSkipSslValidationCredentialsProvider
				.canHandle(""git@github.com:org/repo"")).as(
						""GitSkipSslValidationCredentialsProvider only handles HTTPS uris"")
						.isFalse();
	}
",non-flaky,5
104183,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testIsInteractive,"	@Test
	public void testIsInteractive() {
		assertThat(this.skipSslValidationCredentialsProvider.isInteractive()).as(
				""GitSkipSslValidationCredentialsProvider with no delegate requires no user interaction"")
				.isFalse();
	}
",non-flaky,5
104184,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testIsInteractiveWithDelegate,"	@Test
	public void testIsInteractiveWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		when(this.mockDelegateCredentialsProvider.isInteractive()).thenReturn(true);

		assertThat(this.skipSslValidationCredentialsProvider.isInteractive()).as(
				""With a delegate provider, isInteractive value depends on the delegate"")
				.isTrue();
	}
",non-flaky,5
104185,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsSslFailureInformationalMessage,"	@Test
	public void testSupportsSslFailureInformationalMessage() {
		CredentialItem informationalMessage = new CredentialItem.InformationalMessage(
				""text "" + JGitText.get().sslFailureTrustExplanation + "" more text"");
		assertThat(this.skipSslValidationCredentialsProvider
				.supports(informationalMessage)).as(
						""GitSkipSslValidationCredentialsProvider should always support SSL failure InformationalMessage"")
						.isTrue();

		informationalMessage = new CredentialItem.InformationalMessage(""unrelated"");
		assertThat(this.skipSslValidationCredentialsProvider
				.supports(informationalMessage)).as(
						""GitSkipSslValidationCredentialsProvider should not support unrelated InformationalMessage items"")
						.isFalse();
	}
",non-flaky,5
104186,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsSslFailureInformationalMessageWithDelegate,"	@Test
	public void testSupportsSslFailureInformationalMessageWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		testSupportsSslFailureInformationalMessage();
	}
",non-flaky,5
104187,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsSslValidationYesNoTypes,"	@Test
	public void testSupportsSslValidationYesNoTypes() {
		CredentialItem yesNoType = new CredentialItem.YesNoType(
				JGitText.get().sslTrustNow);
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust now YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(
				MessageFormat.format(JGitText.get().sslTrustForRepo, ""/a/path.git""));
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust repo YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(JGitText.get().sslTrustAlways);
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust always YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(""unrelated"");
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should not support unrelated YesNoType items"")
				.isFalse();
	}
",non-flaky,5
104188,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsYesNoTypeWithDelegate,"	@Test
	public void testSupportsYesNoTypeWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		testSupportsSslValidationYesNoTypes();
	}
",non-flaky,5
104189,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsUnrelatedCredentialItemTypes,"	@Test
	public void testSupportsUnrelatedCredentialItemTypes() {
		CredentialItem usernameCredentialItem = new CredentialItem.Username();

		boolean supportsItems = this.skipSslValidationCredentialsProvider
				.supports(usernameCredentialItem);

		assertThat(supportsItems).as(
				""Credential item types not related to SSL validation skipping should not be supported"")
				.isFalse();
	}
",non-flaky,5
104190,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsUnrelatedCredentialItemTypesWithDelegate,"	@Test
	public void testSupportsUnrelatedCredentialItemTypesWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);
		CredentialItem usernameCredentialItem = new CredentialItem.Username();

		when(this.mockDelegateCredentialsProvider.supports(usernameCredentialItem))
				.thenReturn(true);

		boolean supportsItems = this.skipSslValidationCredentialsProvider
				.supports(usernameCredentialItem);

		assertThat(supportsItems).as(
				""GitSkipSslValidationCredentialsProvider must support the types supported by its delegate CredentialsProvider"")
				.isTrue();
	}
",non-flaky,5
281,soot-oss_soot,TestDominance.TestSimpleDiamond,"@Test
public void TestSimpleDiamond() {
    Node x = new Node(4);
    Node n = new Node(1).addkid(new Node(2).addkid(x)).addkid(new Node(3).addkid(x));
    Graph g = new Graph(n);
    MHGDominatorsFinder<Node> finder = new MHGDominatorsFinder<Node>(g);
    DominatorTree<Node> tree = new DominatorTree<Node>(finder);
    assertThat(tree.getHeads().size(), is(1));
    DominatorNode<Node> head = tree.getHeads().get(0);
    assertThat(head.getGode().id, is(1));
    Set<Integer> kids = kid_ids(head);
    assertThat(kids.size(), is(3));
    assertThat(kids, contains(2, 3, 4));
}",unordered collections,3
156081,soot-oss_soot,HelloTestingFrameworkTest.findsTarget,"  @Test
  public void findsTarget() {
    final SootMethod sootMethod = prepareTarget(""<"" + TEST_TARGET_CLASS + "": void helloWorld()>"", TEST_TARGET_CLASS);
    Assert.assertNotNull(""Could not find target method. System test setup seems to be incorrect."", sootMethod);
    Assert.assertTrue(sootMethod.isConcrete());
    Assert.assertNotNull(sootMethod.retrieveActiveBody());
  }
",non-flaky,5
156082,soot-oss_soot,PolymorphicDispatchTest.findsTarget,"  @Test
  public void findsTarget() {
    String methodSignature = methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""unambiguousMethod"", """");
    final SootMethod sootMethod = prepareTarget(methodSignature, TEST_TARGET_CLASS);
    Assert.assertTrue(sootMethod.isConcrete());

    Body body = sootMethod.retrieveActiveBody();
    Assert.assertNotNull(body);
    // validate individual method
    body.validate();

    for (Unit u : body.getUnits()) {
      if (u instanceof AssignStmt) {
        Value right = ((AssignStmt) u).getRightOp();
        if (right instanceof InvokeExpr) {
          SootMethod m = ((InvokeExpr) right).getMethodRef().resolve();
          Assert.assertFalse(m.isPhantom());
          Assert.assertTrue(m.isDeclared());
          if (m.getName().equals(""invoke"")) {
            Assert.assertTrue(m.isNative());
          }
        }
      }
    }
  }
",non-flaky,5
156083,soot-oss_soot,PolymorphicDispatchTest.handlesAmbiguousMethod,"  @Test
  public void handlesAmbiguousMethod() {
    String methodSignature = methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""ambiguousMethod"", """");
    final SootMethod sootMethod = prepareTarget(methodSignature, TEST_TARGET_CLASS);
    Assert.assertTrue(sootMethod.isConcrete());

    Body body = sootMethod.retrieveActiveBody();
    Assert.assertNotNull(body);
    // validate individual method
    body.validate();

    for (Unit u : body.getUnits()) {
      if (u instanceof AssignStmt) {
        Value right = ((AssignStmt) u).getRightOp();
        if (right instanceof InvokeExpr) {
          SootMethod m = ((InvokeExpr) right).getMethodRef().resolve();
          Assert.assertFalse(m.isPhantom());
          Assert.assertTrue(m.isDeclared());
          if (m.getName().equals(""invoke"")) {
            Assert.assertTrue(m.isNative());
          }
        }
      }
    }
  }
",non-flaky,5
156084,soot-oss_soot,PropagateLineNumberTagTest.nullAssignment,"  @Test
  public void nullAssignment() {
    SootMethod target =
        prepareTarget(
            methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""nullAssignment""),
            TEST_TARGET_CLASS);

    Body body = target.retrieveActiveBody();

    Optional<Unit> unit =
        body.getUnits().stream()
            .filter(
                u ->
                    u.toString()
                        .equals(
                            ""staticinvoke <soot.jimple.PropagateLineNumberTag: soot.jimple.PropagateLineNumberTag$A foo(soot.jimple.PropagateLineNumberTag$A)>(null)""))
            .findFirst();

    assertTrue(unit.isPresent());

    List<ValueBox> useBoxes = unit.get().getUseBoxes();

    assertEquals(2, useBoxes.size());
    ValueBox valueBox = useBoxes.get(0);
    assertTrue(valueBox instanceof ImmediateBox);
    assertEquals(1, valueBox.getTags().size());
    assertTrue(valueBox.getTags().get(0) instanceof LineNumberTag);
    assertEquals(33, valueBox.getJavaSourceStartLineNumber());
  }
",non-flaky,5
156085,soot-oss_soot,PropagateLineNumberTagTest.transitiveNullAssignment,"  @Test
  public void transitiveNullAssignment() {
    SootMethod target =
        prepareTarget(
            methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""transitiveNullAssignment""),
            TEST_TARGET_CLASS);

    Body body = target.retrieveActiveBody();

    // first call to foo
    Optional<Unit> unit =
        body.getUnits().stream()
            .filter(
                u ->
                    u.toString()
                        .equals(
                            ""staticinvoke <soot.jimple.PropagateLineNumberTag: soot.jimple.PropagateLineNumberTag$A foo(soot.jimple.PropagateLineNumberTag$A)>(null)""))
            .findFirst();

    assertTrue(unit.isPresent());

    List<ValueBox> useBoxes = unit.get().getUseBoxes();

    assertEquals(2, useBoxes.size());
    ValueBox valueBox = useBoxes.get(0);
    assertTrue(valueBox instanceof ImmediateBox);
    assertEquals(1, valueBox.getTags().size());
    assertTrue(valueBox.getTags().get(0) instanceof LineNumberTag);
    assertEquals(39, valueBox.getJavaSourceStartLineNumber());

    // second call to foo
    unit =
        body.getUnits().stream()
            .filter(
                u ->
                    u.toString()
                        .equals(
                            ""staticinvoke <soot.jimple.PropagateLineNumberTag: soot.jimple.PropagateLineNumberTag$A foo(soot.jimple.PropagateLineNumberTag$A)>(null)""))
            .skip(1)
            .findFirst();

    assertTrue(unit.isPresent());
    useBoxes = unit.get().getUseBoxes();
    assertEquals(2, useBoxes.size());
    valueBox = useBoxes.get(0);
    assertTrue(valueBox instanceof ImmediateBox);
    assertEquals(1, valueBox.getTags().size());
    assertTrue(valueBox.getTags().get(0) instanceof LineNumberTag);
    assertEquals(39, valueBox.getJavaSourceStartLineNumber());
  }
",non-flaky,5
156086,soot-oss_soot,MultiCallGraphVirtualEdgesTest.TestAsyncTaskBasicCG,"    @Test
    public void TestAsyncTaskBasicCG() {
        prepareTarget(methodSigFromComponents(TARGET_CLASS, TARGET_METHOD), TARGET_CLASS);
        boolean found = false;
        for (Edge edge : Scene.v().getCallGraph()) {
            //String sig = edge.getTgt().method().toString();
            System.out.println(edge);
            String sig = edge.getTgt().method().toString();

            if (edge.toString().contains(""AHelper"") && edge.toString().contains(""handle""))
                found = true;
        }

        //Assert.assertTrue(found);
    }
",non-flaky,5
156087,soot-oss_soot,AndroidCallGraphVirtualEdgesTest.TestAsyncTaskBasicCG,"    @Test
    public void TestAsyncTaskBasicCG() {
        prepareTarget(methodSigFromComponents(TARGET_CLASS, TARGET_METHOD), TARGET_CLASS);

        asyncFuncMaps.clear();
        asyncFuncMaps.put(""doInBackground"", DO_IN_BG);
        asyncFuncMaps.put(""onPreExecute"", ON_PRE_EXE);
        asyncFuncMaps.put(""onPostExecute"", ON_POS_EXE);
        asyncFuncMaps.put(""onProgressUpdate"", ON_PRO_UPD);

        int full = 0, ret = 0;
        for(String key: asyncFuncMaps.keySet())
        {
            full |= asyncFuncMaps.get(key);
        }

        for (Edge edge : Scene.v().getCallGraph()) {
            String sig = edge.getTgt().method().toString();
            for (String key : asyncFuncMaps.keySet()) {
                if (sig.contains(key))
                    ret |= asyncFuncMaps.get(key);
            }
        }

        //The four functions shall all appear in call graph
        Assert.assertEquals(ret, full);
    }
",non-flaky,5
156088,soot-oss_soot,TypeBasedReflectionModelAnySubTypeTest.anySubTypePointsToResolution,"  @Test
  public void anySubTypePointsToResolution() {
    SootMethod entryPoint = prepareTarget(TEST_PTA_ENTRY_POINT, TEST_PACKAGE);
    commonInvokeTest(entryPoint);
  }
",non-flaky,5
156089,soot-oss_soot,TypeBasedReflectionModelAnySubTypeTest.anySubTypeTypestateResolution,"  @Test
  public void anySubTypeTypestateResolution() {
    SootMethod entryPoint = prepareTarget(TEST_TYPESTATE_ENTRY_POINT, TEST_PACKAGE);
    commonInvokeTest(entryPoint);
  }
",non-flaky,5
156090,soot-oss_soot,AsmMethodSourceTest.iterator,"  @Test
  public void iterator() {
    // statements at the beginning of a for loop should have the line number as for the branching
    // statement and not the last line number after the branch that leads outside the loop
    SootMethod target = prepareTarget(methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""iterator""), TEST_TARGET_CLASS);

    Body body = target.retrieveActiveBody();

    Optional<Unit> unit = body.getUnits().stream()
        .filter(u -> u.toString().contains(""<java.util.Iterator: java.lang.Object next()>()"")).findFirst();

    Assert.assertTrue(unit.isPresent());

    Assert.assertEquals(31, unit.get().getJavaSourceStartLineNumber());
  }
",non-flaky,5
156091,soot-oss_soot,AsmMethodSourceTest.localNaming,"  @Test
  public void localNaming() {
    // This test ensures that local names are preserved in the Jimple code.
    final String className = ""soot.asm.LocalNaming"";
    final String[] params = { ""java.lang.String"", ""java.lang.Integer"", ""byte[]"", ""java.lang.StringBuilder"" };
    SootMethod target = prepareTarget(methodSigFromComponents(className, ""void"", ""localNaming"", params), className);
    Body body = target.retrieveActiveBody();
    Set<String> localNames = body.getLocals().stream().map(Local::getName).collect(Collectors.toSet());

    // All expected Local names are present
    Assert.assertTrue(localNames.contains(""alpha""));
    Assert.assertTrue(localNames.contains(""beta""));
    Assert.assertTrue(localNames.contains(""gamma""));
    Assert.assertTrue(localNames.contains(""delta""));
    Assert.assertTrue(localNames.contains(""epsilon""));
    Assert.assertTrue(localNames.contains(""zeta""));
    Assert.assertTrue(localNames.contains(""eta""));
    Assert.assertTrue(localNames.contains(""theta""));
    Assert.assertTrue(localNames.contains(""iota""));
    Assert.assertTrue(localNames.contains(""omega""));

    // No Local name contains ""$stack""
    Assert.assertTrue(localNames.stream().allMatch(n -> !n.contains(""$stack"")));
  }
",non-flaky,5
156092,soot-oss_soot,AsmMethodSourceTest.testSilsDisabled,"  @Test
  public void testSilsDisabled() {
    final String className = ""soot.asm.LocalNaming"";
    final String[] params = {};
    SootMethod target = prepareTarget(methodSigFromComponents(className, ""void"", ""test"", params), className);
    Body body = target.retrieveActiveBody();
    Set<String> localNames = body.getLocals().stream().map(Local::getName).collect(Collectors.toSet());
    // test if all expected Local names are present
    Assert.assertTrue(localNames.contains(""d""));
    Assert.assertTrue(localNames.contains(""f""));
    Assert.assertTrue(localNames.contains(""arr""));
  }
",non-flaky,5
156093,soot-oss_soot,AsmMethodSourceTest.testInner,"  @Test
  public void testInner() {
    NopStmt[] nops = new NopStmt[6];
    for (int i = 0; i < nops.length; i++) {
      nops[i] = Jimple.v().newNopStmt();
    }
    UnitPatchingChain chainNew = new UnitPatchingChain(new HashChain<Unit>());
    UnitContainer container = new UnitContainer(nops[0], new UnitContainer(nops[1], new UnitContainer(nops[2]), nops[3]),
        nops[4], new UnitContainer(nops[5]));
    AsmMethodSource.emitUnits(container, chainNew);
    UnitPatchingChain chainOld = new UnitPatchingChain(new HashChain<Unit>());
    oldEmitImplementation(container, chainOld);

    Assert.assertEquals(chainOld.size(), chainNew.size());
    Iterator<Unit> itO = chainOld.iterator();
    Iterator<Unit> itN = chainNew.iterator();
    while (itO.hasNext()) {
      Unit oo = itO.next();
      Unit nn = itN.next();
      if (oo != nn) {
        Assert.fail();
      }
    }
  }
",non-flaky,5
156094,soot-oss_soot,AsmInnerClassTest.nonInner,"  @Test
  public void nonInner() {
    // statements at the beginning of a for loop should have the line number as for the branching
    // statement and not the last line number after the branch that leads outside the loop
    SootMethod target =
        prepareTarget(
            methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""method""), TEST_TARGET_CLASS);
    assertEquals(2, Scene.v().getApplicationClasses().size());
    assertFalse(target.getDeclaringClass().hasOuterClass());
    assertFalse(target.getDeclaringClass().isInnerClass());
    InnerClassTag tag = (InnerClassTag) target.getDeclaringClass().getTag(InnerClassTag.NAME);
    // the class has inner classes
    assertNotNull(tag);
  }
",non-flaky,5
156095,soot-oss_soot,AsmInnerClassTest.InnerStatic,"  @Test
  public void InnerStatic() {
    SootMethod target2 =
        prepareTarget(
            methodSigFromComponents(TEST_TARGET_CLASS + ""$Inner"", ""void"", ""<init>""),
            TEST_TARGET_CLASS + ""$Inner"");
    assertEquals(2, Scene.v().getApplicationClasses().size());
    assertTrue(target2.getDeclaringClass().hasOuterClass());
    assertTrue(target2.getDeclaringClass().isInnerClass());
    InnerClassTag tag2 = (InnerClassTag) target2.getDeclaringClass().getTag(InnerClassTag.NAME);
    assertNotNull(tag2);
    assertEquals(""soot/asm/ScopeFinderTarget$Inner"", tag2.getInnerClass());
    assertEquals(""soot/asm/ScopeFinderTarget"", tag2.getOuterClass());
    assertTrue(Modifier.isStatic(tag2.getAccessFlags()));
  }
",non-flaky,5
156096,soot-oss_soot,AsmInnerClassTest.InnerStaticInner,"  @Test
  public void InnerStaticInner() {
    SootMethod target3 =
        prepareTarget(
            methodSigFromComponents(TEST_TARGET_CLASS + ""$Inner$InnerInner"", ""void"", ""method""),
            TEST_TARGET_CLASS + ""$Inner$InnerInner"");
    // one dummy
    assertEquals(2, Scene.v().getApplicationClasses().size());
    assertTrue(target3.getDeclaringClass().hasOuterClass());
    assertTrue(target3.getDeclaringClass().isInnerClass());
    InnerClassTag innerClassTag = null;
    for (Tag tag : target3.getDeclaringClass().getTags()) {
      // FIXME: we have multiple innerclasstags? for a parent it makes sense but for a child class?
      if (tag instanceof InnerClassTag) {
        boolean inner =
            ((InnerClassTag) tag)
                .getInnerClass()
                .equals(""soot/asm/ScopeFinderTarget$Inner$InnerInner"");
        if (inner) {
          innerClassTag = (InnerClassTag) tag;
          break;
        }
      }
    }
    assertNotNull(innerClassTag);
    assertEquals(""soot/asm/ScopeFinderTarget$Inner$InnerInner"", innerClassTag.getInnerClass());
    assertEquals(""soot/asm/ScopeFinderTarget$Inner"", innerClassTag.getOuterClass());
    assertFalse(Modifier.isStatic(innerClassTag.getAccessFlags()));
  }
",non-flaky,5
156097,soot-oss_soot,AsmMethodSourceOrigNamesTest.testWriterToUTF8Buffered1,"  @Test
  public void testWriterToUTF8Buffered1() {
    final String clazz = ""org.apache.xml.serializer.WriterToUTF8Buffered"";
    final String[] params = { ""char[]"", ""int"", ""int"" };
    runXalanTest(prepareTarget(methodSigFromComponents(clazz, ""void"", ""write"", params), clazz));
  }
",non-flaky,5
156098,soot-oss_soot,AsmMethodSourceOrigNamesTest.testWriterToUTF8Buffered2,"  @Test
  public void testWriterToUTF8Buffered2() {
    final String clazz = ""org.apache.xml.serializer.WriterToUTF8Buffered"";
    final String[] params = { ""java.lang.String"" };
    runXalanTest(prepareTarget(methodSigFromComponents(clazz, ""void"", ""write"", params), clazz));
  }
",non-flaky,5
156099,soot-oss_soot,AsmMethodSourceOrigNamesTest.testElemApplyTemplates,"  @Test
  public void testElemApplyTemplates() {
    final String clazz = ""org.apache.xalan.templates.ElemApplyTemplates"";
    final String[] params = { ""org.apache.xalan.transformer.TransformerImpl"" };
    runXalanTest(prepareTarget(methodSigFromComponents(clazz, ""void"", ""transformSelectedNodes"", params), clazz));
  }
",non-flaky,5
156100,soot-oss_soot,AsmMethodSourceOrigNamesTest.testXNodeSet,"  @Test
  public void testXNodeSet() {
    final String clazz = ""org.apache.xpath.objects.XNodeSet"";
    final String[] params = { ""org.apache.xpath.objects.XObject"", ""org.apache.xpath.objects.Comparator"" };
    runXalanTest(prepareTarget(methodSigFromComponents(clazz, ""boolean"", ""compare"", params), clazz));
  }
",non-flaky,5
156101,soot-oss_soot,SilsTest.testSilsEnabled,"  @Test
  public void testSilsEnabled() {
    final String className = ""soot.asm.LocalNaming"";
    final String[] params = {};
    SootMethod target = prepareTarget(methodSigFromComponents(className, ""void"", ""test"", params), className);
    Body body = target.retrieveActiveBody();
    Set<String> localNames = body.getLocals().stream().map(Local::getName).collect(Collectors.toSet());
    // test if all expected Local names are present
    // currently d, f are not preserved.
    Assert.assertTrue(localNames.contains(""d""));
    Assert.assertTrue(localNames.contains(""f""));
    Assert.assertTrue(localNames.contains(""arr""));
  }
",non-flaky,5
156102,soot-oss_soot,ResolveFieldInitializersTest.initializedInMethodRef,"  @Test
  public void initializedInMethodRef() {
    prepareTarget(methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""<init>""), TEST_TARGET_CLASS);
    SootClass sootClass = Scene.v().getSootClass(""java.util.ArrayDeque"");
    assertEquals(SootClass.SIGNATURES, sootClass.resolvingLevel());
  }
",non-flaky,5
156103,soot-oss_soot,ResolveFieldInitializersTest.initializedInConstructor,"  @Test
  public void initializedInConstructor() {
    prepareTarget(methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""<init>""), TEST_TARGET_CLASS);
    SootClass sootClass = Scene.v().getSootClass(""java.util.LinkedList"");
    assertEquals(SootClass.SIGNATURES, sootClass.resolvingLevel());
  }
",non-flaky,5
156104,soot-oss_soot,DefaultInterfaceTest.SubClassTest,"  @Test
  public void SubClassTest() throws FileNotFoundException, UnsupportedEncodingException {		
	  
	  String testClass = ""soot.defaultInterfaceMethods.JavaNCSSCheck"";
	  String abstractClass = ""soot.defaultInterfaceDifferentPackage.AbstractCheck"";
	  String classToAnalyze = ""soot.defaultInterfaceDifferentPackage.AbstractCheck"";
	  final SootMethod target =
		        prepareTarget(
		            methodSigFromComponents(testClass, voidType, mainClass),
		            testClass,
		            classToAnalyze);
		
		ArrayList<Edge> edges = GetCallGraph();
		
		assertEquals(edges.get(0).getTgt(), Scene.v().getMethod(""<soot.defaultInterfaceDifferentPackage.AbstractCheck: void log(java.lang.String,java.lang.String)>""));		
		
	}
",non-flaky,5
156105,soot-oss_soot,DefaultInterfaceTest.simpleDefaultInterfaceTest,"  @Test
  public void simpleDefaultInterfaceTest() {

    String testClass = ""soot.defaultInterfaceMethods.SimpleDefaultInterface"";
    String defaultClass = ""soot.defaultInterfaceMethods.Default"";
    String classToAnalyze = ""soot.defaultInterfaceMethods.Default"";

    final SootMethod target =
        prepareTarget(
            methodSigFromComponents(testClass, voidType, mainClass),
            testClass,
            classToAnalyze);

    SootMethod defaultMethod =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.Default: void target()>"");
    Body body = target.retrieveActiveBody();
    SootMethod targetMethod = resolveMethodRefInBody(body.getUnits(), ""void target()"");
    SootMethod resolvedMethod =
        VirtualCalls.v()
            .resolveNonSpecial(Scene.v().getRefType(testClass), defaultMethod.makeRef(), false);
    SootMethod concreteImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveConcreteDispatch(Scene.v().getSootClass(testClass), defaultMethod);
    SootMethod concreteImplViaResolveMethod =
        Scene.v()
            .getFastHierarchy()
            .resolveMethod(Scene.v().getSootClass(testClass), defaultMethod, false);
    Set<SootMethod> abstractImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(Scene.v().getSootClass(defaultClass), defaultMethod);

    boolean edgePresent = checkInEdges(defaultMethod, target);
    final ReachableMethods reachableMethods = Scene.v().getReachableMethods();
    /* Arguments for assert function */

    assertEquals(defaultMethod, resolvedMethod);
    assertEquals(defaultMethod, targetMethod);
    assertEquals(defaultMethod.getName(), ""target"");
    assertNotNull(defaultMethod);
    assertTrue(reachableMethods.contains(defaultMethod));
    assertTrue(edgePresent);
    assertEquals(defaultMethod, concreteImpl);
    assertEquals(concreteImpl, concreteImplViaResolveMethod);
    assertTrue(
        abstractImpl.contains(
            Scene.v().getMethod(""<soot.defaultInterfaceMethods.Default: void target()>"")));
  }
",non-flaky,5
156106,soot-oss_soot,DefaultInterfaceTest.interfaceSameSignatureTest,"  @Test
  public void interfaceSameSignatureTest() {
    String testClassSig = ""soot.defaultInterfaceMethods.InterfaceSameSignature"";
    String interfaceReadSig = ""soot.defaultInterfaceMethods.Read"";
    String interfaceWriteSig = ""soot.defaultInterfaceMethods.Write"";    

    final SootMethod target =
        prepareTarget(
            methodSigFromComponents(testClassSig, voidType, mainClass),
            testClassSig,
            interfaceReadSig,
            interfaceWriteSig);

    SootClass testClass = Scene.v().getSootClass(testClassSig);
    SootClass interfaceRead = Scene.v().getSootClass(interfaceReadSig);
    SootClass interfaceWrite = Scene.v().getSootClass(interfaceWriteSig);

    SootMethod mainPrintMethod =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.InterfaceSameSignature: void print()>"");
    SootMethod readInterfacePrint =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.Read: void print()>"");
    SootMethod writeInterfacePrint =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.Write: void print()>"");
    SootMethod readInterfaceRead =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.Read: void read()>"");
    SootMethod writeInterfaceWrite =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.Write: void write()>"");

    Body mainBody = target.retrieveActiveBody();
    Body mainPrintBody = mainPrintMethod.retrieveActiveBody();

    SootMethod refMainMethod = resolveMethodRefInBody(mainBody.getUnits(), ""void print()"");
    SootMethod refWritePrintMethod =
        resolveMethodRefInBody(
            mainPrintBody.getUnits(), ""soot.defaultInterfaceMethods.Write: void print()"");
    SootMethod refReadPrintMethod =
        resolveMethodRefInBody(
            mainPrintBody.getUnits(), ""soot.defaultInterfaceMethods.Read: void print()"");
    SootMethod refDefaultRead = resolveMethodRefInBody(mainBody.getUnits(), ""void read()"");
    SootMethod refDefaultWrite = resolveMethodRefInBody(mainBody.getUnits(), ""void write()"");

    SootMethod resolvedMainMethod =
        VirtualCalls.v()
            .resolveNonSpecial(
                Scene.v().getRefType(testClassSig), mainPrintMethod.makeRef(), false);
    SootMethod resolvedWritePrintMethod =
        VirtualCalls.v()
            .resolveNonSpecial(
                Scene.v().getRefType(testClassSig), writeInterfacePrint.makeRef(), false);
    SootMethod resolvedReadPrintMethod =
        VirtualCalls.v()
            .resolveNonSpecial(
                Scene.v().getRefType(testClassSig), readInterfacePrint.makeRef(), false);
    SootMethod resolvedDefaultReadMethod =
        VirtualCalls.v()
            .resolveNonSpecial(
                Scene.v().getRefType(testClassSig), readInterfaceRead.makeRef(), false);
    SootMethod resolvedDefaultWriteMethod =
        VirtualCalls.v()
            .resolveNonSpecial(
                Scene.v().getRefType(testClassSig), writeInterfaceWrite.makeRef(), false);

    FastHierarchy fh = Scene.v().getFastHierarchy();
    SootMethod concreteImplMainPrint = fh.resolveConcreteDispatch(testClass, mainPrintMethod);
    SootMethod concreteImplWritePrint = fh.resolveConcreteDispatch(testClass, refWritePrintMethod);
    SootMethod concreteImplReadPrint = fh.resolveConcreteDispatch(testClass, refReadPrintMethod);
    SootMethod concreteImplDefaultRead = fh.resolveConcreteDispatch(testClass, refDefaultRead);
    SootMethod concreteImplDefaultWrite = fh.resolveConcreteDispatch(testClass, refDefaultWrite);

    assertEquals(
        Sets.newHashSet(readInterfaceRead),
        fh.resolveAbstractDispatch(interfaceRead, refDefaultRead));
    assertEquals(
        Sets.newHashSet(writeInterfaceWrite),
        fh.resolveAbstractDispatch(interfaceWrite, refDefaultWrite));
    assertEquals(
        Sets.newHashSet(mainPrintMethod),
        fh.resolveAbstractDispatch(interfaceRead, refReadPrintMethod));
    assertEquals(
        Sets.newHashSet(mainPrintMethod),
        fh.resolveAbstractDispatch(interfaceWrite, refWritePrintMethod));

    /* Edges should be present */
    boolean edgeMainPrintToReadPrint = checkInEdges(readInterfacePrint, mainPrintMethod);
    boolean edgeMainPrintToWritePrint = checkInEdges(writeInterfacePrint, mainPrintMethod);
    boolean edgeMainMethodToPrint = checkInEdges(mainPrintMethod, target);
    boolean edgeMainMethodToReadMethod = checkInEdges(readInterfaceRead, target);
    boolean edgeMainMethodToWriteMethod = checkInEdges(writeInterfaceWrite, target);

    /* Edges should not be present */
    boolean edgeMainMethodToReadPrint = checkInEdges(readInterfacePrint, target);
    boolean edgeMainMethodToWritePrint = checkInEdges(writeInterfacePrint, target);

    final ReachableMethods reachableMethods = Scene.v().getReachableMethods();

    /* Arguments for assert function */
    Map<SootMethod, String> targetMethods =
        new HashMap<SootMethod, String>() {
          {
            put(mainPrintMethod, ""print"");
            put(readInterfacePrint, ""print"");
            put(writeInterfacePrint, ""print"");
            put(readInterfaceRead, ""read"");
            put(writeInterfaceWrite, ""write"");
          }
        };

    Map<SootMethod, SootMethod> resolvedMethods =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(mainPrintMethod, resolvedMainMethod);
            put(mainPrintMethod, resolvedWritePrintMethod);
            put(mainPrintMethod, resolvedReadPrintMethod);
            put(readInterfaceRead, resolvedDefaultReadMethod);
            put(writeInterfaceWrite, resolvedDefaultWriteMethod);
          }
        };

    Map<SootMethod, SootMethod> methodRef =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(mainPrintMethod, refMainMethod);
            put(writeInterfacePrint, refWritePrintMethod);
            put(readInterfacePrint, refReadPrintMethod);
            put(readInterfaceRead, refDefaultRead);
            put(writeInterfaceWrite, refDefaultWrite);
          }
        };

    Map<SootMethod, SootMethod> concreteImpl =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(mainPrintMethod, concreteImplMainPrint);
            put(mainPrintMethod, concreteImplWritePrint);
            put(mainPrintMethod, concreteImplReadPrint);
            put(readInterfaceRead, concreteImplDefaultRead);
            put(writeInterfaceWrite, concreteImplDefaultWrite);
          }
        };

    ArrayList<Boolean> edgePresent =
        new ArrayList<Boolean>() {
          {
            add(edgeMainPrintToReadPrint);
            add(edgeMainPrintToWritePrint);
            add(edgeMainMethodToPrint);
            add(edgeMainMethodToReadMethod);
            add(edgeMainMethodToWriteMethod);
          }
        };

    ArrayList<Boolean> edgeNotPresent =
        new ArrayList<Boolean>() {
          {
            add(edgeMainMethodToReadPrint);
            add(edgeMainMethodToWritePrint);
          }
        };

    for (Map.Entry<SootMethod, String> targetMethod : targetMethods.entrySet()) {
      assertNotNull(targetMethod.getKey());
    }
    for (Map.Entry<SootMethod, SootMethod> virtualResolvedMethod : resolvedMethods.entrySet()) {
      assertEquals(virtualResolvedMethod.getKey(), virtualResolvedMethod.getValue());
    }
    for (Map.Entry<SootMethod, SootMethod> methodRef1 : methodRef.entrySet()) {
      assertEquals(methodRef1.getKey(), methodRef1.getValue());
    }
    for (Map.Entry<SootMethod, String> targetMethod : targetMethods.entrySet()) {
      assertEquals(targetMethod.getKey().getName(), targetMethod.getValue());
    }
    for (Map.Entry<SootMethod, String> targetMethod : targetMethods.entrySet()) {
      assertTrue(reachableMethods.contains(targetMethod.getKey()));
    }
    for (boolean isPresent : edgePresent) {
      assertTrue(isPresent);
    }
    for (boolean notPresent : edgeNotPresent) {
      assertFalse(notPresent);
    }
    for (Map.Entry<SootMethod, SootMethod> concreteImpl1 : concreteImpl.entrySet()) {
      assertEquals(concreteImpl1.getKey(), concreteImpl1.getValue());
    }
  }
",non-flaky,5
156107,soot-oss_soot,DefaultInterfaceTest.classInterfaceWithSameSignatureTest,"  @Test
  public void classInterfaceWithSameSignatureTest() {
    String testClass = ""soot.defaultInterfaceMethods.ClassInterfaceSameSignature"";
    String defaultClass = ""soot.defaultInterfaceMethods.HelloWorld"";

    final SootMethod target =
        prepareTarget(
            methodSigFromComponents(testClass, voidType, mainClass),
            testClass,
            defaultClass);

    SootMethod mainPrintMethod =
        Scene.v()
            .getMethod(""<soot.defaultInterfaceMethods.ClassInterfaceSameSignature: void print()>"");
    SootMethod defaultPrintMethod =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.HelloWorld: void print()>"");

    Body mainBody = target.retrieveActiveBody();
    SootMethod refMainMethod = resolveMethodRefInBody(mainBody.getUnits(), ""void print()"");
    SootMethod resolvedMethod =
        VirtualCalls.v()
            .resolveNonSpecial(
                Scene.v().getRefType(testClass), defaultPrintMethod.makeRef(), false);
    SootMethod concreteImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveConcreteDispatch(Scene.v().getSootClass(testClass), defaultPrintMethod);
    Set<SootMethod> abstractImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(Scene.v().getSootClass(defaultClass), defaultPrintMethod);
    boolean edgeMainMethodToMainPrint = checkInEdges(mainPrintMethod, target);
    boolean edgeMainPrintToDefaultPrint = checkInEdges(defaultPrintMethod, target);
    final ReachableMethods reachableMethods = Scene.v().getReachableMethods();

    Map<SootMethod, String> targetMethods =
        new HashMap<SootMethod, String>() {
          {
            put(mainPrintMethod, ""print"");
            put(defaultPrintMethod, ""print"");
          }
        };

    ArrayList<Boolean> edgePresent =
        new ArrayList<Boolean>() {
          {
            add(edgeMainMethodToMainPrint);
          }
        };

    for (Map.Entry<SootMethod, String> targetMethod : targetMethods.entrySet()) {
      assertNotNull(targetMethod.getKey());
    }
    assertEquals(mainPrintMethod, resolvedMethod);
    assertEquals(mainPrintMethod, refMainMethod);
    for (Map.Entry<SootMethod, String> targetMethod : targetMethods.entrySet()) {
      assertEquals(targetMethod.getKey().getName(), targetMethod.getValue());
    }

    assertTrue(reachableMethods.contains(mainPrintMethod));

    for (boolean isPresent : edgePresent) {
      assertTrue(isPresent);
    }
    assertEquals(mainPrintMethod, concreteImpl);
    assertTrue(
        abstractImpl.contains(
            Scene.v()
                .getMethod(
                    ""<soot.defaultInterfaceMethods.ClassInterfaceSameSignature: void print()>"")));
  }
",non-flaky,5
156108,soot-oss_soot,DefaultInterfaceTest.superClassInterfaceWithSameSignatureTest,"  @Test
  public void superClassInterfaceWithSameSignatureTest() {
    String testClass = ""soot.defaultInterfaceMethods.SuperClassInterfaceSameSignature"";
    String defaultClass = ""soot.defaultInterfaceMethods.PrintInterface"";
    String defaultSuperClass = ""soot.defaultInterfaceMethods.DefaultPrint"";
    String superClassImplementsInterface = ""soot.defaultInterfaceMethods.SuperClassImplementsInterface"";

    final SootMethod target =
        prepareTarget(
            methodSigFromComponents(testClass, voidType, mainClass),
            testClass,
            defaultClass,
            superClassImplementsInterface);

    SootMethod defaultSuperMainMethod =
        Scene.v()
            .getMethod(""<soot.defaultInterfaceMethods.SuperClassImplementsInterface: void main()>"");
    SootMethod mainMethod =
        Scene.v()
            .getMethod(
                ""<soot.defaultInterfaceMethods.SuperClassImplementsInterface: void print()>"");
    SootMethod defaultMethod =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.PrintInterface: void print()>"");
    SootMethod defaultSuperClassMethod =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.DefaultPrint: void print()>"");

    Body mainBody = target.retrieveActiveBody();
    SootMethod refMainMethod = resolveMethodRefInBody(mainBody.getUnits(), ""void print()"");

    SootMethod resolvedMethod =
        VirtualCalls.v()
            .resolveNonSpecial(Scene.v().getRefType(testClass), defaultMethod.makeRef(), false);
    SootMethod resolvedSuperClassDefaultMethod =
        VirtualCalls.v()
            .resolveNonSpecial(
                Scene.v().getRefType(testClass), defaultSuperClassMethod.makeRef(), false);

    SootMethod concreteImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveConcreteDispatch(Scene.v().getSootClass(testClass), defaultMethod);

    Set<SootMethod> abstractImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(Scene.v().getSootClass(defaultClass), defaultMethod);
    Set<SootMethod> abstractImplSuperClass =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(
                Scene.v().getSootClass(defaultSuperClass), defaultSuperClassMethod);

    boolean edgeMainToSuperClassPrint = checkInEdges(mainMethod, target);
    boolean edgeMainToDefaultPrint = checkInEdges(defaultMethod, target);
    boolean edgeMainToSuperDefaultPrint = checkInEdges(defaultSuperClassMethod, target);
    boolean edgeSuperMainToSuperPrint =
        checkInEdges(defaultSuperClassMethod, defaultSuperMainMethod);

    final ReachableMethods reachableMethods = Scene.v().getReachableMethods();

    List<SootMethod> targetMethods =
        new ArrayList<SootMethod>() {
          {
            add(mainMethod);
            add(defaultMethod);
            add(defaultSuperClassMethod);
          }
        };

    ArrayList<Boolean> edgeNotPresent =
        new ArrayList<Boolean>() {
          {
            add(edgeMainToDefaultPrint);
            add(edgeMainToSuperDefaultPrint);
            add(edgeSuperMainToSuperPrint);
          }
        };

    Map<SootMethod, SootMethod> resolvedMethods =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(mainMethod, resolvedMethod);
            put(resolvedSuperClassDefaultMethod, resolvedMethod);
          }
        };

    for (SootMethod targetMethod : targetMethods) {
      assertNotNull(targetMethod);
    }
    assertEquals(targetMethods.get(0), refMainMethod);
    assertEquals(targetMethods.get(0).getName(), ""print"");
    assertTrue(edgeMainToSuperClassPrint);
    for (boolean notPresent : edgeNotPresent) {
      assertFalse(notPresent);
    }
    assertEquals(targetMethods.get(0), concreteImpl);
    assertNotEquals(targetMethods.get(1), concreteImpl);
    assertTrue(
        abstractImpl.contains(
            Scene.v()
                .getMethod(
                    ""<soot.defaultInterfaceMethods.SuperClassImplementsInterface: void print()>"")));
    assertTrue(
        abstractImplSuperClass.contains(
            Scene.v()
                .getMethod(
                    ""<soot.defaultInterfaceMethods.SuperClassImplementsInterface: void print()>"")));
  }
",non-flaky,5
156109,soot-oss_soot,DefaultInterfaceTest.derivedInterfacesTest,"  @Test
  public void derivedInterfacesTest() {
    String testClassSig = ""soot.defaultInterfaceMethods.DerivedInterfaces"";
    String defaultInterfaceOneSig = ""soot.defaultInterfaceMethods.InterfaceTestOne"";
    String defaultInterfaceTwoSig = ""soot.defaultInterfaceMethods.InterfaceTestTwo"";

    final SootMethod target =
        prepareTarget(
            methodSigFromComponents(testClassSig, voidType, mainClass),
            testClassSig,
            defaultInterfaceOneSig,
            defaultInterfaceTwoSig);

    FastHierarchy fh = Scene.v().getFastHierarchy();
    SootClass testClass = Scene.v().getSootClass(testClassSig);
    SootClass defaultInterfaceOne = Scene.v().getSootClass(defaultInterfaceOneSig);
    SootClass defaultInterfaceTwo = Scene.v().getSootClass(defaultInterfaceTwoSig);

    SootMethod interfaceOnePrint =
        Scene.v().getMethod(methodSigFromComponents(defaultInterfaceOneSig, ""void print()""));
    SootMethod interfaceTwoPrint =
        Scene.v().getMethod(methodSigFromComponents(defaultInterfaceTwoSig, ""void print()""));

    SootMethod refMainMethod =
        resolveMethodRefInBody(target.retrieveActiveBody().getUnits(), ""void print()"");

    SootMethod interfaceOneResolvedMethod =
        VirtualCalls.v().resolveNonSpecial(testClass.getType(), interfaceOnePrint.makeRef(), false);
    SootMethod interfaceTwoResolvedMethod =
        VirtualCalls.v().resolveNonSpecial(testClass.getType(), interfaceTwoPrint.makeRef(), false);

    SootMethod concreteImplInterfaceOne = fh.resolveConcreteDispatch(testClass, interfaceOnePrint);
    SootMethod concreteImplInterfaceTwo = fh.resolveConcreteDispatch(testClass, interfaceTwoPrint);

    Set<SootMethod> abstractImplInterfaceOne =
        fh.resolveAbstractDispatch(defaultInterfaceOne, interfaceOnePrint);

    boolean edgeMainToInterfaceTwoPrint = checkInEdges(interfaceTwoPrint, target);
    boolean edgeMainToInterfaceOnePrint = checkInEdges(interfaceOnePrint, target);

    final ReachableMethods reachableMethods = Scene.v().getReachableMethods();

    List<SootMethod> targetMethods =
        new ArrayList<SootMethod>() {
          {
            add(interfaceOnePrint);
            add(interfaceTwoPrint);
          }
        };

    Map<SootMethod, SootMethod> resolvedMethods =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(interfaceTwoPrint, interfaceOneResolvedMethod);
            put(interfaceTwoPrint, interfaceTwoResolvedMethod);
          }
        };

    Map<SootMethod, SootMethod> concreteImplTrue =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(interfaceTwoPrint, concreteImplInterfaceOne);
            put(interfaceTwoPrint, concreteImplInterfaceTwo);
          }
        };

    Map<SootMethod, SootMethod> concreteImplNotTrue =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(interfaceOnePrint, concreteImplInterfaceOne);
            put(interfaceOnePrint, concreteImplInterfaceTwo);
          }
        };

    for (SootMethod targetMethod : targetMethods) {
      Assert.assertNotNull(targetMethod);
    }
    assertEquals(targetMethods.get(1), refMainMethod);
    assertEquals(targetMethods.get(1).getName(), ""print"");
    assertFalse(edgeMainToInterfaceOnePrint);
    assertTrue(edgeMainToInterfaceTwoPrint);
    assertTrue(reachableMethods.contains(targetMethods.get(1)));
    assertFalse(reachableMethods.contains(targetMethods.get(0)));
    for (Map.Entry<SootMethod, SootMethod> virtualResolvedMethod : resolvedMethods.entrySet()) {
      assertEquals(virtualResolvedMethod.getKey(), virtualResolvedMethod.getValue());
    }
    for (Map.Entry<SootMethod, SootMethod> concreteImpl : concreteImplTrue.entrySet()) {
      assertEquals(concreteImpl.getKey(), concreteImpl.getValue());
    }
    for (Map.Entry<SootMethod, SootMethod> concreteImpl : concreteImplNotTrue.entrySet()) {
      assertNotEquals(concreteImpl.getKey(), concreteImpl.getValue());
    }
    assertEquals(Sets.newHashSet(targetMethods.get(1)), abstractImplInterfaceOne);

    assertEquals(
        Sets.newHashSet(targetMethods.get(1)),
        fh.resolveAbstractDispatch(defaultInterfaceTwo, interfaceTwoPrint));
  }
",non-flaky,5
156110,soot-oss_soot,DefaultInterfaceTest.interfaceInheritanceTest,"  @Test
  public void interfaceInheritanceTest() {
    String testClass = ""soot.defaultInterfaceMethods.InterfaceInheritance"";
    String defaultClass = ""soot.defaultInterfaceMethods.InterfaceTestA"";
    String defaultInterface = ""soot.defaultInterfaceMethods.InterfaceTestB"";

    final SootMethod target =
        prepareTarget(
            methodSigFromComponents(testClass, voidType, mainClass),
            testClass,
            defaultClass,
            defaultInterface);

    SootMethod interfaceTestAPrint =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.InterfaceTestA: void print()>"");
    SootMethod mainPrintMessageMethod =
        Scene.v()
            .getMethod(""<soot.defaultInterfaceMethods.InterfaceInheritance: void printMessage()>"");
    Body mainBody = target.retrieveActiveBody();
    SootMethod refMainMethod = resolveMethodRefInBody(mainBody.getUnits(), ""void print()"");
    SootMethod resolvedMethod =
        VirtualCalls.v()
            .resolveNonSpecial(
                Scene.v().getRefType(testClass), interfaceTestAPrint.makeRef(), false);
    SootMethod concreteImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveConcreteDispatch(Scene.v().getSootClass(testClass), interfaceTestAPrint);
    Set<SootMethod> abstractImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(Scene.v().getSootClass(defaultClass), interfaceTestAPrint);

    boolean edgeMainToInterfaceTestAPrint = checkInEdges(interfaceTestAPrint, target);
    boolean edgeMainToMainPrintMessage = checkInEdges(mainPrintMessageMethod, target);
    final ReachableMethods reachableMethods = Scene.v().getReachableMethods();

    List<SootMethod> targetMethods =
        new ArrayList<SootMethod>() {
          {
            add(interfaceTestAPrint);
            add(mainPrintMessageMethod);
          }
        };

    for (SootMethod targetMethod : targetMethods) {
      Assert.assertNotNull(targetMethod);
    }
    assertEquals(targetMethods.get(0), refMainMethod);
    assertEquals(targetMethods.get(0).getName(), ""print"");
    assertTrue(edgeMainToInterfaceTestAPrint);
    assertFalse(edgeMainToMainPrintMessage);
    assertTrue(reachableMethods.contains(targetMethods.get(0)));
    assertFalse(reachableMethods.contains(targetMethods.get(1)));
    assertEquals(targetMethods.get(0), resolvedMethod);
    assertEquals(targetMethods.get(0), concreteImpl);
    assertTrue(abstractImpl.contains(targetMethods.get(0)));
  }
",non-flaky,5
156111,soot-oss_soot,DefaultInterfaceTest.interfaceReAbstractionTest,"  @Test
  public void interfaceReAbstractionTest() {
    String testClass = ""soot.defaultInterfaceMethods.InterfaceReAbstracting"";
    String defaultClass = ""soot.defaultInterfaceMethods.InterfaceA"";
    String defaultInterface = ""soot.defaultInterfaceMethods.InterfaceB"";

    final SootMethod target =
        prepareTarget(
            methodSigFromComponents(testClass, ""void"", ""main""),
            testClass,
            defaultClass,
            defaultInterface);

    SootMethod interfaceAPrint =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.InterfaceA: void print()>"");
    SootMethod mainMethodPrint =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.InterfaceReAbstracting: void print()>"");

    Body mainBody = target.retrieveActiveBody();
    SootMethod refMainMethod = resolveMethodRefInBody(mainBody.getUnits(), ""void print()"");
    SootMethod resolvedMethod =
        VirtualCalls.v()
            .resolveNonSpecial(Scene.v().getRefType(testClass), interfaceAPrint.makeRef(), false);
    SootMethod concreteImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveConcreteDispatch(Scene.v().getSootClass(testClass), interfaceAPrint);
    Set<SootMethod> abstractImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(Scene.v().getSootClass(defaultClass), interfaceAPrint);

    boolean edgeMainMethodToMainPrint = checkInEdges(mainMethodPrint, target);
    boolean edgeMainMethodToInterfaceAPrint = checkInEdges(interfaceAPrint, target);
    final ReachableMethods reachableMethods = Scene.v().getReachableMethods();

    List<SootMethod> targetMethods =
        new ArrayList<SootMethod>() {
          {
            add(mainMethodPrint);
            add(interfaceAPrint);
          }
        };

    for (SootMethod targetMethod : targetMethods) {
      Assert.assertNotNull(targetMethod);
    }
    assertEquals(targetMethods.get(0), refMainMethod);
    assertEquals(targetMethods.get(0).getName(), ""print"");
    assertTrue(edgeMainMethodToMainPrint);
    assertFalse(edgeMainMethodToInterfaceAPrint);
    assertTrue(reachableMethods.contains(targetMethods.get(0)));
    assertFalse(reachableMethods.contains(targetMethods.get(1)));
    assertEquals(targetMethods.get(0), resolvedMethod);
    assertEquals(targetMethods.get(0), concreteImpl);
    assertNotEquals(targetMethods.get(1), concreteImpl);
    assertEquals(
        Sets.newHashSet(
            Scene.v()
                .getMethod(""<soot.defaultInterfaceMethods.InterfaceReAbstracting: void print()>"")),
        abstractImpl);
  }
",non-flaky,5
156112,soot-oss_soot,DefaultInterfaceTest.superClassPreferenceOverDefaultMethodTest,"  @Test
  public void superClassPreferenceOverDefaultMethodTest() {
    String testClass = ""soot.defaultInterfaceMethods.SuperClassPreferenceOverDefaultMethod"";
    String defaultInterfaceOne = ""soot.defaultInterfaceMethods.InterfaceOne"";
    String defaultInterfaceTwo = ""soot.defaultInterfaceMethods.InterfaceTwo"";
    String defaultSuperClass = ""soot.defaultInterfaceMethods.SuperClass"";

    final SootMethod target =
        prepareTarget(
            methodSigFromComponents(testClass, voidType, mainClass),
            testClass,
            defaultInterfaceOne,
            defaultInterfaceTwo,
            defaultSuperClass);

    SootMethod interfaceOnePrint =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.InterfaceOne: void print()>"");
    SootMethod interfaceTwoPrint =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.InterfaceTwo: void print()>"");
    SootMethod superClassPrint =
        Scene.v().getMethod(""<soot.defaultInterfaceMethods.SuperClass: void print()>"");

    Body mainBody = target.retrieveActiveBody();
    SootMethod refMainMethod = resolveMethodRefInBody(mainBody.getUnits(), ""void print()"");

    SootMethod resolvedInterfaceOneDefaultMethod =
        VirtualCalls.v()
            .resolveNonSpecial(Scene.v().getRefType(testClass), interfaceOnePrint.makeRef(), false);
    SootMethod resolvedInterfaceTwoDefaultMethod =
        VirtualCalls.v()
            .resolveNonSpecial(Scene.v().getRefType(testClass), interfaceTwoPrint.makeRef(), false);

    SootMethod concreteImplInterfaceOne =
        Scene.v()
            .getFastHierarchy()
            .resolveConcreteDispatch(Scene.v().getSootClass(testClass), interfaceOnePrint);
    SootMethod concreteImplInterfaceTwo =
        Scene.v()
            .getFastHierarchy()
            .resolveConcreteDispatch(Scene.v().getSootClass(testClass), interfaceTwoPrint);

    Set<SootMethod> abstractImplInterfaceOne =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(
                Scene.v().getSootClass(defaultInterfaceOne), interfaceOnePrint);
    Set<SootMethod> abstractImplInterfaceTwo =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(
                Scene.v().getSootClass(defaultInterfaceTwo), interfaceTwoPrint);

    boolean edgeMainToInterfaceOnePrint = checkInEdges(interfaceOnePrint, target);
    boolean edgeMainToInterfaceTwoPrint = checkInEdges(interfaceTwoPrint, target);
    boolean edgeMainToSuperClassPrint = checkInEdges(superClassPrint, target);

    final ReachableMethods reachableMethods = Scene.v().getReachableMethods();

    List<SootMethod> targetMethods =
        new ArrayList<SootMethod>() {
          {
            add(superClassPrint);
            add(interfaceOnePrint);
            add(interfaceTwoPrint);
          }
        };

    ArrayList<Boolean> edgeNotPresent =
        new ArrayList<Boolean>() {
          {
            add(edgeMainToInterfaceOnePrint);
            add(edgeMainToInterfaceTwoPrint);
          }
        };

    Map<SootMethod, SootMethod> resolvedMethods =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(superClassPrint, resolvedInterfaceOneDefaultMethod);
            put(superClassPrint, resolvedInterfaceTwoDefaultMethod);
          }
        };

    Map<SootMethod, SootMethod> concreteImplTrue =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(superClassPrint, concreteImplInterfaceOne);
            put(superClassPrint, concreteImplInterfaceTwo);
          }
        };

    Map<SootMethod, SootMethod> concreteImplNotTrue =
        new HashMap<SootMethod, SootMethod>() {
          {
            put(interfaceOnePrint, concreteImplInterfaceOne);
            put(interfaceOnePrint, concreteImplInterfaceTwo);
          }
        };

    for (SootMethod targetMethod : targetMethods) {
      assertNotNull(targetMethod);
    }
    assertEquals(targetMethods.get(0), refMainMethod);
    assertEquals(targetMethods.get(0).getName(), ""print"");
    assertTrue(edgeMainToSuperClassPrint);
    for (boolean notPresent : edgeNotPresent) {
      assertFalse(notPresent);
    }
    assertTrue(reachableMethods.contains(targetMethods.get(0)));
    assertFalse(reachableMethods.contains(targetMethods.get(1)));
    assertFalse(reachableMethods.contains(targetMethods.get(2)));
    for (Map.Entry<SootMethod, SootMethod> virtualResolvedMethod : resolvedMethods.entrySet()) {
      assertEquals(virtualResolvedMethod.getKey(), virtualResolvedMethod.getValue());
    }
    for (Map.Entry<SootMethod, SootMethod> concreteImpl : concreteImplTrue.entrySet()) {
      assertEquals(concreteImpl.getKey(), concreteImpl.getValue());
    }
    for (Map.Entry<SootMethod, SootMethod> concreteImpl : concreteImplNotTrue.entrySet()) {
      assertNotEquals(concreteImpl.getKey(), concreteImpl.getValue());
    }
    assertTrue(
        abstractImplInterfaceOne.contains(
            Scene.v().getMethod(""<soot.defaultInterfaceMethods.SuperClass: void print()>"")));
    assertTrue(
        abstractImplInterfaceTwo.contains(
            Scene.v().getMethod(""<soot.defaultInterfaceMethods.SuperClass: void print()>"")));
  }
",non-flaky,5
156113,soot-oss_soot,DefaultInterfaceTest.maximallySpecificSuperInterface,"  @Test
  public void maximallySpecificSuperInterface() {
    String targetClassName = ""soot.defaultInterfaceMethods.MaximallySpecificSuperInterface"";
    String superClass = ""soot.defaultInterfaceMethods.B"";
    String subInterface = ""soot.defaultInterfaceMethods.C"";
    String superInterface = ""soot.defaultInterfaceMethods.D"";

    final SootMethod mainMethod =
        prepareTarget(
            methodSigFromComponents(targetClassName, voidType, mainClass),
            targetClassName,
            superClass,
            subInterface,
            superInterface);

    SootClass testClass = mainMethod.getDeclaringClass();

    SootMethod subInterfacePrint =
        Scene.v().getMethod(methodSigFromComponents(subInterface, ""void print()""));
    SootMethod superInterfacePrint =
        Scene.v().getMethod(methodSigFromComponents(superInterface, ""void print()""));

    SootMethod methodRefResolved =
        resolveMethodRefInBody(mainMethod.retrieveActiveBody().getUnits(), ""void print()"");
    assertEquals(subInterfacePrint, methodRefResolved);

    SootMethod virtualCallsResolved =
        VirtualCalls.v()
            .resolveNonSpecial(testClass.getType(), superInterfacePrint.makeRef(), false);
    assertEquals(subInterfacePrint, virtualCallsResolved);

    SootMethod concreteImplI1 =
        Scene.v().getFastHierarchy().resolveConcreteDispatch(testClass, superInterfacePrint);
    assertEquals(subInterfacePrint, concreteImplI1);

    Set<SootMethod> abstractImpl =
        Scene.v()
            .getFastHierarchy()
            .resolveAbstractDispatch(superInterfacePrint.getDeclaringClass(), superInterfacePrint);
    assertEquals(Sets.newHashSet(subInterfacePrint), abstractImpl);

    assertTrue(checkInEdges(subInterfacePrint, mainMethod));
    assertTrue(Scene.v().getReachableMethods().contains(subInterfacePrint));
  }
",non-flaky,5
156114,soot-oss_soot,SootMethodRefImplTest.testCachingInvalidation,"  @Test
  public void testCachingInvalidation() throws Exception {
    SootMethod m1 = prepareTarget(methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""m1""), TEST_TARGET_CLASS);
    final SootClass clas = m1.getDeclaringClass();

    // There are only 3 methods in the class originally.
    Assert.assertEquals(Arrays.asList(""<init>"", ""m1"", ""m2""),
        clas.getMethods().stream().map(SootMethod::getName).sorted().collect(Collectors.toList()));

    // Ensure the previous value of SootMethodRefImpl#resolveCache
    // is not used if the referenced method itself is modified.
    final Body b = m1.retrieveActiveBody();
    final SootMethodRef mRef = getMethodRef(b);
    Assert.assertEquals(""m2"", mRef.getName());

    // Get the original referenced method appearing in the test source (i.e. ""m2"")
    final SootMethod origM = mRef.resolve();
    Assert.assertTrue(!origM.isPhantom());
    Assert.assertEquals(""m2"", origM.getName());

    // Change the name of the method so the method reference no
    // longer refers to that method.
    origM.setName(""newMethodName"");
    Assert.assertEquals(""newMethodName"", origM.getName());

    // Changing the method itself does not change the reference
    Assert.assertEquals(""m2"", mRef.getName());

    // There are still just 3 methods in the class (but ""m2"" was renamed).
    Assert.assertEquals(Arrays.asList(""<init>"", ""m1"", ""newMethodName""),
        clas.getMethods().stream().map(SootMethod::getName).sorted().collect(Collectors.toList()));

    // When resolving the reference, the cached value is not used since the
    // original method was renamed. It now gives a different method (that was
    // created automatically since a method with the name ""m2"" no longer exists).
    final SootMethod newM = mRef.resolve();
    Assert.assertNotSame(origM, newM);
    Assert.assertEquals(""m2"", newM.getName());

    // There are now 4 methods since resolving ""m2"" created it again.
    Assert.assertEquals(Arrays.asList(""<init>"", ""m1"", ""m2"", ""newMethodName""),
        clas.getMethods().stream().map(SootMethod::getName).sorted().collect(Collectors.toList()));
  }
",non-flaky,5
156115,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.lambdaNoCaptures,"  @Test
  public void lambdaNoCaptures() {
    String testClass = ""soot.lambdaMetaFactory.LambdaNoCaptures"";

    final SootMethod target = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass,
        ""java.util.function.Function"");

    final CallGraph cg = Scene.v().getCallGraph();

    final String metaFactoryClass = getMetaFactoryNameLambda(testClass, TEST_METHOD_NAME);

    final SootMethod bootstrap
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.Function"", ""bootstrap$""));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>""));
    final SootMethod apply
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""apply"", ""java.lang.Object""));
    final SootMethod lambdaBody
        = Scene.v().getMethod(methodSigFromComponents(testClass, ""java.lang.String"", ""lambda$main$0"", ""java.lang.Integer""));
    final SootMethod staticCallee
        = Scene.v().getMethod(methodSigFromComponents(testClass, ""void"", ""staticCallee"", ""java.lang.Integer""));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(
        ""There should be an instance invocation on the synthetic LambdaMetaFactory's implementation of the functional interface in the main method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(apply) && e.kind() == Kind.INTERFACE));
    assertTrue(
        ""There should be a static call to the lambda body implementation in the generated functional interface implementation of the synthetic LambdaMetaFactory"",
        newArrayList(cg.edgesOutOf(apply)).stream().anyMatch(e -> e.getTgt().equals(lambdaBody) && e.isStatic()));

    assertTrue(""There should be a static call to the staticCallee method in actual lambda body implementation"",
        newArrayList(cg.edgesOutOf(lambdaBody)).stream().anyMatch(e -> e.getTgt().equals(staticCallee) && e.isStatic()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156116,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.lambdaWithCaptures,"  @Test
  public void lambdaWithCaptures() {
    String testClass = ""soot.lambdaMetaFactory.LambdaWithCaptures"";

    final SootMethod target
        = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass);

    final CallGraph cg = Scene.v().getCallGraph();

    final String metaFactoryClass = getMetaFactoryNameLambda(testClass, TEST_METHOD_NAME);

    final SootMethod bootstrap = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.Supplier"",
        ""bootstrap$"", testClass, ""java.lang.String""));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>"", testClass, ""java.lang.String""));
    final SootMethod get = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""get""));
    final SootMethod lambdaBody
        = Scene.v().getMethod(methodSigFromComponents(testClass, ""java.lang.String"", ""lambda$main$0"", ""java.lang.String""));
    final SootMethod getString = Scene.v().getMethod(methodSigFromComponents(testClass, ""java.lang.String"", ""getString""));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(
        ""There should be an interface invocation on the synthetic LambdaMetaFactory's implementation of the functional interface in the main method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(get) && e.kind() == Kind.INTERFACE));
    assertTrue(
        ""There should be a virtual call to the lambda body implementation in the generated functional interface implementation of the synthetic LambdaMetaFactory"",
        newArrayList(cg.edgesOutOf(get)).stream().anyMatch(e -> e.getTgt().equals(lambdaBody) && e.isVirtual()));

    assertTrue(""There should be a special call to the getString method in actual lambda body implementation"",
        newArrayList(cg.edgesOutOf(lambdaBody)).stream().anyMatch(e -> e.getTgt().equals(getString) && e.isSpecial()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156117,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.markerInterfaces,"  @Test
  public void markerInterfaces() {
    String testClass = ""soot.lambdaMetaFactory.MarkerInterfaces"";

    final SootMethod target
        = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass);

    final CallGraph cg = Scene.v().getCallGraph();

    final String metaFactoryClass = getMetaFactoryNameLambda(testClass, TEST_METHOD_NAME);

    final SootMethod bootstrap = Scene.v()
        .getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.Supplier"", ""bootstrap$"", testClass));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>"", testClass));
    final SootMethod get = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""get""));
    final SootMethod lambdaBody
        = Scene.v().getMethod(methodSigFromComponents(testClass, ""java.lang.Object"", ""lambda$main$0""));
    final SootMethod getString = Scene.v().getMethod(methodSigFromComponents(testClass, ""java.lang.String"", ""getString""));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(
        ""There should be an interface invocation on the synthetic LambdaMetaFactory's implementation of the functional interface in the main method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(get) && e.kind() == Kind.INTERFACE));
    assertTrue(
        ""There should be a virtual call to the lambda body implementation in the generated functional interface implementation of the synthetic LambdaMetaFactory"",
        newArrayList(cg.edgesOutOf(get)).stream().anyMatch(e -> e.getTgt().equals(lambdaBody) && e.isVirtual()));

    assertTrue(""There should be a virtual call to the getString method in actual lambda body implementation"",
        newArrayList(cg.edgesOutOf(lambdaBody)).stream().anyMatch(e -> e.getTgt().equals(getString) && e.isVirtual()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156118,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.staticMethodRef,"  @Test
  public void staticMethodRef() {
    String testClass = ""soot.lambdaMetaFactory.StaticMethodRef"";

    final SootMethod target
        = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass);

    final CallGraph cg = Scene.v().getCallGraph();

    final String referencedMethodName = ""staticMethod"";

    final String metaFactoryClass = getMetaFactoryNameMethodRef(testClass, referencedMethodName);

    final SootMethod bootstrap
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.Supplier"", ""bootstrap$""));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>""));
    final SootMethod get = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""get""));
    final SootMethod referencedMethod = Scene.v().getMethod(methodSigFromComponents(testClass, ""int"", referencedMethodName));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(
        ""There should be an interface invocation on the synthetic LambdaMetaFactory's implementation of the functional interface in the main method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(get) && e.kind() == Kind.INTERFACE));
    assertTrue(""There should be a static call to the referenced method"",
        newArrayList(cg.edgesOutOf(get)).stream().anyMatch(e -> e.getTgt().equals(referencedMethod) && e.isStatic()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156119,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.privateMethodRef,"  @Test
  public void privateMethodRef() {
    String testClass = ""soot.lambdaMetaFactory.PrivateMethodRef"";

    final SootMethod target
        = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass);

    final CallGraph cg = Scene.v().getCallGraph();

    final String referencedMethodName = ""privateMethod"";

    final String metaFactoryClass = getMetaFactoryNameMethodRef(testClass, referencedMethodName);

    final SootMethod bootstrap = Scene.v()
        .getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.Supplier"", ""bootstrap$"", testClass));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>"", testClass));
    final SootMethod get = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""get""));
    final SootMethod referencedMethod = Scene.v().getMethod(methodSigFromComponents(testClass, ""int"", referencedMethodName));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(
        ""There should be an interface invocation on the synthetic LambdaMetaFactory's implementation of the functional interface in the main method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(get) && e.kind() == Kind.INTERFACE));
    assertTrue(""There should be a virtual call to the referenced method"",
        newArrayList(cg.edgesOutOf(get)).stream().anyMatch(e -> e.getTgt().equals(referencedMethod) && e.isVirtual()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156120,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.publicMethodRef,"  @Test
  public void publicMethodRef() {
    String testClass = ""soot.lambdaMetaFactory.PublicMethodRef"";

    final SootMethod target
        = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass);

    final CallGraph cg = Scene.v().getCallGraph();

    final String referencedMethodName = ""publicMethod"";

    final String metaFactoryClass = getMetaFactoryNameMethodRef(testClass, referencedMethodName);

    final SootMethod bootstrap = Scene.v()
        .getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.Supplier"", ""bootstrap$"", testClass));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>"", testClass));
    final SootMethod get = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""get""));
    final SootMethod referencedMethod = Scene.v().getMethod(methodSigFromComponents(testClass, ""int"", referencedMethodName));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(
        ""There should be an interface invocation on the synthetic LambdaMetaFactory's implementation of the functional interface in the main method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(get) && e.kind() == Kind.INTERFACE));
    assertTrue(""There should be a virtual call to the referenced method"",
        newArrayList(cg.edgesOutOf(get)).stream().anyMatch(e -> e.getTgt().equals(referencedMethod) && e.isVirtual()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156121,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.constructorMethodRef,"  @Test
  public void constructorMethodRef() {
    String testClass = ""soot.lambdaMetaFactory.ConstructorMethodRef"";

    final SootMethod target
        = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass);

    final CallGraph cg = Scene.v().getCallGraph();

    final String referencedMethodName = ""<init>"";

    final String metaFactoryClass = getMetaFactoryNameMethodRef(testClass, referencedMethodName);

    final SootMethod bootstrap
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.Supplier"", ""bootstrap$""));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>""));
    final SootMethod get = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""get""));
    final SootMethod referencedMethod
        = Scene.v().getMethod(methodSigFromComponents(testClass, ""void"", referencedMethodName));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(
        ""There should be an interface invocation on the synthetic LambdaMetaFactory's implementation of the functional interface  in the main method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(get) && e.kind() == Kind.INTERFACE));
    assertTrue(""There should be a special call to the referenced method"",
        newArrayList(cg.edgesOutOf(get)).stream().anyMatch(e -> e.getTgt().equals(referencedMethod) && e.isSpecial()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156122,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.inheritedMethodRef,"  @Test
  public void inheritedMethodRef() {
    String testClass = ""soot.lambdaMetaFactory.InheritedMethodRef"";

    final SootMethod target
        = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass);

    final CallGraph cg = Scene.v().getCallGraph();

    final String referencedMethodName = ""superMethod"";

    final String metaFactoryClass = getMetaFactoryNameLambda(testClass, TEST_METHOD_NAME);

    final SootMethod bootstrap = Scene.v()
        .getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.Supplier"", ""bootstrap$"", testClass));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>"", testClass));
    final SootMethod get = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""get""));
    final SootMethod referencedMethod
        = Scene.v().getMethod(methodSigFromComponents(""soot.lambdaMetaFactory.Super"", ""int"", referencedMethodName));
    final SootMethod lambdaBody
        = Scene.v().getMethod(methodSigFromComponents(testClass, ""java.lang.Integer"", ""lambda$main$0""));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(
        ""There should be an interface invocation on the synthetic LambdaMetaFactory's implementation of the functional interface in the main method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(get) && e.kind() == Kind.INTERFACE));
    //Call is from <soot.lambdaMetaFactory.InheritedMethodRef$lambda_main_0__1
    //to           <soot.lambdaMetaFactory.InheritedMethodRef: java.lang.Integer lambda$main$0()>
    //As such, it needs to be a virtual call.
    assertTrue(
        ""There should be a virtual call to the lambda body implementation in the generated functional interface implementation of the synthetic LambdaMetaFactory"",
        newArrayList(cg.edgesOutOf(get)).stream().anyMatch(e -> e.getTgt().equals(lambdaBody) && e.isVirtual()));
    assertTrue(""There should be a special call to the referenced method"", newArrayList(cg.edgesOutOf(lambdaBody)).stream()
        .anyMatch(e -> e.getTgt().equals(referencedMethod) && e.isSpecial()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156123,soot-oss_soot,AbstractLambdaMetaFactoryCGTest.methodRefWithParameters,"  @Test
  public void methodRefWithParameters() {
    String testClass = ""soot.lambdaMetaFactory.MethodRefWithParameters"";

    final SootMethod target
        = prepareTarget(methodSigFromComponents(testClass, TEST_METHOD_RET, TEST_METHOD_NAME), testClass);

    final CallGraph cg = Scene.v().getCallGraph();

    final String referencedMethodName = ""staticWithCaptures"";

    final String metaFactoryClass = getMetaFactoryNameMethodRef(testClass, referencedMethodName);

    final SootMethod bootstrap
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""java.util.function.BiFunction"", ""bootstrap$""));
    final SootMethod metaFactoryConstructor
        = Scene.v().getMethod(methodSigFromComponents(metaFactoryClass, ""void"", ""<init>""));
    final SootMethod apply = Scene.v().getMethod(
        methodSigFromComponents(metaFactoryClass, ""java.lang.Object"", ""apply"", ""java.lang.Object"", ""java.lang.Object""));
    final SootMethod referencedMethod
        = Scene.v().getMethod(methodSigFromComponents(testClass, ""int"", referencedMethodName, ""int"", ""java.lang.Integer""));

    final List<Edge> edgesFromTarget = newArrayList(cg.edgesOutOf(target));

    assertTrue(""There should be an edge from main to the bootstrap method of the synthetic LambdaMetaFactory"",
        edgesFromTarget.stream().anyMatch(e -> e.tgt().equals(bootstrap) && e.isStatic()));
    assertTrue(""There should be an edge to the constructor of the LambdaMetaFactory in the bootstrap method"",
        newArrayList(cg.edgesOutOf(bootstrap)).stream()
            .anyMatch(e -> e.tgt().equals(metaFactoryConstructor) && e.isSpecial()));
    assertTrue(""There should be an interface invocation on the referenced method"",
        edgesFromTarget.stream().anyMatch(e -> e.getTgt().equals(apply) && e.kind() == Kind.INTERFACE));
    assertTrue(""There should be a static call to the referenced method"",
        newArrayList(cg.edgesOutOf(apply)).stream().anyMatch(e -> e.getTgt().equals(referencedMethod) && e.isStatic()));

    validateAllBodies(target.getDeclaringClass(), bootstrap.getDeclaringClass());
  }
",non-flaky,5
156124,soot-oss_soot,Issue1367Test.constructorReference,"    @Test
    public void constructorReference() {
        String testClass = ""soot.lambdaMetaFactory.Issue1367"";

        final SootMethod target = prepareTarget(
                methodSigFromComponents(testClass, ""java.util.function.Supplier"", ""constructorReference""),
                testClass,
                ""java.util.function.Function"");

        validateAllBodies(target.getDeclaringClass());
    }
",non-flaky,5
156125,soot-oss_soot,Issue1292Test.testNewTest,"  @Test
  public void testNewTest() {
    String testClass = ""soot.lambdaMetaFactory.Issue1292"";
    prepareTarget(
        methodSigFromComponents(testClass, ""void"", ""testNew"", ""java.util.List""),
        testClass,
        ""java.util.function.Function"");
    // if no exception is thrown, everything is working as intended
  }
",non-flaky,5
156126,soot-oss_soot,Issue1146Test.getVertragTest,"  @Test
  public void getVertragTest() {
    String testClass = ""soot.lambdaMetaFactory.Issue1146"";

    final SootMethod target = prepareTarget(
        methodSigFromComponents(testClass, ""soot.lambdaMetaFactory.Issue1146$Vertrag"", ""getVertrag"", ""java.lang.String""),
        testClass, ""java.util.function.Function"");
    // if no exception is thrown, everything is working as intended
  }
",non-flaky,5
156127,soot-oss_soot,Issue1146Test.getVertrag2Test,"  @Test
  public void getVertrag2Test() {
    String testClass = ""soot.lambdaMetaFactory.Issue1146"";

    final SootMethod target = prepareTarget(
        methodSigFromComponents(testClass, ""soot.lambdaMetaFactory.Issue1146$Vertrag"", ""getVertrag2"", ""java.lang.String""),
        testClass, ""java.util.function.Function"");
    // if no exception is thrown, everything is working as intended
  }
",non-flaky,5
156128,soot-oss_soot,LambdaMetaFactoryAdaptTest.parameterBoxing,"  @Test
  public void parameterBoxing() {
    String testClass = ""soot.lambdaMetaFactory.Adapt"";

    final SootMethod target = prepareTarget(methodSigFromComponents(testClass, ""void"", ""parameterBoxingTarget""), testClass);

    // TODO more fine-grained testing

    validateAllBodies(target.getDeclaringClass());
  }
",non-flaky,5
156129,soot-oss_soot,LambdaMetaFactoryAdaptTest.parameterWidening,"  @Test
  public void parameterWidening() {
    String testClass = ""soot.lambdaMetaFactory.Adapt"";

    final SootMethod target = prepareTarget(methodSigFromComponents(testClass, ""void"", ""parameterWidening""), testClass);

    // TODO more fine-grained testing

    validateAllBodies(target.getDeclaringClass());
  }
",non-flaky,5
156130,soot-oss_soot,LambdaMetaFactoryAdaptTest.returnBoxing,"  @Test
  public void returnBoxing() {
    String testClass = ""soot.lambdaMetaFactory.Adapt"";

    final SootMethod target = prepareTarget(methodSigFromComponents(testClass, ""void"", ""returnBoxing""), testClass);

    // TODO more fine-grained testing

    validateAllBodies(target.getDeclaringClass());
  }
",non-flaky,5
156131,soot-oss_soot,LambdaMetaFactoryAdaptTest.returnWidening,"  @Test
  public void returnWidening() {
    String testClass = ""soot.lambdaMetaFactory.Adapt"";

    final SootMethod target = prepareTarget(methodSigFromComponents(testClass, ""void"", ""returnWidening""), testClass);

    // TODO more fine-grained testing

    validateAllBodies(target.getDeclaringClass());
  }
",non-flaky,5
156132,soot-oss_soot,LocalPackerTest.nullAssignment,"  @Test
  public void nullAssignment() {
    SootMethod target =
        prepareTarget(methodSigFromComponents(TEST_TARGET_CLASS, ""void"", ""prefixVariableNames""), TEST_TARGET_CLASS);

    Body body = target.retrieveActiveBody();
    Assert.assertTrue(body instanceof JimpleBody);

    // Assert all local names are distinct
    Assert.assertTrue(body.getLocals().stream().map(Local::getName).distinct().count() == body.getLocalCount());

    LocalPacker.v().transform(body);

    // Assert all local names are distinct
    Assert.assertTrue(body.getLocals().stream().map(Local::getName).distinct().count() == body.getLocalCount());
  }
",non-flaky,5
156133,soot-oss_soot,DexByteCodeInstrutionsTest.InvokePolymorphic1,"  @Test
  public void InvokePolymorphic1() {
    final SootMethod testTarget = prepareTarget(
        methodSigFromComponents(TARGET_CLASS, ""void invokePolymorphicTarget(java.lang.invoke.MethodHandle)""), TARGET_CLASS);

    // We model invokePolymorphic as invokeVirtual
    final List<InvokeExpr> invokes = invokesFromMethod(testTarget);
    Assert.assertEquals(1, invokes.size());
    final InvokeExpr invokePoly = invokes.get(0);
    Assert.assertTrue(invokePoly instanceof VirtualInvokeExpr);
    final SootMethodRef targetMethodRef = invokePoly.getMethodRef();
    Assert.assertEquals(methodSigFromComponents(METHOD_HANDLE_CLASS, METHOD_HANDLE_INVOKE_SUBSIG),
        targetMethodRef.getSignature());
  }
",non-flaky,5
156134,soot-oss_soot,DexByteCodeInstrutionsTest.InvokeCustom1,"  @Test
  public void InvokeCustom1() {
    final SootMethod testTarget
        = prepareTarget(methodSigFromComponents(TARGET_CLASS, ""void invokeCustomTarget()""), TARGET_CLASS);

    // We model invokeCustom as invokeDynamic
    final List<InvokeExpr> invokes = invokesFromMethod(testTarget);
    Assert.assertEquals(1, invokes.size());
    final InvokeExpr invokeCustom = invokes.get(0);
    Assert.assertTrue(invokeCustom instanceof DynamicInvokeExpr);
    final SootMethodRef targetMethodRef = invokeCustom.getMethodRef();
    Assert.assertEquals(methodSigFromComponents(SootClass.INVOKEDYNAMIC_DUMMY_CLASS_NAME, SUPPLIER_GET_SUBSIG),
        targetMethodRef.getSignature());
    final String callToLambdaMethaFactory
        = ""dynamicinvoke \""get\"" <java.util.function.Supplier ()>() <java.lang.invoke.LambdaMetafactory: java.lang.invoke.CallSite metafactory(java.lang.invoke.MethodHandles$Lookup,java.lang.String,java.lang.invoke.MethodType,java.lang.invoke.MethodType,java.lang.invoke.MethodHandle,java.lang.invoke.MethodType)>(methodtype: java.lang.Object __METHODTYPE__(), methodhandle: \""REF_INVOKE_STATIC\"" <soot.dexpler.instructions.DexBytecodeTarget: java.lang.String lambda$invokeCustomTarget$0()>, methodtype: java.lang.String __METHODTYPE__())"";
    Assert.assertEquals(callToLambdaMethaFactory, invokeCustom.toString());
  }
",non-flaky,5
156135,soot-oss_soot,PackManagerTest.testDisableCopyPropagatorInJBPhase,"    @Test
    public void testDisableCopyPropagatorInJBPhase() {
        {
            // default CopyPropagator enabled
            setup();
            Scene.v().loadNecessaryClasses();
            PackManager.v().runBodyPacks();
            SootClass cls = Scene.v().getSootClass(""Example"");
            SootMethod foo = cls.getMethodByName(""foo"");
            List<String> actual = bodyAsStrings(foo.getActiveBody());
            List<String> expected = expectedBody(""r0 := @this: Example"",
                    ""virtualinvoke r0.<Example: void bar(int,int)>(0, 2)"",
                    ""return"");
            assertEquals(expected, actual);
        }
        {
            // disable CopyPropagator
            setup();
            Options.v().setPhaseOption(""jb.sils"", ""enabled:false"");// this transformer calls a lot of other transformers
            Options.v().setPhaseOption(""jb.cp"", ""enabled:false"");
            Scene.v().loadNecessaryClasses();
            PackManager.v().runBodyPacks();
            SootClass cls = Scene.v().getSootClass(""Example"");
            SootMethod foo = cls.getMethodByName(""foo"");
            List<String> actual = bodyAsStrings(foo.getActiveBody());
            List<String> expected = expectedBody(""r0 := @this: Example"",
                    ""b0 = 0"",
                    ""b1 = 2"",
                    ""virtualinvoke r0.<Example: void bar(int,int)>(b0, b1)"",
                    ""return"");
            assertEquals(expected, actual);
        }
    }
",non-flaky,5
156136,soot-oss_soot,PackManagerTest.testDisableUnusedLocalEliminatorInJBPhase,"    @Test
    public void testDisableUnusedLocalEliminatorInJBPhase() {
        {
            // default UnusedLocalEliminator enabled
            setup();
            Scene.v().loadNecessaryClasses();
            PackManager.v().runBodyPacks();
            SootClass cls = Scene.v().getSootClass(""Example"");
            SootMethod bar = cls.getMethodByName(""bar"");
            List<String> actual = bodyAsStrings(bar.getActiveBody());
            List<String> expected = expectedBody(""r1 := @this: Example"",
                    ""i0 := @parameter0: int"",
                    ""i1 := @parameter1: int"",
                    ""i2 = i0 * i1"",
                    ""$r0 = <java.lang.System: java.io.PrintStream out>"",
                    ""virtualinvoke $r0.<java.io.PrintStream: void println(int)>(i2)"",
                    ""return"");
        }
        {
            //disable UnusedLocalEliminator
            setup();
            Options.v().setPhaseOption(""jb.sils"", ""enabled:false"");// this transformer calls a lot of other transformers
            Options.v().setPhaseOption(""jb.cp-ule"", ""enabled:false"");
            Scene.v().loadNecessaryClasses();
            PackManager.v().runBodyPacks();
            SootClass cls = Scene.v().getSootClass(""Example"");
            SootMethod bar = cls.getMethodByName(""bar"");
            List<String> actual = bodyAsStrings(bar.getActiveBody());
            List<String> expected = expectedBody(""r1 := @this: Example"",
                    ""i0 := @parameter0: int"",
                    ""i1 := @parameter1: int"",
                    ""i2 = i0 * i1"",
                    ""z0 = 0"",
                    ""$r0 = <java.lang.System: java.io.PrintStream out>"",
                    ""virtualinvoke $r0.<java.io.PrintStream: void println(int)>(i2)"",
                    ""return"");
        }
    }
",non-flaky,5
156137,soot-oss_soot,LoadingTest.testLoadingJava9to11Class,"  @Test
  public void testLoadingJava9to11Class() {
    G.reset();
    Options.v().set_soot_modulepath(""VIRTUAL_FS_FOR_JDK"");
    Scene.v().loadBasicClasses();

    SootClass klass1
        = SootModuleResolver.v().resolveClass(""java.lang.invoke.VarHandle"", SootClass.BODIES, Optional.of(""java.base""));

    assertTrue(klass1.getName().equals(""java.lang.invoke.VarHandle""));
    assertTrue(klass1.moduleName.equals(""java.base""));

    SootClass klass2 = SootModuleResolver.v().resolveClass(""java.lang.invoke.ConstantBootstraps"", SootClass.BODIES,
        Optional.of(""java.base""));

    assertTrue(klass2.getName().equals(""java.lang.invoke.ConstantBootstraps""));
    assertTrue(klass2.moduleName.equals(""java.base""));

    Scene.v().loadNecessaryClasses();
  }
",non-flaky,5
156138,soot-oss_soot,LoadingTest.testLoadingJava9ClassFromCI,"  @Test
  public void testLoadingJava9ClassFromCI() {
    G.reset();
    Main.main(new String[] { ""-soot-modulepath"", ""VIRTUAL_FS_FOR_JDK"", ""-pp"", ""-src-prec"", ""only-class"",
        ""java.lang.invoke.VarHandle"" });

    SootClass klass = Scene.v().getSootClass(""java.lang.invoke.VarHandle"");
    assertTrue(klass.getName().equals(""java.lang.invoke.VarHandle""));
    assertTrue(klass.moduleName.equals(""java.base""));

  }
",non-flaky,5
156139,soot-oss_soot,LoadingTest.testLoadingJava11ClassFromCI,"  @Test
  public void testLoadingJava11ClassFromCI() {
    G.reset();
    Main.main(new String[] { ""-soot-modulepath"", ""VIRTUAL_FS_FOR_JDK"", ""-pp"", ""-src-prec"", ""only-class"",
        ""java.lang.invoke.ConstantBootstraps"" });

    SootClass klass = Scene.v().getSootClass(""java.lang.invoke.ConstantBootstraps"");
    assertTrue(klass.getName().equals(""java.lang.invoke.ConstantBootstraps""));
    assertTrue(klass.moduleName.equals(""java.base""));

  }
",non-flaky,5
156140,soot-oss_soot,RefTypeTest.testMerge,"	@Test
	public void testMerge() {
		G.reset();
		
		Scene.v().loadNecessaryClasses();
		
		SootClass sc1 = new SootClass(""Class1"");
		SootClass sc2 = new SootClass(""Class2"");
		SootClass sc3 = new SootClass(""Class3"");
		SootClass sc4 = new SootClass(""Class4"");
		SootClass sc5 = new SootClass(""Class5"");
		
		Scene.v().addClass(sc1);
		Scene.v().addClass(sc2);
		Scene.v().addClass(sc3);
		Scene.v().addClass(sc4);
		Scene.v().addClass(sc5);
		
		sc1.setSuperclass(Scene.v().getObjectType().getSootClass());
		sc2.setSuperclass(sc1);
		sc3.setSuperclass(sc2);
		sc4.setSuperclass(sc2);
		sc5.setSuperclass(sc4);
		
		Type tpMerged = sc5.getType().merge(sc3.getType(), Scene.v());
		Assert.assertEquals(""Class2"", ((RefType) tpMerged).getClassName()); 
	}
",non-flaky,5
156141,soot-oss_soot,AbnormalTest.testMethodWithNoInstruction,"    @Test
    public void testMethodWithNoInstruction() {
        setup();
        Options.v().set_output_format(Options.output_format_jimple);
        runTest();
        setup();
        Options.v().set_output_format(Options.output_format_grimp);
        runTest();
        setup();
        Options.v().set_output_format(Options.output_format_baf);
        runTest();
        setup();
        Options.v().set_output_format(Options.output_format_dava);
        runTest();
        setup();
        Options.v().set_output_format(Options.output_format_shimp);
        runTest();
        setup();
        Options.v().set_output_format(Options.output_format_class);
        runTest();
    }
",non-flaky,5
156142,soot-oss_soot,ClassRenamerTest.getName,"  @Test
  public void getName() {
    assertThat(ClassRenamer.v().getName(), equalTo(ClassRenamer.name));
  }
",non-flaky,5
156143,soot-oss_soot,ClassRenamerTest.getDependencies,"  @Test
  public void getDependencies() {
    assertThat(ClassRenamer.v().getDependencies(), equalTo(new String[] { ClassRenamer.name }));
  }
",non-flaky,5
156144,soot-oss_soot,ClassRenamerTest.getPackageName,"  @Test
  public void getPackageName() {
    assertNull(ClassRenamer.getPackageName(""""));
    assertNull(ClassRenamer.getPackageName(null));
    assertNull(ClassRenamer.getPackageName("".""));
    assertNull(ClassRenamer.getPackageName(""ClassName""));
    assertEquals(""com.sable"", ClassRenamer.getPackageName(""com.sable.Soot""));
  }
",non-flaky,5
156145,soot-oss_soot,ClassRenamerTest.getClassName,"  @Test
  public void getClassName() {
    assertNull(ClassRenamer.getClassName(""""));
    assertNull(ClassRenamer.getClassName(null));
    assertNull(ClassRenamer.getClassName("".""));
    assertEquals(""ClassName"", ClassRenamer.getClassName(""ClassName""));
    assertEquals(""Soot"", ClassRenamer.getClassName(""com.sable.Soot""));
    assertNull(ClassRenamer.getClassName(""com.sable.""));
  }
",non-flaky,5
156146,soot-oss_soot,ClassRenamerTest.getOrAddNewName_cachingName,"  @Test
  public void getOrAddNewName_cachingName() {
    ClassRenamer.v().setRemovePackages(false);
    ClassRenamer.v().setRenamePackages(false);

    final String newName = ClassRenamer.v().getOrAddNewName(null, ""ClassName"");
    assertThat(newName, not(containsString(""."")));

    Map<String, String> mapping = ClassRenamer.v().getClassNameMapping((pOldName, pNewName) -> pOldName.equals(""ClassName""));
    assertThat(mapping, hasEntry(""ClassName"", newName));
    assertThat(mapping.size(), equalTo(1));

    assertThat(ClassRenamer.v().getOrAddNewName(null, ""ClassName""), equalTo(newName));

    mapping = ClassRenamer.v().getClassNameMapping((pOldName, pNewName) -> pOldName.equals(""ClassName""));
    assertThat(mapping, hasEntry(""ClassName"", newName));
    assertThat(mapping.size(), equalTo(1));
  }
",non-flaky,5
156147,soot-oss_soot,ClassRenamerTest.getOrAddNewName_cachingPackage,"  @Test
  public void getOrAddNewName_cachingPackage() {
    ClassRenamer.v().setRemovePackages(false);
    ClassRenamer.v().setRenamePackages(false);

    final String newName = ClassRenamer.v().getOrAddNewName(""pac.age"", ""ClassName"");
    assertThat(newName, allOf(startsWith(""pac.age.""), not(endsWith(""ClassName""))));
    assertThat(newName.split(""\\."").length, equalTo(3));

    assertThat(ClassRenamer.v().getOrAddNewName(""pac.age"", ""ClassName""), equalTo(newName));
  }
",non-flaky,5
156148,soot-oss_soot,ClassRenamerTest.getOrAddNewName_nullClassName,"  @Test
  public void getOrAddNewName_nullClassName() {
    ClassRenamer.v().setRemovePackages(false);
    ClassRenamer.v().setRenamePackages(false);

    final String newName = ClassRenamer.v().getOrAddNewName(""pac.age"", null);
    assertThat(newName, startsWith(""pac.age.""));
    assertThat(newName.split(""\\."").length, equalTo(3));

    assertThat(ClassRenamer.v().getOrAddNewName(""pac.age"", null), not(equalTo(newName)));
  }
",non-flaky,5
156149,soot-oss_soot,ClassRenamerTest.getOrAddNewName_renamePackage,"  @Test
  public void getOrAddNewName_renamePackage() {
    ClassRenamer.v().setRemovePackages(false);
    ClassRenamer.v().setRenamePackages(true);

    final String newName = ClassRenamer.v().getOrAddNewName(""pac.age.getOrAddNewName_renamePackage"", ""ClassName"");
    assertThat(newName, allOf(not(startsWith(""pac.age.getOrAddNewName_renamePackage."")), not(endsWith(""ClassName""))));
    assertThat(newName.split(""\\."").length, equalTo(4));

    assertThat(ClassRenamer.v().getOrAddNewName(""pac.age.getOrAddNewName_renamePackage"", ""ClassName""), equalTo(newName));
  }
",non-flaky,5
156150,soot-oss_soot,ClassRenamerTest.getOrAddNewName_renamePackage_nullPackage,"  @Test
  public void getOrAddNewName_renamePackage_nullPackage() {
    ClassRenamer.v().setRemovePackages(false);
    ClassRenamer.v().setRenamePackages(true);

    final String newName = ClassRenamer.v().getOrAddNewName(null, ""ClassName"");
    assertThat(newName, allOf(not(endsWith(""ClassName"")), not(containsString("".""))));

    final String newName0 = ClassRenamer.v().getOrAddNewName(null, ""ClassName"");
    assertThat(newName0, equalTo(newName)); // package names and class names are equal

    final String newName1 = ClassRenamer.v().getOrAddNewName(null, ""ClassName1"");
    assertThat(newName1, not(equalTo(newName)));
    assertThat(newName1.split(""\\."").length, equalTo(2));
    assertThat(newName.split(""\\."")[0], equalTo(newName.split(""\\."")[0])); // package names are equal
  }
",non-flaky,5
156151,soot-oss_soot,ClassRenamerTest.getOrAddNewName_removePackage,"  @Test
  public void getOrAddNewName_removePackage() {
    ClassRenamer.v().setRemovePackages(true);

    String newName = ClassRenamer.v().getOrAddNewName(""a.b.c"", ""ClassName"");
    assertThat(newName, allOf(not(endsWith(""ClassName"")), not(containsString("".""))));

    String packageName = ""a.b.c"";
    for (int i = 0; i < 100; i++) {
      packageName = packageName + "".p"" + i;
      newName = ClassRenamer.v().getOrAddNewName(packageName, ""ClassName"");
      assertThat(newName, allOf(not(endsWith(""ClassName"")), not(containsString("".""))));
    }
  }
",non-flaky,5
156152,soot-oss_soot,MethodHandleTest.testConstant,"  @Test
  public void testConstant() throws Throwable {

    // First generate a classfile with a MethodHnadle
    ClassWriter cv = new ClassWriter(ClassWriter.COMPUTE_FRAMES | ClassWriter.COMPUTE_MAXS);
    cv.visit(Opcodes.V1_7, Opcodes.ACC_PUBLIC, ""HelloMethodHandles"", null, Type.getInternalName(Object.class), null);
    MethodVisitor mv = cv.visitMethod(Opcodes.ACC_STATIC | Opcodes.ACC_PUBLIC, ""getSquareRoot"",
        Type.getMethodDescriptor(Type.getType(java.lang.invoke.MethodHandle.class)), null, null);

    mv.visitCode();

    mv.visitLdcInsn(new Handle(Opcodes.H_INVOKESTATIC, Type.getInternalName(Math.class), ""sqrt"",
        Type.getMethodDescriptor(Type.DOUBLE_TYPE, Type.DOUBLE_TYPE), false));

    mv.visitInsn(Opcodes.ARETURN);
    mv.visitEnd();

    cv.visitEnd();

    File tempDir = Files.createTempDir();
    File classFile = new File(tempDir, ""HelloMethodHandles.class"");
    Files.write(cv.toByteArray(), classFile);

    G.reset();

    String[] commandLine = { ""-pp"", ""-cp"", tempDir.getAbsolutePath(), ""-O"", ""HelloMethodHandles"", };

    System.out.println(""Command Line: "" + Arrays.toString(commandLine));

    Main.main(commandLine);

    Class<?> clazz = validateClassFile(""HelloMethodHandles"");
    java.lang.invoke.MethodHandle methodHandle
        = (java.lang.invoke.MethodHandle) clazz.getMethod(""getSquareRoot"").invoke(null);

    assertThat((Double) methodHandle.invoke(16.0), equalTo(4.0));
  }
",non-flaky,5
156153,soot-oss_soot,MethodHandleTest.testInvoke,"  @Test
  public void testInvoke() throws IOException, ClassNotFoundException {

    // First generate a classfile with a MethodHnadle
    ClassWriter cv = new ClassWriter(ClassWriter.COMPUTE_FRAMES | ClassWriter.COMPUTE_MAXS);
    cv.visit(Opcodes.V1_7, Opcodes.ACC_PUBLIC, ""UniformDistribution"", null, Type.getInternalName(Object.class), null);

    MethodVisitor mv
        = cv.visitMethod(Opcodes.ACC_STATIC | Opcodes.ACC_PUBLIC, ""sample"", Type.getMethodDescriptor(Type.DOUBLE_TYPE,
            Type.getType(java.lang.invoke.MethodHandle.class) /* rng method */, Type.DOUBLE_TYPE /* max */), null, null);

    mv.visitCode();

    mv.visitVarInsn(Opcodes.ALOAD, 0); // load MethodHandle
    mv.visitInsn(Opcodes.ACONST_NULL); // null string... (just to test signatures with class names)


    // Call MethodHandle.invoke() with polymorphic signature: ()D
    mv.visitMethodInsn(Opcodes.INVOKEVIRTUAL, Type.getInternalName(java.lang.invoke.MethodHandle.class), ""invoke"",
        Type.getMethodDescriptor(Type.DOUBLE_TYPE, Type.getType(String.class)), false);

    mv.visitVarInsn(Opcodes.DLOAD, 1);
    mv.visitInsn(Opcodes.DMUL);
    mv.visitInsn(Opcodes.DRETURN);
    mv.visitEnd();
    cv.visitEnd();

    File tempDir = Files.createTempDir();
    File classFile = new File(tempDir, ""UniformDistribution.class"");
    Files.write(cv.toByteArray(), classFile);

    G.reset();

    String[] commandLine = { ""-pp"", ""-cp"", tempDir.getAbsolutePath(), ""-O"", ""UniformDistribution"" };

    System.out.println(""Command Line: "" + Arrays.toString(commandLine));

    Main.main(commandLine);
    validateClassFile(""UniformDistribution"");

  }
",non-flaky,5
156154,soot-oss_soot,TypeBasedReflectionModelTest.constantBase,"    @Test
    public void constantBase() {
        genericLocalVsStringConstantTest(true);
    }
",non-flaky,5
156155,soot-oss_soot,TypeBasedReflectionModelTest.localBase,"    @Test
    public void localBase() {
        genericLocalVsStringConstantTest(false);
    }
",non-flaky,5
156156,soot-oss_soot,TypeBasedReflectionModelTest.staticBase,"    @Test
    public void staticBase() {
        //TODO
    }
",non-flaky,5
156157,soot-oss_soot,DeadAssignmentEliminatorTest.keepArrayLength,"  @Test
  public void keepArrayLength() {
    // create test method and body
    SootClass cl = new SootClass(""TestClass"", Modifier.PUBLIC);
    SootMethod method = new SootMethod(""testMethod"", Collections.singletonList(RefType.v(""java.lang.Object"")),
        ArrayType.v(IntType.v(), 1), Modifier.PUBLIC);
    cl.addMethod(method);
    JimpleBody body = Jimple.v().newBody(method);
    method.setActiveBody(body);

    // create locals
    Chain<Local> locals = body.getLocals();
    Local a = Jimple.v().newLocal(""a"", ArrayType.v(IntType.v(), 1));
    locals.add(a);
    Local b = Jimple.v().newLocal(""b"", IntType.v());
    locals.add(b);

    // create code
    UnitPatchingChain units = body.getUnits();
    Unit identity0 = Jimple.v().newIdentityStmt(a, Jimple.v().newParameterRef(RefType.v(""java.lang.Object""), 0));
    units.add(identity0);
    Unit cast0 = Jimple.v().newAssignStmt(b, Jimple.v().newLengthExpr(a));
    units.add(cast0);
    Unit ret = Jimple.v().newReturnStmt(b);
    units.add(ret);

    // execute transform
    DeadAssignmentEliminator.v().internalTransform(body, ""testPhase"", Collections.emptyMap());

    // check resulting code (length statement should be preserved)
    Iterator<Unit> it = units.iterator();
    assertEquals(identity0, it.next());
    assertEquals(cast0, it.next());
    assertEquals(ret, it.next());
    assertEquals(3, units.size());
  }
",non-flaky,5
156158,soot-oss_soot,DeadAssignmentEliminatorTest.keepEssentialCast,"  @Test
  public void keepEssentialCast() {
    // create test method and body
    SootClass cl = new SootClass(""TestClass"", Modifier.PUBLIC);
    SootMethod method = new SootMethod(""testMethod"", Collections.singletonList(RefType.v(""java.lang.Object"")),
        ArrayType.v(IntType.v(), 1), Modifier.PUBLIC);
    cl.addMethod(method);
    JimpleBody body = Jimple.v().newBody(method);
    method.setActiveBody(body);

    // create locals
    Chain<Local> locals = body.getLocals();
    Local a = Jimple.v().newLocal(""a"", IntType.v());
    locals.add(a);
    Local b = Jimple.v().newLocal(""b"", IntType.v());
    locals.add(b);
    Local c = Jimple.v().newLocal(""c"", IntType.v());
    locals.add(c);
    Local d = Jimple.v().newLocal(""d"", IntType.v());
    locals.add(d);

    // create code
    UnitPatchingChain units = body.getUnits();
    Unit identity0 = Jimple.v().newIdentityStmt(a, Jimple.v().newParameterRef(RefType.v(""java.lang.Object""), 0));
    units.add(identity0);
    Unit cast0 = Jimple.v().newAssignStmt(b, Jimple.v().newCastExpr(a, ArrayType.v(IntType.v(), 1)));
    units.add(cast0);
    Unit cast1 = Jimple.v().newAssignStmt(c, Jimple.v().newCastExpr(a, RefType.v(""java.lang.Number"")));
    units.add(cast1);
    Unit cast2 = Jimple.v().newAssignStmt(d, Jimple.v().newCastExpr(NullConstant.v(), RefType.v(""java.lang.Number"")));
    units.add(cast2);
    Unit ret = Jimple.v().newReturnStmt(b);
    units.add(ret);

    // execute transform
    DeadAssignmentEliminator.v().internalTransform(body, ""testPhase"", Collections.emptyMap());

    // check resulting code (cast should be removed)
    Iterator<Unit> it = units.iterator();
    assertEquals(identity0, it.next());
    assertEquals(cast0, it.next());
    assertEquals(cast1, it.next());
    assertEquals(ret, it.next());
    assertEquals(4, units.size());
  }
",non-flaky,5
156159,soot-oss_soot,DeadAssignmentEliminatorTest.removePrimitiveCast,"  @Test
  public void removePrimitiveCast() {
    // create test method and body
    SootClass cl = new SootClass(""TestClass"", Modifier.PUBLIC);
    SootMethod method = new SootMethod(""testMethod"", Arrays.asList(IntType.v(), IntType.v()), IntType.v(), Modifier.PUBLIC);
    cl.addMethod(method);
    JimpleBody body = Jimple.v().newBody(method);
    method.setActiveBody(body);

    // create locals
    Chain<Local> locals = body.getLocals();
    Local a = Jimple.v().newLocal(""a"", IntType.v());
    locals.add(a);
    Local b = Jimple.v().newLocal(""b"", IntType.v());
    locals.add(b);
    Local c = Jimple.v().newLocal(""c"", IntType.v());
    locals.add(c);
    Local d = Jimple.v().newLocal(""d"", DoubleType.v());
    locals.add(d);

    // create code
    UnitPatchingChain units = body.getUnits();
    Unit identity0 = Jimple.v().newIdentityStmt(a, Jimple.v().newParameterRef(IntType.v(), 0));
    units.add(identity0);
    Unit identity1 = Jimple.v().newIdentityStmt(b, Jimple.v().newParameterRef(IntType.v(), 1));
    units.add(identity1);
    Unit addition = Jimple.v().newAssignStmt(c, Jimple.v().newAddExpr(a, b));
    units.add(addition);
    Unit cast = Jimple.v().newAssignStmt(d, Jimple.v().newCastExpr(a, DoubleType.v()));
    units.add(cast);
    Unit ret = Jimple.v().newReturnStmt(c);
    units.add(ret);

    // execute transform
    DeadAssignmentEliminator.v().internalTransform(body, ""testPhase"", Collections.emptyMap());

    // check resulting code (cast should be removed)
    Iterator<Unit> it = units.iterator();
    assertEquals(identity0, it.next());
    assertEquals(identity1, it.next());
    assertEquals(addition, it.next());
    assertEquals(ret, it.next());
    assertEquals(4, units.size());
  }
",non-flaky,5
156160,soot-oss_soot,TypingMinimizeTest.testMostCommonTypingPairs_1,"  @Test
  public void testMostCommonTypingPairs_1() {

    logger.debug(""Starting Object Random Minimize"");

    List<Typing> typingList = new ArrayList<>();
    Type Type1 = serializableType;
    Type Type2 = comparableType;
    Local x1 = new JimpleLocal(""$x1"", null);
    Typing resultTyping;

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, Type1);
    typingList.add(typing1);
    resultTyping = typing1;

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, Type2);
    typingList.add(typing2);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(2, typingList.size());
    assertEquals(resultTyping, typingList.get(0));
  }
",non-flaky,5
156161,soot-oss_soot,TypingMinimizeTest.testMostCommonTypingPairs_2,"  @Test
  public void testMostCommonTypingPairs_2() {

    logger.debug(""Starting Object Random Minimize"");

    List<Typing> typingList = new ArrayList<>();

    Type Type1 = serializableType;
    Type Type2 = comparableType;
    Type Type3 = numberType;
    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, Type1);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, Type2);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, Type3);
    typingList.add(typing3);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(2, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing2, typing3));
  }
",non-flaky,5
156162,soot-oss_soot,TypingMinimizeTest.testMostCommonTypingPairs_3,"  @Test
  public void testMostCommonTypingPairs_3() {

    List<Typing> typingList = new ArrayList<>();
    Type Type1 = randomAccessType;
    Type Type2 = listType;
    Type Type3 = abstractListType;
    Type Type4 = objectType;
    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, Type1);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, Type2);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, Type3);
    typingList.add(typing3);

    Typing typing4 = new Typing(Arrays.asList(x1));
    typing4.set(x1, Type4);
    typingList.add(typing4);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(2, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing1, typing3));
  }
",non-flaky,5
156163,soot-oss_soot,TypingMinimizeTest.testMostCommonTypingPairs_4,"  @Test
  public void testMostCommonTypingPairs_4() {

    List<Typing> typingList = new ArrayList<>();

    Type Type1 = cloneableType;
    Type Type2 = serializableType;
    Type Type3 = abstractMapType;

    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, Type1);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, Type2);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, Type3);
    typingList.add(typing3);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(3, typingList.size());

  }
",non-flaky,5
156164,soot-oss_soot,TypingMinimizeTest.testHugeCommonTypingPair,"  @Test
  public void testHugeCommonTypingPair() {

    List<Typing> typingList = new ArrayList<>();

    Type Type1 = serializableType;
    Type Type2 = comparableType;
    Local x1 = new JimpleLocal(""$x1"", null);
    Local x2 = new JimpleLocal(""$x2"", null);
    Local x3 = new JimpleLocal(""$x3"", null);

    Typing typing1 = new Typing(Arrays.asList(x1, x2, x3));
    typing1.set(x1, Type1);
    typing1.set(x2, Type1);
    typing1.set(x3, Type1);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1, x2, x3));
    typing2.set(x1, Type2);
    typing2.set(x2, Type1);
    typing2.set(x3, Type1);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1, x2, x3));
    typing3.set(x1, Type1);
    typing3.set(x2, Type2);
    typing3.set(x3, Type1);
    typingList.add(typing3);

    Typing typing4 = new Typing(Arrays.asList(x1, x2, x3));
    typing4.set(x1, Type1);
    typing4.set(x2, Type1);
    typing4.set(x3, Type2);
    typingList.add(typing4);

    Typing typing5 = new Typing(Arrays.asList(x1, x2, x3));
    typing5.set(x1, Type2);
    typing5.set(x2, Type2);
    typing5.set(x3, Type1);
    typingList.add(typing5);

    Typing typing6 = new Typing(Arrays.asList(x1, x2, x3));
    typing6.set(x1, Type2);
    typing6.set(x2, Type1);
    typing6.set(x3, Type2);
    typingList.add(typing6);

    Typing typing7 = new Typing(Arrays.asList(x1, x2, x3));
    typing7.set(x1, Type1);
    typing7.set(x2, Type2);
    typing7.set(x3, Type2);
    typingList.add(typing7);

    Typing typing8 = new Typing(Arrays.asList(x1, x2, x3));
    typing8.set(x1, Type2);
    typing8.set(x2, Type2);
    typing8.set(x3, Type2);
    typingList.add(typing8);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(8, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing1, typing2, typing3, typing4, typing5, typing6, typing7, typing8));
  }
",non-flaky,5
156165,soot-oss_soot,TypingMinimizeTest.testAbstractInterfaceTyping,"  @Test
  public void testAbstractInterfaceTyping() {

    List<Typing> typingList = new ArrayList<>();

    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, interfaceType);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, abstractClass_Interface2Type);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, class_AbstractInterfaceClassType);
    typingList.add(typing3);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(1, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing3));
  }
",non-flaky,5
156166,soot-oss_soot,TypingMinimizeTest.testAbstractAbstractTyping,"  @Test
  public void testAbstractAbstractTyping() {

    logger.debug(""Starting Object Random Minimize"");

    List<Typing> typingList = new ArrayList<>();
    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, interfaceType);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, abstractClass_Interface1Type);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, abstractClass_Interface2Type);
    typingList.add(typing3);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(1, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing3));
  }
",non-flaky,5
156167,soot-oss_soot,TypingMinimizeTest.testJavaInterfaceTyping,"  @Test
  public void testJavaInterfaceTyping() {

    List<Typing> typingList = new ArrayList<>();

    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, interfaceType);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, integerType);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, numberType);
    typingList.add(typing3);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(2, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing2, typing1));
  }
",non-flaky,5
156168,soot-oss_soot,TypingMinimizeTest.testInterfaceInterfaceTyping,"  @Test
  public void testInterfaceInterfaceTyping() {

    List<Typing> typingList = new ArrayList<>();

    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, interfaceType);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, interfaceInterfaceType);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, numberType);
    typingList.add(typing3);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(2, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing2, typing3));
  }
",non-flaky,5
156169,soot-oss_soot,TypingMinimizeTest.testAllRelatedClassesTyping,"  @Test
  public void testAllRelatedClassesTyping() {

    List<Typing> typingList = new ArrayList<>();
    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, objectType);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, stringType);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, comparableType);
    typingList.add(typing3);

    Typing typing4 = new Typing(Arrays.asList(x1));
    typing4.set(x1, abstractClass_Interface2Type);
    typingList.add(typing4);

    Typing typing5 = new Typing(Arrays.asList(x1));
    typing5.set(x1, class_AbstractInterfaceClassType);
    typingList.add(typing5);

    Typing typing6 = new Typing(Arrays.asList(x1));
    typing6.set(x1, abstractClass_Interface1Type);
    typingList.add(typing6);

    Typing typing7 = new Typing(Arrays.asList(x1));
    typing7.set(x1, class_InterfaceType);
    typingList.add(typing7);

    Typing typing8 = new Typing(Arrays.asList(x1));
    typing8.set(x1, abstractType);
    typingList.add(typing8);

    Typing typing9 = new Typing(Arrays.asList(x1));
    typing9.set(x1, class_AbstractType);
    typingList.add(typing9);

    Typing typing10 = new Typing(Arrays.asList(x1));
    typing10.set(x1, fatherClassType);
    typingList.add(typing10);

    Typing typing11 = new Typing(Arrays.asList(x1));
    typing11.set(x1, childClassType);
    typingList.add(typing11);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(5, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing2, typing5, typing7, typing9, typing11));
  }
",non-flaky,5
156170,soot-oss_soot,TypingMinimizeTest.testAllNonRelatedClassesTyping,"  @Test
  public void testAllNonRelatedClassesTyping() {

    List<Typing> typingList = new ArrayList<>();
    Local x1 = new JimpleLocal(""$x1"", null);

    Typing typing1 = new Typing(Arrays.asList(x1));
    typing1.set(x1, objectType);
    typingList.add(typing1);

    Typing typing2 = new Typing(Arrays.asList(x1));
    typing2.set(x1, stringType);
    typingList.add(typing2);

    Typing typing3 = new Typing(Arrays.asList(x1));
    typing3.set(x1, cloneableType);
    typingList.add(typing3);

    Typing typing4 = new Typing(Arrays.asList(x1));
    typing4.set(x1, integerType);
    typingList.add(typing4);

    Typing typing5 = new Typing(Arrays.asList(x1));
    typing5.set(x1, processType);
    typingList.add(typing5);

    Typing typing6 = new Typing(Arrays.asList(x1));
    typing6.set(x1, interfaceType);
    typingList.add(typing6);

    Typing typing7 = new Typing(Arrays.asList(x1));
    typing7.set(x1, abstractType);
    typingList.add(typing7);

    Typing typing8 = new Typing(Arrays.asList(x1));
    typing8.set(x1, fatherClassType);
    typingList.add(typing8);

    getTypingStrategy().minimize(typingList, new BytecodeHierarchy());

    assertEquals(7, typingList.size());
    assertThat(typingList, containsInAnyOrder(typing2, typing3, typing4, typing5, typing6, typing7, typing8));
  }
",non-flaky,5
156171,soot-oss_soot,EntryPointsTest.testClinitOf,"	@Test
	public void testClinitOf() {
		Path cp = Paths.get(""src"", ""test"", ""resources"", ""Clinit"", ""bin"");
		G.reset();
		Options.v().set_prepend_classpath(true);
		Options.v().set_process_dir(Collections.singletonList(cp.toFile().getAbsolutePath()));
		Options.v().set_src_prec(Options.src_prec_class);
		Options.v().set_allow_phantom_refs(true);
		Options.v().set_ignore_resolving_levels(true);
		Options.v().setPhaseOption(""cg.spark"", ""on"");
		Options.v().setPhaseOption(""cg.spark"", ""string-constants:true"");
		Options.v().set_whole_program(true);
		Scene.v().loadNecessaryClasses();
		SootMethod mainMethod = Scene.v().getMainMethod();
		Scene.v().setEntryPoints(Collections.singletonList(mainMethod));
		PackManager.v().getPack(""cg"").apply();
		CallGraph cg = Scene.v().getCallGraph();
		boolean found = false;
		for (Edge edge : cg) {
			if (edge.getSrc().method().getSignature().equals(""<soot.Main: void main(java.lang.String[])>"")) {
				if (edge.getTgt().method().getSignature().equals(""<soot.A: void <clinit>()>"")) { // A1 is used in main
					found = true;
					break;
				}
			}
		}
		assertTrue(found);
		SootClass a1 = Scene.v().getSootClassUnsafe(""soot.A1"");
		SootClass a = Scene.v().getSootClassUnsafe(""soot.A"");
		assertTrue(a1 != null);
		List<String> clinits1 = new ArrayList<>();
		EntryPoints.v().clinitsOf(a1).forEach(e -> {
			clinits1.add(e.toString());
		});
		List<String> clinits = new ArrayList<>();
		EntryPoints.v().clinitsOf(a).forEach(e -> {
			clinits.add(e.toString());
		});
		assertEquals(clinits1, clinits);
	}
",non-flaky,5
156172,soot-oss_soot,ModuleUtilTest.ownPackage,"    @Test
    public void ownPackage() {
        G.reset();
        ModuleUtil moduleUtil = ModuleUtil.v();
        ModuleScene moduleScene = ModuleScene.v();

        SootModuleInfo moduleA = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleA"");
        moduleA.addExportedPackage(""de.upb"");
        moduleScene.addClassSilent(moduleA);


        String foundModule = moduleUtil.declaringModule(""de.upb"", ""moduleA"");
        Assert.assertEquals(""moduleA"", foundModule);
    }
",non-flaky,5
156173,soot-oss_soot,ModuleUtilTest.simpleExport,"    @Test
    public void simpleExport() {
        G.reset();
        ModuleScene moduleScene = ModuleScene.v();

        SootModuleInfo moduleA = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleA"");
        moduleA.addExportedPackage(""de.upb"");
        moduleScene.addClassSilent(moduleA);

        SootModuleInfo moduleB = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleB"");
        moduleB.getRequiredModules().put(moduleA, Modifier.REQUIRES_STATIC);
        moduleScene.addClassSilent(moduleB);

        ModuleUtil moduleUtil = ModuleUtil.v();
        String foundModule = moduleUtil.declaringModule(""de.upb.A"", ""moduleB"");
        Assert.assertEquals(""moduleA"", foundModule);

    }
",non-flaky,5
156174,soot-oss_soot,ModuleUtilTest.simpleRequiresTransitiveExport,"    @Test
    public void simpleRequiresTransitiveExport() {
        G.reset();
        ModuleScene moduleScene = ModuleScene.v();

        SootModuleInfo moduleA = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleA"");
        moduleA.addExportedPackage(""de.upb"");
        moduleScene.addClassSilent(moduleA);

        SootModuleInfo moduleB = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleB"");
        moduleB.getRequiredModules().put(moduleA, Modifier.REQUIRES_TRANSITIVE);
        moduleScene.addClassSilent(moduleB);


        SootModuleInfo moduleC = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleC"");
        moduleC.getRequiredModules().put(moduleB, Modifier.REQUIRES_STATIC);
        moduleScene.addClassSilent(moduleC);

        ModuleUtil moduleUtil = ModuleUtil.v();
        String foundModule = moduleUtil.declaringModule(""de.upb.A"", ""moduleC"");
        Assert.assertEquals(""moduleA"", foundModule);

    }
",non-flaky,5
156175,soot-oss_soot,ModuleUtilTest.TwoLevelRequiresTransitiveExport,"    @Test
    public void TwoLevelRequiresTransitiveExport() {
        G.reset();
        ModuleScene moduleScene = ModuleScene.v();

        SootModuleInfo moduleA = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleA"");
        moduleA.addExportedPackage(""de.upb"");
        moduleScene.addClassSilent(moduleA);

        SootModuleInfo moduleB = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleB"");
        moduleB.getRequiredModules().put(moduleA, Modifier.REQUIRES_TRANSITIVE);
        moduleScene.addClassSilent(moduleB);


        SootModuleInfo moduleC = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleC"");
        moduleC.getRequiredModules().put(moduleB, Modifier.REQUIRES_TRANSITIVE);
        moduleScene.addClassSilent(moduleC);


        SootModuleInfo moduleD = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleD"");
        moduleD.getRequiredModules().put(moduleC, Modifier.REQUIRES_STATIC);
        moduleScene.addClassSilent(moduleD);

        ModuleUtil moduleUtil = ModuleUtil.v();
        String foundModule = moduleUtil.declaringModule(""de.upb.A"", ""moduleD"");
        // output should be D, because module C, does NOT REQUIERS TRANSITIVE module B
        Assert.assertEquals(""moduleA"", foundModule);

    }
",non-flaky,5
156176,soot-oss_soot,ModuleUtilTest.TwoLevelRequiresTransitiveExportFailing,"    @Test
    public void TwoLevelRequiresTransitiveExportFailing() {
        G.reset();
        ModuleScene moduleScene = ModuleScene.v();

        SootModuleInfo moduleA = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleA"");
        moduleA.addExportedPackage(""de.upb"");
        moduleScene.addClassSilent(moduleA);

        SootModuleInfo moduleB = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleB"");
        moduleB.getRequiredModules().put(moduleA, Modifier.REQUIRES_TRANSITIVE);
        moduleScene.addClassSilent(moduleB);


        SootModuleInfo moduleC = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleC"");
        moduleC.getRequiredModules().put(moduleB, Modifier.REQUIRES_STATIC);
        moduleScene.addClassSilent(moduleC);


        SootModuleInfo moduleD = new SootModuleInfo(SootModuleInfo.MODULE_INFO, ""moduleD"");
        moduleD.getRequiredModules().put(moduleC, Modifier.REQUIRES_STATIC);
        moduleScene.addClassSilent(moduleD);

        ModuleUtil moduleUtil = ModuleUtil.v();
        String foundModule = moduleUtil.declaringModule(""de.upb.A"", ""moduleD"");
        // output should be D, because module C, does NOT REQUIERS TRANSITIVE module B
        Assert.assertEquals(""moduleD"", foundModule);

    }
",non-flaky,5
156177,soot-oss_soot,AbstractASMBackendTest.runTestAndCompareOutput,"  @Test
  public void runTestAndCompareOutput() throws IOException {
    runSoot();
    String comparisonOutput = createComparison();

    /*
     * Print output for comparison to file for debugging purposes.
     */
    File compareFile = new File(""sootOutput/"" + getTargetClass() + "".asm.compare"");
    PrintWriter ow = new PrintWriter(compareFile);
    ow.print(comparisonOutput);
    ow.flush();
    ow.close();

    File targetFile = new File(""sootOutput/"" + getTargetClass() + "".asm"");
    assertTrue(String.format(""Soot output file %s not found"", targetFile.getAbsolutePath()), targetFile.exists());
    Scanner sootOutput = new Scanner(targetFile);
    Scanner compareOutput = new Scanner(comparisonOutput);

    try {
      System.out.println(
          String.format(""Comparing files %s and %s..."", compareFile.getAbsolutePath(), targetFile.getAbsolutePath()));
      int line = 1;
      while (compareOutput.hasNextLine()) {
        // Soot-output must have as much lines as the compared output.
        assertTrue(String.format(""Too few lines in Soot-output for class %s! Current line: %d. Comparison output: %s"",
            getTargetClass(), line, comparisonOutput), sootOutput.hasNextLine());

        // Get both lines
        String compare = compareOutput.nextLine();

        String output = sootOutput.nextLine();

        // Compare lines
        assertTrue(String.format(""Expected line %s, but got %s in line %d for class %s"", compare.trim(), output.trim(), line,
            getTargetClass()), compare.equals(output));
        ++line;
      }

      assertFalse(String.format(""Too many lines in Soot-output for class %s!"", getTargetClass()), sootOutput.hasNextLine());
      System.out.println(""File comparison successful."");
    } finally {
      sootOutput.close();
      compareOutput.close();
    }
  }
",non-flaky,5
156178,soot-oss_soot,MinimalJavaVersionTest.testMinimalVersionAnnotation,"  @Test
  public void testMinimalVersionAnnotation() {
    thrown.expect(IllegalArgumentException.class);
    thrown.expectMessage(""Enforced Java version 1.3 too low to support required features (1.5 required)"");
    runSoot(""soot.asm.backend.targets.AnnotatedClass"", ""1.3"");

  }
",non-flaky,5
156179,soot-oss_soot,MinimalJavaVersionTest.testSufficientUserVersion,"  @Test
  public void testSufficientUserVersion() {
    try {
      runSoot(""soot.asm.backend.targets.AnnotatedClass"", ""1.7"");
      return;
    } catch (RuntimeException e) {
      fail(""Version 1.7 should be sufficient for features of pkg.AnnotatedClass!"");
    }
  }
",non-flaky,5
156180,soot-oss_soot,ConstantPoolTest.loadClass,"  @Test
  public void loadClass() {
    G.reset();
    // Location of the rt.jar
    String rtJar = System.getProperty(""java.home"") + File.separator + ""lib"" + File.separator + ""rt.jar"";

    // Run Soot and print output to .asm-files.
    Main.main(new String[] { ""-cp"", getClassPathFolder() + File.pathSeparator + rtJar, ""-process-dir"", getTargetFolder(),
        ""-src-prec"", ""only-class"", ""-output-format"", ""class"", ""-asm-backend"", ""-allow-phantom-refs"", ""-java-version"",
        getRequiredJavaVersion(), getTargetClass() });

    File file = new File(""./sootOutput/ConstantPool.class"");
    URL[] urls = null;
    try {
      URL url = file.toURI().toURL();
      urls = new URL[] { url };
      URLClassLoader cl = new URLClassLoader(urls);

      cl.loadClass(getTargetClass());

      // cl.close();
      // Java 6 backwards compatibility hack
      try {
        for (Method m : URLClassLoader.class.getDeclaredMethods()) {
          if (m.getName().equals(""close"")) {
            m.invoke(cl);
            break;
          }
        }
      } catch (Exception e) {
      }
      return;

    } catch (MalformedURLException e) {
      logger.error(e.getMessage(), e);
    } catch (ClassNotFoundException e) {
      logger.error(e.getMessage(), e);
    }

    fail();

  }
",non-flaky,5
346,eclipse_jetty.project,project.MavenMetadataTest.testIsExpiredTimestampYesterday,"@Test
public void testIsExpiredTimestampYesterday() {
    LocalDateTime yesterday = LocalDateTime.now().minusDays(1);
    String timestamp = getTimestampFormatter().format(yesterday);
    assertTrue(MavenMetadata.isExpiredTimestamp(timestamp), ""Timestamp should be stale: "" + timestamp);
}",time,2
170452,eclipse_jetty.project,ObjectMBeanTest.before,"    @BeforeEach
    public void before()
    {
        container = new MBeanContainer(ManagementFactory.getPlatformMBeanServer());
    }
",non-flaky,5
170453,eclipse_jetty.project,ObjectMBeanTest.after,"    @AfterEach
    public void after()
    {
        container.destroy();
        container = null;
    }
",non-flaky,5
170454,eclipse_jetty.project,ObjectMBeanTest.testMBeanForNull,"    @Test
    public void testMBeanForNull()
    {
        Object mBean = container.mbeanFor(null);
        assertNull(mBean);
    }
",non-flaky,5
170455,eclipse_jetty.project,ObjectMBeanTest.testMBeanForString,"    @Test
    public void testMBeanForString()
    {
        String obj = ""foo"";
        Object mbean = container.mbeanFor(obj);
        assertNotNull(mbean);
        container.beanAdded(null, obj);
        ObjectName objectName = container.findMBean(obj);
        assertNotNull(objectName);
    }
",non-flaky,5
170456,eclipse_jetty.project,ObjectMBeanTest.testMBeanForStringArray,"    @Test
    public void testMBeanForStringArray()
    {
        String[] obj = {""a"", ""b""};
        Object mbean = container.mbeanFor(obj);
        assertNotNull(mbean);
        container.beanAdded(null, obj);
        ObjectName objectName = container.findMBean(obj);
        assertNotNull(objectName);
    }
",non-flaky,5
170457,eclipse_jetty.project,ObjectMBeanTest.testMBeanForIntArray,"    @Test
    public void testMBeanForIntArray()
    {
        int[] obj = {0, 1, 2};
        Object mbean = container.mbeanFor(obj);
        assertNotNull(mbean);
        container.beanAdded(null, obj);
        ObjectName objectName = container.findMBean(obj);
        assertNotNull(objectName);
    }
",non-flaky,5
170458,eclipse_jetty.project,ObjectMBeanTest.testMetaDataCaching,"    @Test
    public void testMetaDataCaching()
    {
        Derived derived = new Derived();
        ObjectMBean derivedMBean = (ObjectMBean)container.mbeanFor(derived);
        ObjectMBean derivedMBean2 = (ObjectMBean)container.mbeanFor(derived);
        assertNotSame(derivedMBean, derivedMBean2);
        assertSame(derivedMBean.metaData(), derivedMBean2.metaData());
    }
",non-flaky,5
170459,eclipse_jetty.project,ObjectMBeanTest.testDerivedAttributes,"    @Test
    public void testDerivedAttributes() throws Exception
    {
        Derived derived = new Derived();
        Managed managed = derived.getManagedInstance();
        ObjectMBean derivedMBean = (ObjectMBean)container.mbeanFor(derived);
        ObjectMBean managedMBean = (ObjectMBean)container.mbeanFor(managed);

        container.beanAdded(null, derived);
        container.beanAdded(null, managed);

        MBeanInfo derivedInfo = derivedMBean.getMBeanInfo();
        assertNotNull(derivedInfo);
        MBeanInfo managedInfo = managedMBean.getMBeanInfo();
        assertNotNull(managedInfo);

        assertEquals(""com.acme.Derived"", derivedInfo.getClassName(), ""name does not match"");
        assertEquals(""Test the mbean stuff"", derivedInfo.getDescription(), ""description does not match"");
        assertEquals(5, derivedInfo.getAttributes().length, ""attribute count does not match"");
        assertEquals(""Full Name"", derivedMBean.getAttribute(""fname""), ""attribute values does not match"");

        derivedMBean.setAttribute(new Attribute(""fname"", ""Fuller Name""));
        assertEquals(""Fuller Name"", derivedMBean.getAttribute(""fname""), ""set attribute value does not match"");
        assertEquals(""goop"", derivedMBean.getAttribute(""goop""), ""proxy attribute values do not match"");
    }
",non-flaky,5
170460,eclipse_jetty.project,ObjectMBeanTest.testDerivedOperations,"    @Test
    public void testDerivedOperations() throws Exception
    {
        Derived derived = new Derived();
        ObjectMBean mbean = (ObjectMBean)container.mbeanFor(derived);

        container.beanAdded(null, derived);

        MBeanInfo info = mbean.getMBeanInfo();
        assertEquals(5, info.getOperations().length, ""operation count does not match"");

        MBeanOperationInfo[] operationInfos = info.getOperations();
        boolean publish = false;
        boolean doodle = false;
        boolean good = false;
        for (MBeanOperationInfo operationInfo : operationInfos)
        {
            if (""publish"".equals(operationInfo.getName()))
            {
                publish = true;
                assertEquals(""publish something"", operationInfo.getDescription(), ""description doesn't match"");
            }

            if (""doodle"".equals(operationInfo.getName()))
            {
                doodle = true;
                assertEquals(""Doodle something"", operationInfo.getDescription(), ""description doesn't match"");
                MBeanParameterInfo[] parameterInfos = operationInfo.getSignature();
                assertEquals(""A description of the argument"", parameterInfos[0].getDescription(), ""parameter description doesn't match"");
                assertEquals(""doodle"", parameterInfos[0].getName(), ""parameter name doesn't match"");
            }

            // This is a proxied operation on the MBean wrapper.
            if (""good"".equals(operationInfo.getName()))
            {
                good = true;
                assertEquals(""test of proxy operations"", operationInfo.getDescription(), ""description does not match"");
                assertEquals(""not bad"", mbean.invoke(""good"", new Object[]{}, new String[]{}), ""execution contexts wrong"");
            }
        }

        assertTrue(publish, ""publish operation was not not found"");
        assertTrue(doodle, ""doodle operation was not not found"");
        assertTrue(good, ""good operation was not not found"");
    }
",non-flaky,5
170461,eclipse_jetty.project,ObjectMBeanTest.testMethodNameMining,"    @Test
    public void testMethodNameMining()
    {
        assertEquals(""fullName"", MetaData.toAttributeName(""getFullName""));
        assertEquals(""fullName"", MetaData.toAttributeName(""getfullName""));
        assertEquals(""fullName"", MetaData.toAttributeName(""isFullName""));
        assertEquals(""fullName"", MetaData.toAttributeName(""isfullName""));
        assertEquals(""fullName"", MetaData.toAttributeName(""setFullName""));
        assertEquals(""fullName"", MetaData.toAttributeName(""setfullName""));
        assertEquals(""fullName"", MetaData.toAttributeName(""FullName""));
        assertEquals(""fullName"", MetaData.toAttributeName(""fullName""));
    }
",non-flaky,5
170462,eclipse_jetty.project,MBeanContainerLifeCycleTest.prepare,"    @BeforeEach
    public void prepare() throws Exception
    {
        container = new ContainerLifeCycle();
        mbeanServer = ManagementFactory.getPlatformMBeanServer();
        MBeanContainer mbeanContainer = new MBeanContainer(mbeanServer);
        container.addBean(mbeanContainer);
        container.start();
    }
",non-flaky,5
170463,eclipse_jetty.project,MBeanContainerLifeCycleTest.dispose,"    @AfterEach
    public void dispose() throws Exception
    {
        container.stop();
    }
",non-flaky,5
170464,eclipse_jetty.project,MBeanContainerLifeCycleTest.testAddBeanRegistersMBeanRemoveBeanUnregistersMBean,"    @Test
    public void testAddBeanRegistersMBeanRemoveBeanUnregistersMBean() throws Exception
    {
        // Adding a bean to the container should register the MBean.
        QueuedThreadPool bean = new QueuedThreadPool();
        container.addBean(bean);

        String pkg = bean.getClass().getPackage().getName();
        Set<ObjectName> objectNames = mbeanServer.queryNames(ObjectName.getInstance(pkg + "":*""), null);
        assertEquals(1, objectNames.size());

        // Removing the bean should unregister the MBean.
        container.removeBean(bean);
        objectNames = mbeanServer.queryNames(ObjectName.getInstance(pkg + "":*""), null);
        assertEquals(0, objectNames.size());
    }
",non-flaky,5
170465,eclipse_jetty.project,MBeanContainerLifeCycleTest.testStoppingContainerDoesNotUnregistersMBeans,"    @Test
    public void testStoppingContainerDoesNotUnregistersMBeans() throws Exception
    {
        QueuedThreadPool bean = new QueuedThreadPool();
        container.addBean(bean, true);

        String pkg = bean.getClass().getPackage().getName();
        Set<ObjectName> objectNames = mbeanServer.queryNames(ObjectName.getInstance(pkg + "":*""), null);
        // QueuedThreadPool and ThreadPoolBudget.
        assertEquals(2, objectNames.size());

        container.stop();

        objectNames = mbeanServer.queryNames(ObjectName.getInstance(pkg + "":*""), null);
        assertEquals(2, objectNames.size());

        // Remove the MBeans to start clean on the next test.
        objectNames.forEach(objectName ->
        {
            try
            {
                mbeanServer.unregisterMBean(objectName);
            }
            catch (Throwable ignored)
            {
            }
        });
    }
",non-flaky,5
170466,eclipse_jetty.project,MBeanContainerLifeCycleTest.testDestroyingContainerUnregistersMBeans,"    @Test
    public void testDestroyingContainerUnregistersMBeans() throws Exception
    {
        QueuedThreadPool bean = new QueuedThreadPool();
        container.addBean(bean, true);

        String pkg = bean.getClass().getPackage().getName();
        Set<ObjectName> objectNames = mbeanServer.queryNames(ObjectName.getInstance(pkg + "":*""), null);
        // QueuedThreadPool and ThreadPoolBudget.
        assertEquals(2, objectNames.size());

        container.stop();
        container.destroy();

        objectNames = mbeanServer.queryNames(ObjectName.getInstance(pkg + "":*""), null);
        assertEquals(0, objectNames.size());
    }
",non-flaky,5
170467,eclipse_jetty.project,PojoTest.testOpenPojo,"    @Test
    public void testOpenPojo()
    {
        Validator validator = ValidatorBuilder.create().with(new SetterTester()).with(new GetterTester()).build();
        List<Class> classes = Arrays.asList(MBeanContainer.class, ObjectMBean.class);
        for (Class clazz : classes)
        {
            validator.validate(PojoClassFactory.getPojoClass(clazz));
        }
    }
",non-flaky,5
170468,eclipse_jetty.project,ObjectMBeanUtilTest.setUp,"    @BeforeEach
    public void setUp()
    {
        container = new MBeanContainer(ManagementFactory.getPlatformMBeanServer());
        derivedExtended = new DerivedExtended();
        objectMBean = (ObjectMBean)container.mbeanFor(derivedExtended);
        objectMBeanInfo = objectMBean.getMBeanInfo();
    }
",non-flaky,5
170469,eclipse_jetty.project,ObjectMBeanUtilTest.testBasicOperations,"    @Test
    public void testBasicOperations()
    {
        assertEquals(derivedExtended, objectMBean.getManagedObject(), ""Managed objects should be equal"");
        assertNull(objectMBean.getObjectName(), ""This method call always returns null in the actual code"");
        assertNull(objectMBean.getObjectNameBasis(), ""This method call always returns null in the actual code"");
        assertNull(objectMBean.getObjectContextBasis(), ""This method call always returns null in the actual code"");
        assertEquals(container, objectMBean.getMBeanContainer(), ""Mbean container should be equal"");
        assertEquals(""Test the mbean extended stuff"", objectMBeanInfo.getDescription(), ""Mbean description must be equal to : Test the mbean extended stuff"");
    }
",non-flaky,5
170470,eclipse_jetty.project,ObjectMBeanUtilTest.testGetAttributeMBeanException,"    @Test
    public void testGetAttributeMBeanException() throws Exception
    {
        Attribute attribute = new Attribute(""doodle4"", ""charu"");
        objectMBean.setAttribute(attribute);

        MBeanException e = assertThrows(MBeanException.class, () -> objectMBean.getAttribute(""doodle4""));

        assertNotNull(e, ""An InvocationTargetException must have occurred by now as doodle4() internally throwing exception"");
    }
",non-flaky,5
170471,eclipse_jetty.project,ObjectMBeanUtilTest.testGetAttributeAttributeNotFoundException,"    @Test
    public void testGetAttributeAttributeNotFoundException()
    {
        AttributeNotFoundException e = assertThrows(AttributeNotFoundException.class, () -> objectMBean.getAttribute(""ffname""));

        assertNotNull(e, ""An AttributeNotFoundException must have occurred by now as there is no attribute with the name ffname in bean"");
    }
",non-flaky,5
170472,eclipse_jetty.project,ObjectMBeanUtilTest.testSetAttributeWithCorrectAttrName,"    @Test
    public void testSetAttributeWithCorrectAttrName() throws Exception
    {
        Attribute attribute = new Attribute(""fname"", ""charu"");
        objectMBean.setAttribute(attribute);

        String value = (String)objectMBean.getAttribute(""fname"");

        assertEquals(""charu"", value, ""Attribute(fname) value must be equal to charu"");
    }
",non-flaky,5
170473,eclipse_jetty.project,ObjectMBeanUtilTest.testSetAttributeNullCheck,"    @Test
    public void testSetAttributeNullCheck() throws Exception
    {
        objectMBean.setAttribute(null);

        AttributeNotFoundException e = assertThrows(AttributeNotFoundException.class, () -> objectMBean.getAttribute(null));

        assertNotNull(e, ""An AttributeNotFoundException must have occurred by now as there is no attribute with the name null"");
    }
",non-flaky,5
170474,eclipse_jetty.project,ObjectMBeanUtilTest.testSetAttributeAttributeWithWrongAttrName,"    @Test
    public void testSetAttributeAttributeWithWrongAttrName()
    {
        attribute = new Attribute(""fnameee"", ""charu"");

        AttributeNotFoundException e = assertThrows(AttributeNotFoundException.class, () -> objectMBean.setAttribute(attribute));

        assertNotNull(e, ""An AttributeNotFoundException must have occurred by now as there is no attribute "" + ""with the name ffname in bean"");
    }
",non-flaky,5
170475,eclipse_jetty.project,ObjectMBeanUtilTest.testSetAttributesWithCorrectValues,"    @Test
    public void testSetAttributesWithCorrectValues()
    {
        AttributeList attributes = getAttributes(""fname"", ""vijay"");
        objectMBean.setAttributes(attributes);

        attributes = objectMBean.getAttributes(new String[]{""fname""});

        assertEquals(1, attributes.size());
        assertEquals(""vijay"", ((Attribute)(attributes.get(0))).getValue(), ""Fname value must be equal to vijay"");
    }
",non-flaky,5
170476,eclipse_jetty.project,ObjectMBeanUtilTest.testSetAttributesForArrayTypeAttribute,"    @Test
    public void testSetAttributesForArrayTypeAttribute() throws Exception
    {
        Derived[] deriveds = getArrayTypeAttribute();

        derivedManaged.setAddresses(deriveds);
        mBeanDerivedManaged.getMBeanInfo();

        assertNotNull(mBeanDerivedManaged.getAttribute(""addresses""), ""Address object shouldn't be null"");
    }
",non-flaky,5
170477,eclipse_jetty.project,ObjectMBeanUtilTest.testSetAttributesForCollectionTypeAttribute,"    @Test
    public void testSetAttributesForCollectionTypeAttribute() throws Exception
    {
        ArrayList<Derived> aliasNames = new ArrayList<>(Arrays.asList(getArrayTypeAttribute()));

        derivedManaged.setAliasNames(aliasNames);
        mBeanDerivedManaged.getMBeanInfo();

        assertNotNull(mBeanDerivedManaged.getAttribute(""aliasNames""), ""Address object shouldn't be null"");
        assertNull(mBeanDerivedManaged.getAttribute(""derived""), ""Derived object shouldn't registered with container so its value will be null"");
    }
",non-flaky,5
170478,eclipse_jetty.project,ObjectMBeanUtilTest.testSetAttributesException,"    @Test
    public void testSetAttributesException()
    {
        AttributeList attributes = getAttributes(""fnameee"", ""charu"");

        attributes = objectMBean.setAttributes(attributes);

        // Original code eating the exception and returning zero size list
        assertEquals(0, attributes.size(), ""As there is no attribute with the name fnameee, this should return empty"");
    }
",non-flaky,5
170479,eclipse_jetty.project,ObjectMBeanUtilTest.testInvokeMBeanException,"    @Test
    public void testInvokeMBeanException()
    {
        ReflectionException e = assertThrows(ReflectionException.class, () -> objectMBean.invoke(""doodle2"", new Object[0], new String[0]));

        assertNotNull(e, ""An ReflectionException must have occurred by now as doodle2() in Derived bean is private"");
    }
",non-flaky,5
170480,eclipse_jetty.project,ObjectMBeanUtilTest.testInvokeReflectionException,"    @Test
    public void testInvokeReflectionException()
    {
        MBeanException e = assertThrows(MBeanException.class, () -> objectMBean.invoke(""doodle1"", new Object[0], new String[0]));

        assertNotNull(e, ""MBeanException is null"");
    }
",non-flaky,5
170481,eclipse_jetty.project,ObjectMBeanUtilTest.testInvoke,"    @Test
    public void testInvoke() throws Exception
    {
        String value = (String)objectMBean.invoke(""good"", new Object[0], new String[0]);

        assertEquals(""not bad"", value, ""Method(good) invocation on objectMBean must return not bad"");
    }
",non-flaky,5
170482,eclipse_jetty.project,ObjectMBeanUtilTest.testInvokeNoSuchMethodException,"    @Test
    public void testInvokeNoSuchMethodException()
    {
        // DerivedMBean contains a managed method with the name good,
        // we must call this method without any arguments.
        ReflectionException e = assertThrows(ReflectionException.class, () ->
            objectMBean.invoke(""good"", new Object[0], new String[]{
                ""int aone""
            }));

        assertNotNull(e, ""A ReflectionException must have occurred by now as we cannot call a method with wrong signature"");
    }
",non-flaky,5
170483,eclipse_jetty.project,ObjectMBeanUtilTest.testToAttributeName,"    @Test
    public void testToAttributeName()
    {
        assertEquals(""fullName"", MetaData.toAttributeName(""isfullName""));
    }
",non-flaky,5
170484,eclipse_jetty.project,ConnectorServerTest.tearDown,"    @AfterEach
    public void tearDown() throws Exception
    {
        if (connectorServer != null)
            connectorServer.stop();
    }
",non-flaky,5
170485,eclipse_jetty.project,ConnectorServerTest.testAddressAfterStart,"    @Test
    public void testAddressAfterStart() throws Exception
    {
        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi:///jndi/rmi:///jmxrmi""), objectName);
        connectorServer.start();

        JMXServiceURL address = connectorServer.getAddress();
        assertTrue(address.toString().matches(""service:jmx:rmi://[^:]+:\\d+/jndi/rmi://[^:]+:\\d+/jmxrmi""));
    }
",non-flaky,5
170486,eclipse_jetty.project,ConnectorServerTest.testNoRegistryHostBindsToHost,"    @Test
    public void testNoRegistryHostBindsToHost() throws Exception
    {
        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi:///jndi/rmi:///jmxrmi""), objectName);
        connectorServer.start();

        // Verify that I can connect to the RMI registry using a non-loopback address.
        new Socket(InetAddress.getLocalHost(), 1099).close();
        assertThrows(ConnectException.class, () ->
        {
            // Verify that I cannot connect to the RMI registry using the loopback address.
            new Socket(InetAddress.getLoopbackAddress(), 1099).close();
        });
    }
",non-flaky,5
170487,eclipse_jetty.project,ConnectorServerTest.testNoRegistryHostNonDefaultRegistryPort,"    @Test
    public void testNoRegistryHostNonDefaultRegistryPort() throws Exception
    {
        ServerSocket serverSocket = new ServerSocket(0);
        int registryPort = serverSocket.getLocalPort();
        serverSocket.close();
        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi:///jndi/rmi://:"" + registryPort + ""/jmxrmi""), objectName);
        connectorServer.start();

        // Verify that I can connect to the RMI registry using a non-loopback address.
        new Socket(InetAddress.getLocalHost(), registryPort).close();
        assertThrows(ConnectException.class, () ->
        {
            // Verify that I cannot connect to the RMI registry using the loopback address.
            new Socket(InetAddress.getLoopbackAddress(), registryPort).close();
        });
    }
",non-flaky,5
170488,eclipse_jetty.project,ConnectorServerTest.testAnyRegistryHostBindsToAny,"    @Test
    public void testAnyRegistryHostBindsToAny() throws Exception
    {
        ServerSocket serverSocket = new ServerSocket(0);
        int registryPort = serverSocket.getLocalPort();
        serverSocket.close();
        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi:///jndi/rmi://0.0.0.0:"" + registryPort + ""/jmxrmi""), objectName);
        connectorServer.start();

        // Verify that I can connect to the RMI registry using a non-loopback address.
        new Socket(InetAddress.getLocalHost(), registryPort).close();
        // Verify that I can connect to the RMI registry using the loopback address.
        new Socket(InetAddress.getLoopbackAddress(), registryPort).close();
    }
",non-flaky,5
170489,eclipse_jetty.project,ConnectorServerTest.testLocalhostRegistryBindsToLoopback,"    @Test
    public void testLocalhostRegistryBindsToLoopback() throws Exception
    {
        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi:///jndi/rmi://localhost:1099/jmxrmi""), objectName);
        connectorServer.start();

        InetAddress localHost = InetAddress.getLocalHost();
        if (!localHost.isLoopbackAddress())
        {
            assertThrows(ConnectException.class, () ->
            {
                // Verify that I cannot connect to the RMIRegistry using a non-loopback address.
                new Socket(localHost, 1099);
            });
        }

        InetAddress loopback = InetAddress.getLoopbackAddress();
        new Socket(loopback, 1099).close();
    }
",non-flaky,5
170490,eclipse_jetty.project,ConnectorServerTest.testNoRMIHostBindsToHost,"    @Test
    public void testNoRMIHostBindsToHost() throws Exception
    {
        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi:///jndi/rmi:///jmxrmi""), objectName);
        connectorServer.start();

        // Verify that I can connect to the RMI server using a non-loopback address.
        new Socket(InetAddress.getLocalHost(), connectorServer.getAddress().getPort()).close();
        assertThrows(ConnectException.class, () ->
        {
            // Verify that I cannot connect to the RMI server using the loopback address.
            new Socket(InetAddress.getLoopbackAddress(), connectorServer.getAddress().getPort()).close();
        });
    }
",non-flaky,5
170491,eclipse_jetty.project,ConnectorServerTest.testAnyRMIHostBindsToAny,"    @Test
    public void testAnyRMIHostBindsToAny() throws Exception
    {
        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi://0.0.0.0/jndi/rmi:///jmxrmi""), objectName);
        connectorServer.start();

        // Verify that I can connect to the RMI server using a non-loopback address.
        new Socket(InetAddress.getLocalHost(), connectorServer.getAddress().getPort()).close();
        // Verify that I can connect to the RMI server using the loopback address.
        new Socket(InetAddress.getLoopbackAddress(), connectorServer.getAddress().getPort()).close();
    }
",non-flaky,5
170492,eclipse_jetty.project,ConnectorServerTest.testLocalhostRMIBindsToLoopback,"    @Test
    public void testLocalhostRMIBindsToLoopback() throws Exception
    {
        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi://localhost/jndi/rmi://localhost:1099/jmxrmi""), objectName);
        connectorServer.start();
        JMXServiceURL address = connectorServer.getAddress();

        InetAddress localHost = InetAddress.getLocalHost();
        if (!localHost.isLoopbackAddress())
        {
            assertThrows(ConnectException.class, () ->
            {
                // Verify that I cannot connect to the RMIRegistry using a non-loopback address.
                new Socket(localHost, address.getPort());
            });
        }

        InetAddress loopback = InetAddress.getLoopbackAddress();
        new Socket(loopback, address.getPort()).close();
    }
",non-flaky,5
170493,eclipse_jetty.project,ConnectorServerTest.testRMIServerPort,"    @Test
    public void testRMIServerPort() throws Exception
    {
        ServerSocket server = new ServerSocket(0);
        int port = server.getLocalPort();
        server.close();

        connectorServer = new ConnectorServer(new JMXServiceURL(""service:jmx:rmi://localhost:"" + port + ""/jndi/rmi:///jmxrmi""), objectName);
        connectorServer.start();

        JMXServiceURL address = connectorServer.getAddress();
        assertEquals(port, address.getPort());

        InetAddress loopback = InetAddress.getLoopbackAddress();
        new Socket(loopback, port).close();
    }
",non-flaky,5
170494,eclipse_jetty.project,ConnectorServerTest.testRMIServerAndRMIRegistryOnSameHostAndSamePort,"    @Test
    public void testRMIServerAndRMIRegistryOnSameHostAndSamePort() throws Exception
    {
        // RMI can multiplex connections on the same address and port for different
        // RMI objects, in this case the RMI registry and the RMI server. In this
        // case, the RMIServerSocketFactory will be invoked only once.
        // The case with different address and same port is already covered by TCP,
        // that can listen to 192.168.0.1:1099 and 127.0.0.1:1099 without problems.

        String host = ""localhost"";
        ServerSocket serverSocket = new ServerSocket(0);
        int port = serverSocket.getLocalPort();
        serverSocket.close();

        connectorServer = new ConnectorServer(new JMXServiceURL(""rmi"", host, port, ""/jndi/rmi://"" + host + "":"" + port + ""/jmxrmi""), objectName);
        connectorServer.start();

        JMXServiceURL address = connectorServer.getAddress();
        assertEquals(port, address.getPort());
    }
",non-flaky,5
170495,eclipse_jetty.project,ConnectorServerTest.testJMXOverTLS,"    @Test
    public void testJMXOverTLS() throws Exception
    {
        SslContextFactory.Server sslContextFactory = new SslContextFactory.Server();
        String keyStorePath = MavenTestingUtils.getTestResourcePath(""keystore.p12"").toString();
        String keyStorePassword = ""storepwd"";
        sslContextFactory.setKeyStorePath(keyStorePath);
        sslContextFactory.setKeyStorePassword(keyStorePassword);
        sslContextFactory.start();

        // The RMIClientSocketFactory is stored within the RMI stub.
        // When using TLS, the stub is deserialized in a possibly different
        // JVM that does not have access to the server keystore, and there
        // is no way to provide TLS configuration during the deserialization
        // of the stub. Therefore the client must provide system properties
        // to specify the TLS configuration. For this test it needs the
        // trustStore because the server certificate is self-signed.
        // The server needs to contact the RMI registry and therefore also
        // needs these system properties.
        System.setProperty(""javax.net.ssl.trustStore"", keyStorePath);
        System.setProperty(""javax.net.ssl.trustStorePassword"", keyStorePassword);

        connectorServer = new ConnectorServer(new JMXServiceURL(""rmi"", null, 1100, ""/jndi/rmi://localhost:1100/jmxrmi""), null, objectName, sslContextFactory);
        connectorServer.start();

        // The client needs to talk TLS to the RMI registry to download
        // the RMI server stub, and this is independent from JMX.
        // The RMI server stub then contains the SslRMIClientSocketFactory
        // needed to talk to the RMI server.
        Map<String, Object> clientEnv = new HashMap<>();
        clientEnv.put(ConnectorServer.RMI_REGISTRY_CLIENT_SOCKET_FACTORY_ATTRIBUTE, new SslRMIClientSocketFactory());
        try (JMXConnector client = JMXConnectorFactory.connect(connectorServer.getAddress(), clientEnv))
        {
            client.getMBeanServerConnection().queryNames(null, null);
        }
    }
",non-flaky,5
170496,eclipse_jetty.project,MBeanContainerTest.setUp,"    @BeforeEach
    public void setUp()
    {
        mbeanServer = ManagementFactory.getPlatformMBeanServer();
        mbeanContainer = new MBeanContainer(mbeanServer);
    }
",non-flaky,5
170497,eclipse_jetty.project,MBeanContainerTest.testMakeName,"    @Test
    public void testMakeName()
    {
        beanName = ""mngd:bean"";

        beanName = mbeanContainer.makeName(beanName);

        assertEquals(""mngd_bean"", beanName, ""Bean name should be mngd_bean"");
    }
",non-flaky,5
170498,eclipse_jetty.project,MBeanContainerTest.testFindBean,"    @Test
    public void testFindBean()
    {
        managed = getManaged();

        objectName = mbeanContainer.findMBean(managed);
        assertNotNull(objectName);

        assertEquals(managed, mbeanContainer.findBean(objectName), ""Bean must be added"");
        assertNull(mbeanContainer.findBean(null), ""It must return null as there is no bean with the name null"");
    }
",non-flaky,5
170499,eclipse_jetty.project,MBeanContainerTest.testMBeanContainer,"    @Test
    public void testMBeanContainer()
    {
        assertNotNull(mbeanContainer, ""Container shouldn't be null"");
    }
",non-flaky,5
170500,eclipse_jetty.project,MBeanContainerTest.testGetMBeanServer,"    @Test
    public void testGetMBeanServer()
    {
        assertEquals(mbeanServer, mbeanContainer.getMBeanServer(), ""MBean server Instance must be equal"");
    }
",non-flaky,5
170501,eclipse_jetty.project,MBeanContainerTest.testDomain,"    @Test
    public void testDomain()
    {
        String domain = ""Test"";

        mbeanContainer.setDomain(domain);

        assertEquals(domain, mbeanContainer.getDomain(), ""Domain name must be Test"");
    }
",non-flaky,5
170502,eclipse_jetty.project,MBeanContainerTest.testBeanAdded,"    @Test
    public void testBeanAdded()
    {
        setBeanAdded();

        objectName = mbeanContainer.findMBean(managed);

        assertTrue(mbeanServer.isRegistered(objectName), ""Bean must have been registered"");
    }
",non-flaky,5
170503,eclipse_jetty.project,MBeanContainerTest.testBeanAddedNullCheck,"    @Test
    public void testBeanAddedNullCheck()
    {
        setBeanAdded();
        Integer mbeanCount = mbeanServer.getMBeanCount();

        mbeanContainer.beanAdded(null, null);

        assertEquals(mbeanCount, mbeanServer.getMBeanCount(), ""MBean count must not change after beanAdded(null, null) call"");
    }
",non-flaky,5
170504,eclipse_jetty.project,MBeanContainerTest.testBeanRemoved,"    @Test
    public void testBeanRemoved()
    {
        setUpBeanRemoved();

        mbeanContainer.beanRemoved(null, managed);

        assertNull(mbeanContainer.findMBean(managed), ""Bean shouldn't be registered with container as we removed the bean"");
    }
",non-flaky,5
170505,eclipse_jetty.project,MBeanContainerTest.testBeanRemovedInstanceNotFoundException,"    @Test
    public void testBeanRemovedInstanceNotFoundException() throws Exception
    {
        // given
        setUpBeanRemoved();
        objectName = mbeanContainer.findMBean(managed);

        // when
        mbeanContainer.getMBeanServer().unregisterMBean(objectName);

        // then
        assertFalse(mbeanServer.isRegistered(objectName), ""Bean must not have been registered as we unregistered the bean"");
        // this flow covers InstanceNotFoundException. Actual code just eating
        // the exception. i.e Actual code just printing the stacktrace, whenever
        // an exception of type InstanceNotFoundException occurs.
        mbeanContainer.beanRemoved(null, managed);
    }
",non-flaky,5
170506,eclipse_jetty.project,MBeanContainerTest.testDump,"    @Test
    public void testDump()
    {
        assertNotNull(mbeanContainer.dump(), ""Dump operation shouldn't return null if operation is success"");
    }
",non-flaky,5
170507,eclipse_jetty.project,MBeanContainerTest.testDestroy,"    @Test
    public void testDestroy()
    {
        setUpDestroy();

        objectName = mbeanContainer.findMBean(managed);
        mbeanContainer.destroy();

        assertFalse(mbeanContainer.getMBeanServer().isRegistered(objectName), ""Unregistered bean - managed"");
    }
",non-flaky,5
170508,eclipse_jetty.project,MBeanContainerTest.testDestroyInstanceNotFoundException,"    @Test
    public void testDestroyInstanceNotFoundException() throws Exception
    {
        setUpDestroy();

        objectName = mbeanContainer.findMBean(managed);
        mbeanContainer.getMBeanServer().unregisterMBean(objectName);

        assertFalse(mbeanContainer.getMBeanServer().isRegistered(objectName), ""Unregistered bean - managed"");
        // this flow covers InstanceNotFoundException. Actual code just eating
        // the exception. i.e Actual code just printing the stacktrace, whenever
        // an exception of type InstanceNotFoundException occurs.
        mbeanContainer.destroy();
    }
",non-flaky,5
170509,eclipse_jetty.project,MBeanContainerTest.testNonManagedLifecycleNotUnregistered,"    @Test
    public void testNonManagedLifecycleNotUnregistered() throws Exception
    {
        testNonManagedObjectNotUnregistered(new ContainerLifeCycle());
    }
",non-flaky,5
170510,eclipse_jetty.project,MBeanContainerTest.testNonManagedPojoNotUnregistered,"    @Test
    public void testNonManagedPojoNotUnregistered() throws Exception
    {
        testNonManagedObjectNotUnregistered(new Object());
    }
",non-flaky,5
170511,eclipse_jetty.project,TestAnnotationParser.handle,"    @Test
    public void testSampleAnnotation() throws Exception
    {
        String[] classNames = new String[]{""org.eclipse.jetty.annotations.ClassA""};
        AnnotationParser parser = new AnnotationParser();

        class SampleAnnotationHandler extends AnnotationParser.AbstractHandler
        {
            private List<String> methods = Arrays.asList(""a"", ""b"", ""c"", ""d"", ""l"");

            @Override
            public void handle(ClassInfo info, String annotation)
            {
                if (annotation == null || !""org.eclipse.jetty.annotations.Sample"".equals(annotation))
                    return;

                assertEquals(""org.eclipse.jetty.annotations.ClassA"", info.getClassName());
            }
",non-flaky,5
170512,eclipse_jetty.project,TestAnnotationParser.handle,"    @Test
    public void testMultiAnnotation() throws Exception
    {
        String[] classNames = new String[]{""org.eclipse.jetty.annotations.ClassB""};
        AnnotationParser parser = new AnnotationParser();

        class MultiAnnotationHandler extends AnnotationParser.AbstractHandler
        {
            @Override
            public void handle(ClassInfo info, String annotation)
            {
                if (annotation == null || !""org.eclipse.jetty.annotations.Multi"".equals(annotation))
                    return;
                assertTrue(""org.eclipse.jetty.annotations.ClassB"".equals(info.getClassName()));
            }
",non-flaky,5
170513,eclipse_jetty.project,TestAnnotationParser.testHiddenFilesInJar,"    @Test
    public void testHiddenFilesInJar() throws Exception
    {
        File badClassesJar = MavenTestingUtils.getTestResourceFile(""bad-classes.jar"");
        AnnotationParser parser = new AnnotationParser();
        Set<Handler> emptySet = Collections.emptySet();
        parser.parse(emptySet, badClassesJar.toURI());
        // only the valid classes inside bad-classes.jar should be parsed. If any invalid classes are parsed and exception would be thrown here
    }
",non-flaky,5
170514,eclipse_jetty.project,TestAnnotationParser.testModuleInfoClassInJar,"    @Test
    public void testModuleInfoClassInJar() throws Exception
    {
        File badClassesJar = MavenTestingUtils.getTestResourceFile(""jdk9/slf4j-api-1.8.0-alpha2.jar"");
        AnnotationParser parser = new AnnotationParser();
        Set<Handler> emptySet = Collections.emptySet();
        parser.parse(emptySet, badClassesJar.toURI());
        // Should throw no exceptions, and happily skip the module-info.class files
    }
",non-flaky,5
170515,eclipse_jetty.project,TestAnnotationParser.testJep238MultiReleaseInJar,"    @Test
    public void testJep238MultiReleaseInJar() throws Exception
    {
        File badClassesJar = MavenTestingUtils.getTestResourceFile(""jdk9/log4j-api-2.9.0.jar"");
        AnnotationParser parser = new AnnotationParser();
        Set<Handler> emptySet = Collections.emptySet();
        parser.parse(emptySet, badClassesJar.toURI());
        // Should throw no exceptions, and skip the META-INF/versions/9/* files
    }
",non-flaky,5
170516,eclipse_jetty.project,TestAnnotationParser.testJep238MultiReleaseInJarJDK10,"    @Test
    public void testJep238MultiReleaseInJarJDK10() throws Exception
    {
        File jdk10Jar = MavenTestingUtils.getTestResourceFile(""jdk10/multirelease-10.jar"");
        AnnotationParser parser = new AnnotationParser();
        DuplicateClassScanHandler handler = new DuplicateClassScanHandler();
        Set<Handler> handlers = Collections.singleton(handler);
        parser.parse(handlers, new PathResource(jdk10Jar));
        // Should throw no exceptions
    }
",non-flaky,5
170517,eclipse_jetty.project,TestAnnotationParser.testBasedirExclusion,"    @Test
    public void testBasedirExclusion() throws Exception
    {
        // Build up basedir, which itself has a path segment that violates java package and classnaming.
        // The basedir should have no effect on annotation scanning.
        // Intentionally using a base director name that starts with a "".""
        // This mimics what you see in jenkins, hudson, hadoop, solr, camel, and selenium for their 
        // installed and/or managed webapps
        File basedir = testdir.getPathFile("".base/workspace/classes"").toFile();
        FS.ensureEmpty(basedir);

        // Copy in class that is known to have annotations.
        copyClass(ClassA.class, basedir);

        // Setup Tracker
        TrackingAnnotationHandler tracker = new TrackingAnnotationHandler(Sample.class.getName());

        // Setup annotation scanning
        AnnotationParser parser = new AnnotationParser();

        // Parse
        parser.parse(Collections.singleton(tracker), basedir.toURI());

        // Validate
        assertThat(""Found Class"", tracker.foundClasses, contains(ClassA.class.getName()));
    }
",non-flaky,5
170518,eclipse_jetty.project,TestAnnotationParser.testScanDuplicateClassesInJars,"    @Test
    public void testScanDuplicateClassesInJars() throws Exception
    {
        Resource testJar = Resource.newResource(MavenTestingUtils.getTestResourceFile(""tinytest.jar""));
        Resource testJar2 = Resource.newResource(MavenTestingUtils.getTestResourceFile(""tinytest_copy.jar""));
        AnnotationParser parser = new AnnotationParser();
        DuplicateClassScanHandler handler = new DuplicateClassScanHandler();
        Set<Handler> handlers = Collections.singleton(handler);
        parser.parse(handlers, testJar);
        parser.parse(handlers, testJar2);
        List<String> locations = handler.getParsedList(""org.acme.ClassOne"");
        assertNotNull(locations);
        assertEquals(2, locations.size());
        assertTrue(!(locations.get(0).equals(locations.get(1))));
    }
",non-flaky,5
170519,eclipse_jetty.project,TestAnnotationParser.testScanDuplicateClasses,"    @Test
    public void testScanDuplicateClasses() throws Exception
    {
        Resource testJar = Resource.newResource(MavenTestingUtils.getTestResourceFile(""tinytest.jar""));
        File testClasses = new File(MavenTestingUtils.getTargetDir(), ""test-classes"");
        AnnotationParser parser = new AnnotationParser();
        DuplicateClassScanHandler handler = new DuplicateClassScanHandler();
        Set<Handler> handlers = Collections.singleton(handler);
        parser.parse(handlers, testJar);
        parser.parse(handlers, Resource.newResource(testClasses));
        List<String> locations = handler.getParsedList(""org.acme.ClassOne"");
        assertNotNull(locations);
        assertEquals(2, locations.size());
        assertTrue(!(locations.get(0).equals(locations.get(1))));
    }
",non-flaky,5
170520,eclipse_jetty.project,TestAnnotationConfiguration.setup,"    @BeforeEach
    public void setup() throws Exception
    {
        web25 = MavenTestingUtils.getTestResourceFile(""web25.xml"");
        web31false = MavenTestingUtils.getTestResourceFile(""web31false.xml"");
        web31true = MavenTestingUtils.getTestResourceFile(""web31true.xml"");

        // prepare an sci that will be on the webapp's classpath
        jarDir = new File(MavenTestingUtils.getTestResourcesDir().getParentFile(), ""jar"");
        testSciJar = new File(jarDir, ""test-sci.jar"");
        assertTrue(testSciJar.exists());

        testContainerSciJar = new File(jarDir, ""test-sci-for-container-path.jar"");
        testWebInfClassesJar = new File(jarDir, ""test-sci-for-webinf.jar"");

        // unpack some classes to pretend that are in WEB-INF/classes
        unpacked = new File(MavenTestingUtils.getTargetTestingDir(), ""test-sci-for-webinf"");
        unpacked.mkdirs();
        FS.cleanDirectory(unpacked);
        JAR.unpack(testWebInfClassesJar, unpacked);
        webInfClasses = Resource.newResource(unpacked);

        containerLoader = new URLClassLoader(new URL[]{
            testContainerSciJar.toURI().toURL()
        }, Thread.currentThread().getContextClassLoader());

        targetClasses = Resource.newResource(MavenTestingUtils.getTargetDir().toURI()).addPath(""/test-classes"");

        classes = Arrays.asList(new Resource[]{webInfClasses, targetClasses});

        webAppLoader = new URLClassLoader(new URL[]{
            testSciJar.toURI().toURL(), targetClasses.getURI().toURL(), webInfClasses.getURI().toURL()
        },
            containerLoader);
    }
",non-flaky,5
170521,eclipse_jetty.project,TestAnnotationConfiguration.testAnnotationScanControl,"    @Test
    public void testAnnotationScanControl() throws Exception
    {
        //check that a 2.5 webapp with configurationDiscovered will discover annotations
        TestableAnnotationConfiguration config25 = new TestableAnnotationConfiguration();
        WebAppContext context25 = new WebAppContext();
        context25.setClassLoader(Thread.currentThread().getContextClassLoader());
        context25.setAttribute(AnnotationConfiguration.MULTI_THREADED, Boolean.FALSE);
        context25.setAttribute(AnnotationConfiguration.MAX_SCAN_WAIT, 0);
        context25.setConfigurationDiscovered(false);
        context25.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web25)));
        context25.getServletContext().setEffectiveMajorVersion(2);
        context25.getServletContext().setEffectiveMinorVersion(5);
        config25.configure(context25);
        config25.assertAnnotationDiscovery(false);

        //check that a 2.5 webapp discover annotations
        TestableAnnotationConfiguration config25b = new TestableAnnotationConfiguration();
        WebAppContext context25b = new WebAppContext();
        context25b.setClassLoader(Thread.currentThread().getContextClassLoader());
        context25b.setAttribute(AnnotationConfiguration.MULTI_THREADED, Boolean.FALSE);
        context25b.setAttribute(AnnotationConfiguration.MAX_SCAN_WAIT, 0);
        context25b.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web25)));
        context25b.getServletContext().setEffectiveMajorVersion(2);
        context25b.getServletContext().setEffectiveMinorVersion(5);
        config25b.configure(context25b);
        config25b.assertAnnotationDiscovery(true);

        //check that a 3.x webapp with metadata true won't discover annotations
        TestableAnnotationConfiguration config31 = new TestableAnnotationConfiguration();
        WebAppContext context31 = new WebAppContext();
        context31.setClassLoader(Thread.currentThread().getContextClassLoader());
        context31.setAttribute(AnnotationConfiguration.MULTI_THREADED, Boolean.FALSE);
        context31.setAttribute(AnnotationConfiguration.MAX_SCAN_WAIT, 0);
        context31.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web31true)));
        context31.getServletContext().setEffectiveMajorVersion(3);
        context31.getServletContext().setEffectiveMinorVersion(1);
        config31.configure(context31);
        config31.assertAnnotationDiscovery(false);

        //check that a 3.x webapp with metadata false will discover annotations
        TestableAnnotationConfiguration config31b = new TestableAnnotationConfiguration();
        WebAppContext context31b = new WebAppContext();
        context31b.setClassLoader(Thread.currentThread().getContextClassLoader());
        context31b.setAttribute(AnnotationConfiguration.MULTI_THREADED, Boolean.FALSE);
        context31b.setAttribute(AnnotationConfiguration.MAX_SCAN_WAIT, 0);
        context31b.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web31false)));
        context31b.getServletContext().setEffectiveMajorVersion(3);
        context31b.getServletContext().setEffectiveMinorVersion(1);
        config31b.configure(context31b);
        config31b.assertAnnotationDiscovery(true);
    }
",non-flaky,5
170522,eclipse_jetty.project,TestAnnotationConfiguration.testServerAndWebappSCIs,"    @Test
    public void testServerAndWebappSCIs() throws Exception
    {
        ClassLoader old = Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(webAppLoader);

        try
        {
            AnnotationConfiguration config = new AnnotationConfiguration();
            WebAppContext context = new WebAppContext();
            List<ServletContainerInitializer> scis;

            //test 3.1 webapp loads both server and app scis
            context.setClassLoader(webAppLoader);
            context.getMetaData().addWebInfResource(Resource.newResource(testSciJar.toURI().toURL()));
            context.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web31true)));
            context.getMetaData().setWebInfClassesResources(classes);
            context.getServletContext().setEffectiveMajorVersion(3);
            context.getServletContext().setEffectiveMinorVersion(1);
            scis = config.getNonExcludedInitializers(context);
            assertNotNull(scis);
            assertEquals(3, scis.size());
            assertEquals(""com.acme.ServerServletContainerInitializer"", scis.get(0).getClass().getName()); //container path
            assertEquals(""com.acme.webinf.WebInfClassServletContainerInitializer"", scis.get(1).getClass().getName()); // web-inf
            assertEquals(""com.acme.initializer.FooInitializer"", scis.get(2).getClass().getName()); //web-inf jar no web-fragment
        }
        finally
        {
            Thread.currentThread().setContextClassLoader(old);
        }
    }
",non-flaky,5
170523,eclipse_jetty.project,TestAnnotationConfiguration.createServletContainerInitializerAnnotationHandlers,"    @Test
    public void testClassScanHandlersForSCIs() throws Exception
    {
        //test that SCIs with a @HandlesTypes that is an annotation registers
        //handlers for the scanning phase that will capture the class hierarchy,
        //and also capture all classes that contain the annotation
        ClassLoader old = Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(webAppLoader);

        try
        {
            class MyAnnotationConfiguration extends AnnotationConfiguration
            {

                @Override
                public void createServletContainerInitializerAnnotationHandlers(WebAppContext context, List<ServletContainerInitializer> scis) throws Exception
                {
                    super.createServletContainerInitializerAnnotationHandlers(context, scis);
                    //check class hierarchy scanner handler is registered
                    assertNotNull(_classInheritanceHandler);
                    //check 
                    assertEquals(1, _containerInitializerAnnotationHandlers.size());
                    ContainerInitializerAnnotationHandler handler = _containerInitializerAnnotationHandlers.get(0);
                    assertThat(handler._holder.toString(), containsString(""com.acme.initializer.FooInitializer""));
                    assertEquals(""com.acme.initializer.Foo"", handler._annotation.getName());
                }
",non-flaky,5
170524,eclipse_jetty.project,TestAnnotationConfiguration.testMetaDataCompleteSCIs,"    @Test
    public void testMetaDataCompleteSCIs() throws Exception
    {
        ClassLoader old = Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(webAppLoader);

        try
        {
            AnnotationConfiguration config = new AnnotationConfiguration();
            WebAppContext context = new WebAppContext();
            List<ServletContainerInitializer> scis;
            // test a 3.1 webapp with metadata-complete=false loads both server
            // and webapp scis
            context.setClassLoader(webAppLoader);
            context.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web31false)));
            context.getMetaData().setWebInfClassesResources(classes);
            context.getMetaData().addWebInfResource(Resource.newResource(testSciJar.toURI().toURL()));
            context.getServletContext().setEffectiveMajorVersion(3);
            context.getServletContext().setEffectiveMinorVersion(1);
            scis = config.getNonExcludedInitializers(context);
            assertNotNull(scis);
            assertEquals(3, scis.size());
            assertEquals(""com.acme.ServerServletContainerInitializer"", scis.get(0).getClass().getName()); // container
            // path
            assertEquals(""com.acme.webinf.WebInfClassServletContainerInitializer"", scis.get(1).getClass().getName()); // web-inf
            assertEquals(""com.acme.initializer.FooInitializer"", scis.get(2).getClass().getName()); // web-inf
            // jar
            // no
            // web-fragment
        }
        finally
        {
            Thread.currentThread().setContextClassLoader(old);
        }
    }
",non-flaky,5
170525,eclipse_jetty.project,TestAnnotationConfiguration.testRelativeOrderingWithSCIs,"    @Test
    public void testRelativeOrderingWithSCIs() throws Exception
    {
        // test a 3.1 webapp with RELATIVE ORDERING loads sci from
        // equivalent of WEB-INF/classes first as well as container path

        ClassLoader old = Thread.currentThread().getContextClassLoader();

        File orderedFragmentJar = new File(jarDir, ""test-sci-with-ordering.jar"");
        assertTrue(orderedFragmentJar.exists());
        URLClassLoader orderedLoader = new URLClassLoader(new URL[]{
            orderedFragmentJar.toURI().toURL(), testSciJar.toURI().toURL(),
            targetClasses.getURI().toURL(), webInfClasses.getURI().toURL()
        },
            containerLoader);
        Thread.currentThread().setContextClassLoader(orderedLoader);

        try
        {
            AnnotationConfiguration config = new AnnotationConfiguration();
            WebAppContext context = new WebAppContext();
            List<ServletContainerInitializer> scis;
            context.setClassLoader(orderedLoader);
            context.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web31true)));
            RelativeOrdering ordering = new RelativeOrdering(context.getMetaData());
            context.getMetaData().setOrdering(ordering);
            context.getMetaData().addWebInfResource(Resource.newResource(orderedFragmentJar.toURI().toURL()));
            context.getMetaData().addWebInfResource(Resource.newResource(testSciJar.toURI().toURL()));
            context.getMetaData().setWebInfClassesResources(classes);
            context.getMetaData().orderFragments();
            context.getServletContext().setEffectiveMajorVersion(3);
            context.getServletContext().setEffectiveMinorVersion(1);
            scis = config.getNonExcludedInitializers(context);
            assertNotNull(scis);
            assertEquals(4, scis.size());
            assertEquals(""com.acme.ServerServletContainerInitializer"", scis.get(0).getClass().getName()); //container path
            assertEquals(""com.acme.webinf.WebInfClassServletContainerInitializer"", scis.get(1).getClass().getName()); // web-inf
            assertEquals(""com.acme.ordering.AcmeServletContainerInitializer"", scis.get(2).getClass().getName()); // first
            assertEquals(""com.acme.initializer.FooInitializer"", scis.get(3).getClass().getName()); //other in ordering
        }
        finally
        {
            Thread.currentThread().setContextClassLoader(old);
        }
    }
",non-flaky,5
170526,eclipse_jetty.project,TestAnnotationConfiguration.testDiscoveredFalseWithSCIs,"    @Test
    public void testDiscoveredFalseWithSCIs() throws Exception
    {
        ClassLoader old = Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(webAppLoader);
        try
        {
            //test 2.5 webapp with configurationDiscovered=false loads only server scis
            AnnotationConfiguration config = new AnnotationConfiguration();
            WebAppContext context = new WebAppContext();
            List<ServletContainerInitializer> scis;
            context.setConfigurationDiscovered(false);
            context.setClassLoader(webAppLoader);
            context.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web25)));
            context.getMetaData().setWebInfClassesResources(classes);
            context.getMetaData().addWebInfResource(Resource.newResource(testSciJar.toURI().toURL()));
            context.getServletContext().setEffectiveMajorVersion(2);
            context.getServletContext().setEffectiveMinorVersion(5);
            scis = config.getNonExcludedInitializers(context);
            assertNotNull(scis);
            for (ServletContainerInitializer s : scis)
            {
                //should not have any of the web-inf lib scis in here
                assertFalse(s.getClass().getName().equals(""com.acme.ordering.AcmeServletContainerInitializer""));
                assertFalse(s.getClass().getName().equals(""com.acme.initializer.FooInitializer""));
                //NOTE: should also not have the web-inf classes scis in here either, but due to the
                //way the test is set up, the sci we're pretending is in web-inf classes will actually
                //NOT be loaded by the webapp's classloader, but rather by the junit classloader, so
                //it looks as if it is a container class.
            }
        }
        finally
        {
            Thread.currentThread().setContextClassLoader(old);
        }
    }
",non-flaky,5
170527,eclipse_jetty.project,TestAnnotationConfiguration.testDiscoveredTrueWithSCIs,"    @Test
    public void testDiscoveredTrueWithSCIs() throws Exception
    {
        ClassLoader old = Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(webAppLoader);
        try
        {
            //test 2.5 webapp with configurationDiscovered=true loads both server and webapp scis
            AnnotationConfiguration config = new AnnotationConfiguration();
            WebAppContext context = new WebAppContext();
            List<ServletContainerInitializer> scis;
            context.setConfigurationDiscovered(true);
            context.setClassLoader(webAppLoader);
            context.getMetaData().setWebDescriptor(new WebDescriptor(Resource.newResource(web25)));
            context.getMetaData().setWebInfClassesResources(classes);
            context.getMetaData().addWebInfResource(Resource.newResource(testSciJar.toURI().toURL()));
            context.getServletContext().setEffectiveMajorVersion(2);
            context.getServletContext().setEffectiveMinorVersion(5);
            scis = config.getNonExcludedInitializers(context);
            assertNotNull(scis);
            assertEquals(3, scis.size());
            assertEquals(""com.acme.ServerServletContainerInitializer"", scis.get(0).getClass().getName()); //container path
            assertEquals(""com.acme.webinf.WebInfClassServletContainerInitializer"", scis.get(1).getClass().getName()); // web-inf
            assertEquals(""com.acme.initializer.FooInitializer"", scis.get(2).getClass().getName()); //web-inf jar no web-fragment
        }
        finally
        {
            Thread.currentThread().setContextClassLoader(old);
        }
    }
",non-flaky,5
170528,eclipse_jetty.project,TestSecurityAnnotationConversions.testDenyAllOnClass,"    @Test
    public void testDenyAllOnClass() throws Exception
    {

        WebAppContext wac = makeWebAppContext(DenyServlet.class.getCanonicalName(), ""denyServlet"", new String[]{
            ""/foo/*"", ""*.foo""
        });

        //Assume we found 1 servlet with a @HttpConstraint with value=EmptyRoleSemantic.DENY security annotation
        ServletSecurityAnnotationHandler annotationHandler = new ServletSecurityAnnotationHandler(wac);
        AnnotationIntrospector introspector = new AnnotationIntrospector(wac);
        introspector.registerHandler(annotationHandler);

        //set up the expected outcomes:
        //1 ConstraintMapping per ServletMapping pathSpec
        Constraint expectedConstraint = new Constraint();
        expectedConstraint.setAuthenticate(true);
        expectedConstraint.setDataConstraint(Constraint.DC_NONE);

        ConstraintMapping[] expectedMappings = new ConstraintMapping[2];

        expectedMappings[0] = new ConstraintMapping();
        expectedMappings[0].setConstraint(expectedConstraint);
        expectedMappings[0].setPathSpec(""/foo/*"");

        expectedMappings[1] = new ConstraintMapping();
        expectedMappings[1].setConstraint(expectedConstraint);
        expectedMappings[1].setPathSpec(""*.foo"");

        introspector.introspect(new DenyServlet(), null);

        compareResults(expectedMappings, ((ConstraintAware)wac.getSecurityHandler()).getConstraintMappings());
    }
",non-flaky,5
170529,eclipse_jetty.project,TestSecurityAnnotationConversions.testPermitAll,"    @Test
    public void testPermitAll() throws Exception
    {
        //Assume we found 1 servlet with a @ServletSecurity security annotation
        WebAppContext wac = makeWebAppContext(PermitServlet.class.getCanonicalName(), ""permitServlet"", new String[]{
            ""/foo/*"", ""*.foo""
        });

        ServletSecurityAnnotationHandler annotationHandler = new ServletSecurityAnnotationHandler(wac);
        AnnotationIntrospector introspector = new AnnotationIntrospector(wac);
        introspector.registerHandler(annotationHandler);

        //set up the expected outcomes - no constraints at all as per Servlet Spec 3.1 pg 129
        //1 ConstraintMapping per ServletMapping pathSpec

        ConstraintMapping[] expectedMappings = new ConstraintMapping[]{};
        PermitServlet permit = new PermitServlet();
        introspector.introspect(permit, null);

        compareResults(expectedMappings, ((ConstraintAware)wac.getSecurityHandler()).getConstraintMappings());
    }
",non-flaky,5
170530,eclipse_jetty.project,TestSecurityAnnotationConversions.testRolesAllowedWithTransportGuarantee,"    @Test
    public void testRolesAllowedWithTransportGuarantee() throws Exception
    {
        //Assume we found 1 servlet with annotation with roles defined and
        //and a TransportGuarantee

        WebAppContext wac = makeWebAppContext(RolesServlet.class.getCanonicalName(), ""rolesServlet"", new String[]{
            ""/foo/*"", ""*.foo""
        });

        ServletSecurityAnnotationHandler annotationHandler = new ServletSecurityAnnotationHandler(wac);
        AnnotationIntrospector introspector = new AnnotationIntrospector(wac);
        introspector.registerHandler(annotationHandler);

        //set up the expected outcomes:compareResults
        //1 ConstraintMapping per ServletMapping
        Constraint expectedConstraint = new Constraint();
        expectedConstraint.setAuthenticate(true);
        expectedConstraint.setRoles(new String[]{""tom"", ""dick"", ""harry""});
        expectedConstraint.setDataConstraint(Constraint.DC_CONFIDENTIAL);

        ConstraintMapping[] expectedMappings = new ConstraintMapping[2];
        expectedMappings[0] = new ConstraintMapping();
        expectedMappings[0].setConstraint(expectedConstraint);
        expectedMappings[0].setPathSpec(""/foo/*"");

        expectedMappings[1] = new ConstraintMapping();
        expectedMappings[1].setConstraint(expectedConstraint);
        expectedMappings[1].setPathSpec(""*.foo"");
        introspector.introspect(new RolesServlet(), null);
        compareResults(expectedMappings, ((ConstraintAware)wac.getSecurityHandler()).getConstraintMappings());
    }
",non-flaky,5
170531,eclipse_jetty.project,TestSecurityAnnotationConversions.testMethodAnnotation,"    @Test
    public void testMethodAnnotation() throws Exception
    {
        //ServletSecurity annotation with HttpConstraint of TransportGuarantee.CONFIDENTIAL, and a list of rolesAllowed, and
        //an HttpMethodConstraint for GET method that permits all and has TransportGuarantee.NONE (ie is default)

        WebAppContext wac = makeWebAppContext(Method1Servlet.class.getCanonicalName(), ""method1Servlet"", new String[]{
            ""/foo/*"", ""*.foo""
        });

        //set up the expected outcomes: - a Constraint for the RolesAllowed on the class
        //with userdata constraint of DC_CONFIDENTIAL
        //and mappings for each of the pathSpecs
        Constraint expectedConstraint1 = new Constraint();
        expectedConstraint1.setAuthenticate(true);
        expectedConstraint1.setRoles(new String[]{""tom"", ""dick"", ""harry""});
        expectedConstraint1.setDataConstraint(Constraint.DC_CONFIDENTIAL);

        //a Constraint for the PermitAll on the doGet method with a userdata
        //constraint of DC_CONFIDENTIAL inherited from the class
        Constraint expectedConstraint2 = new Constraint();
        expectedConstraint2.setDataConstraint(Constraint.DC_NONE);

        ConstraintMapping[] expectedMappings = new ConstraintMapping[4];
        expectedMappings[0] = new ConstraintMapping();
        expectedMappings[0].setConstraint(expectedConstraint1);
        expectedMappings[0].setPathSpec(""/foo/*"");
        expectedMappings[0].setMethodOmissions(new String[]{""GET""});
        expectedMappings[1] = new ConstraintMapping();
        expectedMappings[1].setConstraint(expectedConstraint1);
        expectedMappings[1].setPathSpec(""*.foo"");
        expectedMappings[1].setMethodOmissions(new String[]{""GET""});

        expectedMappings[2] = new ConstraintMapping();
        expectedMappings[2].setConstraint(expectedConstraint2);
        expectedMappings[2].setPathSpec(""/foo/*"");
        expectedMappings[2].setMethod(""GET"");
        expectedMappings[3] = new ConstraintMapping();
        expectedMappings[3].setConstraint(expectedConstraint2);
        expectedMappings[3].setPathSpec(""*.foo"");
        expectedMappings[3].setMethod(""GET"");

        AnnotationIntrospector introspector = new AnnotationIntrospector(wac);
        ServletSecurityAnnotationHandler annotationHandler = new ServletSecurityAnnotationHandler(wac);
        introspector.registerHandler(annotationHandler);
        introspector.introspect(new Method1Servlet(), null);
        compareResults(expectedMappings, ((ConstraintAware)wac.getSecurityHandler()).getConstraintMappings());
    }
",non-flaky,5
170532,eclipse_jetty.project,TestSecurityAnnotationConversions.testMethodAnnotation2,"    @Test
    public void testMethodAnnotation2() throws Exception
    {
        //A ServletSecurity annotation that has HttpConstraint of CONFIDENTIAL with defined roles, but a
        //HttpMethodConstraint for GET that permits all, but also requires CONFIDENTIAL
        WebAppContext wac = makeWebAppContext(Method2Servlet.class.getCanonicalName(), ""method2Servlet"", new String[]{
            ""/foo/*"", ""*.foo""
        });

        AnnotationIntrospector introspector = new AnnotationIntrospector(wac);
        ServletSecurityAnnotationHandler annotationHandler = new ServletSecurityAnnotationHandler(wac);
        introspector.registerHandler(annotationHandler);

        //set up the expected outcomes: - a Constraint for the RolesAllowed on the class
        //with userdata constraint of DC_CONFIDENTIAL
        //and mappings for each of the pathSpecs
        Constraint expectedConstraint1 = new Constraint();
        expectedConstraint1.setAuthenticate(true);
        expectedConstraint1.setRoles(new String[]{""tom"", ""dick"", ""harry""});
        expectedConstraint1.setDataConstraint(Constraint.DC_CONFIDENTIAL);

        //a Constraint for the Permit on the GET method with a userdata
        //constraint of DC_CONFIDENTIAL
        Constraint expectedConstraint2 = new Constraint();
        expectedConstraint2.setDataConstraint(Constraint.DC_CONFIDENTIAL);

        ConstraintMapping[] expectedMappings = new ConstraintMapping[4];
        expectedMappings[0] = new ConstraintMapping();
        expectedMappings[0].setConstraint(expectedConstraint1);
        expectedMappings[0].setPathSpec(""/foo/*"");
        expectedMappings[0].setMethodOmissions(new String[]{""GET""});
        expectedMappings[1] = new ConstraintMapping();
        expectedMappings[1].setConstraint(expectedConstraint1);
        expectedMappings[1].setPathSpec(""*.foo"");
        expectedMappings[1].setMethodOmissions(new String[]{""GET""});

        expectedMappings[2] = new ConstraintMapping();
        expectedMappings[2].setConstraint(expectedConstraint2);
        expectedMappings[2].setPathSpec(""/foo/*"");
        expectedMappings[2].setMethod(""GET"");
        expectedMappings[3] = new ConstraintMapping();
        expectedMappings[3].setConstraint(expectedConstraint2);
        expectedMappings[3].setPathSpec(""*.foo"");
        expectedMappings[3].setMethod(""GET"");

        introspector.introspect(new Method2Servlet(), null);
        compareResults(expectedMappings, ((ConstraintAware)wac.getSecurityHandler()).getConstraintMappings());
    }
",non-flaky,5
170533,eclipse_jetty.project,TestAnnotationDecorator.testAnnotationDecorator,"    @Test
    public void testAnnotationDecorator() throws Exception
    {
        assertThrows(NullPointerException.class, () ->
        {
            new AnnotationDecorator(null);
        });

        WebAppContext context = new WebAppContext();
        AnnotationDecorator decorator = new AnnotationDecorator(context);
        ServletE servlet = new ServletE();
        //test without BaseHolder metadata
        decorator.decorate(servlet);
        LifeCycleCallbackCollection callbacks = (LifeCycleCallbackCollection)context.getAttribute(LifeCycleCallbackCollection.LIFECYCLE_CALLBACK_COLLECTION);
        assertNotNull(callbacks);
        assertFalse(callbacks.getPreDestroyCallbacks().isEmpty());

        //reset
        context.removeAttribute(LifeCycleCallbackCollection.LIFECYCLE_CALLBACK_COLLECTION);

        //test with BaseHolder metadata, should not introspect with metdata-complete==true
        context.getMetaData().setWebDescriptor(new TestWebDescriptor(MetaData.Complete.True));
        assertTrue(context.getMetaData().isMetaDataComplete());
        ServletHolder holder = new ServletHolder(new Source(Source.Origin.DESCRIPTOR, """"));
        holder.setHeldClass(ServletE.class);
        context.getServletHandler().addServlet(holder);
        DecoratedObjectFactory.associateInfo(holder);
        decorator = new AnnotationDecorator(context);
        decorator.decorate(servlet);
        DecoratedObjectFactory.disassociateInfo();
        callbacks = (LifeCycleCallbackCollection)context.getAttribute(LifeCycleCallbackCollection.LIFECYCLE_CALLBACK_COLLECTION);
        assertNull(callbacks);

        //reset
        context.removeAttribute(LifeCycleCallbackCollection.LIFECYCLE_CALLBACK_COLLECTION);

        //test with BaseHolder metadata, should introspect with metadata-complete==false
        context.getMetaData().setWebDescriptor(new TestWebDescriptor(MetaData.Complete.False));
        DecoratedObjectFactory.associateInfo(holder);
        decorator = new AnnotationDecorator(context);
        decorator.decorate(servlet);
        DecoratedObjectFactory.disassociateInfo();
        callbacks = (LifeCycleCallbackCollection)context.getAttribute(LifeCycleCallbackCollection.LIFECYCLE_CALLBACK_COLLECTION);
        assertNotNull(callbacks);
        assertFalse(callbacks.getPreDestroyCallbacks().isEmpty());
    }
",non-flaky,5
170534,eclipse_jetty.project,TestAnnotationInheritance.destroy,"    @AfterEach
    public void destroy() throws Exception
    {
        classNames.clear();
        InitialContext ic = new InitialContext();
        Context comp = (Context)ic.lookup(""java:comp"");
        comp.destroySubcontext(""env"");
    }
",non-flaky,5
170535,eclipse_jetty.project,TestAnnotationInheritance.testParseClassNames,"    @Test
    public void testParseClassNames() throws Exception
    {
        classNames.add(ClassA.class.getName());
        classNames.add(ClassB.class.getName());

        SampleHandler handler = new SampleHandler();
        AnnotationParser parser = new AnnotationParser();
        parser.parse(Collections.singleton(handler), classNames);

        //check we got  2 class annotations
        assertEquals(2, handler.annotatedClassNames.size());

        //check we got all annotated methods on each class
        assertEquals(7, handler.annotatedMethods.size());
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.a""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.b""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.c""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.d""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.l""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassB.a""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassB.c""));

        //check we got all annotated fields on each class
        assertEquals(1, handler.annotatedFields.size());
        assertEquals(""org.eclipse.jetty.annotations.ClassA.m"", handler.annotatedFields.get(0));
    }
",non-flaky,5
170536,eclipse_jetty.project,TestAnnotationInheritance.testParseClass,"    @Test
    public void testParseClass() throws Exception
    {
        SampleHandler handler = new SampleHandler();
        AnnotationParser parser = new AnnotationParser();
        parser.parse(Collections.singleton(handler), ClassB.class, true);

        //check we got  2 class annotations
        assertEquals(2, handler.annotatedClassNames.size());

        //check we got all annotated methods on each class
        assertEquals(7, handler.annotatedMethods.size());
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.a""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.b""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.c""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.d""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassA.l""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassB.a""));
        assertTrue(handler.annotatedMethods.contains(""org.eclipse.jetty.annotations.ClassB.c""));

        //check we got all annotated fields on each class
        assertEquals(1, handler.annotatedFields.size());
        assertEquals(""org.eclipse.jetty.annotations.ClassA.m"", handler.annotatedFields.get(0));
    }
",non-flaky,5
170537,eclipse_jetty.project,TestAnnotationInheritance.testTypeInheritanceHandling,"    @Test
    public void testTypeInheritanceHandling() throws Exception
    {
        Map<String, Set<String>> map = new ConcurrentHashMap<>();

        AnnotationParser parser = new AnnotationParser();
        ClassInheritanceHandler handler = new ClassInheritanceHandler(map);

        class Foo implements InterfaceD
        {
        }

        classNames.clear();
        classNames.add(ClassA.class.getName());
        classNames.add(ClassB.class.getName());
        classNames.add(InterfaceD.class.getName());
        classNames.add(Foo.class.getName());

        parser.parse(Collections.singleton(handler), classNames);

        assertNotNull(map);
        assertFalse(map.isEmpty());
        assertEquals(2, map.size());

        assertThat(map, hasKey(""org.eclipse.jetty.annotations.ClassA""));
        assertThat(map, hasKey(""org.eclipse.jetty.annotations.InterfaceD""));
        Set<String> classes = map.get(""org.eclipse.jetty.annotations.ClassA"");
        assertThat(classes, contains(""org.eclipse.jetty.annotations.ClassB""));

        classes = map.get(""org.eclipse.jetty.annotations.InterfaceD"");
        assertThat(classes, containsInAnyOrder(""org.eclipse.jetty.annotations.ClassB"",
            Foo.class.getName()));
    }
",non-flaky,5
170538,eclipse_jetty.project,TestAnnotationIntrospector.testIsIntrospectable,"    @Test
    public void testIsIntrospectable() throws Exception
    {
        try (StacklessLogging ignore = new StacklessLogging(AnnotationIntrospector.class))
        {
            WebAppContext wac = new WebAppContext();
            AnnotationIntrospector introspector = new AnnotationIntrospector(wac);
            //can't introspect nothing
            assertFalse(introspector.isIntrospectable(null, null));

            //can introspect if no metadata to say otherwise
            assertTrue(introspector.isIntrospectable(new Object(), null));

            //can introspect if metdata isn't a BaseHolder
            assertTrue(introspector.isIntrospectable(new Object(), new Object()));

            //an EMBEDDED sourced servlet can be introspected
            ServletHolder holder = new ServletHolder();
            holder.setHeldClass(ServletE.class);
            assertTrue(introspector.isIntrospectable(new ServletE(), holder));

            //a JAVAX API sourced servlet can be introspected
            holder = new ServletHolder(Source.JAVAX_API);
            holder.setHeldClass(ServletE.class);
            assertTrue(introspector.isIntrospectable(new ServletE(), holder));

            //an ANNOTATION sourced servlet can be introspected
            holder = new ServletHolder(new Source(Source.Origin.ANNOTATION, ServletE.class.getName()));
            holder.setHeldClass(ServletE.class);
            assertTrue(introspector.isIntrospectable(new ServletE(), holder));

            //a DESCRIPTOR sourced servlet can be introspected if web.xml metdata-complete==false
            File file = MavenTestingUtils.getTestResourceFile(""web31false.xml"");
            Resource resource = Resource.newResource(file);
            wac.getMetaData().setWebDescriptor(new WebDescriptor(resource));
            holder = new ServletHolder(new Source(Source.Origin.DESCRIPTOR, resource.toString()));
            assertTrue(introspector.isIntrospectable(new ServletE(), holder));

            //a DESCRIPTOR sourced servlet can be introspected if web-fragment.xml medata-complete==false && web.xml metadata-complete==false
            file = MavenTestingUtils.getTestResourceFile(""web-fragment4false.xml"");
            resource = Resource.newResource(file);
            wac.getMetaData().addFragmentDescriptor(Resource.newResource(file.getParentFile()), new FragmentDescriptor(resource));
            holder = new ServletHolder(new Source(Source.Origin.DESCRIPTOR, resource.toString()));
            assertTrue(introspector.isIntrospectable(new ServletE(), holder));

            //a DESCRIPTOR sourced servlet cannot be introspected if web-fragment.xml medata-complete==true (&& web.xml metadata-complete==false)
            file = MavenTestingUtils.getTestResourceFile(""web-fragment4true.xml"");
            resource = Resource.newResource(file);
            wac.getMetaData().addFragmentDescriptor(Resource.newResource(file.getParentFile()), new FragmentDescriptor(resource));
            holder = new ServletHolder(new Source(Source.Origin.DESCRIPTOR, resource.toString()));
            assertFalse(introspector.isIntrospectable(new ServletE(), holder));

            //a DESCRIPTOR sourced servlet cannot be introspected if web.xml medata-complete==true
            file = MavenTestingUtils.getTestResourceFile(""web31true.xml"");
            resource = Resource.newResource(file);
            wac.getMetaData().setWebDescriptor(new WebDescriptor(resource));
            holder = new ServletHolder(new Source(Source.Origin.DESCRIPTOR, resource.toString()));
            assertFalse(introspector.isIntrospectable(new ServletE(), holder));
        }
    }
",non-flaky,5
170539,eclipse_jetty.project,TestDiscoveredServletContainerInitializerHolder.test,"    @Test
    public void test() throws Exception
    {
        //SCI with @HandlesTypes[Ordinary, Sample]
        SampleServletContainerInitializer sci = new SampleServletContainerInitializer();
        
        DiscoveredServletContainerInitializerHolder holder = 
            new DiscoveredServletContainerInitializerHolder(new Source(Source.Origin.ANNOTATION, sci.getClass().getName()),
            sci);

        //add the @HandlesTypes to the holder
        holder.addStartupClasses(Ordinary.class, Sample.class);
        
        //pretend scanned and discovered that ASample has the Sample annotation
        holder.addStartupClasses(ASample.class.getName());
        
        //pretend we scanned the entire class hierarchy and found:
        //   com.acme.tom and com.acme.dick both extend Ordinary
        //   ASample has subclass BSample
        Map<String, Set<String>> classMap = new HashMap<>();
        classMap.put(Ordinary.class.getName(), new HashSet(Arrays.asList(""com.acme.tom"", ""com.acme.dick"")));
        classMap.put(ASample.class.getName(), new HashSet(Arrays.asList(BSample.class.getName())));
        holder.resolveClasses(classMap);
        
        //we should now have the following classes that will be passed to the SampleServletContainerInitializer.onStartup
        String toString = holder.toString();
        assertThat(toString, containsString(""com.acme.tom""));
        assertThat(toString, containsString(""com.acme.dick""));
        assertThat(toString, containsString(ASample.class.getName()));
        assertThat(toString, containsString(BSample.class.getName()));
        assertThat(toString, containsString(""applicable=[],annotated=[]""));
    }
",non-flaky,5
170540,eclipse_jetty.project,TestServletAnnotations.testServletAnnotation,"    @Test
    public void testServletAnnotation() throws Exception
    {
        List<String> classes = new ArrayList<String>();
        classes.add(""org.eclipse.jetty.annotations.ServletC"");
        AnnotationParser parser = new AnnotationParser();

        WebAppContext wac = new WebAppContext();
        List<DiscoveredAnnotation> results = new ArrayList<DiscoveredAnnotation>();

        TestWebServletAnnotationHandler handler = new TestWebServletAnnotationHandler(wac, results);

        parser.parse(Collections.singleton(handler), classes);

        assertEquals(1, results.size());
        assertTrue(results.get(0) instanceof WebServletAnnotation);

        results.get(0).apply();

        ServletHolder[] holders = wac.getServletHandler().getServlets();
        assertNotNull(holders);
        assertEquals(1, holders.length);

        // Verify servlet annotations
        ServletHolder cholder = holders[0];
        assertThat(""Servlet Name"", cholder.getName(), is(""CServlet""));
        assertThat(""InitParameter[x]"", cholder.getInitParameter(""x""), is(""y""));
        assertThat(""Init Order"", cholder.getInitOrder(), is(2));
        assertThat(""Async Supported"", cholder.isAsyncSupported(), is(false));

        // Verify mappings
        ServletMapping[] mappings = wac.getServletHandler().getServletMappings();
        assertNotNull(mappings);
        assertEquals(1, mappings.length);
        String[] paths = mappings[0].getPathSpecs();
        assertNotNull(paths);
        assertEquals(2, paths.length);
    }
",non-flaky,5
170541,eclipse_jetty.project,TestServletAnnotations.testWebServletAnnotationOverrideDefault,"    @Test
    public void testWebServletAnnotationOverrideDefault() throws Exception
    {
        //if the existing servlet mapping TO A DIFFERENT SERVLET IS from a default descriptor we
        //DO allow the annotation to replace the mapping.

        WebAppContext wac = new WebAppContext();
        ServletHolder defaultServlet = new ServletHolder();
        defaultServlet.setClassName(""org.eclipse.jetty.servlet.DefaultServlet"");
        defaultServlet.setName(""default"");
        wac.getServletHandler().addServlet(defaultServlet);

        ServletMapping m = new ServletMapping();
        m.setPathSpec(""/"");
        m.setServletName(""default"");
        m.setFromDefaultDescriptor(true);  //this mapping will be from a default descriptor
        wac.getServletHandler().addServletMapping(m);

        WebServletAnnotation annotation = new WebServletAnnotation(wac, ""org.eclipse.jetty.annotations.ServletD"", null);
        annotation.apply();

        //test that as the original servlet mapping had only 1 pathspec, then the whole
        //servlet mapping should be deleted as that pathspec will be remapped to the DServlet
        ServletMapping[] resultMappings = wac.getServletHandler().getServletMappings();
        assertNotNull(resultMappings);
        assertEquals(1, resultMappings.length);
        assertEquals(2, resultMappings[0].getPathSpecs().length);
        resultMappings[0].getServletName().equals(""DServlet"");
        for (String s : resultMappings[0].getPathSpecs())
        {
            assertThat(s, anyOf(is(""/""), is(""/bah/*"")));
        }
    }
",non-flaky,5
170542,eclipse_jetty.project,TestServletAnnotations.testWebServletAnnotationReplaceDefault,"    @Test
    public void testWebServletAnnotationReplaceDefault() throws Exception
    {
        //if the existing servlet mapping TO A DIFFERENT SERVLET IS from a default descriptor we
        //DO allow the annotation to replace the mapping.
        WebAppContext wac = new WebAppContext();
        ServletHolder defaultServlet = new ServletHolder();
        defaultServlet.setClassName(""org.eclipse.jetty.servlet.DefaultServlet"");
        defaultServlet.setName(""default"");
        wac.getServletHandler().addServlet(defaultServlet);

        ServletMapping m = new ServletMapping();
        m.setPathSpec(""/"");
        m.setServletName(""default"");
        m.setFromDefaultDescriptor(true);  //this mapping will be from a default descriptor
        wac.getServletHandler().addServletMapping(m);

        ServletMapping m2 = new ServletMapping();
        m2.setPathSpec(""/other"");
        m2.setServletName(""default"");
        m2.setFromDefaultDescriptor(true);  //this mapping will be from a default descriptor
        wac.getServletHandler().addServletMapping(m2);

        WebServletAnnotation annotation = new WebServletAnnotation(wac, ""org.eclipse.jetty.annotations.ServletD"", null);
        annotation.apply();

        //test that only the mapping for ""/"" was removed from the mappings to the default servlet
        ServletMapping[] resultMappings = wac.getServletHandler().getServletMappings();
        assertNotNull(resultMappings);
        assertEquals(2, resultMappings.length);
        for (ServletMapping r : resultMappings)
        {
            if (r.getServletName().equals(""default""))
            {
                assertEquals(1, r.getPathSpecs().length);
                assertEquals(""/other"", r.getPathSpecs()[0]);
            }
            else if (r.getServletName().equals(""DServlet""))
            {
                assertEquals(2, r.getPathSpecs().length);
                for (String p : r.getPathSpecs())
                {
                    if (!p.equals(""/"") && !p.equals(""/bah/*""))
                        fail(""Unexpected path"");
                }
            }
            else
                fail(""Unexpected servlet mapping: "" + r);
        }
    }
",non-flaky,5
170543,eclipse_jetty.project,TestServletAnnotations.testWebServletAnnotationNotOverride,"    @Test
    public void testWebServletAnnotationNotOverride() throws Exception
    {
        //if the existing servlet mapping TO A DIFFERENT SERVLET IS NOT from a default descriptor we
        //DO NOT allow the annotation to replace the mapping
        WebAppContext wac = new WebAppContext();
        ServletHolder servlet = new ServletHolder();
        servlet.setClassName(""org.eclipse.jetty.servlet.FooServlet"");
        servlet.setName(""foo"");
        wac.getServletHandler().addServlet(servlet);
        ServletMapping m = new ServletMapping();
        m.setPathSpec(""/"");
        m.setServletName(""foo"");
        wac.getServletHandler().addServletMapping(m);

        WebServletAnnotation annotation = new WebServletAnnotation(wac, ""org.eclipse.jetty.annotations.ServletD"", null);
        annotation.apply();

        ServletMapping[] resultMappings = wac.getServletHandler().getServletMappings();
        assertEquals(2, resultMappings.length);
        for (ServletMapping r : resultMappings)
        {
            if (r.getServletName().equals(""DServlet""))
            {
                assertEquals(2, r.getPathSpecs().length);
            }
            else if (r.getServletName().equals(""foo""))
            {
                assertEquals(1, r.getPathSpecs().length);
            }
            else
                fail(""Unexpected servlet name: "" + r);
        }
    }
",non-flaky,5
170544,eclipse_jetty.project,TestServletAnnotations.testWebServletAnnotationIgnore,"    @Test
    public void testWebServletAnnotationIgnore() throws Exception
    {
        //an existing servlet OF THE SAME NAME has even 1 non-default mapping we can't use
        //any of the url mappings in the annotation
        WebAppContext wac = new WebAppContext();
        ServletHolder servlet = new ServletHolder();
        servlet.setClassName(""org.eclipse.jetty.servlet.OtherDServlet"");
        servlet.setName(""DServlet"");
        wac.getServletHandler().addServlet(servlet);

        ServletMapping m = new ServletMapping();
        m.setPathSpec(""/default"");
        m.setFromDefaultDescriptor(true);
        m.setServletName(""DServlet"");
        wac.getServletHandler().addServletMapping(m);

        ServletMapping m2 = new ServletMapping();
        m2.setPathSpec(""/other"");
        m2.setServletName(""DServlet"");
        wac.getServletHandler().addServletMapping(m2);

        WebServletAnnotation annotation = new WebServletAnnotation(wac, ""org.eclipse.jetty.annotations.ServletD"", null);
        annotation.apply();

        ServletMapping[] resultMappings = wac.getServletHandler().getServletMappings();
        assertEquals(2, resultMappings.length);

        for (ServletMapping r : resultMappings)
        {
            assertEquals(1, r.getPathSpecs().length);
            if (!r.getPathSpecs()[0].equals(""/default"") && !r.getPathSpecs()[0].equals(""/other""))
                fail(""Unexpected path in mapping: "" + r);
        }
    }
",non-flaky,5
170545,eclipse_jetty.project,TestServletAnnotations.testWebServletAnnotationNoMappings,"    @Test
    public void testWebServletAnnotationNoMappings() throws Exception
    {
        //an existing servlet OF THE SAME NAME has no mappings, therefore all mappings in the annotation
        //should be accepted
        WebAppContext wac = new WebAppContext();
        ServletHolder servlet = new ServletHolder();
        servlet.setName(""foo"");
        wac.getServletHandler().addServlet(servlet);

        WebServletAnnotation annotation = new WebServletAnnotation(wac, ""org.eclipse.jetty.annotations.ServletD"", null);
        annotation.apply();

        ServletMapping[] resultMappings = wac.getServletHandler().getServletMappings();
        assertEquals(1, resultMappings.length);
        assertEquals(2, resultMappings[0].getPathSpecs().length);
        for (String s : resultMappings[0].getPathSpecs())
        {
            assertThat(s, anyOf(is(""/""), is(""/bah/*"")));
        }
    }
",non-flaky,5
170546,eclipse_jetty.project,TestServletAnnotations.testDeclareRoles,"    @Test
    public void testDeclareRoles()
        throws Exception
",non-flaky,5
170547,eclipse_jetty.project,TestResourceAnnotations.init,"    @BeforeEach
    public void init() throws Exception
    {
        server = new Server();
        wac = new WebAppContext();
        wac.setServer(server);
        injections = new InjectionCollection();
        wac.setAttribute(InjectionCollection.INJECTION_COLLECTION, injections);
        InitialContext ic = new InitialContext();
        comp = (Context)ic.lookup(""java:comp"");
        env = comp.createSubcontext(""env"");
    }
",non-flaky,5
170548,eclipse_jetty.project,TestResourceAnnotations.destroy,"    @AfterEach
    public void destroy() throws Exception
    {
        comp.destroySubcontext(""env"");
    }
",non-flaky,5
170549,eclipse_jetty.project,TestResourceAnnotations.testResourceAnnotations,"    @Test
    public void testResourceAnnotations()
        throws Exception
",non-flaky,5
170550,eclipse_jetty.project,TestResourceAnnotations.testResourcesAnnotation,"    @Test
    public void testResourcesAnnotation()
        throws Exception
",non-flaky,5
170551,eclipse_jetty.project,TestRunAsAnnotation.testRunAsAnnotation,"    @Test
    public void testRunAsAnnotation() throws Exception
    {
        WebAppContext wac = new WebAppContext();
        
        //pre-add a servlet but not by descriptor
        ServletHolder holder = new ServletHolder();
        holder.setName(""foo1"");
        holder.setHeldClass(ServletC.class);
        holder.setInitOrder(1); //load on startup
        wac.getServletHandler().addServletWithMapping(holder, ""/foo/*"");
        
        //add another servlet of the same class, but as if by descriptor
        ServletHolder holder2 = new ServletHolder();
        holder2.setName(""foo2"");
        holder2.setHeldClass(ServletC.class);
        holder2.setInitOrder(1);
        wac.getServletHandler().addServletWithMapping(holder2, ""/foo2/*"");
        Resource fakeXml = Resource.newResource(new File(MavenTestingUtils.getTargetTestingDir(""run-as""), ""fake.xml""));
        wac.getMetaData().setOrigin(holder2.getName() + "".servlet.run-as"", new WebDescriptor(fakeXml));
        
        AnnotationIntrospector parser = new AnnotationIntrospector(wac);
        RunAsAnnotationHandler handler = new RunAsAnnotationHandler(wac);
        parser.registerHandler(handler);
        parser.introspect(new ServletC(), null);
        
        assertEquals(""admin"", holder.getRunAsRole());
        assertEquals(null, holder2.getRunAsRole());
    }
",non-flaky,5
358,OryxProject_oryx,ALSServingInputProducerIT.testALSInputProducer,"@Test
public void testALSInputProducer() throws Exception {
    Map<String, Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.serving.application-resources"", ""\""com.cloudera.oryx.app.serving,com.cloudera.oryx.app.serving.als\"""");
    overlayConfig.put(""oryx.serving.model-manager-class"", ALSServingModelManager.class.getName());
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());
    startMessaging();
    startServer(config);
    @SuppressWarnings(""unchecked"")
    TopicProducer<String, String> inputProducer = ((TopicProducer<String, String>) (getServingLayer().getContext().getServletContext().getAttribute(INPUT_PRODUCER_KEY)));
    String[] inputs = new String[]{ ""abc,123,1.5"", ""xyz,234,-0.5"", ""AB,10,0"" };
    List<Pair<String, String>> keyMessages;
    try (final CloseableIterator<Pair<String, String>> data = new ConsumeData(INPUT_TOPIC, getZKPort()).iterator()) {
        log.info(""Starting consumer thread"");
        ConsumeTopicRunnable consumeInput = new ConsumeTopicRunnable(data);
        new Thread(consumeInput).start();
        Thread.sleep(3000);
        for (String input : inputs) {
            inputProducer.send("""", input);
        }
        Thread.sleep(1000);
        keyMessages = consumeInput.getKeyMessages();
    }
    for (int i = 0; i < keyMessages.size(); i++) {
        Pair<String, String> keyMessage = keyMessages.get(i);
        assertEquals("""", keyMessage.getFirst());
        assertEquals(inputs[i], keyMessage.getSecond());
    }
    assertEquals(inputs.length, keyMessages.size());
}",async wait,0
176832,OryxProject_oryx,RandomManagerTest.testRandomState,"  @Test
  public void testRandomState() {
    // Really, a test that the random generator state is reset in tests
    RandomGenerator generator = RandomManager.getRandom();
    assertEquals(1553355631, generator.nextInt());
    assertNotEquals(1553355631, generator.nextInt());
  }
",non-flaky,5
176833,OryxProject_oryx,RandomManagerRandomTest.testRandomState,"  @Test
  public void testRandomState() {
    RandomGenerator generator = RandomManager.getRandom();
    double unseededValue = generator.nextDouble();
    RandomManager.useTestSeed();
    double seededValue = generator.nextDouble();
    assertNotEquals(unseededValue, seededValue);
    assertEquals(seededValue, RandomManager.getRandom().nextDouble());
  }
",non-flaky,5
176834,OryxProject_oryx,LinearSystemSolverTest.testSolveFToD,"  @Test
  public void testSolveFToD() {
    RealMatrix a = new Array2DRowRealMatrix(new double[][] {
        {1.3, -2.0, 3.0},
        {2.0, 0.0, 5.0},
        {0.0, -1.5, 5.5},
    });
    Solver solver = new LinearSystemSolver().getSolver(a);
    assertNotNull(solver);
    double[] y = solver.solveFToD(new float[] {1.0f, 2.0f, 6.5f});
    assertArrayEquals(
        new double[] {-1.9560439560439564,0.002197802197802894,1.1824175824175824}, y);
  }
",non-flaky,5
176835,OryxProject_oryx,LinearSystemSolverTest.testSolveDToD,"  @Test
  public void testSolveDToD() {
    RealMatrix a = new Array2DRowRealMatrix(new double[][] {
        {1.3, -2.0, 3.0},
        {2.0, 0.0, 5.0},
        {0.0, -1.5, 5.5},
    });
    Solver solver = new LinearSystemSolver().getSolver(a);
    assertNotNull(solver);
    double[] y = solver.solveDToD(new double[]{1.0, 2.0, 6.5});
    assertArrayEquals(
        new double[] {-1.9560439560439564,0.002197802197802894,1.1824175824175824}, y);
  }
",non-flaky,5
176836,OryxProject_oryx,LinearSystemSolverTest.testIsNonSingular,"  @Test
  public void testIsNonSingular() {
    RealMatrix nonSingular = new Array2DRowRealMatrix(new double[][] {
        {1.3, -2.0, 3.0},
        {2.0, 0.0, 5.0},
        {0.0, -1.5, 5.5},
    });
    assertTrue(new LinearSystemSolver().isNonSingular(nonSingular));
    RealMatrix singular = new Array2DRowRealMatrix(new double[][] {
        {1.3, -2.0, 3.0},
        {2.6, -4.0, 6.0},
        {0.0, -1.5, 5.5},
    });
    assertFalse(new LinearSystemSolver().isNonSingular(singular));
  }
",non-flaky,5
176837,OryxProject_oryx,LinearSystemSolverTest.testApparentRank,"  @Test
  public void testApparentRank() {
    RealMatrix nearSingular = new Array2DRowRealMatrix(new double[][] {
        {1.31, -2.0, 3.0},
        {2.6, -4.01, 6.01},
        {0.0, -1.5, 5.5},
    });
    try {
      new LinearSystemSolver().getSolver(nearSingular);
    } catch (SingularMatrixSolverException smse) {
      assertEquals(2, smse.getApparentRank());
    }
  }
",non-flaky,5
176838,OryxProject_oryx,VectorMathTest.testDotFF,"  @Test
  public void testDotFF() {
    assertEquals(5.35, VectorMath.dot(VEC1, VEC2), FLOAT_EPSILON);
  }
",non-flaky,5
176839,OryxProject_oryx,VectorMathTest.testDotDF,"  @Test
  public void testDotDF() {
    assertEquals(5.35, VectorMath.dot(VEC1D, VEC2), FLOAT_EPSILON);
  }
",non-flaky,5
176840,OryxProject_oryx,VectorMathTest.testToFloats,"  @Test
  public void testToFloats() {
    assertArrayEquals(new float[] {1.2f}, VectorMath.toFloats(1.2), FLOAT_EPSILON);
  }
",non-flaky,5
176841,OryxProject_oryx,VectorMathTest.testToDoubles,"  @Test
  public void testToDoubles() {
    assertArrayEquals(new double[] {1.2}, VectorMath.toDoubles(1.2f), FLOAT_EPSILON);
  }
",non-flaky,5
176842,OryxProject_oryx,VectorMathTest.testParseVector,"  @Test
  public void testParseVector() {
    assertArrayEquals(
        new double[] {-1.0, 2.01, 3.5},
        VectorMath.parseVector(new String[] {""-1.0"", ""2.01"", ""3.5""}));
  }
",non-flaky,5
176843,OryxProject_oryx,VectorMathTest.testSmall,"  @Test
  public void testSmall() {
    float[] a = { 1.0e-24f };
    assertEquals(1.0e-24 * 1.0e-24, VectorMath.dot(a, a));
  }
",non-flaky,5
176844,OryxProject_oryx,VectorMathTest.testBig,"  @Test
  public void testBig() {
    float[] a = { 1.0e20f };
    assertEquals((double) 1.0e20f * (double) 1.0e20f, VectorMath.dot(a, a));
  }
",non-flaky,5
176845,OryxProject_oryx,VectorMathTest.testNorm,"  @Test
  public void testNorm() {
    assertEquals(0.0, VectorMath.norm(new float[] {0.0f}), FLOAT_EPSILON);
    assertEquals(3.674234614174767, VectorMath.norm(VEC1), FLOAT_EPSILON);
    assertEquals(10.72800074571213, VectorMath.norm(VEC2), FLOAT_EPSILON);
  }
",non-flaky,5
176846,OryxProject_oryx,VectorMathTest.testTransposeTimesSelf,"  @Test
  public void testTransposeTimesSelf() {
    Map<Integer,float[]> a = new HashMap<>();
    a.put(-1, new float[] {1.3f, -2.0f, 3.0f});
    a.put(1, new float[] {2.0f, 0.0f, 5.0f});
    a.put(3, new float[] {0.0f, -1.5f, 5.5f});
    RealMatrix ata = VectorMath.transposeTimesSelf(a.values());
    RealMatrix expected = new Array2DRowRealMatrix(new double[][] {
        {5.69, -2.6, 13.9},
        {-2.6, 6.25, -14.25},
        {13.9, -14.25, 64.25}
    });
    for (int row = 0; row < 3; row++) {
      for (int col = 0; col < 3; col++) {
        assertEquals(expected.getEntry(row, col), ata.getEntry(row, col), FLOAT_EPSILON);
      }
    }
  }
",non-flaky,5
176847,OryxProject_oryx,DoubleWeightedMeanTest.testNone,"  @Test
  public void testNone() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    assertEquals(0, mean.getN());
    assertTrue(Double.isNaN(mean.getResult()));
  }
",non-flaky,5
176848,OryxProject_oryx,DoubleWeightedMeanTest.testOne,"  @Test
  public void testOne() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    mean.increment(1.5);
    assertEquals(1, mean.getN());
    assertEquals(1.5, mean.getResult());
    assertEquals(""1.5"", mean.toString());
  }
",non-flaky,5
176849,OryxProject_oryx,DoubleWeightedMeanTest.testWeighted,"  @Test
  public void testWeighted() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    mean.increment(0.2, 4.0);
    mean.increment(-0.1, 2.0);
    assertEquals(2, mean.getN());
    assertEquals(0.1, mean.getResult());
  }
",non-flaky,5
176850,OryxProject_oryx,DoubleWeightedMeanTest.testNegative,"  @Test
  public void testNegative() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    mean.increment(-0.1, 2.1);
    mean.increment(0.1, 2.1);
    assertEquals(2, mean.getN());
    assertEquals(0.0, mean.getResult());
  }
",non-flaky,5
176851,OryxProject_oryx,DoubleWeightedMeanTest.testComplex,"  @Test
  public void testComplex() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    for (int i = 1; i <= 5; i++) {
      mean.increment(1.0 / (i + 1), i);
    }
    assertEquals(5, mean.getN());
    assertEquals((1.0/2.0 + 2.0/3.0 + 3.0/4.0 + 4.0/5.0 + 5.0/6.0) / 15.0, mean.getResult());
  }
",non-flaky,5
176852,OryxProject_oryx,DoubleWeightedMeanTest.testCopyEquals,"  @Test
  public void testCopyEquals() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    mean.increment(0.2, 4.0);
    mean.increment(-0.1, 2.0);
    DoubleWeightedMean copy = mean.copy();
    assertEquals(copy, mean);
    assertEquals(copy.hashCode(), mean.hashCode());
    DoubleWeightedMean zero = new DoubleWeightedMean();
    mean.clear();
    assertEquals(zero, mean);
  }
",non-flaky,5
176853,OryxProject_oryx,JVMUtilsTest.close,"  @Test
  public void testShutdownHook() {
    // Can't really test this except to verify that no exception is thrown now or at shutdown
    JVMUtils.closeAtShutdown(new Closeable() {
      @Override
      public void close() {
        // do nothing
      }
",non-flaky,5
176854,OryxProject_oryx,JVMUtilsTest.testUsedMemory,"  @Test
  public void testUsedMemory() {
    // Reasonable guess
    assertTrue(JVMUtils.getUsedMemory() >= 1L << 20);
  }
",non-flaky,5
176855,OryxProject_oryx,LoggingTest.doRun,"  @Test(expected = IllegalStateException.class)
  public void testLoggingRunnableException() {
    new LoggingRunnable() {
      @Override
      public void doRun() throws IOException {
        throw buildIOE();
      }
",non-flaky,5
176856,OryxProject_oryx,LoggingTest.doCall,"  @Test
  public void testLoggingCallable() {
    Integer result = new LoggingCallable<Integer>() {
      @Override
      public Integer doCall() {
        return 3;
      }
",non-flaky,5
176857,OryxProject_oryx,LoggingTest.doCall,"  @Test(expected = IllegalStateException.class)
  public void testLoggingCallableException() {
    new LoggingCallable<Void>() {
      @Override
      public Void doCall() throws IOException {
        throw buildIOE();
      }
",non-flaky,5
176858,OryxProject_oryx,LoggingTest.doCall,"  @Test(expected = IllegalStateException.class)
  public void testLoggingVoidCallableException() {
    new LoggingVoidCallable() {
      @Override
      public void doCall() throws IOException {
        throw buildIOE();
      }
",non-flaky,5
176859,OryxProject_oryx,LangUtilsTest.testHashDouble,"  @Test
  public void testHashDouble() {
    for (int i = 0; i < 1000; i++) {
      assertEquals(Double.valueOf(i).hashCode(), LangUtils.hashDouble(i));
    }
  }
",non-flaky,5
176860,OryxProject_oryx,AutoLockTest.testClose,"  @Test
  public void testClose() {
    ReentrantLock lock = new ReentrantLock();
    assertFalse(lock.isHeldByCurrentThread());
    AutoLock al = new AutoLock(lock);
    assertTrue(lock.isHeldByCurrentThread());
    al.close();
    assertFalse(lock.isHeldByCurrentThread());
  }
",non-flaky,5
176861,OryxProject_oryx,AutoLockTest.testAutoClose,"  @Test
  public void testAutoClose() {
    ReentrantLock lock = new ReentrantLock();
    assertFalse(lock.isHeldByCurrentThread());
    try (AutoLock al = new AutoLock(lock)) {
      assertTrue(lock.isHeldByCurrentThread());
    }
    assertFalse(lock.isHeldByCurrentThread());
  }
",non-flaky,5
176862,OryxProject_oryx,ClassUtilsTest.testLoadClass,"  @Test
  public void testLoadClass() {
    assertSame(ArrayList.class, ClassUtils.loadClass(ArrayList.class.getName()));
  }
",non-flaky,5
176863,OryxProject_oryx,ClassUtilsTest.testLoadClass2,"  @Test
  public void testLoadClass2() {
    assertSame(ArrayList.class, ClassUtils.loadClass(ArrayList.class.getName(), List.class));
  }
",non-flaky,5
176864,OryxProject_oryx,ClassUtilsTest.testLoadInstanceOf,"  @Test
  public void testLoadInstanceOf() {
    assertTrue(ClassUtils.loadInstanceOf(HashSet.class) instanceof HashSet);
  }
",non-flaky,5
176865,OryxProject_oryx,ClassUtilsTest.testLoadInstanceOf2,"  @Test
  public void testLoadInstanceOf2() {
    assertTrue(ClassUtils.loadInstanceOf(HashSet.class.getName(), Set.class) instanceof HashSet);
  }
",non-flaky,5
176866,OryxProject_oryx,ClassUtilsTest.testInstantiateWithArgs,"  @Test
  public void testInstantiateWithArgs() {
    Number n = ClassUtils.loadInstanceOf(Integer.class.getName(),
        Number.class,
        new Class<?>[]{int.class},
        new Object[]{3});
    assertEquals(3, n.intValue());
  }
",non-flaky,5
176867,OryxProject_oryx,ClassUtilsTest.testNoSuchMethod,"  @Test(expected = IllegalArgumentException.class)
  public void testNoSuchMethod() {
    ClassUtils.loadInstanceOf(Long.class.getName(), Long.class);
  }
",non-flaky,5
176868,OryxProject_oryx,ClassUtilsTest.tesInvocationException,"  @Test(expected = IllegalStateException.class)
  public void tesInvocationException() {
    ClassUtils.loadInstanceOf(String.class.getName(),
                              String.class,
                              new Class<?>[] { char[].class },
                              new Object[] { null });
  }
",non-flaky,5
176869,OryxProject_oryx,ClassUtilsTest.testExists,"  @Test
  public void testExists() {
    assertTrue(ClassUtils.classExists(""java.lang.String""));
    assertTrue(ClassUtils.classExists(""com.cloudera.oryx.common.lang.ClassUtils""));
    assertFalse(ClassUtils.classExists(""java.Foo""));
  }
",non-flaky,5
176870,OryxProject_oryx,PMMLUtilsTest.testSkeleton,"  @Test
  public void testSkeleton() {
    PMML pmml = PMMLUtils.buildSkeletonPMML();
    assertEquals(""Oryx"", pmml.getHeader().getApplication().getName());
    assertNotNull(pmml.getHeader().getTimestamp());
  }
",non-flaky,5
176871,OryxProject_oryx,PMMLUtilsTest.testReadWrite,"  @Test
  public void testReadWrite() throws Exception {
    Path tempModelFile = Files.createTempFile(getTempDir(), ""model"", "".pmml.gz"");
    PMML model = buildDummyModel();
    PMMLUtils.write(model, tempModelFile);
    assertTrue(Files.exists(tempModelFile));
    PMML model2 = PMMLUtils.read(tempModelFile);
    List<Model> models = model2.getModels();
    assertEquals(1, models.size());
    assertTrue(models.get(0) instanceof TreeModel);
    TreeModel treeModel = (TreeModel) models.get(0);
    assertEquals(123.0, treeModel.getNode().getRecordCount().doubleValue());
    assertEquals(MiningFunctionType.CLASSIFICATION, treeModel.getFunctionName());
  }
",non-flaky,5
176872,OryxProject_oryx,PMMLUtilsTest.testToString,"  @Test
  public void testToString() throws Exception {
    PMML model = buildDummyModel();
    model.getHeader().setTimestamp(null);
    assertEquals(""<?xml version=\""1.0\"" encoding=\""UTF-8\"" standalone=\""yes\""?>\n"" +
                 ""<PMML version=\""4.2.1\"" xmlns=\""http://www.dmg.org/PMML-4_2\"">\n"" +
                 ""    <Header>\n"" +
                 ""        <Application name=\""Oryx\""/>\n"" +
                 ""    </Header>\n"" +
                 ""    <TreeModel functionName=\""classification\"">\n"" +
                 ""        <Node recordCount=\""123.0\""/>\n"" +
                 ""    </TreeModel>\n"" +
                 ""</PMML>\n"",
                 PMMLUtils.toString(model));
  }
",non-flaky,5
176873,OryxProject_oryx,PMMLUtilsTest.testFromString,"  @Test
  public void testFromString() throws Exception {
    PMML model = buildDummyModel();
    PMML model2 = PMMLUtils.fromString(PMMLUtils.toString(model));
    assertEquals(model.getHeader().getApplication().getName(),
                 model2.getHeader().getApplication().getName());
    assertEquals(model.getModels().get(0).getFunctionName(),
                 model2.getModels().get(0).getFunctionName());
  }
",non-flaky,5
176874,OryxProject_oryx,IOUtilsTest.testDeleteRecursively,"  @Test
  public void testDeleteRecursively() throws IOException {
    Path testDir = createTestDirs();
    IOUtils.deleteRecursively(testDir);
    assertFalse(Files.exists(testDir));
    assertFalse(Files.exists(testDir.resolve(""subFile1"")));
  }
",non-flaky,5
176875,OryxProject_oryx,IOUtilsTest.testListFiles,"  @Test
  public void testListFiles() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*"");
    assertEquals(2, files.size());
    assertTrue(files.contains(testDir.resolve(""subFile1"")));
    assertFalse(files.contains(testDir.resolve("".hidden"")));
    assertTrue(files.contains(testDir.resolve(""subDir1"")));
  }
",non-flaky,5
176876,OryxProject_oryx,IOUtilsTest.testListFiles2,"  @Test
  public void testListFiles2() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, """");
    assertEquals(2, files.size());
    assertTrue(files.contains(testDir.resolve(""subFile1"")));
    assertFalse(files.contains(testDir.resolve("".hidden"")));
    assertTrue(files.contains(testDir.resolve(""subDir1"")));
  }
",non-flaky,5
176877,OryxProject_oryx,IOUtilsTest.testListSubdirs,"  @Test
  public void testListSubdirs() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*/*"");
    assertEquals(2, files.size());
    assertTrue(files.contains(testDir.resolve(""subDir1"").resolve(""subFile2"")));
    assertTrue(files.contains(testDir.resolve(""subDir1"").resolve(""subDir2"")));
  }
",non-flaky,5
176878,OryxProject_oryx,IOUtilsTest.testListSubdirs2,"  @Test
  public void testListSubdirs2() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*/subFile*"");
    assertEquals(1, files.size());
    assertTrue(files.contains(testDir.resolve(""subDir1"").resolve(""subFile2"")));
  }
",non-flaky,5
176879,OryxProject_oryx,IOUtilsTest.testOrder,"  @Test
  public void testOrder() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*/*"");
    assertEquals(testDir.resolve(""subDir1"").resolve(""subDir2""), files.get(0));
    assertEquals(testDir.resolve(""subDir1"").resolve(""subFile2""), files.get(1));
  }
",non-flaky,5
176880,OryxProject_oryx,IOUtilsTest.testReadLines,"  @Test
  public void testReadLines() throws IOException {
    Path tempDir = getTempDir();
    Path textFile = tempDir.resolve(""file.txt"");
    Files.write(textFile, Arrays.asList(""foo"", ""bar"", ""baz""), StandardCharsets.UTF_8);
    Iterator<String> it = IOUtils.readLines(textFile).iterator();
    assertTrue(it.hasNext());
    assertEquals(""foo"", it.next());
    assertTrue(it.hasNext());
    assertEquals(""bar"", it.next());
    assertTrue(it.hasNext());
    assertEquals(""baz"", it.next());
    assertFalse(it.hasNext());
  }
",non-flaky,5
176881,OryxProject_oryx,IOUtilsTest.testChooseFreePort,"  @Test
  public void testChooseFreePort() throws IOException {
    int freePort = IOUtils.chooseFreePort();
    assertTrue(freePort >= 1024 && freePort < 65536);
    try (ServerSocket socket = new ServerSocket(freePort, 0)) {
      assertEquals(freePort, socket.getLocalPort());
    }
  }
",non-flaky,5
176882,OryxProject_oryx,IOUtilsTest.testDistinctFreePorts,"  @Test
  public void testDistinctFreePorts() throws IOException {
    // This whole thing probably won't work unless successive calls really do return
    // different ports instead of reusing free ephemeral ports.
    Set<Integer> ports = new HashSet<>();
    for (int i = 0; i < 10; i++) {
      ports.add(IOUtils.chooseFreePort());
    }
    assertEquals(10, ports.size());
  }
",non-flaky,5
176883,OryxProject_oryx,ConfigUtilsTest.testDefaultConfig,"  @Test
  public void testDefaultConfig() {
    Config config = ConfigUtils.getDefault();
    assertEquals(""yarn-client"", config.getString(""oryx.batch.streaming.master""));
  }
",non-flaky,5
176884,OryxProject_oryx,ConfigUtilsTest.testSerialize,"  @Test
  public void testSerialize() {
    String serialized = ConfigUtils.serialize(ConfigUtils.getDefault());
    assertTrue(serialized.contains(""update-class""));
    Config deserialized = ConfigUtils.deserialize(serialized);
    assertEquals(
        ConfigUtils.getDefault().getString(""oryx.serving.api.port""),
        deserialized.getString(""oryx.serving.api.port""));
  }
",non-flaky,5
176885,OryxProject_oryx,ConfigUtilsTest.testOptionalString,"  @Test
  public void testOptionalString() {
    assertNull(ConfigUtils.getOptionalString(ConfigUtils.getDefault(), ""nonexistent""));
  }
",non-flaky,5
176886,OryxProject_oryx,ConfigUtilsTest.testOptionalStringList,"  @Test
  public void testOptionalStringList() {
    assertNull(ConfigUtils.getOptionalStringList(ConfigUtils.getDefault(), ""nonexistent""));
  }
",non-flaky,5
176887,OryxProject_oryx,ConfigUtilsTest.testOverlayOn,"  @Test
  public void testOverlayOn() {
    Map<String,Object> overlay = new HashMap<>();
    overlay.put(""foo"", ""bar"");
    Config config = ConfigUtils.overlayOn(overlay, ConfigUtils.getDefault());
    assertEquals(""bar"", config.getString(""foo""));
  }
",non-flaky,5
176888,OryxProject_oryx,ConfigUtilsTest.testSetPath,"  @Test
  public void testSetPath() throws Exception {
    Path cwd = Paths.get(""."");
    Map<String,Object> map = new HashMap<>();
    ConfigUtils.set(map, ""cwd"", cwd);
    ConfigUtils.set(map, ""temp"", Paths.get(""/tmp""));
    assertEquals(""\"""" + cwd.toRealPath(LinkOption.NOFOLLOW_LINKS).toUri() + ""\"""", map.get(""cwd""));
    assertEquals(""\""file:///tmp/\"""", map.get(""temp""));
  }
",non-flaky,5
176889,OryxProject_oryx,ConfigUtilsTest.testRedact,"  @Test
  public void testRedact() {
    String redacted = ConfigUtils.redact(""  password=foo \nPassword=foo\nPASSWORD = foo\n"" +
                                             "" the-password= foo \nThe-Password =foo"");
    assertFalse(redacted.contains(""foo""));
    assertTrue(redacted.contains(""*****""));
    assertTrue(redacted.contains(""password=""));
    assertTrue(redacted.contains(""Password=""));
    assertTrue(redacted.contains(""PASSWORD = ""));
    assertTrue(redacted.contains(""the-password= ""));
    assertTrue(redacted.contains(""The-Password =""));
  }
",non-flaky,5
176890,OryxProject_oryx,PairTest.testEquals,"  @Test
  public void testEquals() {
    assertEquals(new Pair<>(3.0, ""foo""), new Pair<>(3.0, ""foo""));
    assertEquals(new Pair<>(null, null), new Pair<>(null, null));
    assertFalse(new Pair<>(3.0, ""foo"").equals(new Pair<>(4.0, ""foo"")));
    assertNotEquals(new Pair<>(3.0, ""foo""), new Pair<>(""foo"", 3.0));
    assertNotEquals(""3.0,foo"", new Pair<>(3.0, ""foo""));
  }
",non-flaky,5
176891,OryxProject_oryx,PairTest.testHashCode,"  @Test
  public void testHashCode() {
    assertEquals(new Pair<>(3.0, ""foo"").hashCode(), new Pair<>(3.0, ""foo"").hashCode());
    assertEquals(new Pair<>(null, null).hashCode(), new Pair<>(null, null).hashCode());
  }
",non-flaky,5
176892,OryxProject_oryx,PairTest.testToString,"  @Test
  public void testToString() {
    assertEquals(""3.0,foo"", new Pair<>(3.0, ""foo"").toString());
  }
",non-flaky,5
176893,OryxProject_oryx,AndPredicateTest.testAnd,"  @Test
  public void testAnd() {
    NotContainsPredicate<String> a = new NotContainsPredicate<>(Arrays.asList(""foo""));
    NotContainsPredicate<String> b = new NotContainsPredicate<>(Arrays.asList(""bar"", ""baz""));
    AndPredicate<String> and = new AndPredicate<>(a, b);
    assertFalse(and.test(""foo""));
    assertFalse(and.test(""bar""));
    assertFalse(and.test(""baz""));
    assertTrue(and.test(""bing""));
  }
",non-flaky,5
176894,OryxProject_oryx,NotContainsPredicateTest.testPredicate,"  @Test
  public void testPredicate() {
    Collection<Integer> contains = Arrays.asList(1, 3, 5);
    NotContainsPredicate<Integer> predicate = new NotContainsPredicate<>(contains);
    assertTrue(predicate.test(2));
    assertFalse(predicate.test(5));
  }
",non-flaky,5
176895,OryxProject_oryx,PairComparatorsTest.testByFirst,"  @Test
  public void testByFirst() {
    List<Pair<Integer,String>> pairs = Arrays.asList(
        new Pair<>(3, ""foo""),
        new Pair<>(4, ""bing""),
        new Pair<>(1, ""baz""),
        new Pair<>(2, ""whizz"")
    );
    Collections.sort(pairs, PairComparators.<Integer>byFirst());
    assertEquals(1, pairs.get(0).getFirst().intValue());
    assertEquals(2, pairs.get(1).getFirst().intValue());
    assertEquals(""baz"", pairs.get(0).getSecond());
    assertEquals(""whizz"", pairs.get(1).getSecond());
  }
",non-flaky,5
176896,OryxProject_oryx,PairComparatorsTest.testBySecond,"  @Test
  public void testBySecond() {
    List<Pair<Integer,String>> pairs = Arrays.asList(
        new Pair<>(3, ""foo""),
        new Pair<>(4, ""bing""),
        new Pair<>(1, ""baz""),
        new Pair<>(2, ""whizz"")
    );
    Collections.sort(pairs, PairComparators.<String>bySecond());
    assertEquals(1, pairs.get(0).getFirst().intValue());
    assertEquals(4, pairs.get(1).getFirst().intValue());
    assertEquals(""baz"", pairs.get(0).getSecond());
    assertEquals(""bing"", pairs.get(1).getSecond());
  }
",non-flaky,5
176897,OryxProject_oryx,KeyOnlyBiPredicateTest.test,"  @Test
  public void testKeyOnly() {
    ObjObjMap<String,String> map = HashObjObjMaps.newMutableMap(
        new String[]{""foo"", ""bar"", ""baz""},
        new String[]{""1"", ""3"", ""4""}
    );
    map.removeIf(new KeyOnlyBiPredicate<String, String>(new Predicate<String>() {
      @Override
      public boolean test(String s) {
        return s.startsWith(""b"");
      }
",non-flaky,5
176898,OryxProject_oryx,TextUtilsTest.testParseJSON,"  @Test
  public void testParseJSON() throws Exception {
    assertArrayEquals(new String[] {""a"", ""1"", ""foo""},
                      TextUtils.parseJSONArray(""[\""a\"",\""1\"",\""foo\""]""));
    assertArrayEquals(new String[] {""a"", ""1"", ""foo"", """"},
                      TextUtils.parseJSONArray(""[\""a\"",\""1\"",\""foo\"",\""\""]""));
    assertArrayEquals(new String[] {""2.3""}, TextUtils.parseJSONArray(""[\""2.3\""]""));
    assertArrayEquals(new String[] {}, TextUtils.parseJSONArray(""[]""));
  }
",non-flaky,5
176899,OryxProject_oryx,TextUtilsTest.testParseDelimited,"  @Test
  public void testParseDelimited() throws Exception {
    assertArrayEquals(new String[] {""a"", ""1"", ""foo""}, TextUtils.parseDelimited(""a,1,foo"", ','));
    assertArrayEquals(new String[] {""a"", ""1"", ""foo"", """"},
                      TextUtils.parseDelimited(""a,1,foo,"", ','));
    assertArrayEquals(new String[] {""2.3""}, TextUtils.parseDelimited(""2.3"", ','));
    assertArrayEquals(new String[] {""\""a\""""}, TextUtils.parseDelimited(""\""\""\""a\""\""\"""", ','));
    assertArrayEquals(new String[] {""\"""", ""\""\""""},
                      TextUtils.parseDelimited(""\""\""\""\"" \""\""\""\""\""\"""", ' '));
    // Different from JSON, sort of:
    assertArrayEquals(new String[] {""""}, TextUtils.parseDelimited("""", ','));
    assertArrayEquals(new String[] {""a"", ""1,"", "",foo""},
                      TextUtils.parseDelimited(""a\t1,\t,foo"", '\t'));
    assertArrayEquals(new String[] {""a"", ""1"", ""foo"", """"},
                      TextUtils.parseDelimited(""a 1 foo "", ' '));
    assertArrayEquals(new String[] {""-1.0"", ""a\"" \""b""},
                      TextUtils.parseDelimited(""-1.0 a\""\\ \""b"", ' '));
    assertArrayEquals(new String[] {""-1.0"", ""a\""b\""c""},
                      TextUtils.parseDelimited(""-1.0 \""a\\\""b\\\""c\"""", ' '));

  }
",non-flaky,5
176900,OryxProject_oryx,TextUtilsTest.testParsePMMLDelimited,"  @Test
  public void testParsePMMLDelimited() {
    assertArrayEquals(new String[] {""1"", ""22"", ""3""}, TextUtils.parsePMMLDelimited(""1 22 3""));
    assertArrayEquals(new String[] {""ab"", ""a b"", ""with \""quotes\"" ""},
                      TextUtils.parsePMMLDelimited(""ab  \""a b\""   \""with \\\""quotes\\\"" \"" ""));
    assertArrayEquals(new String[] {""\"" \""""},
                      TextUtils.parsePMMLDelimited(""\""\\\"" \\\""\""""));
    assertArrayEquals(new String[] {"" c\"" d \""e "", "" c\"" d \""e ""},
                      TextUtils.parsePMMLDelimited("" \"" c\\\"" d \\\""e \"" \"" c\\\"" d \\\""e \"" ""));
  }
",non-flaky,5
176901,OryxProject_oryx,TextUtilsTest.testJoinDelimited,"  @Test
  public void testJoinDelimited() {
    assertEquals(""1,2,3"", TextUtils.joinDelimited(Arrays.asList(""1"", ""2"", ""3""), ','));
    assertEquals(""\""a,b\"""", TextUtils.joinDelimited(Arrays.asList(""a,b""), ','));
    assertEquals(""\""\""\""a\""\""\"""", TextUtils.joinDelimited(Arrays.asList(""\""a\""""), ','));
    assertEquals(""1 2 3"", TextUtils.joinDelimited(Arrays.asList(""1"", ""2"", ""3""), ' '));
    assertEquals(""\""1 \"" \""2 \"" 3"", TextUtils.joinDelimited(Arrays.asList(""1 "", ""2 "", ""3""), ' '));
    assertEquals(""\""\""\""a\""\""\"""", TextUtils.joinDelimited(Arrays.asList(""\""a\""""), ' '));
    assertEquals(""\""\""\""\"" \""\""\""\""\""\"""",
                 TextUtils.joinDelimited(Arrays.asList(""\"""", ""\""\""""), ' '));
    assertEquals("""", TextUtils.joinDelimited(Collections.emptyList(), '\t'));
  }
",non-flaky,5
176902,OryxProject_oryx,TextUtilsTest.testJoinPMMLDelimited,"  @Test
  public void testJoinPMMLDelimited() {
    assertEquals(""ab \""a b\"" \""with \\\""quotes\\\"" \"""",
                 TextUtils.joinPMMLDelimited(Arrays.asList(""ab"", ""a b"", ""with \""quotes\"" "")));
    assertEquals(""1 22 3"",
                 TextUtils.joinPMMLDelimited(Arrays.asList(""1"", ""22"", ""3"")));
    assertEquals(""\"" c\\\"" d \\\""e \"" \"" c\\\"" d \\\""e \"""",
                 TextUtils.joinPMMLDelimited(Arrays.asList("" c\"" d \""e "", "" c\"" d \""e "")));
  }
",non-flaky,5
176903,OryxProject_oryx,TextUtilsTest.testJoinPMMLDelimitedNumbers,"  @Test
  public void testJoinPMMLDelimitedNumbers() {
    assertEquals(""-1.0 2.01 3.5"",
                 TextUtils.joinPMMLDelimitedNumbers(Arrays.asList(-1.0, 2.01, 3.5)));
  }
",non-flaky,5
176904,OryxProject_oryx,TextUtilsTest.testJoinJSON,"  @Test
  public void testJoinJSON() {
    assertEquals(""[\""1\"",\""2\"",\""3\""]"", TextUtils.joinJSON(Arrays.asList(""1"", ""2"", ""3"")));
    assertEquals(""[\""1 \"",\""2 \"",\""3\""]"", TextUtils.joinJSON(Arrays.asList(""1 "", ""2 "", ""3"")));
    assertEquals(""[]"", TextUtils.joinJSON(Collections.emptyList()));
  }
",non-flaky,5
176905,OryxProject_oryx,TextUtilsTest.testJSONList,"  @Test
  public void testJSONList() {
    List<Object> list = new ArrayList<>();
    list.add(""foo"");
    list.add(2);
    assertEquals(""[\""A\"",[\""foo\"",2],\""B\""]"", TextUtils.joinJSON(Arrays.asList(""A"", list, ""B"")));
  }
",non-flaky,5
176906,OryxProject_oryx,TextUtilsTest.testJSONMap,"  @Test
  public void testJSONMap() {
    Map<Object,Object> map = new HashMap<>();
    map.put(1, ""bar"");
    map.put(""foo"", 2);
    assertEquals(""[\""A\"",{\""1\"":\""bar\"",\""foo\"":2},\""B\""]"",
                 TextUtils.joinJSON(Arrays.asList(""A"", map, ""B"")));
  }
",non-flaky,5
176907,OryxProject_oryx,RatingToTupleDoubleTest.testFunction,"  @Test
  public void testFunction() {
    Tuple2<Tuple2<Integer,Integer>,Double> tuple =
        new RatingToTupleDouble().call(new Rating(1, 2, 3.0));
    assertEquals(1, tuple._1()._1().intValue());
    assertEquals(2, tuple._1()._2().intValue());
    assertEquals(3.0, tuple._2().doubleValue());
  }
",non-flaky,5
176908,OryxProject_oryx,ALSHyperParamTuningIT.testHyperParameterTuning,"  @Test
  public void testHyperParameterTuning() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir =  tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", ALSUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    // Choose pairs of values where the best is predictable
    overlayConfig.put(""oryx.als.hyperparams.features"", ""[1,"" + TEST_FEATURES + ""]"");
    overlayConfig.put(""oryx.ml.eval.candidates"", 2);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(config,
                                    new FeaturesALSDataGenerator(TEST_ELEMENTS,
                                                                 TEST_ELEMENTS,
                                                                 TEST_FEATURES),
                                    DATA_TO_WRITE,
                                    WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);
    assertEquals(8, pmml.getExtensions().size());
    assertNotNull(AppPMMLUtils.getExtensionValue(pmml, ""X""));
    assertNotNull(AppPMMLUtils.getExtensionValue(pmml, ""Y""));
    Map<String,Object> expected = new HashMap<>();
    expected.put(""features"", TEST_FEATURES);
    expected.put(""lambda"", 0.001);
    expected.put(""implicit"", true);
    expected.put(""alpha"", 1.0);
    checkExtensions(pmml, expected);
  }
",non-flaky,5
176909,OryxProject_oryx,ALSModelContentIT.testModelContent,"  @Test
  public void testModelContent() throws Exception {
    Path tempDir = getTempDir();
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", ALSUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", tempDir.resolve(""data""));
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", 10);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", 1);
    overlayConfig.put(""oryx.ml.eval.test-fraction"", 0);
    overlayConfig.put(""oryx.als.implicit"", false);
    overlayConfig.put(""oryx.als.hyperparams.lambda"", 0.0001);
    overlayConfig.put(""oryx.als.hyperparams.features"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    ModelContentDataGenerator generator = new ModelContentDataGenerator();
    List<Pair<String, String>> updates = startServerProduceConsumeTopics(
        config,
        generator,
        generator.getSentData().size(),
        20);

    Collection<String> modelUsers = null;
    Collection<String> modelItems = null;
    Map<String,Collection<String>> knownUsersItems = new HashMap<>();

    for (Pair<String, String> km : updates) {
      String type = km.getFirst();
      String value = km.getSecond();
      log.debug(""{} = {}"", type, value);

      if (""UP"".equals(type)) {

        List<?> update = MAPPER.readValue(value, List.class);
        if (""X"".equals(update.get(0).toString())) {
          String userID = update.get(1).toString();
          @SuppressWarnings(""unchecked"")
          Collection<String> userKnownItems = (Collection<String>) update.get(3);
          knownUsersItems.put(userID, new ArrayList<>(userKnownItems));
        }

      } else { // ""MODEL""

        PMML pmml = PMMLUtils.fromString(value);
        modelUsers = AppPMMLUtils.getExtensionContent(pmml, ""XIDs"");
        modelItems = AppPMMLUtils.getExtensionContent(pmml, ""YIDs"");

      }

    }

    assertContainsSame(Arrays.asList(""A0"", ""B1"", ""C2""), modelUsers);
    assertContainsSame(Arrays.asList(""A0"", ""B1"", ""C2"", ""D3""), modelItems);
    assertContainsSame(Arrays.asList(""A0"", ""B1"", ""C2"", ""D3""), knownUsersItems.get(""A0""));
    assertContainsSame(Arrays.asList(""C2"", ""D3""), knownUsersItems.get(""B1""));
    assertContainsSame(Arrays.asList(""D3""), knownUsersItems.get(""C2""));
  }
",non-flaky,5
176910,OryxProject_oryx,ALSUpdateIT.testALS,"  @Test
  public void testALS() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir =  tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", ALSUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.als.implicit"", false);
    overlayConfig.put(""oryx.als.hyperparams.lambda"", LAMBDA);
    overlayConfig.put(""oryx.als.hyperparams.features"", FEATURES);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    List<Pair<String,String>> updates = startServerProduceConsumeTopics(
        config,
        new RandomALSDataGenerator(NUM_USERS_ITEMS, NUM_USERS_ITEMS, 1, 5),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    int generations = modelInstanceDirs.size();
    checkIntervals(generations, DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    List<Collection<String>> userIDs = new ArrayList<>();
    userIDs.add(Collections.<String>emptySet()); // Add dummy empty set as prior value
    List<Collection<String>> productIDs = new ArrayList<>();
    productIDs.add(Collections.<String>emptySet()); // Add dummy empty set as prior value

    for (Path modelInstanceDir : modelInstanceDirs) {
      Path modelFile = modelInstanceDir.resolve(MLUpdate.MODEL_FILE_NAME);
      assertTrue(""Model file should exist: "" + modelFile, Files.exists(modelFile));
      assertTrue(""Model file should not be empty: "" + modelFile, Files.size(modelFile) > 0);
      PMMLUtils.read(modelFile); // Shouldn't throw exception
      Path xDir = modelInstanceDir.resolve(""X"");
      assertTrue(Files.exists(xDir));
      userIDs.add(checkFeatures(xDir, userIDs.get(userIDs.size() - 1)));
      Path yDir = modelInstanceDir.resolve(""Y"");
      assertTrue(Files.exists(yDir));
      productIDs.add(checkFeatures(yDir, productIDs.get(productIDs.size() - 1)));
    }
    // Remove dummy empty sets
    userIDs.remove(0);
    productIDs.remove(0);

    Collection<String> expectedUsers = null;
    Collection<String> expectedProducts = null;
    Collection<String> seenUsers = null;
    Collection<String> seenProducts = null;
    Collection<String> lastModelUsers = null;
    Collection<String> lastModelProducts = null;
    int whichGeneration = -1;
    for (Pair<String,String> km : updates) {

      String type = km.getFirst();
      String value = km.getSecond();

      log.debug(""{} = {}"", type, value);

      boolean isModel = ""MODEL"".equals(type);
      boolean isUpdate = ""UP"".equals(type);
      assertTrue(isModel || isUpdate);

      if (isUpdate) {

        assertNotNull(seenUsers);
        assertNotNull(seenProducts);

        List<?> update = MAPPER.readValue(value, List.class);
        // First field is X or Y, depending on whether it's a user or item vector
        String whichMatrixField = update.get(0).toString();
        boolean isUser = ""X"".equals(whichMatrixField);
        boolean isProduct = ""Y"".equals(whichMatrixField);
        // Next is user/item ID
        String id = update.get(1).toString();
        assertTrue(isUser || isProduct);
        if (isUser) {
          seenUsers.add(id);
        } else {
          seenProducts.add(id);
        }
        // Verify that feature vector are valid floats
        for (float f : MAPPER.convertValue(update.get(2), float[].class)) {
          assertTrue(!Float.isNaN(f) && !Float.isInfinite(f));
        }

        if (isUser) {
          // Only known-items for users exist now, not known users for items
          @SuppressWarnings(""unchecked"")
          Collection<String> knownUsersItems = (Collection<String>) update.get(3);
          assertFalse(knownUsersItems.isEmpty());
          for (String known : knownUsersItems) {
            int i = ALSUtilsTest.stringIDtoID(known);
            assertTrue(i >= 0 && i < NUM_USERS_ITEMS);
          }
        }

      } else {

        PMML pmml = PMMLUtils.fromString(value);

        checkHeader(pmml.getHeader());

        assertEquals(7, pmml.getExtensions().size());
        Map<String,Object> expected = new HashMap<>();
        expected.put(""features"", FEATURES);
        expected.put(""lambda"", LAMBDA);
        expected.put(""implicit"", false);
        checkExtensions(pmml, expected);

        // See if users/item sets seen in updates match what was expected from output
        assertContainsSame(expectedUsers, seenUsers);
        assertContainsSame(expectedProducts, seenProducts);

        // Also check key sets reported in model
        assertContainsSame(expectedUsers, lastModelUsers);
        assertContainsSame(expectedProducts, lastModelProducts);

        // Update for next round
        whichGeneration++;
        expectedUsers = userIDs.get(whichGeneration);
        expectedProducts = productIDs.get(whichGeneration);
        seenUsers = new HashSet<>();
        seenProducts = new HashSet<>();
        lastModelUsers = AppPMMLUtils.getExtensionContent(pmml, ""XIDs"");
        lastModelProducts = AppPMMLUtils.getExtensionContent(pmml, ""YIDs"");

      }
    }

  }
",non-flaky,5
176911,OryxProject_oryx,KMeansUpdateIT.testKMeans,"  @Test
  public void testKMeans() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", KMeansUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.kmeans.hyperparams.k"", NUM_CLUSTERS);
    overlayConfig.put(""oryx.kmeans.iterations"", 5);
    overlayConfig.put(""oryx.input-schema.num-features"", NUM_FEATURES);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[]"");
    overlayConfig.put(""oryx.kmeans.evaluation-strategy"", EVALUATION_STRATEGY);

    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    List<Pair<String, String>> updates = startServerProduceConsumeTopics(
        config,
        new RandomKMeansDataGenerator(NUM_FEATURES),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    int generations = modelInstanceDirs.size();
    checkIntervals(generations, DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    for (Path modelInstanceDir : modelInstanceDirs) {
      Path modelFile = modelInstanceDir.resolve(MLUpdate.MODEL_FILE_NAME);
      assertTrue(""Model file should exist: "" + modelFile, Files.exists(modelFile));
      assertTrue(""Model file should not be empty: "" + modelFile, Files.size(modelFile) > 0);
      PMMLUtils.read(modelFile); // Shouldn't throw exception
    }

    InputSchema schema = new InputSchema(config);

    for (Pair<String,String> km : updates) {

      String type = km.getFirst();
      String value = km.getSecond();

      assertEquals(""MODEL"", type);

      PMML pmml = PMMLUtils.fromString(value);

      checkHeader(pmml.getHeader());

      checkDataDictionary(schema, pmml.getDataDictionary());

      Model rootModel = pmml.getModels().get(0);

      ClusteringModel clusteringModel = (ClusteringModel) rootModel;

      // Check if Basic hyperparameters match
      assertEquals(NUM_CLUSTERS, clusteringModel.getNumberOfClusters().intValue());
      assertEquals(NUM_CLUSTERS, clusteringModel.getClusters().size());
      assertEquals(NUM_FEATURES, clusteringModel.getClusteringFields().size());
      assertEquals(ComparisonMeasure.Kind.DISTANCE,
                   clusteringModel.getComparisonMeasure().getKind());
      assertEquals(NUM_FEATURES, clusteringModel.getClusters().get(0).getArray().getN().intValue());
      for (Cluster cluster : clusteringModel.getClusters()) {
        assertTrue(cluster.getSize() > 0);
      }
    }
  }
",non-flaky,5
176912,OryxProject_oryx,KMeansHyperParamTuningIT.testKMeans,"  @Test
  public void testKMeans() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", KMeansUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.kmeans.hyperparams.k"", ""[2,100]"");
    overlayConfig.put(""oryx.kmeans.iterations"", 20);
    overlayConfig.put(""oryx.kmeans.runs"", 20);
    overlayConfig.put(""oryx.input-schema.num-features"", NUM_FEATURES);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[]"");
    overlayConfig.put(""oryx.ml.eval.candidates"", 3);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    overlayConfig.put(""oryx.kmeans.evaluation-strategy"", EVALUATION_STRATEGY);

    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(
        config,
        new RandomKMeansDataGenerator(NUM_FEATURES),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);
    Model rootModel = pmml.getModels().get(0);
    ClusteringModel clusteringModel = (ClusteringModel) rootModel;

    // Should have picked highest k
    assertEquals(100, clusteringModel.getNumberOfClusters().intValue());
  }
",non-flaky,5
176913,OryxProject_oryx,KMeansEvalIT.testFetchSampleEvalData,"  @Test
  public void testFetchSampleEvalData() {
    JavaRDD<Vector> evalData = SilhouetteCoefficient.fetchSampleData(getRddOfVectors());
    assertEquals(6, evalData.count());
  }
",non-flaky,5
176914,OryxProject_oryx,KMeansEvalIT.testDunnIndexForClustering,"  @Test
  public void testDunnIndexForClustering() {
    List<ClusterInfo> clusters = getClusters();
    DunnIndex dunnIndex = new DunnIndex(clusters);
    double eval = dunnIndex.evaluate(getRddOfVectors());
    log.info(""Dunn Index for {} clusters: {}"", clusters.size(), eval);
    assertEquals(1.7142857142857142, eval);
  }
",non-flaky,5
176915,OryxProject_oryx,KMeansEvalIT.testDaviesBouldinIndexForClustering,"  @Test
  public void testDaviesBouldinIndexForClustering() {
    List<ClusterInfo> clusters = getClusters();
    DaviesBouldinIndex daviesBouldinIndex = new DaviesBouldinIndex(clusters);
    double eval = daviesBouldinIndex.evaluate(getRddOfVectors());
    log.info(""Davies Bouldin Index for {} clusters: {}"", clusters.size(), eval);
    assertEquals(0.638888888888889, eval);
  }
",non-flaky,5
176916,OryxProject_oryx,KMeansEvalIT.testSilhouetteCoefficientForClustering,"  @Test
  public void testSilhouetteCoefficientForClustering() {
    List<ClusterInfo> clusters = getClusters();
    SilhouetteCoefficient silhouetteCoefficient = new SilhouetteCoefficient(clusters);
    double eval = silhouetteCoefficient.evaluate(getRddOfVectors());
    log.info(""Silhouette Coefficient for {} clusters: {}"", clusters.size(), eval);
    assertEquals(0.48484126984126985, eval);
  }
",non-flaky,5
176917,OryxProject_oryx,KMeansEvalIT.testComputeSilhouetteCoefficient,"  @Test
  public void testComputeSilhouetteCoefficient() {
    assertEquals(5.0, SilhouetteCoefficient.calcSilhouetteCoefficient(-0.8, 0.2));
    assertEquals(-1.25, SilhouetteCoefficient.calcSilhouetteCoefficient(0.8, -0.2));
    assertEquals(0.0, SilhouetteCoefficient.calcSilhouetteCoefficient(1.5, 1.5));
    assertEquals(1.0, SilhouetteCoefficient.calcSilhouetteCoefficient(1.5, Double.POSITIVE_INFINITY));
    assertEquals(-1.0, SilhouetteCoefficient.calcSilhouetteCoefficient(Double.POSITIVE_INFINITY, 1.5));
  }
",non-flaky,5
176918,OryxProject_oryx,RDFNumericHyperParamTuningIT.testRDF,"  @Test
  public void testRDF() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", RDFUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.rdf.num-trees"", NUM_TREES);
    // Low values like 1 are deliberately bad, won't work
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", ""[1,"" + MAX_DEPTH + ""]"");
    overlayConfig.put(""oryx.rdf.hyperparams.max-split-candidates"", MAX_SPLIT_CANDIDATES);
    overlayConfig.put(""oryx.rdf.hyperparams.impurity"", IMPURITY);
    overlayConfig.put(""oryx.input-schema.num-features"", 5);
    overlayConfig.put(""oryx.input-schema.numeric-features"", ""[\""4\""]"");
    overlayConfig.put(""oryx.input-schema.id-features"", ""[\""0\""]"");
    overlayConfig.put(""oryx.input-schema.target-feature"", ""\""4\"""");
    overlayConfig.put(""oryx.ml.eval.candidates"", 2);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(
        config,
        new RandomNumericRDFDataGenerator(3),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);

    assertEquals(3, pmml.getExtensions().size());
    Map<String,Object> expected = new HashMap<>();
    expected.put(""maxSplitCandidates"", MAX_SPLIT_CANDIDATES);
    expected.put(""maxDepth"", MAX_DEPTH);
    expected.put(""impurity"", IMPURITY);
    checkExtensions(pmml, expected);

    Pair<DecisionForest,CategoricalValueEncodings> forestEncoding = RDFPMMLUtils.read(pmml);
    DecisionForest forest = forestEncoding.getFirst();
    CategoricalValueEncodings encoding = forestEncoding.getSecond();

    for (int f1 = 0; f1 <= 1; f1++) {
      for (int f2 = 0; f2 <= 1; f2++) {
        for (int f3 = 0; f3 <= 1; f3++) {
          NumericPrediction prediction = (NumericPrediction) forest.predict(new Example(null,
              null,
              CategoricalFeature.forEncoding(encoding.getValueEncodingMap(1).get(f1 == 1 ? ""A"" : ""B"")),
              CategoricalFeature.forEncoding(encoding.getValueEncodingMap(2).get(f2 == 1 ? ""A"" : ""B"")),
              CategoricalFeature.forEncoding(encoding.getValueEncodingMap(3).get(f3 == 1 ? ""A"" : ""B""))));
          int expectedCount = f1 + f2 + f3;
          if (expectedCount == 3) {
            // TODO this might be a bug in Spark RDF. The tree never creates a node for all
            // positive classes even though it should. Plenty of nodes, info gain, etc.
            assertEquals(2, Math.round(prediction.getPrediction()));
          } else {
            assertEquals(expectedCount, Math.round(prediction.getPrediction()));
          }
        }
      }
    }

  }
",non-flaky,5
176919,OryxProject_oryx,RDFUpdateIT.testRDF,"  @Test
  public void testRDF() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", RDFUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.rdf.num-trees"", NUM_TREES);
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", MAX_DEPTH);
    overlayConfig.put(""oryx.rdf.hyperparams.max-split-candidates"", MAX_SPLIT_CANDIDATES);
    overlayConfig.put(""oryx.rdf.hyperparams.impurity"", IMPURITY);
    overlayConfig.put(""oryx.input-schema.num-features"", 5);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[\""4\""]"");
    overlayConfig.put(""oryx.input-schema.id-features"", ""[\""0\""]"");
    overlayConfig.put(""oryx.input-schema.target-feature"", ""\""4\"""");

    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    List<Pair<String, String>> updates = startServerProduceConsumeTopics(
        config,
        new RandomCategoricalRDFDataGenerator(3),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    int generations = modelInstanceDirs.size();
    checkIntervals(generations, DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    for (Path modelInstanceDir : modelInstanceDirs) {
      Path modelFile = modelInstanceDir.resolve(MLUpdate.MODEL_FILE_NAME);
      assertTrue(""Model file should exist: "" + modelFile, Files.exists(modelFile));
      assertTrue(""Model file should not be empty: "" + modelFile, Files.size(modelFile) > 0);
      PMMLUtils.read(modelFile); // Shouldn't throw exception
    }

    InputSchema schema = new InputSchema(config);

    for (Pair<String,String> km : updates) {

      String type = km.getFirst();
      String value = km.getSecond();

      assertEquals(""MODEL"", type);

      PMML pmml = PMMLUtils.fromString(value);

      checkHeader(pmml.getHeader());

      assertEquals(3, pmml.getExtensions().size());
      Map<String,Object> expected = new HashMap<>();
      expected.put(""maxDepth"", MAX_DEPTH);
      expected.put(""maxSplitCandidates"", MAX_SPLIT_CANDIDATES);
      expected.put(""impurity"", IMPURITY);
      checkExtensions(pmml, expected);

      checkDataDictionary(schema, pmml.getDataDictionary());

      Model rootModel = pmml.getModels().get(0);
      if (rootModel instanceof TreeModel) {
        assertEquals(NUM_TREES, 1);
        TreeModel treeModel = (TreeModel) rootModel;
        checkTreeModel(treeModel);
      } else if (rootModel instanceof MiningModel) {
        MiningModel miningModel = (MiningModel) rootModel;
        Segmentation segmentation = miningModel.getSegmentation();
        if (schema.isClassification()) {
          assertEquals(MultipleModelMethodType.WEIGHTED_MAJORITY_VOTE,
                       segmentation.getMultipleModelMethod());
        } else {
          assertEquals(MultipleModelMethodType.WEIGHTED_AVERAGE,
                       segmentation.getMultipleModelMethod());
        }
        List<Segment> segments = segmentation.getSegments();
        assertEquals(NUM_TREES, segments.size());
        for (int i = 0; i < segments.size(); i++) {
          Segment segment = segments.get(i);
          assertEquals(Integer.toString(i), segment.getId());
          assertTrue(segment.getPredicate() instanceof True);
          assertEquals(1.0, segment.getWeight());
          assertTrue(segment.getModel() instanceof TreeModel);
          checkTreeModel((TreeModel) segment.getModel());
        }

      } else {
        fail(""Wrong model type: "" + rootModel.getClass());
        return;
      }

      if (schema.isClassification()) {
        assertEquals(MiningFunctionType.CLASSIFICATION, rootModel.getFunctionName());
      } else {
        assertEquals(MiningFunctionType.REGRESSION, rootModel.getFunctionName());
      }

      checkMiningSchema(schema, rootModel.getMiningSchema());

    }
  }
",non-flaky,5
176920,OryxProject_oryx,RDFCategoricalHyperParamTuningIT.testRDF,"  @Test
  public void testRDF() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", RDFUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.rdf.num-trees"", 10);
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", MAX_DEPTH);
    // Low values like 1 are deliberately bad, won't work
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", ""[1,"" + MAX_DEPTH + ""]"");
    overlayConfig.put(""oryx.rdf.hyperparams.max-split-candidates"", MAX_SPLIT_CANDIDATES);
    overlayConfig.put(""oryx.input-schema.num-features"", 5);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[\""4\""]"");
    overlayConfig.put(""oryx.input-schema.id-features"", ""[\""0\""]"");
    overlayConfig.put(""oryx.input-schema.target-feature"", ""\""4\"""");
    overlayConfig.put(""oryx.ml.eval.candidates"", 3);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(
        config,
        new RandomCategoricalRDFDataGenerator(3),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);

    assertEquals(3, pmml.getExtensions().size());
    Map<String,Object> expected = new HashMap<>();
    expected.put(""maxSplitCandidates"", MAX_SPLIT_CANDIDATES);
    expected.put(""maxDepth"", MAX_DEPTH);
    expected.put(""impurity"", IMPURITY);
    checkExtensions(pmml, expected);

    Pair<DecisionForest,CategoricalValueEncodings> forestEncoding = RDFPMMLUtils.read(pmml);
    DecisionForest forest = forestEncoding.getFirst();
    CategoricalValueEncodings encoding = forestEncoding.getSecond();
    Map<String,Integer> targetEncoding = encoding.getValueEncodingMap(4);

    int[] zeroOne = { 0, 1 };
    for (int f1 : zeroOne) {
      for (int f2 : zeroOne) {
        for (int f3 : zeroOne) {
          CategoricalPrediction prediction =
              (CategoricalPrediction) forest.predict(new Example(null,
                                                                 null,
                                                                 NumericFeature.forValue(f1),
                                                                 NumericFeature.forValue(f2),
                                                                 NumericFeature.forValue(f3)));
          boolean expectedPositive = f1 == 1 && f2 == 1 && f3 == 1;
          assertEquals(targetEncoding.get(Boolean.toString(expectedPositive)).intValue(),
                       prediction.getMostProbableCategoryEncoding());
        }
      }
    }

  }
",non-flaky,5
176921,OryxProject_oryx,MultiRescorerProviderTest.testMultiRecommendRescorer,"  @Test
  public void testMultiRecommendRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    
    Rescorer provider = multi.getRecommendRescorer(Collections.singletonList(""ABCDE""), null);
    assertNull(provider);

    Rescorer provider2 = multi.getRecommendRescorer(Collections.singletonList(""AB""), null);
    assertNotNull(provider2);
    assertFalse(provider2 instanceof MultiRescorer);
    assertTrue(provider2.isFiltered(""ABC""));
    assertFalse(provider2.isFiltered(""AB""));

    Rescorer provider3 = multi.getRecommendRescorer(Collections.singletonList(""ABCDEF""), null);
    assertNotNull(provider3);
    assertTrue(provider3 instanceof MultiRescorer);
    assertTrue(provider3.isFiltered(""ABC""));
    assertTrue(provider3.isFiltered(""AB""));
    assertFalse(provider3.isFiltered(""ABCDEFABCDEF""));
  }
",non-flaky,5
176922,OryxProject_oryx,MultiRescorerProviderTest.testMultiRecommendToAnonymousRescorer,"  @Test
  public void testMultiRecommendToAnonymousRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    
    Rescorer provider = multi.getRecommendToAnonymousRescorer(
        Collections.singletonList(""ABCDE""), null);
    assertNull(provider);

    Rescorer provider2 =
        multi.getRecommendToAnonymousRescorer(Collections.singletonList(""AB""), null);
    assertNotNull(provider2);
    assertFalse(provider2 instanceof MultiRescorer);
    assertTrue(provider2.isFiltered(""ABC""));
    assertFalse(provider2.isFiltered(""AB""));

    Rescorer provider3 =
        multi.getRecommendToAnonymousRescorer(Collections.singletonList(""ABCDEF""), null);
    assertNotNull(provider3);
    assertTrue(provider3 instanceof MultiRescorer);
    assertTrue(provider3.isFiltered(""ABC""));
    assertTrue(provider3.isFiltered(""AB""));
    assertFalse(provider3.isFiltered(""ABCDEF""));
  }
",non-flaky,5
176923,OryxProject_oryx,MultiRescorerProviderTest.testMultiMostPopularItemsRescorer,"  @Test
  public void testMultiMostPopularItemsRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    Rescorer provider = multi.getMostPopularItemsRescorer(null);
    assertNotNull(provider);
    assertTrue(provider instanceof MultiRescorer);
    assertTrue(provider.isFiltered(""ABC""));
    assertTrue(provider.isFiltered(""AB""));
    assertFalse(provider.isFiltered(""ABCDEF""));
  }
",non-flaky,5
176924,OryxProject_oryx,MultiRescorerProviderTest.testMultiMostActiveUsersRescorer,"  @Test
  public void testMultiMostActiveUsersRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    Rescorer provider = multi.getMostActiveUsersRescorer(null);
    assertNotNull(provider);
    assertTrue(provider instanceof MultiRescorer);
    assertTrue(provider.isFiltered(""ABC""));
    assertTrue(provider.isFiltered(""AB""));
    assertFalse(provider.isFiltered(""ABCDEF""));
  }
",non-flaky,5
176925,OryxProject_oryx,MultiRescorerProviderTest.testMultiMostSimilarItemsRescorer,"  @Test
  public void testMultiMostSimilarItemsRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    Rescorer provider = multi.getMostSimilarItemsRescorer(null);
    assertNotNull(provider);
    assertTrue(provider instanceof MultiRescorer);
    assertTrue(provider.isFiltered(""ABC""));
    assertTrue(provider.isFiltered(""ABCDE""));
    assertFalse(provider.isFiltered(""ABCDEFABCDEF""));
  }
",non-flaky,5
176926,OryxProject_oryx,AbstractRescorerProviderTest.testDefault,"  @Test
  public void testDefault() {
    RescorerProvider noop = new NullProvider1();
    assertNull(noop.getMostActiveUsersRescorer(null));
    assertNull(noop.getMostPopularItemsRescorer(null));
    assertNull(noop.getMostSimilarItemsRescorer(null));
    assertNull(noop.getRecommendRescorer(null, null));
    assertNull(noop.getRecommendToAnonymousRescorer(null, null));
  }
",non-flaky,5
176927,OryxProject_oryx,AbstractRescorerProviderTest.testLoad,"  @Test
  public void testLoad() {
    RescorerProvider provider = AbstractRescorerProvider.loadRescorerProviders(
        ""com.cloudera.oryx.app.als.NullProvider2"");
    assertTrue(provider instanceof NullProvider2);
    RescorerProvider multiProvider = AbstractRescorerProvider.loadRescorerProviders(
        ""com.cloudera.oryx.app.als.NullProvider1,com.cloudera.oryx.app.als.NullProvider2"");
    assertTrue(multiProvider instanceof MultiRescorerProvider);
  }
",non-flaky,5
176928,OryxProject_oryx,AbstractRescorerProviderTest.testNoClass,"  @Test(expected = IllegalArgumentException.class)
  public void testNoClass() {
    AbstractRescorerProvider.loadRescorerProviders(""noSuchClass"");
  }
",non-flaky,5
176929,OryxProject_oryx,AbstractRescorerProviderTest.testWrongClass,"  @Test(expected = ClassCastException.class)
  public void testWrongClass() {
    AbstractRescorerProvider.loadRescorerProviders(
        ""com.cloudera.oryx.app.als.AbstractRescorerProviderTest"");
  }
",non-flaky,5
176930,OryxProject_oryx,ServingLayerTest.testServingLayer,"  @Test
  public void testServingLayer() throws Exception {
    Map<String,Object> overlay = buildOverlay();
    Config config = ConfigUtils.overlayOn(overlay, ConfigUtils.getDefault());
    doTestServingLayer(config);
  }
",non-flaky,5
176931,OryxProject_oryx,ServingLayerTest.testServingLayerSecure,"  @Test
  public void testServingLayerSecure() throws Exception {
    Path keystoreFile = SecureAPIConfigIT.buildKeystoreFile();
    Map<String,Object> overlay = buildOverlay();
    overlay.put(""oryx.serving.api.keystore-file"", ""\"""" + keystoreFile + ""\"""");
    overlay.put(""oryx.serving.api.keystore-password"", ""oryxpass"");
    Config config = ConfigUtils.overlayOn(overlay, ConfigUtils.getDefault());
    try {
      doTestServingLayer(config);
    } finally {
      Files.delete(Paths.get(config.getString(""oryx.serving.api.keystore-file"")));
    }
  }
",non-flaky,5
45,Tencent_Firestorm,HealthCheckCoordinatorGrpcTest.healthCheckTest,"@Test
public void healthCheckTest() throws Exception {
    RssGetShuffleAssignmentsRequest request = new RssGetShuffleAssignmentsRequest(""1"", 1, 1, 1, 1, Sets.newHashSet(SHUFFLE_SERVER_VERSION));
    Uninterruptibles.sleepUninterruptibly(3, TimeUnit.SECONDS);
    assertEquals(2, coordinatorClient.getShuffleServerList().getServersCount());
    List<ServerNode> nodes = coordinators.get(0).getClusterManager().getServerList(Sets.newHashSet(SHUFFLE_SERVER_VERSION));
    assertEquals(2, coordinatorClient.getShuffleServerList().getServersCount());
    assertEquals(2, nodes.size());
    RssGetShuffleAssignmentsResponse response = coordinatorClient.getShuffleAssignments(request);
    assertFalse(response.getPartitionToServers().isEmpty());
    for (ServerNode node : nodes) {
        assertTrue(node.isHealthy());
    }
    byte[] bytes = new byte[writeDataSize];
    new Random().nextBytes(bytes);
    try (final FileOutputStream out = new FileOutputStream(tempDataFile)) {
        out.write(bytes);
    }
    Uninterruptibles.sleepUninterruptibly(3, TimeUnit.SECONDS);
    CoordinatorTestUtils.waitForRegister(coordinatorClient, 2);
    nodes = coordinators.get(0).getClusterManager().getServerList(Sets.newHashSet(SHUFFLE_SERVER_VERSION));
    for (ServerNode node : nodes) {
        assertFalse(node.isHealthy());
    }
    assertEquals(0, nodes.size());
    response = coordinatorClient.getShuffleAssignments(request);
    assertEquals(INTERNAL_ERROR, response.getStatusCode());
    tempDataFile.delete();
    int i = 0;
    do {
        Uninterruptibles.sleepUninterruptibly(3, TimeUnit.SECONDS);
        nodes = coordinators.get(0).getClusterManager().getServerList(Sets.newHashSet(SHUFFLE_SERVER_VERSION));
        i++;
        if (i == 10) {
            fail();
        }
    } while (nodes.size() != 2 );
    for (ServerNode node : nodes) {
        assertTrue(node.isHealthy());
    }
    assertEquals(2, nodes.size());
    response = coordinatorClient.getShuffleAssignments(request);
    assertFalse(response.getPartitionToServers().isEmpty());
}",async wait,0
76920,Tencent_Firestorm,RssShuffleUtilsTest.compressionTest,"  @Test
  public void compressionTest() {
    List<Integer> testSizes = Lists.newArrayList(
        1, 1024, 128 * 1024, 512 * 1024, 1024 * 1024, 4 * 1024 * 1024);
    for (int size : testSizes) {
      singleTest(size);
    }
  }
",non-flaky,5
76921,Tencent_Firestorm,RssShuffleUtilsTest.odfsConfigurationTest,"  @Test
  public void odfsConfigurationTest() {
    SparkConf conf = new SparkConf();
    Configuration conf1 = RssShuffleUtils.newHadoopConfiguration(conf);
    assertFalse(conf1.getBoolean(""dfs.namenode.odfs.enable"", false));
    assertEquals(""org.apache.hadoop.fs.Hdfs"", conf1.get(""fs.AbstractFileSystem.hdfs.impl""));

    conf.set(RssClientConfig.RSS_OZONE_DFS_NAMENODE_ODFS_ENABLE, ""true"");
    conf1 = RssShuffleUtils.newHadoopConfiguration(conf);
    assertTrue(conf1.getBoolean(""dfs.namenode.odfs.enable"", false));
    assertEquals(""org.apache.hadoop.odfs.HdfsOdfsFilesystem"", conf1.get(""fs.hdfs.impl""));
    assertEquals(""org.apache.hadoop.odfs.HdfsOdfs"", conf1.get(""fs.AbstractFileSystem.hdfs.impl""));

    conf.set(RssClientConfig.RSS_OZONE_FS_HDFS_IMPL, ""expect_odfs_impl"");
    conf.set(RssClientConfig.RSS_OZONE_FS_ABSTRACT_FILE_SYSTEM_HDFS_IMPL, ""expect_odfs_abstract_impl"");
    conf1 = RssShuffleUtils.newHadoopConfiguration(conf);
    assertEquals(""expect_odfs_impl"", conf1.get(""fs.hdfs.impl""));
    assertEquals(""expect_odfs_abstract_impl"", conf1.get(""fs.AbstractFileSystem.hdfs.impl""));
  }
",non-flaky,5
76922,Tencent_Firestorm,RssShuffleDataIteratorTest.readTest1,"  @Test
  public void readTest1() throws Exception {
    String basePath = HDFS_URI + ""readTest1"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test1"", conf);

    Map<String, String> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 2, 5, expectedData,
        blockIdBitmap, ""key"", KRYO_SERIALIZER, 0);

    RssShuffleDataIterator rssShuffleDataIterator = getDataIterator(basePath, blockIdBitmap, taskIdBitmap);

    validateResult(rssShuffleDataIterator, expectedData, 10);

    blockIdBitmap.add(ClientUtils.getBlockId(0, 0, Constants.MAX_SEQUENCE_NO));
    rssShuffleDataIterator = getDataIterator(basePath, blockIdBitmap, taskIdBitmap);
    int recNum = 0;
    try {
      // can't find all expected block id, data loss
      while (rssShuffleDataIterator.hasNext()) {
        rssShuffleDataIterator.next();
        recNum++;
      }
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().startsWith(""Blocks read inconsistent:""));
    }
    assertEquals(10, recNum);
  }
",non-flaky,5
76923,Tencent_Firestorm,RssShuffleDataIteratorTest.readTest2,"  @Test
  public void readTest2() throws Exception {
    String basePath = HDFS_URI + ""readTest2"";
    HdfsShuffleWriteHandler writeHandler1 =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test2_1"", conf);
    HdfsShuffleWriteHandler writeHandler2 =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test2_2"", conf);

    Map<String, String> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler1, 2, 5, expectedData,
        blockIdBitmap, ""key1"", KRYO_SERIALIZER, 0);
    writeTestData(writeHandler2, 2, 5, expectedData,
        blockIdBitmap, ""key2"", KRYO_SERIALIZER, 0);

    RssShuffleDataIterator rssShuffleDataIterator = getDataIterator(basePath, blockIdBitmap, taskIdBitmap);

    validateResult(rssShuffleDataIterator, expectedData, 20);
    assertEquals(20, rssShuffleDataIterator.getShuffleReadMetrics().recordsRead());
    assertEquals(256, rssShuffleDataIterator.getShuffleReadMetrics().remoteBytesRead());
    assertTrue(rssShuffleDataIterator.getShuffleReadMetrics().fetchWaitTime() > 0);
  }
",non-flaky,5
76924,Tencent_Firestorm,RssShuffleDataIteratorTest.readTest3,"  @Test
  public void readTest3() throws Exception {
    String basePath = HDFS_URI + ""readTest3"";
    HdfsShuffleWriteHandler writeHandler1 =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test3_1"", conf);
    HdfsShuffleWriteHandler writeHandler2 =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test3_2"", conf);

    Map<String, String> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler1, 2, 5, expectedData,
        blockIdBitmap, ""key1"", KRYO_SERIALIZER, 0);
    writeTestData(writeHandler2, 2, 5, expectedData,
        blockIdBitmap, ""key2"", KRYO_SERIALIZER, 0);

    // duplicate file created, it should be used in product environment
    String shuffleFolder = basePath + ""/appId/0/0-1"";
    FileUtil.copy(fs, new Path(shuffleFolder + ""/test3_1_0.data""), fs,
        new Path(shuffleFolder + ""/test3_1_0.cp.data""), false, conf);
    FileUtil.copy(fs, new Path(shuffleFolder + ""/test3_1_0.index""), fs,
        new Path(shuffleFolder + ""/test3_1_0.cp.index""), false, conf);
    FileUtil.copy(fs, new Path(shuffleFolder + ""/test3_2_0.data""), fs,
        new Path(shuffleFolder + ""/test3_2_0.cp.data""), false, conf);
    FileUtil.copy(fs, new Path(shuffleFolder + ""/test3_2_0.index""), fs,
        new Path(shuffleFolder + ""/test3_2_0.cp.index""), false, conf);

    RssShuffleDataIterator rssShuffleDataIterator = getDataIterator(basePath, blockIdBitmap, taskIdBitmap);

    validateResult(rssShuffleDataIterator, expectedData, 20);
  }
",non-flaky,5
76925,Tencent_Firestorm,RssShuffleDataIteratorTest.readTest4,"  @Test
  public void readTest4() throws Exception {
    String basePath = HDFS_URI + ""readTest4"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test1"", conf);

    Map<String, String> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 2, 5, expectedData,
        blockIdBitmap, ""key"", KRYO_SERIALIZER, 0);

    RssShuffleDataIterator rssShuffleDataIterator = getDataIterator(basePath, blockIdBitmap, taskIdBitmap);
    // data file is deleted after iterator initialization
    Path dataFile = new Path(basePath + ""/appId/0/0-1/test1_0.data"");
    fs.delete(dataFile, true);
    // sleep to wait delete operation
    Thread.sleep(10000);
    try {
      fs.listStatus(dataFile);
      fail(""Index file should be deleted"");
    } catch (Exception e) {
    }

    try {
      while (rssShuffleDataIterator.hasNext()) {
        rssShuffleDataIterator.next();
      }
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().startsWith(""Blocks read inconsistent: expected""));
    }
  }
",non-flaky,5
76926,Tencent_Firestorm,RssShuffleDataIteratorTest.readTest5,"  @Test
  public void readTest5() throws Exception {
    String basePath = HDFS_URI + ""readTest5"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test"", conf);

    Map<String, String> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 2, 5, expectedData,
        blockIdBitmap, ""key"", KRYO_SERIALIZER, 0);

    RssShuffleDataIterator rssShuffleDataIterator = getDataIterator(basePath, blockIdBitmap, taskIdBitmap);
    // index file is deleted after iterator initialization, it should be ok, all index infos are read already
    Path indexFile = new Path(basePath + ""/appId/0/0-1/test.index"");
    fs.delete(indexFile, true);
    // sleep to wait delete operation
    Thread.sleep(10000);
    try {
      fs.listStatus(indexFile);
      fail(""Index file should be deleted"");
    } catch (Exception e) {
    }
    validateResult(rssShuffleDataIterator, expectedData, 10);
  }
",non-flaky,5
76927,Tencent_Firestorm,RssShuffleDataIteratorTest.readTest7,"  @Test
  public void readTest7() throws Exception {
    String basePath = HDFS_URI + ""readTest7"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test"", conf);

    Map<String, String> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 2, 5, expectedData,
        blockIdBitmap, ""key"", KRYO_SERIALIZER, 0);

    RssShuffleDataIterator rssShuffleDataIterator = getDataIterator(basePath, blockIdBitmap, taskIdBitmap);

    // crc32 is incorrect
    try (MockedStatic<ChecksumUtils> checksumUtilsMock = Mockito.mockStatic(ChecksumUtils.class)) {
      checksumUtilsMock.when(() -> ChecksumUtils.getCrc32((ByteBuffer) any())).thenReturn(-1L);
      try {
        while (rssShuffleDataIterator.hasNext()) {
          rssShuffleDataIterator.next();
        }
        fail(EXPECTED_EXCEPTION_MESSAGE);
      } catch (Exception e) {
        assertTrue(e.getMessage().startsWith(""Unexpected crc value""));
      }
    }
  }
",non-flaky,5
76928,Tencent_Firestorm,WriteBufferTest.test,"  @Test
  public void test() {
    WriterBuffer wb = new WriterBuffer(32);
    assertEquals(0, wb.getMemoryUsed());
    assertEquals(0, wb.getDataLength());

    serializeData(""key"", ""value"");
    // size of serialized kv is 12
    wb.addRecord(serializedData, serializedDataLength);
    assertEquals(32, wb.getMemoryUsed());
    assertEquals(12, wb.getDataLength());
    wb.addRecord(serializedData, serializedDataLength);
    assertEquals(32, wb.getMemoryUsed());
    // case: data size < output buffer size, when getData(), [] + buffer with 24b = 24b
    assertEquals(24, wb.getData().length);
    wb.addRecord(serializedData, serializedDataLength);
    // case: data size > output buffer size, when getData(), [1 buffer] + buffer with 12 = 36b
    assertEquals(36, wb.getData().length);
    assertEquals(64, wb.getMemoryUsed());
    wb.addRecord(serializedData, serializedDataLength);
    wb.addRecord(serializedData, serializedDataLength);
    // case: data size > output buffer size, when getData(), 2 buffer + output with 12b = 60b
    assertEquals(60, wb.getData().length);
    assertEquals(96, wb.getMemoryUsed());

    wb = new WriterBuffer(32);

    serializeData(""key1111111111111111111111111111"", ""value222222222222222222222222222"");
    wb.addRecord(serializedData, serializedDataLength);
    assertEquals(67, wb.getMemoryUsed());
    assertEquals(67, wb.getDataLength());

    serializeData(""key"", ""value"");
    wb.addRecord(serializedData, serializedDataLength);
    // 67 + 32
    assertEquals(99, wb.getMemoryUsed());
    // 67 + 12
    assertEquals(79, wb.getDataLength());
    assertEquals(79, wb.getData().length);

    wb.addRecord(serializedData, serializedDataLength);
    assertEquals(99, wb.getMemoryUsed());
    assertEquals(91, wb.getDataLength());
    assertEquals(91, wb.getData().length);
  }
",non-flaky,5
76929,Tencent_Firestorm,WriteBufferManagerTest.addRecordTest,"  @Test
  public void addRecordTest() {
    SparkConf conf = getConf();
    WriteBufferManager wbm = createManager(conf);
    wbm.setShuffleWriteMetrics(new ShuffleWriteMetrics());
    String testKey = ""Key"";
    String testValue = ""Value"";
    List<ShuffleBlockInfo> result = wbm.addRecord(0, testKey, testValue);
    // single buffer is not full, there is no data return
    assertEquals(0, result.size());
    assertEquals(512, wbm.getAllocatedBytes());
    assertEquals(32, wbm.getUsedBytes());
    assertEquals(0, wbm.getInSendListBytes());
    assertEquals(1, wbm.getBuffers().size());
    wbm.addRecord(0, testKey, testValue);
    wbm.addRecord(0, testKey, testValue);
    wbm.addRecord(0, testKey, testValue);
    result = wbm.addRecord(0, testKey, testValue);
    // single buffer is full
    assertEquals(1, result.size());
    assertEquals(512, wbm.getAllocatedBytes());
    assertEquals(96, wbm.getUsedBytes());
    assertEquals(96, wbm.getInSendListBytes());
    assertEquals(0, wbm.getBuffers().size());
    wbm.addRecord(0, testKey, testValue);
    wbm.addRecord(1, testKey, testValue);
    wbm.addRecord(2, testKey, testValue);
    // single buffer is not full, and less than spill size
    assertEquals(512, wbm.getAllocatedBytes());
    assertEquals(192, wbm.getUsedBytes());
    assertEquals(96, wbm.getInSendListBytes());
    assertEquals(3, wbm.getBuffers().size());
    // all buffer size > spill size
    wbm.addRecord(3, testKey, testValue);
    wbm.addRecord(4, testKey, testValue);
    result = wbm.addRecord(5, testKey, testValue);
    assertEquals(6, result.size());
    assertEquals(512, wbm.getAllocatedBytes());
    assertEquals(288, wbm.getUsedBytes());
    assertEquals(288, wbm.getInSendListBytes());
    assertEquals(0, wbm.getBuffers().size());
    // free memory
    wbm.freeAllocatedMemory(96);
    assertEquals(416, wbm.getAllocatedBytes());
    assertEquals(192, wbm.getUsedBytes());
    assertEquals(192, wbm.getInSendListBytes());

    assertEquals(11, wbm.getShuffleWriteMetrics().recordsWritten());
    assertTrue(wbm.getShuffleWriteMetrics().bytesWritten() > 0);

    wbm.freeAllocatedMemory(192);
    wbm.addRecord(0, testKey, testValue);
    wbm.addRecord(1, testKey, testValue);
    wbm.addRecord(2, testKey, testValue);
    result = wbm.clear();
    assertEquals(3, result.size());
    assertEquals(224, wbm.getAllocatedBytes());
    assertEquals(96, wbm.getUsedBytes());
    assertEquals(96, wbm.getInSendListBytes());
  }
",non-flaky,5
76930,Tencent_Firestorm,WriteBufferManagerTest.createBlockIdTest,"  @Test
  public void createBlockIdTest() {
    SparkConf conf = getConf();
    WriteBufferManager wbm = createManager(conf);
    WriterBuffer mockWriterBuffer = mock(WriterBuffer.class);
    when(mockWriterBuffer.getData()).thenReturn(new byte[]{});
    when(mockWriterBuffer.getMemoryUsed()).thenReturn(0);
    ShuffleBlockInfo sbi = wbm.createShuffleBlock(0, mockWriterBuffer);
    // seqNo = 0, partitionId = 0, taskId = 0
    assertEquals(0L, sbi.getBlockId());

    // seqNo = 1, partitionId = 0, taskId = 0
    sbi = wbm.createShuffleBlock(0, mockWriterBuffer);
    assertEquals(17592186044416L, sbi.getBlockId());

    // seqNo = 0, partitionId = 1, taskId = 0
    sbi = wbm.createShuffleBlock(1, mockWriterBuffer);
    assertEquals(1048576L, sbi.getBlockId());

    // seqNo = 1, partitionId = 1, taskId = 0
    sbi = wbm.createShuffleBlock(1, mockWriterBuffer);
    assertEquals(17592187092992L, sbi.getBlockId());
  }
",non-flaky,5
76931,Tencent_Firestorm,RssShuffleReaderTest.readTest,"  @Test
  public void readTest() throws Exception {

    String basePath = HDFS_URI + ""readTest1"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 0, basePath, ""test"", conf);
    HdfsShuffleWriteHandler writeHandler1 =
        new HdfsShuffleWriteHandler(""appId"", 0, 1, 1, basePath, ""test"", conf);

    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    Map<String, String> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
    writeTestData(writeHandler, 2, 5, expectedData,
        blockIdBitmap, ""key"", KRYO_SERIALIZER, 0);



    TaskContext contextMock = mock(TaskContext.class);
    RssShuffleHandle handleMock = mock(RssShuffleHandle.class);
    ShuffleDependency dependencyMock = mock(ShuffleDependency.class);
    when(handleMock.getAppId()).thenReturn(""appId"");
    when(handleMock.getDependency()).thenReturn(dependencyMock);
    when(handleMock.getShuffleId()).thenReturn(1);
    when(dependencyMock.serializer()).thenReturn(KRYO_SERIALIZER);
    when(contextMock.attemptNumber()).thenReturn(1);
    when(contextMock.taskAttemptId()).thenReturn(1L);
    when(contextMock.taskMetrics()).thenReturn(new TaskMetrics());
    doNothing().when(contextMock).killTaskIfInterrupted();
    when(dependencyMock.aggregator()).thenReturn(Option.empty());
    when(dependencyMock.keyOrdering()).thenReturn(Option.empty());
    when(dependencyMock.mapSideCombine()).thenReturn(false);

    Map<Integer, Roaring64NavigableMap> partitionToExpectBlocks = Maps.newHashMap();
    partitionToExpectBlocks.put(0, blockIdBitmap);
    RssShuffleReader rssShuffleReaderSpy = spy(new RssShuffleReader<String, String>(
        0,
        1,
        0,
        Integer.MAX_VALUE,
        contextMock,
        handleMock,
        basePath,
        1000,
        conf,
        StorageType.HDFS.name(),
        1000,
        1,
        partitionToExpectBlocks,
        taskIdBitmap,
        new ShuffleReadMetrics()));
    validateResult(rssShuffleReaderSpy.read(), expectedData, 10);

    writeTestData(writeHandler1, 2, 4, expectedData,
        blockIdBitmap1, ""another_key"", KRYO_SERIALIZER, 1);
    partitionToExpectBlocks.put(1, blockIdBitmap1);
    RssShuffleReader rssShuffleReaderSpy1 = spy(new RssShuffleReader<String, String>(
        0,
        2,
        0,
        Integer.MAX_VALUE,
        contextMock,
        handleMock,
        basePath,
        1000,
        conf,
        StorageType.HDFS.name(),
        1000,
        2,
        partitionToExpectBlocks,
        taskIdBitmap,
        new ShuffleReadMetrics()));
    validateResult(rssShuffleReaderSpy1.read(), expectedData, 18);

    RssShuffleReader rssShuffleReaderSpy2 = spy(new RssShuffleReader<String, String>(
        0,
        2,
        0,
        Integer.MAX_VALUE,
        contextMock,
        handleMock,
        basePath,
        1000,
        conf,
        StorageType.HDFS.name(),
        1000,
        2,
        partitionToExpectBlocks,
        Roaring64NavigableMap.bitmapOf(),
        new ShuffleReadMetrics()));
    validateResult(rssShuffleReaderSpy2.read(), Maps.newHashMap(), 0);
  }
",non-flaky,5
76932,Tencent_Firestorm,RssShuffleWriterTest.checkBlockSendResultTest,"  @Test
  public void checkBlockSendResultTest() {
    SparkConf conf = new SparkConf();
    conf.setAppName(""testApp"")
        .setMaster(""local[2]"")
        .set(RssClientConfig.RSS_TEST_FLAG, ""true"")
        .set(RssClientConfig.RSS_WRITER_SEND_CHECK_TIMEOUT, ""10000"")
        .set(RssClientConfig.RSS_WRITER_SEND_CHECK_INTERVAL, ""1000"")
        .set(RssClientConfig.RSS_COORDINATOR_QUORUM, ""127.0.0.1:12345,127.0.0.1:12346"");
    // init SparkContext
    SparkContext sc = SparkContext.getOrCreate(conf);
    Map<String, Set<Long>> failBlocks = Maps.newConcurrentMap();
    Map<String, Set<Long>> successBlocks = Maps.newConcurrentMap();
    Serializer kryoSerializer = new KryoSerializer(conf);
    RssShuffleManager manager = TestUtils.createShuffleManager(
        conf,
        false,
        null,
        successBlocks,
        failBlocks);

    ShuffleWriteClient mockShuffleWriteClient = mock(ShuffleWriteClient.class);
    Partitioner mockPartitioner = mock(Partitioner.class);
    RssShuffleHandle mockHandle = mock(RssShuffleHandle.class);
    ShuffleDependency mockDependency = mock(ShuffleDependency.class);
    when(mockHandle.getDependency()).thenReturn(mockDependency);
    when(mockPartitioner.numPartitions()).thenReturn(2);
    TaskMemoryManager mockTaskMemoryManager = mock(TaskMemoryManager.class);
    when(mockHandle.getPartitionToServers()).thenReturn(Maps.newHashMap());
    when(mockDependency.partitioner()).thenReturn(mockPartitioner);

    BufferManagerOptions bufferOptions = new BufferManagerOptions(conf);
    WriteBufferManager bufferManager = new WriteBufferManager(
        0, 0, bufferOptions, kryoSerializer,
        Maps.newHashMap(), mockTaskMemoryManager, new ShuffleWriteMetrics());
    WriteBufferManager bufferManagerSpy = spy(bufferManager);

    RssShuffleWriter rssShuffleWriter = new RssShuffleWriter(""appId"", 0, ""taskId"", 1L,
        bufferManagerSpy, (new TaskMetrics()).shuffleWriteMetrics(),
        manager, conf, mockShuffleWriteClient, mockHandle);
    doReturn(1000000L).when(bufferManagerSpy).acquireMemory(anyLong());

    // case 1: all blocks are sent successfully
    successBlocks.put(""taskId"", Sets.newHashSet(1L, 2L, 3L));
    rssShuffleWriter.checkBlockSendResult(Sets.newHashSet(1L, 2L, 3L));
    successBlocks.clear();

    // case 2: partial blocks aren't sent before spark.rss.writer.send.check.timeout,
    // Runtime exception will be thrown
    successBlocks.put(""taskId"", Sets.newHashSet(1L, 2L));
    thrown.expect(RuntimeException.class);
    thrown.expectMessage(StringStartsWith.startsWith(""Timeout:""));
    rssShuffleWriter.checkBlockSendResult(Sets.newHashSet(1L, 2L, 3L));
    successBlocks.clear();

    // case 3: partial blocks are sent failed, Runtime exception will be thrown
    successBlocks.put(""taskId"", Sets.newHashSet(1L, 2L));
    failBlocks.put(""taskId"", Sets.newHashSet(3L));
    thrown.expect(RuntimeException.class);
    thrown.expectMessage(StringStartsWith.startsWith(""Send failed:""));
    rssShuffleWriter.checkBlockSendResult(Sets.newHashSet(1L, 2L, 3L));
    successBlocks.clear();
    failBlocks.clear();

    sc.stop();
  }
",non-flaky,5
76933,Tencent_Firestorm,RssShuffleWriterTest.onReceive,"  @Test
  public void writeTest() throws Exception {
    SparkConf conf = new SparkConf();
    conf.setAppName(""testApp"").setMaster(""local[2]"")
        .set(RssClientConfig.RSS_WRITER_SERIALIZER_BUFFER_SIZE, ""32"")
        .set(RssClientConfig.RSS_WRITER_BUFFER_SIZE, ""32"")
        .set(RssClientConfig.RSS_TEST_FLAG, ""true"")
        .set(RssClientConfig.RSS_WRITER_BUFFER_SEGMENT_SIZE, ""64"")
        .set(RssClientConfig.RSS_WRITER_SEND_CHECK_TIMEOUT, ""10000"")
        .set(RssClientConfig.RSS_WRITER_SEND_CHECK_INTERVAL, ""1000"")
        .set(RssClientConfig.RSS_WRITER_BUFFER_SPILL_SIZE, ""128"")
        .set(RssClientConfig.RSS_COORDINATOR_QUORUM, ""127.0.0.1:12345,127.0.0.1:12346"");
    // init SparkContext
    List<ShuffleBlockInfo> shuffleBlockInfos = Lists.newArrayList();
    SparkContext sc = SparkContext.getOrCreate(conf);
    Map<String, Set<Long>> successBlockIds = Maps.newConcurrentMap();
    EventLoop<AddBlockEvent> testLoop = new EventLoop<AddBlockEvent>(""test"") {
      @Override
      public void onReceive(AddBlockEvent event) {
        assertEquals(""taskId"", event.getTaskId());
        shuffleBlockInfos.addAll(event.getShuffleDataInfoList());
        Set<Long> blockIds = event.getShuffleDataInfoList().parallelStream()
            .map(sdi -> sdi.getBlockId()).collect(Collectors.toSet());
        successBlockIds.putIfAbsent(event.getTaskId(), Sets.newConcurrentHashSet());
        successBlockIds.get(event.getTaskId()).addAll(blockIds);
      }
",non-flaky,5
76934,Tencent_Firestorm,RssShuffleWriterTest.onReceive,"  @Test
  public void postBlockEventTest() throws Exception {
    WriteBufferManager mockBufferManager = mock(WriteBufferManager.class);
    ShuffleDependency mockDependency = mock(ShuffleDependency.class);
    ShuffleWriteMetrics mockMetrics = mock(ShuffleWriteMetrics.class);
    Partitioner mockPartitioner = mock(Partitioner.class);
    when(mockDependency.partitioner()).thenReturn(mockPartitioner);
    SparkConf sparkConf = new SparkConf();
    when(mockPartitioner.numPartitions()).thenReturn(2);
    List<AddBlockEvent> events = Lists.newArrayList();

    EventLoop<AddBlockEvent> eventLoop = new EventLoop<AddBlockEvent>(""test"") {
      @Override
      public void onReceive(AddBlockEvent event) {
        events.add(event);
      }
",non-flaky,5
76935,Tencent_Firestorm,RssShuffleReaderTest.readTest,"  @Test
  public void readTest() throws Exception {

    String basePath = HDFS_URI + ""readTest1"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test"", conf);

    Map<String, String> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
     writeTestData(writeHandler, 2, 5, expectedData,
        blockIdBitmap, ""key"", KRYO_SERIALIZER, 0);

    TaskContext contextMock = mock(TaskContext.class);
    RssShuffleHandle handleMock = mock(RssShuffleHandle.class);
    ShuffleDependency dependencyMock = mock(ShuffleDependency.class);
    when(handleMock.getAppId()).thenReturn(""appId"");
    when(handleMock.getShuffleId()).thenReturn(1);
    when(handleMock.getDependency()).thenReturn(dependencyMock);
    when(dependencyMock.serializer()).thenReturn(KRYO_SERIALIZER);
    when(contextMock.taskAttemptId()).thenReturn(1L);
    when(contextMock.attemptNumber()).thenReturn(1);
    when(contextMock.taskMetrics()).thenReturn(new TaskMetrics());
    doNothing().when(contextMock).killTaskIfInterrupted();
    when(dependencyMock.mapSideCombine()).thenReturn(false);
    when(dependencyMock.aggregator()).thenReturn(Option.empty());
    when(dependencyMock.keyOrdering()).thenReturn(Option.empty());

    RssShuffleReader rssShuffleReaderSpy = spy(new RssShuffleReader<String, String>(0, 1, contextMock,
        handleMock, basePath, 1000, conf, StorageType.HDFS.name(),
        1000, 2, 10, blockIdBitmap, taskIdBitmap));

    validateResult(rssShuffleReaderSpy.read(), expectedData, 10);
  }
",non-flaky,5
76936,Tencent_Firestorm,RssShuffleWriterTest.checkBlockSendResultTest,"  @Test
  public void checkBlockSendResultTest() {
    SparkConf conf = new SparkConf();
    conf.setAppName(""testApp"")
        .setMaster(""local[2]"")
        .set(RssClientConfig.RSS_TEST_FLAG, ""true"")
        .set(RssClientConfig.RSS_WRITER_SEND_CHECK_TIMEOUT, ""10000"")
        .set(RssClientConfig.RSS_WRITER_SEND_CHECK_INTERVAL, ""1000"")
        .set(RssClientConfig.RSS_COORDINATOR_QUORUM, ""127.0.0.1:12345,127.0.0.1:12346"");
    // init SparkContext
    SparkContext sc = SparkContext.getOrCreate(conf);
    RssShuffleManager manager = new RssShuffleManager(conf, false);

    Serializer kryoSerializer = new KryoSerializer(conf);
    ShuffleWriteClient mockShuffleWriteClient = mock(ShuffleWriteClient.class);
    Partitioner mockPartitioner = mock(Partitioner.class);
    ShuffleDependency mockDependency = mock(ShuffleDependency.class);
    RssShuffleHandle mockHandle = mock(RssShuffleHandle.class);
    when(mockHandle.getDependency()).thenReturn(mockDependency);
    when(mockDependency.partitioner()).thenReturn(mockPartitioner);
    when(mockPartitioner.numPartitions()).thenReturn(2);
    when(mockHandle.getPartitionToServers()).thenReturn(Maps.newHashMap());
    TaskMemoryManager mockTaskMemoryManager = mock(TaskMemoryManager.class);

    BufferManagerOptions bufferOptions = new BufferManagerOptions(conf);
    WriteBufferManager bufferManager = new WriteBufferManager(
        0, 0, bufferOptions, kryoSerializer,
        Maps.newHashMap(), mockTaskMemoryManager, new ShuffleWriteMetrics());
    WriteBufferManager bufferManagerSpy = spy(bufferManager);
    doReturn(1000000L).when(bufferManagerSpy).acquireMemory(anyLong());

    RssShuffleWriter rssShuffleWriter = new RssShuffleWriter(""appId"", 0, ""taskId"", 1L,
        bufferManagerSpy, (new TaskMetrics()).shuffleWriteMetrics(),
        manager, conf, mockShuffleWriteClient, mockHandle);

    // case 1: all blocks are sent successfully
    manager.addSuccessBlockIds(""taskId"", Sets.newHashSet(1L, 2L, 3L));
    rssShuffleWriter.checkBlockSendResult(Sets.newHashSet(1L, 2L, 3L));
    manager.clearCachedBlockIds();

    // case 2: partial blocks aren't sent before spark.rss.writer.send.check.timeout,
    // Runtime exception will be thrown
    manager.addSuccessBlockIds(""taskId"", Sets.newHashSet(1L, 2L));
    thrown.expect(RuntimeException.class);
    thrown.expectMessage(StringStartsWith.startsWith(""Timeout:""));
    rssShuffleWriter.checkBlockSendResult(Sets.newHashSet(1L, 2L, 3L));

    manager.clearCachedBlockIds();

    // case 3: partial blocks are sent failed, Runtime exception will be thrown
    manager.addSuccessBlockIds(""taskId"", Sets.newHashSet(1L, 2L));
    manager.addFailedBlockIds(""taskId"", Sets.newHashSet(3L));
    thrown.expect(RuntimeException.class);
    thrown.expectMessage(StringStartsWith.startsWith(""Send failed:""));
    rssShuffleWriter.checkBlockSendResult(Sets.newHashSet(1L, 2L, 3L));
    manager.clearCachedBlockIds();

    sc.stop();
  }
",non-flaky,5
76937,Tencent_Firestorm,RssShuffleWriterTest.onReceive,"  @Test
  public void writeTest() throws Exception {
    SparkConf conf = new SparkConf();
    conf.setAppName(""testApp"").setMaster(""local[2]"")
        .set(RssClientConfig.RSS_TEST_FLAG, ""true"")
        .set(RssClientConfig.RSS_WRITER_BUFFER_SIZE, ""32"")
        .set(RssClientConfig.RSS_WRITER_SERIALIZER_BUFFER_SIZE, ""32"")
        .set(RssClientConfig.RSS_WRITER_BUFFER_SEGMENT_SIZE, ""64"")
        .set(RssClientConfig.RSS_WRITER_BUFFER_SPILL_SIZE, ""128"")
        .set(RssClientConfig.RSS_WRITER_SEND_CHECK_TIMEOUT, ""10000"")
        .set(RssClientConfig.RSS_WRITER_SEND_CHECK_INTERVAL, ""1000"")
        .set(RssClientConfig.RSS_COORDINATOR_QUORUM, ""127.0.0.1:12345,127.0.0.1:12346"");
    // init SparkContext
    SparkContext sc = SparkContext.getOrCreate(conf);
    RssShuffleManager manager = new RssShuffleManager(conf, false);
    List<ShuffleBlockInfo> shuffleBlockInfos = Lists.newArrayList();

    manager.setEventLoop(new EventLoop<AddBlockEvent>(""test"") {
      @Override
      public void onReceive(AddBlockEvent event) {
        assertEquals(""taskId"", event.getTaskId());
        shuffleBlockInfos.addAll(event.getShuffleDataInfoList());
        Set<Long> blockIds = event.getShuffleDataInfoList().parallelStream()
            .map(sdi -> sdi.getBlockId()).collect(Collectors.toSet());
        manager.addSuccessBlockIds(event.getTaskId(), blockIds);
      }
",non-flaky,5
76938,Tencent_Firestorm,RssShuffleWriterTest.onReceive,"  @Test
  public void postBlockEventTest() throws Exception {
    WriteBufferManager mockBufferManager = mock(WriteBufferManager.class);
    ShuffleWriteMetrics mockMetrics = mock(ShuffleWriteMetrics.class);
    ShuffleDependency mockDependency = mock(ShuffleDependency.class);
    Partitioner mockPartitioner = mock(Partitioner.class);
    RssShuffleManager mockShuffleManager = mock(RssShuffleManager.class);
    when(mockDependency.partitioner()).thenReturn(mockPartitioner);
    when(mockPartitioner.numPartitions()).thenReturn(2);
    List<AddBlockEvent> events = Lists.newArrayList();

    EventLoop<AddBlockEvent> eventLoop = new EventLoop<AddBlockEvent>(""test"") {
      @Override
      public void onReceive(AddBlockEvent event) {
        events.add(event);
      }
",non-flaky,5
76939,Tencent_Firestorm,ClientUtilsTest.getBlockIdTest,"  @Test
  public void getBlockIdTest() {
    // max value of blockId
    assertEquals(
        new Long(9223372036854775807L), ClientUtils.getBlockId(16777215, 1048575, 524287));
    // just a random test
    assertEquals(
        new Long(1759218709299300L), ClientUtils.getBlockId(100, 100, 100));
    // min value of blockId
    assertEquals(
        new Long(0L), ClientUtils.getBlockId(0, 0, 0));
    try {
      ClientUtils.getBlockId(16777216, 0, 0);
      fail(EXCEPTION_EXPECTED);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Can't support partitionId[16777216], the max value should be 16777215""));
    }
    try {
      ClientUtils.getBlockId(0, 1048576, 0);
      fail(EXCEPTION_EXPECTED);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Can't support taskAttemptId[1048576], the max value should be 1048575""));
    }
    try {
      ClientUtils.getBlockId(0, 0, 524288);
      fail(EXCEPTION_EXPECTED);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Can't support sequence[524288], the max value should be 524287""));
    }
  }
",non-flaky,5
76940,Tencent_Firestorm,ClientUtilsTest.getBitmapNumTest,"  @Test
  public void getBitmapNumTest() {
    // max value of taskNum, partitionNum, blockNumPerTaskPerPartition, it is unexpected in real job
    assertEquals(
        2147483647, ClientUtils.getBitmapNum(Integer.MAX_VALUE, Integer.MAX_VALUE, 1000000, 100000000L));
    // taskNum * partitionNum * blockNumPerTaskPerPartition / blockNumPerBitmap > 0
    assertEquals(
        5001, ClientUtils.getBitmapNum(100000, 100000, 50, 100000000L));
    // taskNum * partitionNum * blockNumPerTaskPerPartition / blockNumPerBitmap = 0
    assertEquals(
        1, ClientUtils.getBitmapNum(1999, 1999, 50, 100000000L));
    try {
      ClientUtils.getBitmapNum(1, 1, 1, 19999999L);
      fail(EXCEPTION_EXPECTED);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""blockNumPerBitmap should be greater than""));
    }
    try {
      ClientUtils.getBitmapNum(1, 1, 1000001, 20000000L);
      fail(EXCEPTION_EXPECTED);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""blockNumPerTaskPerPartition should be less than""));
    }
  }
",non-flaky,5
76941,Tencent_Firestorm,ShuffleReadClientImplTest.readTest1,"  @Test
  public void readTest1() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest1"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 1, 1, basePath, ""test1"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 2, 30, 0, expectedData,
        blockIdBitmap);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 100, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();

    blockIdBitmap.addLong(Constants.MAX_TASK_ATTEMPT_ID - 1);
    taskIdBitmap.addLong(Constants.MAX_TASK_ATTEMPT_ID - 1);
    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 100, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());
    TestUtils.validateResult(readClient, expectedData);
    try {
      // can't find all expected block id, data loss
      readClient.checkProcessedBlockIds();
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Blocks read inconsistent:""));
    } finally {
      readClient.close();
    }
  }
",non-flaky,5
76942,Tencent_Firestorm,ShuffleReadClientImplTest.readTest2,"  @Test
  public void readTest2() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest2"";
    HdfsShuffleWriteHandler writeHandler1 =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test2_1"", conf);
    HdfsShuffleWriteHandler writeHandler2 =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test2_2"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler1, 2, 30, 0, expectedData, blockIdBitmap);
    writeTestData(writeHandler2, 2, 30, 0, expectedData, blockIdBitmap);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 1, 100, 2, 10, 1000,
        basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76943,Tencent_Firestorm,ShuffleReadClientImplTest.readTest3,"  @Test
  public void readTest3() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest3"";
    HdfsShuffleWriteHandler writeHandler1 =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test3_1"", conf);
    HdfsShuffleWriteHandler writeHandler2 =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test3_2"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler1, 2, 30, 0, expectedData, blockIdBitmap);
    writeTestData(writeHandler2, 2, 30, 0, expectedData, blockIdBitmap);

    // duplicate file created, it should be used in product environment
    String shuffleFolder = basePath + ""/appId/0/0-1"";
    FileUtil.copy(fs, new Path(shuffleFolder + ""/test3_1_0.data""), fs,
        new Path(basePath + ""/test3_1.cp.data""), false, conf);
    FileUtil.copy(fs, new Path(shuffleFolder + ""/test3_1_0.index""), fs,
        new Path(basePath + ""/test3_1.cp.index""), false, conf);
    FileUtil.copy(fs, new Path(shuffleFolder + ""/test3_2_0.data""), fs,
        new Path(basePath + ""/test3_2.cp.data""), false, conf);
    FileUtil.copy(fs, new Path(shuffleFolder + ""/test3_2_0.index""), fs,
        new Path(basePath + ""/test3_2.cp.index""), false, conf);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 1, 100, 2, 10, 1000,
        basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76944,Tencent_Firestorm,ShuffleReadClientImplTest.readTest4,"  @Test
  public void readTest4() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest4"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test1"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 2, 30, 0, expectedData, blockIdBitmap);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 1, 100, 2, 10, 1000,
        basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());
    Path dataFile = new Path(basePath + ""/appId/0/0-1/test1_0.data"");
    // data file is deleted after readClient checkExpectedBlockIds
    fs.delete(new Path(basePath + ""/appId/0/0-1/test1_0.data""), true);
    // sleep to wait delete operation
    Thread.sleep(10000);

    assertNull(readClient.readShuffleBlockData());
    try {
      fs.listStatus(dataFile);
      fail(""Index file should be deleted"");
    } catch (Exception e) {
    }

    try {
      readClient.checkProcessedBlockIds();
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().startsWith(""Blocks read inconsistent: expected""));
    }
    readClient.close();
  }
",non-flaky,5
76945,Tencent_Firestorm,ShuffleReadClientImplTest.readTest5,"  @Test
  public void readTest5() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest5"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 2, 30, 0, expectedData, blockIdBitmap);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 1, 100, 2, 10, 1000,
        basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());
    // index file is deleted after iterator initialization, it should be ok, all index infos are read already
    Path indexFile = new Path(basePath + ""/appId/0/0-1/test.index"");
    fs.delete(indexFile, true);
    readClient.close();

    assertNull(readClient.readShuffleBlockData());
  }
",non-flaky,5
76946,Tencent_Firestorm,ShuffleReadClientImplTest.readTest7,"  @Test
  public void readTest7() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest7"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test"", conf);

    Map<Long, byte[]> expectedData1 = Maps.newHashMap();
    Map<Long, byte[]> expectedData2 = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 10, 30, 0, expectedData1, blockIdBitmap1);

    Roaring64NavigableMap blockIdBitmap2 = Roaring64NavigableMap.bitmapOf();
    writeTestData(writeHandler, 10, 30, 0, expectedData2, blockIdBitmap2);

    writeTestData(writeHandler, 10, 30, 0, expectedData1, blockIdBitmap1);

    ShuffleReadClientImpl readClient1 = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 0, 100, 2, 10, 100,
        basePath, blockIdBitmap1, taskIdBitmap, Lists.newArrayList(), new Configuration());
    ShuffleReadClientImpl readClient2 = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 1, 100, 2, 10, 100,
        basePath, blockIdBitmap2, taskIdBitmap, Lists.newArrayList(), new Configuration());
    TestUtils.validateResult(readClient1, expectedData1);
    readClient1.checkProcessedBlockIds();
    readClient1.close();

    TestUtils.validateResult(readClient2, expectedData2);
    readClient2.checkProcessedBlockIds();
    readClient2.close();
  }
",non-flaky,5
76947,Tencent_Firestorm,ShuffleReadClientImplTest.readTest8,"  @Test
  public void readTest8() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest8"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 2, 30, 0, expectedData, blockIdBitmap);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 1, 100, 2, 10, 1000,
        basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());
    // crc32 is incorrect
    try (MockedStatic<ChecksumUtils> checksumUtilsMock = Mockito.mockStatic(ChecksumUtils.class)) {
      checksumUtilsMock.when(() -> ChecksumUtils.getCrc32((ByteBuffer) any())).thenReturn(-1L);
      try {
        ByteBuffer bb = readClient.readShuffleBlockData().getByteBuffer();
        while (bb != null) {
          bb = readClient.readShuffleBlockData().getByteBuffer();
        }
        fail(EXPECTED_EXCEPTION_MESSAGE);
      } catch (Exception e) {
        assertTrue(e.getMessage().startsWith(""Unexpected crc value""));
      }
    }
    readClient.close();
  }
",non-flaky,5
76948,Tencent_Firestorm,ShuffleReadClientImplTest.readTest9,"  @Test
  public void readTest9() {
    // empty data
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 1, 100, 2, 10, 1000,
        ""basePath"", Roaring64NavigableMap.bitmapOf(), Roaring64NavigableMap.bitmapOf(),
        Lists.newArrayList(), new Configuration());
    assertNull(readClient.readShuffleBlockData());
    readClient.checkProcessedBlockIds();
  }
",non-flaky,5
76949,Tencent_Firestorm,ShuffleReadClientImplTest.readTest10,"  @Test
  public void readTest10() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest10"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 0, 1, basePath, ""test"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 5, 30, 0, expectedData, blockIdBitmap);
    Roaring64NavigableMap wrongBlockIdBitmap = Roaring64NavigableMap.bitmapOf();
    LongIterator iter = blockIdBitmap.getLongIterator();
    while (iter.hasNext()) {
      wrongBlockIdBitmap.addLong(iter.next() + (1 << Constants.TASK_ATTEMPT_ID_MAX_LENGTH));
    }

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        ""appId"", 0, 0, 100, 2, 10, 100,
        basePath, wrongBlockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());
    assertNull(readClient.readShuffleBlockData());
    try {
      readClient.checkProcessedBlockIds();
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Blocks read inconsistent:""));
    }
  }
",non-flaky,5
76950,Tencent_Firestorm,ShuffleReadClientImplTest.readTest11,"  @Test
  public void readTest11() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest11"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 1, 1, basePath, ""test1"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    writeTestData(writeHandler, 10, 30, 0, expectedData, blockIdBitmap);

    // test with different indexReadLimit to validate result
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 1, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();

    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 2, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();

    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 3, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();

    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 10, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();

    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 11, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76951,Tencent_Firestorm,ShuffleReadClientImplTest.readTest12,"  @Test
  public void readTest12() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest12"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 1, 1, basePath, ""test1"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0, 1);
    writeTestData(writeHandler, 5, 30, 0, expectedData, blockIdBitmap);
    writeTestData(writeHandler, 5, 30, 1, expectedData, blockIdBitmap);
    writeTestData(writeHandler, 5, 30, 2, Maps.newHashMap(), blockIdBitmap);

    // unexpected taskAttemptId should be filtered
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 100, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    assertEquals(15, readClient.getProcessedBlockIds().getLongCardinality());
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76952,Tencent_Firestorm,ShuffleReadClientImplTest.readTest13,"  @Test
  public void readTest13() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest13"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 1, 1, basePath, ""test1"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0, 3);
    writeTestData(writeHandler, 5, 30, 0, expectedData, blockIdBitmap);
    // test case: data generated by speculation task without report result
    writeTestData(writeHandler, 5, 30, 1, Maps.newHashMap(), Roaring64NavigableMap.bitmapOf());
    // test case: data generated by speculation task with report result
    writeTestData(writeHandler, 5, 30, 2, Maps.newHashMap(), blockIdBitmap);
    writeTestData(writeHandler, 5, 30, 3, expectedData, blockIdBitmap);

    // unexpected taskAttemptId should be filtered
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 100, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    assertEquals(20, readClient.getProcessedBlockIds().getLongCardinality());
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76953,Tencent_Firestorm,ShuffleReadClientImplTest.readTest14,"  @Test
  public void readTest14() throws Exception {
    String basePath = HDFS_URI + ""clientReadTest14"";
    HdfsShuffleWriteHandler writeHandler =
        new HdfsShuffleWriteHandler(""appId"", 0, 1, 1, basePath, ""test1"", conf);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0, 2);
    writeDuplicatedData(writeHandler, 5, 30, 0, expectedData, blockIdBitmap);
    writeTestData(writeHandler, 5, 30, 1, Maps.newHashMap(), Roaring64NavigableMap.bitmapOf());
    writeTestData(writeHandler, 5, 30, 2, expectedData, blockIdBitmap);

    // unexpected taskAttemptId should be filtered
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(), ""appId"", 0, 1, 100, 1,
        10, 1000, basePath, blockIdBitmap, taskIdBitmap, Lists.newArrayList(), new Configuration());

    TestUtils.validateResult(readClient, expectedData);
    assertEquals(15, readClient.getProcessedBlockIds().getLongCardinality());
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76954,Tencent_Firestorm,ShuffleWriteClientImplTest.testSendData,"  @Test
  public void testSendData() {
    ShuffleWriteClientImpl shuffleWriteClient =
        new ShuffleWriteClientImpl(""GRPC"", 3, 2000, 4);
    ShuffleServerClient mockShuffleServerClient = mock(ShuffleServerClient.class);
    ShuffleWriteClientImpl spyClient = spy(shuffleWriteClient);
    doReturn(mockShuffleServerClient).when(spyClient).getShuffleServerClient(any());
    when(mockShuffleServerClient.sendShuffleData(any())).thenReturn(
        new RssSendShuffleDataResponse(ResponseStatusCode.NO_BUFFER));

    List<ShuffleServerInfo> shuffleServerInfoList =
        Lists.newArrayList(new ShuffleServerInfo(""id"", ""host"", 0));
    List<ShuffleBlockInfo> shuffleBlockInfoList = Lists.newArrayList(new ShuffleBlockInfo(
        0, 0, 10, 10, 10, new byte[]{1}, shuffleServerInfoList, 10, 100, 0));
    SendShuffleDataResult result = spyClient.sendShuffleData(""appId"", shuffleBlockInfoList);

    assertTrue(result.getFailedBlockIds().contains(10L));
  }
",non-flaky,5
76955,Tencent_Firestorm,JettyServerTest.jettyServerTest,"  @Test
  public void jettyServerTest() throws FileNotFoundException {
    RssBaseConf conf = new RssBaseConf();
    conf.setString(""rss.jetty.http.port"", ""9527"");
    JettyServer jettyServer = new JettyServer(conf);
    Server server = jettyServer.getServer();

    assertEquals(4, server.getBeans().size());
    assertEquals(30000, server.getStopTimeout());
    assertTrue(server.getThreadPool() instanceof ExecutorThreadPool);

    assertEquals(1, server.getConnectors().length);
    assertEquals(server, server.getHandler().getServer());
    assertTrue(server.getConnectors()[0] instanceof ServerConnector);
    ServerConnector connector = (ServerConnector) server.getConnectors()[0];
    assertEquals(9527, connector.getPort());

    assertEquals(1, server.getHandlers().length);
    Handler handler = server.getHandler();
    assertTrue(handler instanceof ServletContextHandler);
  }
",non-flaky,5
76956,Tencent_Firestorm,JettyServerTest.jettyServerStartTest,"  @Test
  public void jettyServerStartTest() throws Exception {
    try {
      RssBaseConf conf = new RssBaseConf();
      conf.setString(""rss.jetty.http.port"", ""9527"");
      JettyServer jettyServer1 = new JettyServer(conf);
      JettyServer jettyServer2 = new JettyServer(conf);
      jettyServer1.start();

      ExitUtils.disableSystemExit();
      final String expectMessage = ""Fail to start jetty http server"";
      final int expectStatus = 1;
      try {
        jettyServer2.start();
      } catch (Exception e) {
        assertEquals(expectMessage, e.getMessage());
        assertEquals(expectStatus, ((ExitException) e).getStatus());
      }

      final Thread t = new Thread(null, () -> {
        throw new AssertionError(""TestUncaughtException"");
      }, ""testThread"");
      t.start();
      t.join();
    } catch (Exception e) {
      e.printStackTrace();
      fail();
    }

  }
",non-flaky,5
76957,Tencent_Firestorm,ConfigOptionTest.testBasicTypes,"  @Test
  public void testBasicTypes() {
    final ConfigOption<Integer> intConfig = ConfigOptions
        .key(""rss.key1"")
        .intType()
        .defaultValue(1000)
        .withDescription(""Int config key1"");
    assertSame(Integer.class, intConfig.getClazz());
    assertEquals(1000, (int) intConfig.defaultValue());
    assertEquals(""Int config key1"", intConfig.description());

    final ConfigOption<Long> longConfig = ConfigOptions
        .key(""rss.key2"")
        .longType()
        .defaultValue(1999L);
    assertTrue(longConfig.hasDefaultValue());
    assertEquals(1999L, (long) longConfig.defaultValue());

    final ConfigOption<String> stringConfig = ConfigOptions
        .key(""rss.key3"")
        .stringType()
        .noDefaultValue();
    assertFalse(stringConfig.hasDefaultValue());
    assertEquals("""", stringConfig.description());

    final ConfigOption<Boolean> booleanConfig = ConfigOptions
        .key(""key4"")
        .booleanType()
        .defaultValue(false)
        .withDescription(""Boolean config key"");
    assertFalse(booleanConfig.defaultValue());
    assertEquals(""Boolean config key"", booleanConfig.description());

    final ConfigOption<Integer> positiveInt = ConfigOptions
        .key(""key5"")
        .intType()
        .checkValue((v) -> {return v > 0;}, ""The value of key5 must be positive"")
        .defaultValue(1)
        .withDescription(""Positive integer key"");
    RssBaseConf conf = new RssBaseConf();
    conf.set(positiveInt, -1);
    boolean isException = false;
    try {
      conf.get(positiveInt);
    } catch (IllegalArgumentException ie) {
      isException = true;
      assertTrue(ie.getMessage().contains(""The value of key5 must be positive""));
    }
    assertTrue(isException);
    conf.set(positiveInt, 1);
    try {
      conf.get(positiveInt);
    } catch (IllegalArgumentException ie) {
      fail();
    }
  }
",non-flaky,5
76958,Tencent_Firestorm,RssConfTest.testOptionWithDefault,"    @Test
    public void testOptionWithDefault() {
        RssConf cfg = new RssConf();
        cfg.setInteger(""int-key"", 11);
        cfg.setString(""string-key"", ""abc"");

        ConfigOption<String> presentStringOption = ConfigOptions
                .key(""string-key"")
                .stringType()
                .defaultValue(""my-beautiful-default"");
        ConfigOption<Integer> presentIntOption = ConfigOptions
                .key(""int-key"")
                .intType()
                .defaultValue(87);

        assertEquals(""abc"", cfg.getString(presentStringOption));
        assertEquals(""abc"", cfg.getValue(presentStringOption));

        assertEquals(11, cfg.getInteger(presentIntOption));
        assertEquals(""11"", cfg.getValue(presentIntOption));
    }
",non-flaky,5
76959,Tencent_Firestorm,RssConfTest.testSetStringAndGetConcreteType,"    @Test
    public void testSetStringAndGetConcreteType() {
        RssConf conf = new RssConf();
        conf.setString(""boolean-type"", ""true"");
        conf.setString(""int-type"", ""1111"");
        conf.setString(""long-type"", ""1000"");
        assertTrue(conf.getBoolean(""boolean-type"", false));
        assertEquals(conf.getInteger(""int-type"", 100), 1111);
        assertEquals(conf.getLong(""long-type"", 222L), 1000L);
    }
",non-flaky,5
76960,Tencent_Firestorm,RssConfTest.testOptionWithNoDefault,"    @Test
    public void testOptionWithNoDefault() {
        RssConf cfg = new RssConf();
        cfg.setInteger(""int-key"", 11);
        cfg.setString(""string-key"", ""abc"");

        ConfigOption<String> presentStringOption = ConfigOptions
                .key(""string-key"")
                .stringType()
                .noDefaultValue();

        assertEquals(""abc"", cfg.getString(presentStringOption));
        assertEquals(""abc"", cfg.getValue(presentStringOption));

        // test getting default when no value is present

        ConfigOption<String> stringOption = ConfigOptions
                .key(""test"")
                .stringType()
                .noDefaultValue();

        // getting strings for null should work
        assertNull(cfg.getValue(stringOption));
        assertNull(cfg.getString(stringOption));

        // overriding the null default should work
        assertEquals(""override"", cfg.getString(stringOption, ""override""));
    }
",non-flaky,5
76961,Tencent_Firestorm,ChecksumUtilsTest.crc32TestWithByte,"  @Test
  public void crc32TestWithByte() {
    byte[] data = new byte[32 * 1024 * 1024];
    new Random().nextBytes(data);
    CRC32 crc32 = new CRC32();
    crc32.update(data);
    long expected = crc32.getValue();
    assertEquals(expected, ChecksumUtils.getCrc32(data));

    data = new byte[32 * 1024];
    new Random().nextBytes(data);
    crc32 = new CRC32();
    crc32.update(data);
    expected = crc32.getValue();
    assertEquals(expected, ChecksumUtils.getCrc32(data));
  }
",non-flaky,5
76962,Tencent_Firestorm,ChecksumUtilsTest.crc32TestWithByteBuff,"  @Test
  public void crc32TestWithByteBuff() throws Exception {
    int length = 32 * 1024 * 1024;
    byte[] data = new byte[length];
    new Random().nextBytes(data);

    String tempDir = Files.createTempDirectory(""rss"").toString();
    File file = new File(tempDir, ""crc_test.txt"");
    file.createNewFile();
    file.deleteOnExit();

    try (FileOutputStream outputStream = new FileOutputStream(file)) {
      outputStream.write(data);
    }

    long expectedChecksum = ChecksumUtils.getCrc32(data);

    // test direct ByteBuffer
    Path path = Paths.get(file.getAbsolutePath());
    FileChannel fileChannel = FileChannel.open(path);
    ByteBuffer buffer = ByteBuffer.allocateDirect(length);
    int bytesRead = fileChannel.read(buffer);
    fileChannel.close();
    assertEquals(length, bytesRead);
    buffer.flip();
    assertEquals(expectedChecksum, ChecksumUtils.getCrc32(buffer));
    assertEquals(length, buffer.position());

    // test heap ByteBuffer
    path = Paths.get(file.getAbsolutePath());
    fileChannel = FileChannel.open(path);
    buffer = ByteBuffer.allocate(length);
    bytesRead = fileChannel.read(buffer);
    fileChannel.close();
    assertEquals(length, bytesRead);
    buffer.flip();
    assertEquals(expectedChecksum, ChecksumUtils.getCrc32(buffer));

  }
",non-flaky,5
76963,Tencent_Firestorm,RssUtilsTest.testGetPropertiesFromFile,"  @Test
  public void testGetPropertiesFromFile() {
    final String filePath = Objects.requireNonNull(
        getClass().getClassLoader().getResource(""rss-defaults.conf"")).getFile();
    Map<String, String> properties = RssUtils.getPropertiesFromFile(filePath);
    assertEquals(""12121"", properties.get(""rss.coordinator.port""));
    assertEquals(""155"", properties.get(""rss.server.heartbeat.interval""));
    assertEquals(""true"", properties.get(""rss.x.y.z""));
    assertEquals(""-XX:+PrintGCDetails-Dkey=value-Dnumbers=\""one two three\"""",
        properties.get(""rss.a.b.c.extraJavaOptions""));
  }
",non-flaky,5
76964,Tencent_Firestorm,RssUtilsTest.testGetHostIp,"  @Test
  public void testGetHostIp() {
    try {
      String address = InetAddress.getLocalHost().getHostAddress();
      String realIp = RssUtils.getHostIp();
      assertNotEquals(""127.0.0.1"", realIp);
      if (!address.equals(""127.0.0.1"")) {
        assertEquals(address, realIp);
      }
    } catch (Exception e) {
      fail(e.getMessage());
    }
  }
",non-flaky,5
76965,Tencent_Firestorm,RssUtilsTest.testSerializeBitmap,"  @Test
  public void testSerializeBitmap() throws Exception {
    Roaring64NavigableMap bitmap1 = Roaring64NavigableMap.bitmapOf(1, 2, 100, 10000);
    byte[] bytes = RssUtils.serializeBitMap(bitmap1);
    Roaring64NavigableMap bitmap2 = RssUtils.deserializeBitMap(bytes);
    assertEquals(bitmap1, bitmap2);
    assertEquals(Roaring64NavigableMap.bitmapOf(), RssUtils.deserializeBitMap(new byte[]{}));
  }
",non-flaky,5
76966,Tencent_Firestorm,RssUtilsTest.testShuffleIndexSegment,"  @Test
  public void testShuffleIndexSegment() {
    ShuffleIndexResult shuffleIndexResult = new ShuffleIndexResult();
    List<ShuffleDataSegment> shuffleDataSegments =
        RssUtils.transIndexDataToSegments(shuffleIndexResult, 1000);
    assertTrue(shuffleDataSegments.isEmpty());

    int readBufferSize = 32;
    int totalLength = 0;
    List<BufferSegment> bufferSegments = Lists.newArrayList();
    int[] dataSegmentLength = new int[]{32, 16, 10, 32, 6};

    for (int i = 0; i < dataSegmentLength.length; ++i) {
      long offset = totalLength;
      int length = dataSegmentLength[i];
      bufferSegments.add(new BufferSegment(i, offset, length, i, i, i));
      totalLength += length;
    }

    // those 5 segment's data length are [32, 16, 10, 32, 6] so the index should be
    // split into 3 ShuffleDataSegment, which are [32, 16 + 10 + 32, 6]
    int expectedTotalSegmentNum = 3;
    ByteBuffer byteBuffer = ByteBuffer.allocate(5 * 40);

    for (BufferSegment bufferSegment : bufferSegments) {
      byteBuffer.putLong(bufferSegment.getOffset());
      byteBuffer.putInt(bufferSegment.getLength());
      byteBuffer.putInt(bufferSegment.getUncompressLength());
      byteBuffer.putLong(bufferSegment.getCrc());
      byteBuffer.putLong(bufferSegment.getBlockId());
      byteBuffer.putLong(bufferSegment.getTaskAttemptId());
    }

    byte[] data = byteBuffer.array();
    shuffleDataSegments = RssUtils.transIndexDataToSegments(new ShuffleIndexResult(data), readBufferSize);
    assertEquals(expectedTotalSegmentNum, shuffleDataSegments.size());

    assertEquals(0, shuffleDataSegments.get(0).getOffset());
    assertEquals(32, shuffleDataSegments.get(0).getLength());
    assertEquals(1, shuffleDataSegments.get(0).getBufferSegments().size());

    assertEquals(32, shuffleDataSegments.get(1).getOffset());
    assertEquals(58, shuffleDataSegments.get(1).getLength());
    assertEquals(3,shuffleDataSegments.get(1).getBufferSegments().size());

    assertEquals(90, shuffleDataSegments.get(2).getOffset());
    assertEquals(6, shuffleDataSegments.get(2).getLength());
    assertEquals(1, shuffleDataSegments.get(2).getBufferSegments().size());
  }
",non-flaky,5
76967,Tencent_Firestorm,ExitUtilsTest.test,"  @Test
  public void test() {
    try {
    final int status = -1;
    final String testExitMessage = ""testExitMessage"";
    try {
      ExitUtils.disableSystemExit();
      ExitUtils.terminate(status, testExitMessage, null, null);
      fail();
    } catch (ExitException e) {
      assertEquals(status, e.getStatus());
      assertEquals(testExitMessage, e.getMessage());
    }

    final Thread t = new Thread(null, () -> {
      throw new AssertionError(""TestUncaughtException"");
    }, ""testThread"");
    t.start();
    t.join();
  } catch (Exception e) {
      e.printStackTrace();
      fail();
    }

  }
",non-flaky,5
76968,Tencent_Firestorm,UnitConverterTest.testByteString,"  @Test
  public void testByteString() {

    assertEquals(10 * PB, UnitConverter.byteStringAs(""10PB"", ByteUnit.BYTE));
    assertEquals(10 * PB, UnitConverter.byteStringAs(""10pb"", ByteUnit.BYTE));
    assertEquals(10 * PB, UnitConverter.byteStringAs(""10pB"", ByteUnit.BYTE));
    assertEquals(10 * PB, UnitConverter.byteStringAs(""10p"", ByteUnit.BYTE));
    assertEquals(10 * PB, UnitConverter.byteStringAs(""10P"", ByteUnit.BYTE));

    assertEquals(10 * TB, UnitConverter.byteStringAs(""10TB"", ByteUnit.BYTE));
    assertEquals(10 * TB, UnitConverter.byteStringAs(""10tb"", ByteUnit.BYTE));
    assertEquals(10 * TB, UnitConverter.byteStringAs(""10tB"", ByteUnit.BYTE));
    assertEquals(10 * TB, UnitConverter.byteStringAs(""10T"", ByteUnit.BYTE));
    assertEquals(10 * TB, UnitConverter.byteStringAs(""10t"", ByteUnit.BYTE));

    assertEquals(10 * GB, UnitConverter.byteStringAs(""10GB"", ByteUnit.BYTE));
    assertEquals(10 * GB, UnitConverter.byteStringAs(""10gb"", ByteUnit.BYTE));
    assertEquals(10 * GB, UnitConverter.byteStringAs(""10gB"", ByteUnit.BYTE));

    assertEquals(10 * MB, UnitConverter.byteStringAs(""10MB"", ByteUnit.BYTE));
    assertEquals(10 * MB, UnitConverter.byteStringAs(""10mb"", ByteUnit.BYTE));
    assertEquals(10 * MB, UnitConverter.byteStringAs(""10mB"", ByteUnit.BYTE));
    assertEquals(10 * MB, UnitConverter.byteStringAs(""10M"", ByteUnit.BYTE));
    assertEquals(10 * MB, UnitConverter.byteStringAs(""10m"", ByteUnit.BYTE));

    assertEquals(10 * KB, UnitConverter.byteStringAs(""10KB"", ByteUnit.BYTE));
    assertEquals(10 * KB, UnitConverter.byteStringAs(""10kb"", ByteUnit.BYTE));
    assertEquals(10 * KB, UnitConverter.byteStringAs(""10Kb"", ByteUnit.BYTE));
    assertEquals(10 * KB, UnitConverter.byteStringAs(""10K"", ByteUnit.BYTE));
    assertEquals(10 * KB, UnitConverter.byteStringAs(""10k"", ByteUnit.BYTE));

    assertEquals(1111, UnitConverter.byteStringAs(""1111"", ByteUnit.BYTE));
  }
",non-flaky,5
76969,Tencent_Firestorm,ArgumentsTest.argTest,"  @Test
  public void argTest() {
    String[] args = {""-c"", confFile};
    Arguments arguments = new Arguments();
    CommandLine commandLine = new CommandLine(arguments);
    commandLine.parseArgs(args);
    assertEquals(confFile, arguments.getConfigFile());
  }
",non-flaky,5
76970,Tencent_Firestorm,ArgumentsTest.argEmptyTest,"  @Test
  public void argEmptyTest() {
    String[] args = new String[0];
    Arguments arguments = new Arguments();
    CommandLine commandLine = new CommandLine(arguments);
    commandLine.parseArgs(args);
    assertNull(arguments.getConfigFile());
  }
",non-flaky,5
76971,Tencent_Firestorm,MetricsManagerTest.testMetricsManager,"  @Test
  public void testMetricsManager() {
    MetricsManager metricsManager = new MetricsManager();
    assertEquals(CollectorRegistry.defaultRegistry, metricsManager.getCollectorRegistry());

    CollectorRegistry expectedRegistry = new CollectorRegistry();
    metricsManager = new MetricsManager(expectedRegistry);
    assertEquals(expectedRegistry, metricsManager.getCollectorRegistry());

    String expectedName1 = ""counter"";
    String expectedHelp1 = ""Counter "" + expectedName1;
    metricsManager.addCounter(expectedName1);

    String expectedName2 = ""name2"";
    String expectedHelp2 = ""Gauge "" + expectedName2;
    String label = ""gaugeLabel"";
    Gauge gauge = metricsManager.addGauge(expectedName2, label);
    gauge.labels(""lv1"").inc();
    gauge.labels(""lv2"").inc();

    Map<String, MetricFamilySamples> metricsSamples = new HashMap<>();
    Enumeration<MetricFamilySamples> mfs = expectedRegistry.metricFamilySamples();
    while (mfs.hasMoreElements()) {
      MetricFamilySamples cur = mfs.nextElement();
      metricsSamples.put(cur.name, cur);
    }

    assertEquals(expectedHelp1, metricsSamples.get(expectedName1).help);
    assertEquals(1, metricsSamples.get(expectedName1).samples.size());

    assertEquals(expectedHelp2, metricsSamples.get(expectedName2).help);
    List<MetricFamilySamples.Sample> f = metricsSamples.get(expectedName2).samples;
    assertEquals(2, metricsSamples.get(expectedName2).samples.size());
    String[] actualLabelValues = metricsSamples
        .get(expectedName2).samples
        .stream().map(i -> i.labelValues.get(0))
        .collect(Collectors.toList()).toArray(new String[0]);
    Arrays.sort(actualLabelValues);
    assertArrayEquals(new String[]{""lv1"", ""lv2""}, actualLabelValues);
  }
",non-flaky,5
76972,Tencent_Firestorm,ShufflePartitionedBlockTest.shufflePartitionedBlockTest,"  @Test
  public void shufflePartitionedBlockTest() {
    byte[] buf = new byte[3];
    new Random().nextBytes(buf);

    ShufflePartitionedBlock b1 = new ShufflePartitionedBlock(1, 1, 2, 3, 1, buf);
    assertEquals(1, b1.getLength());
    assertEquals(2, b1.getCrc());
    assertEquals(3, b1.getBlockId());

    ShufflePartitionedBlock b3 = new ShufflePartitionedBlock(1, 1, 2, 3, 3, buf);
    assertArrayEquals(buf, b3.getData());
  }
",non-flaky,5
76973,Tencent_Firestorm,SparkClientWithLocalTest.readTest1,"  @Test
  public void readTest1() {
    String testAppId = ""localReadTest1"";
    registerApp(testAppId, Lists.newArrayList(new PartitionRange(0, 0)));
    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    createTestData(testAppId, expectedData, blockIdBitmap, taskIdBitmap);
    blockIdBitmap.addLong((1 << Constants.TASK_ATTEMPT_ID_MAX_LENGTH));
    ShuffleReadClientImpl readClient;
    readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(), testAppId, 0, 0, 100, 1,
        10, 1000, """", blockIdBitmap, taskIdBitmap, shuffleServerInfo, null);
    validateResult(readClient, expectedData);
    try {
      // can't find all expected block id, data loss
      readClient.checkProcessedBlockIds();
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Blocks read inconsistent:""));
    } finally {
      readClient.close();
    }
  }
",non-flaky,5
76974,Tencent_Firestorm,SparkClientWithLocalTest.readTest2,"  @Test
  public void readTest2() {
    String testAppId = ""localReadTest2"";
    registerApp(testAppId, Lists.newArrayList(new PartitionRange(0, 0)));

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    List<ShuffleBlockInfo> blocks = createShuffleBlockList(
        0, 0, 0, 2, 30, blockIdBitmap, expectedData, mockSSI);
    sendTestData(testAppId, blocks);
    blocks = createShuffleBlockList(
        0, 0, 0, 2, 30, blockIdBitmap, expectedData, mockSSI);
    sendTestData(testAppId, blocks);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(),
        testAppId, 0, 0, 100, 1, 10, 1000,
        """", blockIdBitmap, taskIdBitmap, shuffleServerInfo, null);

    validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76975,Tencent_Firestorm,SparkClientWithLocalTest.readTest3,"  @Test
  public void readTest3() throws Exception {
    String testAppId = ""localReadTest3"";
    registerApp(testAppId, Lists.newArrayList(new PartitionRange(0, 0)));

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    List<ShuffleBlockInfo> blocks = createShuffleBlockList(
        0, 0, 0, 2, 30, blockIdBitmap, expectedData, mockSSI);
    sendTestData(testAppId, blocks);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(),
        testAppId, 0, 0, 100, 1, 10, 1000,
        """", blockIdBitmap, taskIdBitmap, shuffleServerInfo, null);
    FileUtils.deleteDirectory(new File(DATA_DIR1.getAbsolutePath() + ""/"" + testAppId + ""/0/0-0""));
    FileUtils.deleteDirectory(new File(DATA_DIR2.getAbsolutePath() + ""/"" + testAppId + ""/0/0-0""));
    // sleep to wait delete operation
    Thread.sleep(2000);

    try {
      readClient.readShuffleBlockData();
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Failed to read shuffle index""));
    }
    readClient.close();
  }
",non-flaky,5
76976,Tencent_Firestorm,SparkClientWithLocalTest.readTest4,"  @Test
  public void readTest4() {
    String testAppId = ""localReadTest4"";
    registerApp(testAppId, Lists.newArrayList(new PartitionRange(0, 1)));

    Map<Long, byte[]> expectedData1 = Maps.newHashMap();
    Map<Long, byte[]> expectedData2 = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    List<ShuffleBlockInfo> blocks = createShuffleBlockList(
        0, 0, 0, 10, 30, blockIdBitmap1, expectedData1, mockSSI);
    sendTestData(testAppId, blocks);

    Roaring64NavigableMap blockIdBitmap2 = Roaring64NavigableMap.bitmapOf();
    blocks = createShuffleBlockList(
        0, 1, 0, 10, 30, blockIdBitmap2, expectedData2, mockSSI);
    sendTestData(testAppId, blocks);

    blocks = createShuffleBlockList(
        0, 0, 0, 10, 30, blockIdBitmap1, expectedData1, mockSSI);
    sendTestData(testAppId, blocks);

    ShuffleReadClientImpl readClient1 = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(),
        testAppId, 0, 0, 100, 2, 10, 100,
        """", blockIdBitmap1, taskIdBitmap, shuffleServerInfo, null);
    ShuffleReadClientImpl readClient2 = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(),
        testAppId, 0, 1, 100, 2, 10, 100,
        """", blockIdBitmap2, taskIdBitmap, shuffleServerInfo, null);
    validateResult(readClient1, expectedData1);
    readClient1.checkProcessedBlockIds();
    readClient1.close();

    validateResult(readClient2, expectedData2);
    readClient2.checkProcessedBlockIds();
    readClient2.close();
  }
",non-flaky,5
76977,Tencent_Firestorm,SparkClientWithLocalTest.readTest5,"  @Test
  public void readTest5() {
    String testAppId = ""localReadTest5"";
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(),
        testAppId, 0, 1, 100, 2, 10, 1000,
        """", Roaring64NavigableMap.bitmapOf(), Roaring64NavigableMap.bitmapOf(),
        shuffleServerInfo, null);
    assertNull(readClient.readShuffleBlockData());
    readClient.checkProcessedBlockIds();
  }
",non-flaky,5
76978,Tencent_Firestorm,SparkClientWithLocalTest.readTest6,"  @Test
  public void readTest6() {
    String testAppId = ""localReadTest6"";
    registerApp(testAppId, Lists.newArrayList(new PartitionRange(0, 0)));

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);
    List<ShuffleBlockInfo> blocks = createShuffleBlockList(
        0, 0, 0, 5, 30, blockIdBitmap, expectedData, mockSSI);
    sendTestData(testAppId, blocks);

    Roaring64NavigableMap wrongBlockIdBitmap = Roaring64NavigableMap.bitmapOf();
    LongIterator iter = blockIdBitmap.getLongIterator();
    while (iter.hasNext()) {
      wrongBlockIdBitmap.addLong(iter.next() + (1 << Constants.TASK_ATTEMPT_ID_MAX_LENGTH));
    }

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(),
        testAppId, 0, 0, 100, 1, 10, 100,
        """", wrongBlockIdBitmap, taskIdBitmap, shuffleServerInfo, null);
    assertNull(readClient.readShuffleBlockData());
    try {
      readClient.checkProcessedBlockIds();
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Blocks read inconsistent:""));
    }
  }
",non-flaky,5
76979,Tencent_Firestorm,SparkClientWithLocalTest.readTest7,"  @Test
  public void readTest7() {
    String testAppId = ""localReadTest7"";
    registerApp(testAppId, Lists.newArrayList(new PartitionRange(0, 0)));

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0, 1);

    List<ShuffleBlockInfo> blocks = createShuffleBlockList(
        0, 0, 0, 5, 30, blockIdBitmap, expectedData, mockSSI);
    sendTestData(testAppId, blocks);

    blocks = createShuffleBlockList(
        0, 0, 1, 5, 30, blockIdBitmap, expectedData, mockSSI);
    sendTestData(testAppId, blocks);

    blocks = createShuffleBlockList(
        0, 0, 2, 5, 30, blockIdBitmap, Maps.newHashMap(), mockSSI);
    sendTestData(testAppId, blocks);

    // unexpected taskAttemptId should be filtered
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(), testAppId, 0, 0, 100, 1,
        10, 1000, """", blockIdBitmap, taskIdBitmap, shuffleServerInfo, null);

    validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76980,Tencent_Firestorm,SparkClientWithLocalTest.readTest8,"  @Test
  public void readTest8() {
    String testAppId = ""localReadTest8"";
    registerApp(testAppId, Lists.newArrayList(new PartitionRange(0, 0)));

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0, 3);
    List<ShuffleBlockInfo> blocks = createShuffleBlockList(
        0, 0, 0, 5, 30, blockIdBitmap, expectedData, mockSSI);
    sendTestData(testAppId, blocks);

    // test case: data generated by speculation task without report result
    blocks = createShuffleBlockList(
        0, 0, 1, 5, 30, Roaring64NavigableMap.bitmapOf(), Maps.newHashMap(), mockSSI);
    sendTestData(testAppId, blocks);
    // test case: data generated by speculation task with report result
    blocks = createShuffleBlockList(
        0, 0, 2, 5, 30, blockIdBitmap, Maps.newHashMap(), mockSSI);
    sendTestData(testAppId, blocks);

    blocks = createShuffleBlockList(
        0, 0, 3, 5, 30, Roaring64NavigableMap.bitmapOf(), Maps.newHashMap(), mockSSI);
    sendTestData(testAppId, blocks);

    // unexpected taskAttemptId should be filtered
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(), testAppId, 0, 0, 100, 1,
        10, 1000, """", blockIdBitmap, taskIdBitmap, shuffleServerInfo, null);

    validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76981,Tencent_Firestorm,SparkClientWithLocalTest.readTest9,"  @Test
  public void readTest9() throws Exception {
    String testAppId = ""localReadTest9"";
    registerApp(testAppId, Lists.newArrayList(new PartitionRange(0, 0)));
    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);

    List<ShuffleBlockInfo> blocks;
    ShuffleReadClientImpl readClient;

    createTestData(testAppId, expectedData, blockIdBitmap, taskIdBitmap);
    Roaring64NavigableMap beforeAdded = RssUtils.deserializeBitMap(RssUtils.serializeBitMap(blockIdBitmap));
    // write data by another task, read data again, the cache for index file should be updated
    blocks = createShuffleBlockList(
        0, 0, 1, 3, 25, blockIdBitmap, Maps.newHashMap(), mockSSI);
    sendTestData(testAppId, blocks);
    // test with un-changed expected blockId
    readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(), testAppId, 0, 0, 100, 1,
        10, 1000, """", beforeAdded, taskIdBitmap,
        shuffleServerInfo, null);
    validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();

    // test with changed expected blockId
    readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(), testAppId, 0, 0, 100, 1,
        10, 1000, """", blockIdBitmap, taskIdBitmap,
        shuffleServerInfo, null);
    validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();
  }
",non-flaky,5
76982,Tencent_Firestorm,CombineByKeyTest.combineByKeyTest,"  @Test
  public void combineByKeyTest() throws Exception {
    run();
  }
",non-flaky,5
76983,Tencent_Firestorm,SparkSQLTest.resultCompareTest,"  @Test
  public void resultCompareTest() throws Exception {
    run();
    checkShuffleData();
  }
",non-flaky,5
76984,Tencent_Firestorm,RepartitionTest.resultCompareTest,"  @Test
  public void resultCompareTest() throws Exception {
    run();
  }
",non-flaky,5
76985,Tencent_Firestorm,RepartitionTest.testMemoryRelease,"  @Test
  public void testMemoryRelease() throws Exception {
    String fileName = generateTextFile(10000, 10000);
    SparkConf sparkConf = createSparkConf();
    updateSparkConfWithRss(sparkConf);
    sparkConf.set(""spark.executor.memory"", ""500m"");
    updateRssStorage(sparkConf);

    // oom if there has no memory release
    runSparkApp(sparkConf, fileName);
  }
",non-flaky,5
76986,Tencent_Firestorm,GroupByKeyTest.groupByTest,"  @Test
  public void groupByTest() throws Exception {
    run();
  }
",non-flaky,5
76987,Tencent_Firestorm,SparkFallbackReadTest.resultCompareTest,"  @Test
  public void resultCompareTest() throws Exception {
    run();
    checkShuffleData();
  }
",non-flaky,5
76988,Tencent_Firestorm,ShuffleWithRssClientTest.rpcFailTest,"  @Test
  public void rpcFailTest() throws Exception {
    String testAppId = ""rpcFailTest"";
    shuffleWriteClientImpl.registerShuffle(shuffleServerInfo1,
        testAppId, 0, Lists.newArrayList(new PartitionRange(0, 0)));
    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();

    // simulator a failed server
    ShuffleServerInfo fakeShuffleServerInfo =
        new ShuffleServerInfo(""127.0.0.1-20001"", shuffleServers.get(0).getIp(), SHUFFLE_SERVER_PORT + 100);
    List<ShuffleBlockInfo> blocks = createShuffleBlockList(
        0, 0, 0, 3, 25, blockIdBitmap,
        expectedData, Lists.newArrayList(shuffleServerInfo1, fakeShuffleServerInfo));
    SendShuffleDataResult result = shuffleWriteClientImpl.sendShuffleData(testAppId, blocks);
    Roaring64NavigableMap failedBlockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap succBlockIdBitmap = Roaring64NavigableMap.bitmapOf();
    for (Long blockId : result.getFailedBlockIds()) {
      failedBlockIdBitmap.addLong(blockId);
    }
    for (Long blockId : result.getSuccessBlockIds()) {
      succBlockIdBitmap.addLong(blockId);
    }
    assertEquals(blockIdBitmap, failedBlockIdBitmap);
    assertEquals(blockIdBitmap, succBlockIdBitmap);

    boolean commitResult = shuffleWriteClientImpl.sendCommit(Sets.newHashSet(
        shuffleServerInfo1, fakeShuffleServerInfo), testAppId, 0, 2);
    assertFalse(commitResult);

    Map<Integer, List<Long>> ptb = Maps.newHashMap();
    ptb.put(1, Lists.newArrayList(1L));
    try {
      Map<Integer, List<ShuffleServerInfo>> partitionToServers = Maps.newHashMap();
      partitionToServers.put(1, Lists.newArrayList(
          shuffleServerInfo1, fakeShuffleServerInfo));
      shuffleWriteClientImpl.reportShuffleResult(partitionToServers, testAppId, 0, 0, ptb, 2);
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Report shuffle result is failed for""));
    }
  }
",non-flaky,5
76989,Tencent_Firestorm,ShuffleWithRssClientTest.reportMultipleServerTest,"  @Test
  public void reportMultipleServerTest() throws Exception {
    String testAppId = ""reportMultipleServerTest"";

    shuffleWriteClientImpl.registerShuffle(shuffleServerInfo1,
        testAppId, 1, Lists.newArrayList(new PartitionRange(1, 1)));

    shuffleWriteClientImpl.registerShuffle(shuffleServerInfo2,
        testAppId, 1, Lists.newArrayList(new PartitionRange(2, 2)));

    Map<Integer, List<ShuffleServerInfo>> partitionToServers = Maps.newHashMap();
    partitionToServers.putIfAbsent(1, Lists.newArrayList(shuffleServerInfo1));
    partitionToServers.putIfAbsent(2, Lists.newArrayList(shuffleServerInfo2));
    Map<Integer, List<Long>> partitionToBlocks = Maps.newHashMap();
    List<Long> blockIds = Lists.newArrayList();
    for (int i = 0; i < 5; i++ ) {
      blockIds.add(ClientUtils.getBlockId(1, 0, i));
    }
    partitionToBlocks.put(1, blockIds);
    blockIds = Lists.newArrayList();
    for (int i = 0; i < 7; i++ ) {
      blockIds.add(ClientUtils.getBlockId(2, 0, i));
    }
    partitionToBlocks.put(2, blockIds);
    shuffleWriteClientImpl
        .reportShuffleResult(partitionToServers, testAppId, 1, 0, partitionToBlocks, 1);

    Roaring64NavigableMap bitmap = shuffleWriteClientImpl
        .getShuffleResult(""GRPC"", Sets.newHashSet(shuffleServerInfo1), testAppId,
        1, 0);
    assertTrue(bitmap.isEmpty());

    bitmap = shuffleWriteClientImpl
        .getShuffleResult(""GRPC"", Sets.newHashSet(shuffleServerInfo1), testAppId,
        1, 1);
    assertEquals(5, bitmap.getLongCardinality());
    for (int i = 0; i < 5; i++) {
      assertTrue(bitmap.contains(partitionToBlocks.get(1).get(i)));
    }

    bitmap = shuffleWriteClientImpl
        .getShuffleResult(""GRPC"", Sets.newHashSet(shuffleServerInfo1), testAppId,
        1, 2);
    assertTrue(bitmap.isEmpty());

    bitmap = shuffleWriteClientImpl
        .getShuffleResult(""GRPC"", Sets.newHashSet(shuffleServerInfo2), testAppId,
        1, 0);
    assertTrue(bitmap.isEmpty());

    bitmap = shuffleWriteClientImpl
        .getShuffleResult(""GRPC"", Sets.newHashSet(shuffleServerInfo2), testAppId,
        1, 1);
    assertTrue(bitmap.isEmpty());

    bitmap = shuffleWriteClientImpl
        .getShuffleResult(""GRPC"", Sets.newHashSet(shuffleServerInfo2), testAppId,
        1, 2);
    assertEquals(7, bitmap.getLongCardinality());
    for (int i = 0; i < 7; i++) {
      assertTrue(bitmap.contains(partitionToBlocks.get(2).get(i)));
    }
  }
",non-flaky,5
76990,Tencent_Firestorm,ShuffleWithRssClientTest.writeReadTest,"  @Test
  public void writeReadTest() throws Exception {
    String testAppId = ""writeReadTest"";
    shuffleWriteClientImpl.registerShuffle(shuffleServerInfo1,
        testAppId, 0, Lists.newArrayList(new PartitionRange(0, 0)));
    shuffleWriteClientImpl.registerShuffle(shuffleServerInfo2,
        testAppId, 0, Lists.newArrayList(new PartitionRange(0, 0)));
    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap taskIdBitmap = Roaring64NavigableMap.bitmapOf(0);

    List<ShuffleBlockInfo> blocks = createShuffleBlockList(
        0, 0, 0, 3, 25, blockIdBitmap,
        expectedData, Lists.newArrayList(shuffleServerInfo1, shuffleServerInfo2));
    shuffleWriteClientImpl.sendShuffleData(testAppId, blocks);
    // send 1st commit, finish commit won't be sent to Shuffle server and data won't be persisted to disk
    boolean commitResult = shuffleWriteClientImpl
        .sendCommit(Sets.newHashSet(shuffleServerInfo1, shuffleServerInfo2), testAppId, 0, 2);
    assertTrue(commitResult);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(), testAppId, 0, 0, 100, 1,
        10, 1000, """", blockIdBitmap, taskIdBitmap,
        Lists.newArrayList(shuffleServerInfo1, shuffleServerInfo2), null);

    try {
      readClient.readShuffleBlockData();
      fail(EXPECTED_EXCEPTION_MESSAGE);
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Failed to read shuffle index for""));
    }
    readClient.close();

    // send 2nd commit, data will be persisted to disk
    commitResult = shuffleWriteClientImpl
        .sendCommit(Sets.newHashSet(shuffleServerInfo1, shuffleServerInfo2), testAppId, 0, 2);
    assertTrue(commitResult);
    readClient = new ShuffleReadClientImpl(StorageType.LOCALFILE.name(), testAppId, 0, 0, 100, 1,
        10, 1000, """", blockIdBitmap, taskIdBitmap,
        Lists.newArrayList(shuffleServerInfo1, shuffleServerInfo2), null);
    validateResult(readClient, expectedData);
    readClient.checkProcessedBlockIds();
    readClient.close();

    // commit will be failed because of fakeIp
    commitResult = shuffleWriteClientImpl.sendCommit(Sets.newHashSet(new ShuffleServerInfo(
        ""127.0.0.1-20001"", ""fakeIp"", SHUFFLE_SERVER_PORT)), testAppId, 0, 2);
    assertFalse(commitResult);

    // wait resource to be deleted
    Thread.sleep(6000);

    // commit is ok, but finish shuffle rpc will failed because resource was deleted
    commitResult = shuffleWriteClientImpl
        .sendCommit(Sets.newHashSet(shuffleServerInfo1, shuffleServerInfo2), testAppId, 0, 2);
    assertFalse(commitResult);
  }
",non-flaky,5
76991,Tencent_Firestorm,ShuffleWithRssClientTest.emptyTaskTest,"  @Test
  public void emptyTaskTest() {
    String testAppId = ""emptyTaskTest"";
    shuffleWriteClientImpl.registerShuffle(shuffleServerInfo1,
        testAppId, 0, Lists.newArrayList(new PartitionRange(0, 0)));
    boolean commitResult = shuffleWriteClientImpl
        .sendCommit(Sets.newHashSet(shuffleServerInfo1), testAppId, 0, 2);
    assertTrue(commitResult);
    commitResult = shuffleWriteClientImpl
        .sendCommit(Sets.newHashSet(shuffleServerInfo2), testAppId, 0, 2);
    assertFalse(commitResult);
  }
",non-flaky,5
76992,Tencent_Firestorm,CoordinatorGrpcTest.testGetPartitionToServers,"  @Test
  public void testGetPartitionToServers() {
    GetShuffleAssignmentsResponse testResponse = generateShuffleAssignmentsResponse();

    Map<Integer, List<ShuffleServerInfo>> partitionToServers =
        coordinatorClient.getPartitionToServers(testResponse);

    assertEquals(Arrays.asList(new ShuffleServerInfo(""id1"", ""0.0.0.1"", 100),
        new ShuffleServerInfo(""id2"", ""0.0.0.2"", 100)),
        partitionToServers.get(0));
    assertEquals(Arrays.asList(new ShuffleServerInfo(""id1"", ""0.0.0.1"", 100),
        new ShuffleServerInfo(""id2"", ""0.0.0.2"", 100)),
        partitionToServers.get(1));
    assertEquals(Arrays.asList(new ShuffleServerInfo(""id3"", ""0.0.0.3"", 100),
        new ShuffleServerInfo(""id4"", ""0.0.0.4"", 100)),
        partitionToServers.get(2));
    assertEquals(Arrays.asList(new ShuffleServerInfo(""id3"", ""0.0.0.3"", 100),
        new ShuffleServerInfo(""id4"", ""0.0.0.4"", 100)),
        partitionToServers.get(3));
    assertNull(partitionToServers.get(4));
  }
",non-flaky,5
76993,Tencent_Firestorm,CoordinatorGrpcTest.getShuffleRegisterInfoTest,"  @Test
  public void getShuffleRegisterInfoTest() {
    GetShuffleAssignmentsResponse testResponse = generateShuffleAssignmentsResponse();
    Map<ShuffleServerInfo, List<PartitionRange>> serverToPartitionRanges =
        coordinatorClient.getServerToPartitionRanges(testResponse);
    List<ShuffleRegisterInfo> expected = Arrays.asList(
        new ShuffleRegisterInfo(new ShuffleServerInfo(""id1"", ""0.0.0.1"", 100),
            Lists.newArrayList(new PartitionRange(0, 1))),
        new ShuffleRegisterInfo(new ShuffleServerInfo(""id2"", ""0.0.0.2"", 100),
            Lists.newArrayList(new PartitionRange(0, 1))),
        new ShuffleRegisterInfo(new ShuffleServerInfo(""id3"", ""0.0.0.3"", 100),
            Lists.newArrayList(new PartitionRange(2, 3))),
        new ShuffleRegisterInfo(new ShuffleServerInfo(""id4"", ""0.0.0.4"", 100),
            Lists.newArrayList(new PartitionRange(2, 3))));
    assertEquals(4, serverToPartitionRanges.size());
    for (ShuffleRegisterInfo sri : expected) {
      List<PartitionRange> partitionRanges = serverToPartitionRanges.get(sri.getShuffleServerInfo());
      assertEquals(sri.getPartitionRanges(), partitionRanges);
    }
  }
",non-flaky,5
76994,Tencent_Firestorm,CoordinatorGrpcTest.getShuffleAssignmentsTest,"  @Test
  public void getShuffleAssignmentsTest() throws Exception {
    String appId = ""getShuffleAssignmentsTest"";
    CoordinatorTestUtils.waitForRegister(coordinatorClient,2);
    RssGetShuffleAssignmentsRequest request = new RssGetShuffleAssignmentsRequest(
        appId, 1, 10, 4, 1,
        Sets.newHashSet(Constants.SHUFFLE_SERVER_VERSION));
    RssGetShuffleAssignmentsResponse response = coordinatorClient.getShuffleAssignments(request);
    Set<Integer> expectedStart = Sets.newHashSet(0, 4, 8);

    Map<ShuffleServerInfo, List<PartitionRange>> serverToPartitionRanges = response.getServerToPartitionRanges();
    assertEquals(2, serverToPartitionRanges.size());
    List<PartitionRange> partitionRanges = Lists.newArrayList();
    for (List<PartitionRange> ranges : serverToPartitionRanges.values()) {
      partitionRanges.addAll(ranges);
    }
    for (PartitionRange pr : partitionRanges) {
      switch (pr.getStart()) {
        case 0:
          assertEquals(3, pr.getEnd());
          expectedStart.remove(0);
          break;
        case 4:
          assertEquals(7, pr.getEnd());
          expectedStart.remove(4);
          break;
        case 8:
          assertEquals(11, pr.getEnd());
          expectedStart.remove(8);
          break;
        default:
          fail(""Shouldn't be here"");
      }
    }
    assertTrue(expectedStart.isEmpty());

    request = new RssGetShuffleAssignmentsRequest(
        appId, 1, 10, 4, 2,
        Sets.newHashSet(Constants.SHUFFLE_SERVER_VERSION));
    response = coordinatorClient.getShuffleAssignments(request);
    serverToPartitionRanges = response.getServerToPartitionRanges();
    assertEquals(2, serverToPartitionRanges.size());
    partitionRanges = Lists.newArrayList();
    for (List<PartitionRange> ranges : serverToPartitionRanges.values()) {
      partitionRanges.addAll(ranges);
    }
    assertEquals(6, partitionRanges.size());
    int range0To3 = 0;
    int range4To7 = 0;
    int range8To11 = 0;
    for (PartitionRange pr : partitionRanges) {
      switch (pr.getStart()) {
        case 0:
          assertEquals(3, pr.getEnd());
          range0To3++;
          break;
        case 4:
          assertEquals(7, pr.getEnd());
          range4To7++;
          break;
        case 8:
          assertEquals(11, pr.getEnd());
          range8To11++;
          break;
        default:
          fail(""Shouldn't be here"");
      }
    }
    assertEquals(2, range0To3);
    assertEquals(2, range4To7);
    assertEquals(2, range8To11);

    request = new RssGetShuffleAssignmentsRequest(
        appId, 3, 2, 1, 1,
        Sets.newHashSet(""fake_version""));
    try {
      coordinatorClient.getShuffleAssignments(request);
      fail(""Exception should be thrown"");
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Empty assignment""));
    }
  }
",non-flaky,5
76995,Tencent_Firestorm,CoordinatorGrpcTest.appHeartbeatTest,"  @Test
  public void appHeartbeatTest() throws Exception {
    RssAppHeartBeatResponse response =
        coordinatorClient.sendAppHeartBeat(new RssAppHeartBeatRequest(""appHeartbeatTest1"", 1000));
    assertEquals(ResponseStatusCode.SUCCESS, response.getStatusCode());
    assertEquals(Sets.newHashSet(""appHeartbeatTest1""),
        coordinators.get(0).getApplicationManager().getAppIds());
    coordinatorClient.sendAppHeartBeat(new RssAppHeartBeatRequest(""appHeartbeatTest2"", 1000));
    assertEquals(Sets.newHashSet(""appHeartbeatTest1"", ""appHeartbeatTest2""),
        coordinators.get(0).getApplicationManager().getAppIds());
    int retry = 0;
    while (retry < 5) {
      coordinatorClient.sendAppHeartBeat(new RssAppHeartBeatRequest(""appHeartbeatTest1"", 1000));
      retry++;
      Thread.sleep(1000);
    }
    // appHeartbeatTest2 was removed because of expired
    assertEquals(Sets.newHashSet(""appHeartbeatTest1""),
        coordinators.get(0).getApplicationManager().getAppIds());
  }
",non-flaky,5
76996,Tencent_Firestorm,CoordinatorGrpcTest.shuffleServerHeartbeatTest,"  @Test
  public void shuffleServerHeartbeatTest() throws Exception {
    CoordinatorTestUtils.waitForRegister(coordinatorClient, 2);
    shuffleServers.get(0).stopServer();
    Thread.sleep(5000);
    SimpleClusterManager scm = (SimpleClusterManager) coordinators.get(0).getClusterManager();
    List<ServerNode> nodes = scm.getServerList(Sets.newHashSet(Constants.SHUFFLE_SERVER_VERSION));
    assertEquals(1, nodes.size());
    ServerNode node = nodes.get(0);
    assertTrue(node.getTags().contains(Constants.SHUFFLE_SERVER_VERSION));
    assertTrue(scm.getTagToNodes().get(Constants.SHUFFLE_SERVER_VERSION).contains(node));
    ShuffleServerConf shuffleServerConf = shuffleServers.get(0).getShuffleServerConf();
    shuffleServerConf.setInteger(""rss.rpc.server.port"", SHUFFLE_SERVER_PORT + 2);
    shuffleServerConf.setInteger(""rss.jetty.http.port"", 18082);
    ShuffleServer ss = new ShuffleServer(shuffleServerConf);
    ss.start();
    shuffleServers.set(0, ss);
    Thread.sleep(3000);
    assertEquals(2, coordinators.get(0).getClusterManager().getNodesNum());
  }
",non-flaky,5
76997,Tencent_Firestorm,CoordinatorGrpcTest.rpcMetricsTest,"  @Test
  public void rpcMetricsTest() throws Exception{
    String appId = ""rpcMetricsTest"";
    double oldValue = coordinators.get(0).getGrpcMetrics().getCounterMap()
        .get(CoordinatorGrpcMetrics.HEARTBEAT_METHOD).get();
    CoordinatorTestUtils.waitForRegister(coordinatorClient,2);
    double newValue = coordinators.get(0).getGrpcMetrics().getCounterMap()
        .get(CoordinatorGrpcMetrics.HEARTBEAT_METHOD).get();
    assertTrue(newValue - oldValue > 1);
    assertEquals(0,
        coordinators.get(0).getGrpcMetrics().getGaugeMap()
            .get(CoordinatorGrpcMetrics.HEARTBEAT_METHOD).get(), 0.5);

    RssGetShuffleAssignmentsRequest request = new RssGetShuffleAssignmentsRequest(
        appId, 1, 10, 4, 1,
        Sets.newHashSet(Constants.SHUFFLE_SERVER_VERSION));
    oldValue = coordinators.get(0).getGrpcMetrics().getCounterMap()
        .get(CoordinatorGrpcMetrics.GET_SHUFFLE_ASSIGNMENTS_METHOD).get();
    coordinatorClient.getShuffleAssignments(request);
    newValue = coordinators.get(0).getGrpcMetrics().getCounterMap()
        .get(CoordinatorGrpcMetrics.GET_SHUFFLE_ASSIGNMENTS_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        coordinators.get(0).getGrpcMetrics().getGaugeMap()
            .get(CoordinatorGrpcMetrics.GET_SHUFFLE_ASSIGNMENTS_METHOD).get(), 0.5);
  }
",non-flaky,5
76998,Tencent_Firestorm,PartitionBalanceCoordinatorGrpcTest.getShuffleAssignmentsTest,"  @Test
  public void getShuffleAssignmentsTest() throws Exception {
    CoordinatorTestUtils.waitForRegister(coordinatorClient, 3);
    RssGetShuffleAssignmentsRequest request = new RssGetShuffleAssignmentsRequest(
        ""app1"",
        1,
        1,
        1,
        1,
        Sets.newHashSet(Constants.SHUFFLE_SERVER_VERSION));
    RssGetShuffleAssignmentsResponse response = coordinatorClient.getShuffleAssignments(request);
    assertEquals(1, response.getPartitionToServers().size());
    for (Map.Entry<Integer, List<ShuffleServerInfo>> entry : response.getPartitionToServers().entrySet()) {
      assertEquals(1, entry.getValue().size());
      assertEquals(SHUFFLE_SERVER_PORT + 1, entry.getValue().get(0).getPort());
    }
    request = new RssGetShuffleAssignmentsRequest(
        ""app1"",
        2,
        1,
        1,
        1,
        Sets.newHashSet(Constants.SHUFFLE_SERVER_VERSION));
    response = coordinatorClient.getShuffleAssignments(request);
    assertEquals(1, response.getPartitionToServers().size());
    for (Map.Entry<Integer, List<ShuffleServerInfo>> entry : response.getPartitionToServers().entrySet()) {
      assertEquals(1, entry.getValue().size());
      assertEquals(SHUFFLE_SERVER_PORT + 1, entry.getValue().get(0).getPort());
    }
    request = new RssGetShuffleAssignmentsRequest(
        ""app1"",
        2,
        1,
        1,
        1,
        Sets.newHashSet(Constants.SHUFFLE_SERVER_VERSION));
    response = coordinatorClient.getShuffleAssignments(request);
    assertEquals(1, response.getPartitionToServers().size());
    for (Map.Entry<Integer, List<ShuffleServerInfo>> entry : response.getPartitionToServers().entrySet()) {
      assertEquals(1, entry.getValue().size());
      assertEquals(SHUFFLE_SERVER_PORT, entry.getValue().get(0).getPort());
    }
  }
",non-flaky,5
76999,Tencent_Firestorm,ShuffleServerWithHdfsTest.hdfsWriteReadTest,"  @Test
  public void hdfsWriteReadTest() {
    String appId = ""app_hdfs_read_write"";
    String dataBasePath = HDFS_URI + ""rss/test"";
    RssRegisterShuffleRequest rrsr = new RssRegisterShuffleRequest(appId, 0,
        Lists.newArrayList(new PartitionRange(0, 1)));
    shuffleServerClient.registerShuffle(rrsr);
    rrsr = new RssRegisterShuffleRequest(appId, 0, Lists.newArrayList(new PartitionRange(2, 3)));
    shuffleServerClient.registerShuffle(rrsr);

    Roaring64NavigableMap[] bitmaps = new Roaring64NavigableMap[4];
    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Map<Integer, List<ShuffleBlockInfo>>  dataBlocks = createTestData(bitmaps, expectedData);
    Map<Integer, List<ShuffleBlockInfo>> partitionToBlocks = Maps.newHashMap();
    partitionToBlocks.put(0, dataBlocks.get(0));
    partitionToBlocks.put(1, dataBlocks.get(1));

    Map<Integer, Map<Integer, List<ShuffleBlockInfo>>> shuffleToBlocks = Maps.newHashMap();
    shuffleToBlocks.put(0, partitionToBlocks);

    RssSendShuffleDataRequest rssdr = new RssSendShuffleDataRequest(appId, 3, 1000, shuffleToBlocks);
    shuffleServerClient.sendShuffleData(rssdr);
    assertEquals(456, shuffleServers.get(0).getShuffleBufferManager().getUsedMemory());
    assertEquals(0, shuffleServers.get(0).getShuffleBufferManager().getPreAllocatedSize());
    RssSendCommitRequest rscr = new RssSendCommitRequest(appId, 0);
    shuffleServerClient.sendCommit(rscr);
    RssFinishShuffleRequest rfsr = new RssFinishShuffleRequest(appId, 0);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        appId, 0, 0, 100, 2, 10, 1000,
        dataBasePath, bitmaps[0], Roaring64NavigableMap.bitmapOf(0), Lists.newArrayList(), new Configuration());
    assertNull(readClient.readShuffleBlockData());
    shuffleServerClient.finishShuffle(rfsr);

    partitionToBlocks.clear();
    partitionToBlocks.put(2, dataBlocks.get(2));
    shuffleToBlocks.clear();
    shuffleToBlocks.put(0, partitionToBlocks);
    rssdr = new RssSendShuffleDataRequest(appId, 3, 1000, shuffleToBlocks);
    shuffleServerClient.sendShuffleData(rssdr);
    assertEquals(0, shuffleServers.get(0).getShuffleBufferManager().getPreAllocatedSize());
    rscr = new RssSendCommitRequest(appId, 0);
    shuffleServerClient.sendCommit(rscr);
    rfsr = new RssFinishShuffleRequest(appId, 0);
    shuffleServerClient.finishShuffle(rfsr);

    partitionToBlocks.clear();
    partitionToBlocks.put(3, dataBlocks.get(3));
    shuffleToBlocks.clear();
    shuffleToBlocks.put(0, partitionToBlocks);
    rssdr = new RssSendShuffleDataRequest(appId, 3, 1000, shuffleToBlocks);
    shuffleServerClient.sendShuffleData(rssdr);
    rscr = new RssSendCommitRequest(appId, 0);
    shuffleServerClient.sendCommit(rscr);
    rfsr = new RssFinishShuffleRequest(appId, 0);
    shuffleServerClient.finishShuffle(rfsr);

    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        appId, 0, 0, 100, 2, 10, 1000,
        dataBasePath, bitmaps[0], Roaring64NavigableMap.bitmapOf(0), Lists.newArrayList(), new Configuration());
    validateResult(readClient, expectedData, bitmaps[0]);

    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        appId, 0, 1, 100, 2, 10, 1000,
        dataBasePath, bitmaps[1], Roaring64NavigableMap.bitmapOf(1), Lists.newArrayList(), new Configuration());
    validateResult(readClient, expectedData, bitmaps[1]);

    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        appId, 0, 2, 100, 2, 10, 1000,
        dataBasePath, bitmaps[2], Roaring64NavigableMap.bitmapOf(2), Lists.newArrayList(), new Configuration());
    validateResult(readClient, expectedData, bitmaps[2]);

    readClient = new ShuffleReadClientImpl(StorageType.HDFS.name(),
        appId, 0, 3, 100, 2, 10, 1000,
        dataBasePath, bitmaps[3], Roaring64NavigableMap.bitmapOf(3), Lists.newArrayList(), new Configuration());
    validateResult(readClient, expectedData, bitmaps[3]);
  }
",non-flaky,5
77000,Tencent_Firestorm,MultiStorageFaultToleranceTest.hdfsFaultTolerance,"  @Test
  public void hdfsFaultTolerance() {
    try {
      String appId = ""app_hdfs_fault_tolerance_data"";
      Map<Long, byte[]> expectedData = Maps.newHashMap();
      Map<Integer, List<Integer>> map = Maps.newHashMap();
      map.put(2, Lists.newArrayList(0, 3));
      map.put(3, Lists.newArrayList(3));
      registerShuffle(appId, map);

      Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
      Roaring64NavigableMap blockIdBitmap2 = Roaring64NavigableMap.bitmapOf();
      Roaring64NavigableMap blockIdBitmap3 = Roaring64NavigableMap.bitmapOf();

      List<ShuffleBlockInfo> blocks1 = createShuffleBlockList(
          2, 0, 1,11, 10 * 1024 * 1024, blockIdBitmap1, expectedData);

      List<ShuffleBlockInfo> blocks2 = createShuffleBlockList(
          3, 3, 2,9, 10 * 1024 * 1024, blockIdBitmap2, expectedData);

      List<ShuffleBlockInfo> blocks3 = createShuffleBlockList(
          2, 3, 2,9, 10 * 1024 * 1024, blockIdBitmap3, expectedData);

      assertEquals(0, ShuffleStorageUtils.getStorageIndex(2, appId, 2, 0));
      assertEquals(0, ShuffleStorageUtils.getStorageIndex(2, appId, 3, 3));
      assertEquals(0, ShuffleStorageUtils.getStorageIndex(2, appId, 2, 3));
      assertEquals(1, cluster.getDataNodes().size());
      cluster.stopDataNode(0);
      assertEquals(0, cluster.getDataNodes().size());

      sendSinglePartitionToShuffleServer(appId, 2, 0, 1, blocks1);
      boolean isException = false;
      try {
        sendSinglePartitionToShuffleServer(appId, 3, 3,2, blocks2);
      } catch (RuntimeException re) {
        isException = true;
        assertTrue(re.getMessage().contains(""Fail to finish""));
      }
      assertTrue(isException);

      cluster.startDataNodes(conf, 1, true, HdfsServerConstants.StartupOption.REGULAR,
          null, null, null, false, true);
      assertEquals(1, cluster.getDataNodes().size());

      sendSinglePartitionToShuffleServer(appId, 2, 3, 2, blocks3);

      validateResult(appId, 2, 0, blockIdBitmap1, Roaring64NavigableMap.bitmapOf(1), expectedData);
      validateResult(appId, 2, 3, blockIdBitmap3, Roaring64NavigableMap.bitmapOf(2), expectedData);
    } catch (Exception e) {
      e.printStackTrace();
      fail();
    }
  }
",non-flaky,5
77001,Tencent_Firestorm,MultiStorageFaultToleranceTest.diskFaultTolerance,"  @Test
  public void diskFaultTolerance() {
    String appId = ""app_disk_fault_tolerance_data"";
    Map<Long, byte[]> expectedData = Maps.newHashMap();

    Map<Integer, List<Integer>> map = Maps.newHashMap();
    map.put(2, Lists.newArrayList(1, 3));
    map.put(3, Lists.newArrayList(1));
    registerShuffle(appId, map);

    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap2 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap3 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap4 = Roaring64NavigableMap.bitmapOf();

    List<ShuffleBlockInfo> blocks1 = createShuffleBlockList(
        2, 1, 1,11, 10 * 1024 * 1024, blockIdBitmap1, expectedData);

    List<ShuffleBlockInfo> blocks2 = createShuffleBlockList(
        3, 1, 2,9, 10 * 1024 * 1024, blockIdBitmap2, expectedData);

    List<ShuffleBlockInfo> blocks3 = createShuffleBlockList(
        2, 3, 2,9, 10 * 1024 * 1024, blockIdBitmap3, expectedData);

    List<ShuffleBlockInfo> blocks4 = createShuffleBlockList(
        2, 1, 1, 11, 10 * 1024 * 1024, blockIdBitmap4, expectedData);

    assertEquals(1, ShuffleStorageUtils.getStorageIndex(2, appId, 2, 1));
    assertEquals(1, ShuffleStorageUtils.getStorageIndex(2, appId, 3, 1));
    assertEquals(1, ShuffleStorageUtils.getStorageIndex(2, appId, 2, 3));
    assertEquals(1, ShuffleStorageUtils.getStorageIndex(2, appId, 2, 1));
    try {
      sendSinglePartitionToShuffleServer(appId, 2, 1, 1, blocks1);
      sendSinglePartitionToShuffleServer(appId, 3, 1,2, blocks2);
      sendSinglePartitionToShuffleServer(appId, 2, 3, 2, blocks3);
      sendSinglePartitionToShuffleServer(appId, 2, 1, 1, blocks4);
    } catch (Exception e) {
      e.printStackTrace();
      fail();
    }
    validateResult(appId, 2, 1, blockIdBitmap1, Roaring64NavigableMap.bitmapOf(1), expectedData);
    validateResult(appId, 3, 1, blockIdBitmap2, Roaring64NavigableMap.bitmapOf(2), expectedData);
    validateResult(appId, 2, 3, blockIdBitmap3, Roaring64NavigableMap.bitmapOf(2), expectedData);
  }
",non-flaky,5
77002,Tencent_Firestorm,ShuffleServerWithLocalTest.localWriteReadTest,"  @Test
  public void localWriteReadTest() throws Exception {
    String testAppId = ""localWriteReadTest"";
    RssRegisterShuffleRequest rrsr = new RssRegisterShuffleRequest(testAppId, 0,
        Lists.newArrayList(new PartitionRange(0, 1)));
    shuffleServerClient.registerShuffle(rrsr);
    rrsr = new RssRegisterShuffleRequest(testAppId, 0, Lists.newArrayList(new PartitionRange(2, 3)));
    shuffleServerClient.registerShuffle(rrsr);

    Map<Long, byte[]> expectedData = Maps.newHashMap();

    Roaring64NavigableMap[] bitmaps = new Roaring64NavigableMap[4];
    Map<Integer, List<ShuffleBlockInfo>> partitionToBlocks = createTestData(bitmaps, expectedData);

    Set<Long> expectedBlockIds1 = transBitmapToSet(bitmaps[0]);
    Set<Long> expectedBlockIds2 = transBitmapToSet(bitmaps[1]);
    Set<Long> expectedBlockIds3 = transBitmapToSet(bitmaps[2]);
    Set<Long> expectedBlockIds4 = transBitmapToSet(bitmaps[3]);

    Map<Integer, Map<Integer, List<ShuffleBlockInfo>>> shuffleToBlocks = Maps.newHashMap();
    shuffleToBlocks.put(0, partitionToBlocks);

    RssSendShuffleDataRequest rssdr = new RssSendShuffleDataRequest(
        testAppId, 3, 1000, shuffleToBlocks);
    shuffleServerClient.sendShuffleData(rssdr);
    RssSendCommitRequest rscr = new RssSendCommitRequest(testAppId, 0);
    shuffleServerClient.sendCommit(rscr);
    RssFinishShuffleRequest rfsr = new RssFinishShuffleRequest(testAppId, 0);
    shuffleServerClient.finishShuffle(rfsr);

    ShuffleDataResult sdr  = readShuffleData(
        shuffleServerClient, testAppId, 0, 0, 2,
        10, 1000, 0);
    validateResult(sdr, expectedBlockIds1, expectedData, 0);
    sdr  = readShuffleData(
        shuffleServerClient, testAppId, 0, 1, 2,
        10, 1000, 0);
    validateResult(sdr, expectedBlockIds2, expectedData, 1);
    sdr  = readShuffleData(
        shuffleServerClient, testAppId, 0, 2, 2,
        10, 1000, 0);
    validateResult(sdr, expectedBlockIds3, expectedData, 2);
    sdr  = readShuffleData(
        shuffleServerClient, testAppId, 0, 3, 2,
        10, 1000, 0);
    validateResult(sdr, expectedBlockIds4, expectedData, 3);

    assertEquals(4, shuffleServers.get(0).getShuffleTaskManager()
        .getServerReadHandlers().get(testAppId).size());
    assertNotNull(shuffleServers.get(0).getShuffleTaskManager()
        .getPartitionsToBlockIds().get(testAppId));
    Thread.sleep(8000);
    assertNull(shuffleServers.get(0).getShuffleTaskManager().getServerReadHandlers().get(testAppId));
    assertNull(shuffleServers.get(0).getShuffleTaskManager().getPartitionsToBlockIds().get(testAppId));
  }
",non-flaky,5
77003,Tencent_Firestorm,ShuffleServerGrpcTest.clearResourceTest,"  @Test
  public void clearResourceTest() throws Exception {
    final ShuffleWriteClient shuffleWriteClient =
        ShuffleClientFactory.getInstance().createShuffleWriteClient(
            ""GRPC"", 2, 10000L, 4);
    shuffleWriteClient.registerCoordinators(""127.0.0.1:19999"");
    shuffleWriteClient.registerShuffle(
        new ShuffleServerInfo(""127.0.0.1-20001"", ""127.0.0.1"", 20001),
        ""clearResourceTest1"",
        0,
        Lists.newArrayList(new PartitionRange(0, 1)));

    shuffleWriteClient.sendAppHeartbeat(""clearResourceTest1"", 1000L);
    shuffleWriteClient.sendAppHeartbeat(""clearResourceTest2"", 1000L);

    RssRegisterShuffleRequest rrsr = new RssRegisterShuffleRequest(""clearResourceTest1"", 0,
        Lists.newArrayList(new PartitionRange(0, 1)));
    shuffleServerClient.registerShuffle(rrsr);
    rrsr = new RssRegisterShuffleRequest(""clearResourceTest2"", 0,
        Lists.newArrayList(new PartitionRange(0, 1)));
    shuffleServerClient.registerShuffle(rrsr);
    assertEquals(Sets.newHashSet(""clearResourceTest1"", ""clearResourceTest2""),
        shuffleServers.get(0).getShuffleTaskManager().getAppIds().keySet());

    // Thread will keep refresh clearResourceTest1 in coordinator
    Thread t = new Thread(() -> {
      int i = 0;
      while (i < 20) {
        shuffleWriteClient.sendAppHeartbeat(""clearResourceTest1"", 1000L);
        i++;
        try {
          Thread.sleep(1000);
        } catch (InterruptedException e) {
          return;
        }
      }
    });
    t.start();

    // Heartbeat is sent to coordinator too]
    Thread.sleep(3000);
    shuffleServerClient.registerShuffle(new RssRegisterShuffleRequest(""clearResourceTest1"", 0,
        Lists.newArrayList(new PartitionRange(0, 1))));
    assertEquals(Sets.newHashSet(""clearResourceTest1""),
        coordinators.get(0).getApplicationManager().getAppIds());
    // clearResourceTest2 will be removed because of rss.server.app.expired.withoutHeartbeat
    Thread.sleep(2000);
    assertEquals(Sets.newHashSet(""clearResourceTest1""),
        shuffleServers.get(0).getShuffleTaskManager().getAppIds().keySet());

    // clearResourceTest1 will be removed because of rss.server.app.expired.withoutHeartbeat
    t.interrupt();
    Thread.sleep(8000);
    assertEquals(0, shuffleServers.get(0).getShuffleTaskManager().getAppIds().size());

  }
",non-flaky,5
77004,Tencent_Firestorm,ShuffleServerGrpcTest.shuffleResultTest,"  @Test
  public void shuffleResultTest() throws Exception {
    Map<Integer, List<Long>> partitionToBlockIds = Maps.newHashMap();
    List<Long> blockIds1 = getBlockIdList(1, 3);
    List<Long> blockIds2 = getBlockIdList(2, 2);
    List<Long> blockIds3 = getBlockIdList(3, 1);
    partitionToBlockIds.put(1, blockIds1);
    partitionToBlockIds.put(2, blockIds2);
    partitionToBlockIds.put(3, blockIds3);

    RssReportShuffleResultRequest request =
        new RssReportShuffleResultRequest(""shuffleResultTest"", 0, 0L, partitionToBlockIds, 1);
    try {
      shuffleServerClient.reportShuffleResult(request);
      fail(""Exception should be thrown"");
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""error happened when report shuffle result""));
    }

    RssGetShuffleResultRequest req = new RssGetShuffleResultRequest(""shuffleResultTest"", 1, 1);
    try {
      shuffleServerClient.getShuffleResult(req);
      fail(""Exception should be thrown"");
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Can't get shuffle result""));
    }

    RssRegisterShuffleRequest rrsr = new RssRegisterShuffleRequest(""shuffleResultTest"", 100,
        Lists.newArrayList(new PartitionRange(0, 1)));
    shuffleServerClient.registerShuffle(rrsr);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 0, 1);
    RssGetShuffleResultResponse result = shuffleServerClient.getShuffleResult(req);
    Roaring64NavigableMap blockIdBitmap = result.getBlockIdBitmap();
    assertEquals(Roaring64NavigableMap.bitmapOf(), blockIdBitmap);

    request =
        new RssReportShuffleResultRequest(""shuffleResultTest"", 0, 0L, partitionToBlockIds, 1);
    RssReportShuffleResultResponse response = shuffleServerClient.reportShuffleResult(request);
    assertEquals(ResponseStatusCode.SUCCESS, response.getStatusCode());
    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 0, 1);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    Roaring64NavigableMap expectedP1 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP1, blockIds1);
    assertEquals(expectedP1, blockIdBitmap);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 0, 2);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    Roaring64NavigableMap expectedP2 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP2, blockIds2);
    assertEquals(expectedP2, blockIdBitmap);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 0, 3);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    Roaring64NavigableMap expectedP3 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP3, blockIds3);
    assertEquals(expectedP3, blockIdBitmap);

    partitionToBlockIds = Maps.newHashMap();
    blockIds1 = getBlockIdList(1, 3);
    blockIds2 = getBlockIdList(2, 2);
    blockIds3 = getBlockIdList(3, 1);
    partitionToBlockIds.put(1, blockIds1);
    partitionToBlockIds.put(2, blockIds2);
    partitionToBlockIds.put(3, blockIds3);

    request =
        new RssReportShuffleResultRequest(""shuffleResultTest"", 0, 1L, partitionToBlockIds, 1);
    shuffleServerClient.reportShuffleResult(request);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 0, 1);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    addExpectedBlockIds(expectedP1, blockIds1);
    assertEquals(expectedP1, blockIdBitmap);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 0, 2);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    addExpectedBlockIds(expectedP2, blockIds2);
    assertEquals(expectedP2, blockIdBitmap);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 0, 3);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    addExpectedBlockIds(expectedP3, blockIds3);
    assertEquals(expectedP3, blockIdBitmap);

    request =
        new RssReportShuffleResultRequest(""shuffleResultTest"", 1, 1L, Maps.newHashMap(), 1);
    shuffleServerClient.reportShuffleResult(request);
    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 1, 1);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    assertEquals(Roaring64NavigableMap.bitmapOf(), blockIdBitmap);

    // test with bitmapNum > 1
    partitionToBlockIds = Maps.newHashMap();
    blockIds1 = getBlockIdList(1, 3);
    blockIds2 = getBlockIdList(2, 2);
    blockIds3 = getBlockIdList(3, 1);
    partitionToBlockIds.put(1, blockIds1);
    partitionToBlockIds.put(2, blockIds2);
    partitionToBlockIds.put(3, blockIds3);
    request =
        new RssReportShuffleResultRequest(""shuffleResultTest"", 2, 1L, partitionToBlockIds, 3);
    shuffleServerClient.reportShuffleResult(request);
    // validate bitmap in shuffleTaskManager
    Roaring64NavigableMap[] bitmaps = shuffleServers.get(0).getShuffleTaskManager()
        .getPartitionsToBlockIds().get(""shuffleResultTest"").get(2);
    assertEquals(3, bitmaps.length);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 2, 1);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    expectedP1 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP1, blockIds1);
    assertEquals(expectedP1, blockIdBitmap);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 2, 2);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    expectedP2 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP2, blockIds2);
    assertEquals(expectedP2, blockIdBitmap);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 2, 3);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    expectedP3 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP3, blockIds3);
    assertEquals(expectedP3, blockIdBitmap);

    partitionToBlockIds = Maps.newHashMap();
    blockIds1 = getBlockIdList((int) Constants.MAX_PARTITION_ID, 3);
    blockIds2 = getBlockIdList(2, 2);
    blockIds3 = getBlockIdList(3, 1);
    partitionToBlockIds.put((int) Constants.MAX_PARTITION_ID, blockIds1);
    partitionToBlockIds.put(2, blockIds2);
    partitionToBlockIds.put(3, blockIds3);
    // bimapNum = 2
    request =
        new RssReportShuffleResultRequest(""shuffleResultTest"", 4, 1L, partitionToBlockIds, 2);
    shuffleServerClient.reportShuffleResult(request);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 4, (int) Constants.MAX_PARTITION_ID);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    expectedP1 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP1, blockIds1);
    assertEquals(expectedP1, blockIdBitmap);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 4, 2);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    expectedP2 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP2, blockIds2);
    assertEquals(expectedP2, blockIdBitmap);

    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 4, 3);
    result = shuffleServerClient.getShuffleResult(req);
    blockIdBitmap = result.getBlockIdBitmap();
    expectedP3 = Roaring64NavigableMap.bitmapOf();
    addExpectedBlockIds(expectedP3, blockIds3);
    assertEquals(expectedP3, blockIdBitmap);

    // wait resources are deleted
    Thread.sleep(12000);
    req = new RssGetShuffleResultRequest(""shuffleResultTest"", 1, 1);
    try {
      shuffleServerClient.getShuffleResult(req);
      fail(""Exception should be thrown"");
    } catch (Exception e) {
      assertTrue(e.getMessage().contains(""Can't get shuffle result""));
    }
  }
",non-flaky,5
77005,Tencent_Firestorm,ShuffleServerGrpcTest.registerTest,"  @Test
  public void registerTest() {
    shuffleServerClient.registerShuffle(new RssRegisterShuffleRequest(""registerTest"", 0,
        Lists.newArrayList(new PartitionRange(0, 1))));
    RssGetShuffleResultRequest req = new RssGetShuffleResultRequest(""registerTest"", 0, 0);
    // no exception with getShuffleResult means register successfully
    shuffleServerClient.getShuffleResult(req);
    req = new RssGetShuffleResultRequest(""registerTest"", 0, 1);
    shuffleServerClient.getShuffleResult(req);
    shuffleServerClient.registerShuffle(new RssRegisterShuffleRequest(""registerTest"", 1,
        Lists.newArrayList(new PartitionRange(0, 0), new PartitionRange(1, 1), new PartitionRange(2, 2))));
    req = new RssGetShuffleResultRequest(""registerTest"", 1, 0);
    shuffleServerClient.getShuffleResult(req);
    req = new RssGetShuffleResultRequest(""registerTest"", 1, 1);
    shuffleServerClient.getShuffleResult(req);
    req = new RssGetShuffleResultRequest(""registerTest"", 1, 2);
    shuffleServerClient.getShuffleResult(req);
  }
",non-flaky,5
77006,Tencent_Firestorm,ShuffleServerGrpcTest.sendDataWithoutRegisterTest,"  @Test
  public void sendDataWithoutRegisterTest() throws Exception {
    List<ShuffleBlockInfo> blockInfos = Lists.newArrayList(new ShuffleBlockInfo(0, 0, 0, 100, 0,
        new byte[]{}, Lists.newArrayList(), 0, 100, 0));
    Map<Integer, List<ShuffleBlockInfo>> partitionToBlocks = Maps.newHashMap();
    partitionToBlocks.put(0, blockInfos);
    Map<Integer, Map<Integer, List<ShuffleBlockInfo>>> shuffleToBlocks = Maps.newHashMap();
    shuffleToBlocks.put(0, partitionToBlocks);

    RssSendShuffleDataRequest rssdr = new RssSendShuffleDataRequest(
        ""sendDataWithoutRegisterTest"", 3, 1000, shuffleToBlocks);
    shuffleServerClient.sendShuffleData(rssdr);
    assertEquals(132, shuffleServers.get(0).getPreAllocatedMemory());
    Thread.sleep(10000);
    assertEquals(0, shuffleServers.get(0).getPreAllocatedMemory());
  }
",non-flaky,5
77007,Tencent_Firestorm,ShuffleServerGrpcTest.multipleShuffleResultTest,"  @Test
  public void multipleShuffleResultTest() throws Exception {
    Set<Long> expectedBlockIds = Sets.newConcurrentHashSet();
    RssRegisterShuffleRequest rrsr = new RssRegisterShuffleRequest(""multipleShuffleResultTest"", 100,
        Lists.newArrayList(new PartitionRange(0, 1)));
    shuffleServerClient.registerShuffle(rrsr);

    Runnable r1 = () -> {
      for (int i = 0; i < 100; i++) {
        Map<Integer, List<Long>> ptbs = Maps.newHashMap();
        List<Long> blockIds = Lists.newArrayList();
        Long blockId = ClientUtils.getBlockId(1, 0, i);
        expectedBlockIds.add(blockId);
        blockIds.add(blockId);
        ptbs.put(1, blockIds);
        RssReportShuffleResultRequest req1 =
            new RssReportShuffleResultRequest(""multipleShuffleResultTest"", 1, 0, ptbs, 1);
        shuffleServerClient.reportShuffleResult(req1);
      }
    };
    Runnable r2 = () -> {
      for (int i = 100; i < 200; i++) {
        Map<Integer, List<Long>> ptbs = Maps.newHashMap();
        List<Long> blockIds = Lists.newArrayList();
        Long blockId = ClientUtils.getBlockId(1, 1, i);
        expectedBlockIds.add(blockId);
        blockIds.add(blockId);
        ptbs.put(1, blockIds);
        RssReportShuffleResultRequest req1 =
            new RssReportShuffleResultRequest(""multipleShuffleResultTest"", 1, 1, ptbs, 1);
        shuffleServerClient.reportShuffleResult(req1);
      }
    };
    Runnable r3 = () -> {
      for (int i = 200; i < 300; i++) {
        Map<Integer, List<Long>> ptbs = Maps.newHashMap();
        List<Long> blockIds = Lists.newArrayList();
        Long blockId = ClientUtils.getBlockId(1, 2, i);
        expectedBlockIds.add(blockId);
        blockIds.add(blockId);
        ptbs.put(1, blockIds);
        RssReportShuffleResultRequest req1 =
            new RssReportShuffleResultRequest(""multipleShuffleResultTest"", 1, 2, ptbs, 1);
        shuffleServerClient.reportShuffleResult(req1);
      }
    };
    Thread t1 = new Thread(r1);
    Thread t2 = new Thread(r2);
    Thread t3 = new Thread(r3);
    t1.start();
    t2.start();
    t3.start();
    t1.join();
    t2.join();
    t3.join();

    Roaring64NavigableMap blockIdBitmap = Roaring64NavigableMap.bitmapOf();
    for (Long blockId : expectedBlockIds) {
      blockIdBitmap.addLong(blockId);
    }

    RssGetShuffleResultRequest req = new RssGetShuffleResultRequest(
        ""multipleShuffleResultTest"", 1, 1);
    RssGetShuffleResultResponse result = shuffleServerClient.getShuffleResult(req);
    Roaring64NavigableMap actualBlockIdBitmap = result.getBlockIdBitmap();
    assertEquals(blockIdBitmap, actualBlockIdBitmap);
  }
",non-flaky,5
77008,Tencent_Firestorm,ShuffleServerGrpcTest.rpcMetricsTest,"  @Test
  public void rpcMetricsTest() {
    String appId = ""rpcMetricsTest"";
    int shuffleId = 0;
    double oldGrpcTotal = shuffleServers.get(0).getGrpcMetrics().getCounterGrpcTotal().get();
    double oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().
        get(ShuffleServerGrpcMetrics.REGISTER_SHUFFLE_METHOD).get();
    shuffleServerClient.registerShuffle(new RssRegisterShuffleRequest(appId, shuffleId,
        Lists.newArrayList(new PartitionRange(0, 1))));
    double newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap()
        .get(ShuffleServerGrpcMetrics.REGISTER_SHUFFLE_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.REGISTER_SHUFFLE_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.APP_HEARTBEAT_METHOD).get();
    shuffleServerClient.sendHeartBeat(new RssAppHeartBeatRequest(appId, 10000));
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.APP_HEARTBEAT_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.APP_HEARTBEAT_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.REQUIRE_BUFFER_METHOD).get();
    shuffleServerClient.requirePreAllocation(100, 10, 1000);
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.REQUIRE_BUFFER_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.REQUIRE_BUFFER_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.SEND_SHUFFLE_DATA_METHOD).get();
    List<ShuffleBlockInfo> blockInfos = Lists.newArrayList(new ShuffleBlockInfo(shuffleId, 0, 0, 100, 0,
        new byte[]{}, Lists.newArrayList(), 0, 100, 0));
    Map<Integer, List<ShuffleBlockInfo>> partitionToBlocks = Maps.newHashMap();
    partitionToBlocks.put(0, blockInfos);
    Map<Integer, Map<Integer, List<ShuffleBlockInfo>>> shuffleToBlocks = Maps.newHashMap();
    shuffleToBlocks.put(0, partitionToBlocks);
    RssSendShuffleDataRequest rssdr = new RssSendShuffleDataRequest(
        appId, 3, 1000, shuffleToBlocks);
    shuffleServerClient.sendShuffleData(rssdr);
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.SEND_SHUFFLE_DATA_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.SEND_SHUFFLE_DATA_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.COMMIT_SHUFFLE_TASK_METHOD).get();
    shuffleServerClient.sendCommit(new RssSendCommitRequest(appId, shuffleId));
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.COMMIT_SHUFFLE_TASK_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.COMMIT_SHUFFLE_TASK_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.FINISH_SHUFFLE_METHOD).get();
    shuffleServerClient.finishShuffle(new RssFinishShuffleRequest(appId, shuffleId));
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.FINISH_SHUFFLE_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.FINISH_SHUFFLE_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.REPORT_SHUFFLE_RESULT_METHOD).get();
    Map<Integer, List<Long>> partitionToBlockIds = Maps.newHashMap();
    List<Long> blockIds1 = getBlockIdList(1, 3);
    List<Long> blockIds2 = getBlockIdList(2, 2);
    List<Long> blockIds3 = getBlockIdList(3, 1);
    partitionToBlockIds.put(1, blockIds1);
    partitionToBlockIds.put(2, blockIds2);
    partitionToBlockIds.put(3, blockIds3);
    RssReportShuffleResultRequest request =
        new RssReportShuffleResultRequest(appId, shuffleId, 0L, partitionToBlockIds, 1);
    shuffleServerClient.reportShuffleResult(request);
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.REPORT_SHUFFLE_RESULT_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.REPORT_SHUFFLE_RESULT_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.GET_SHUFFLE_RESULT_METHOD).get();
    shuffleServerClient.getShuffleResult(new RssGetShuffleResultRequest(appId, shuffleId, 1));
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.GET_SHUFFLE_RESULT_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.GET_SHUFFLE_RESULT_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.GET_SHUFFLE_INDEX_METHOD).get();
    try {
      shuffleServerClient.getShuffleIndex(new RssGetShuffleIndexRequest(
          appId, shuffleId, 1, 1, 3));
    } catch (Exception e) {
      // ignore the exception, just test metrics value
    }
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.GET_SHUFFLE_INDEX_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.GET_SHUFFLE_INDEX_METHOD).get(), 0.5);

    oldValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.GET_SHUFFLE_DATA_METHOD).get();
    try {
      shuffleServerClient.getShuffleData(new RssGetShuffleDataRequest(
          appId, shuffleId, 0, 1, 3,
          0, 100));
    } catch (Exception e) {
      // ignore the exception, just test metrics value
    }
    newValue = shuffleServers.get(0).getGrpcMetrics().getCounterMap().get(
        ShuffleServerGrpcMetrics.GET_SHUFFLE_DATA_METHOD).get();
    assertEquals(oldValue + 1, newValue, 0.5);
    assertEquals(0,
        shuffleServers.get(0).getGrpcMetrics().getGaugeMap().get(
            ShuffleServerGrpcMetrics.GET_SHUFFLE_DATA_METHOD).get(), 0.5);

    double newGrpcTotal = shuffleServers.get(0).getGrpcMetrics().getCounterGrpcTotal().get();
    // require buffer will be called one more time when send data
    assertEquals(oldGrpcTotal + 11, newGrpcTotal, 0.5);
    assertEquals(0, shuffleServers.get(0).getGrpcMetrics().getGaugeGrpcOpen().get(), 0.5);
  }
",non-flaky,5
77009,Tencent_Firestorm,MultiStorageTest.readUploadedDataTest,"  @Test
  public void readUploadedDataTest() {
    String appId = ""ap_read_uploaded_data"";
    RssRegisterShuffleRequest rr1 =  new RssRegisterShuffleRequest(appId, 0,
        Lists.newArrayList(new PartitionRange(0, 0)));
    RssRegisterShuffleRequest rr2 =  new RssRegisterShuffleRequest(appId, 0,
        Lists.newArrayList(new PartitionRange(1, 1)));
    RssRegisterShuffleRequest rr3 =  new RssRegisterShuffleRequest(appId, 0,
        Lists.newArrayList(new PartitionRange(2, 2)));
    RssRegisterShuffleRequest rr4 =  new RssRegisterShuffleRequest(appId, 0,
        Lists.newArrayList(new PartitionRange(4, 4)));
    shuffleServerClient.registerShuffle(rr1);
    shuffleServerClient.registerShuffle(rr2);
    shuffleServerClient.registerShuffle(rr3);
    shuffleServerClient.registerShuffle(rr4);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Set<Long> expectedBlock1 = Sets.newHashSet();
    Set<Long> expectedBlock2 = Sets.newHashSet();

    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap2 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap3 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap4 = Roaring64NavigableMap.bitmapOf();

    List<ShuffleBlockInfo> blocks1 = createShuffleBlockList(
        0, 0, 1,3, 25, blockIdBitmap1, expectedData);
    List<ShuffleBlockInfo> blocks2 = createShuffleBlockList(
        0, 1, 1,5,1024 * 1024, blockIdBitmap2, expectedData);
    List<ShuffleBlockInfo> blocks3 = createShuffleBlockList(
        0, 2, 2,4, 25, blockIdBitmap3, expectedData);
    List<ShuffleBlockInfo> blocks4 = createShuffleBlockList(
        0, 4, 3,1, 1024 * 1024, blockIdBitmap4, expectedData);


    blocks1.forEach(b -> expectedBlock1.add(b.getBlockId()));
    blocks2.forEach(b -> expectedBlock2.add(b.getBlockId()));

    Map<Integer, List<ShuffleBlockInfo>> partitionToBlocks = Maps.newHashMap();
    partitionToBlocks.put(0, blocks1);
    partitionToBlocks.put(1, blocks2);
    Map<Integer, Map<Integer, List<ShuffleBlockInfo>>> shuffleToBlocks = Maps.newHashMap();
    shuffleToBlocks.put(0, partitionToBlocks);
    RssSendShuffleDataRequest rs1 = new RssSendShuffleDataRequest(appId, 3, 1000, shuffleToBlocks);
    shuffleServerClient.sendShuffleData(rs1);

    RssSendCommitRequest rc1 = new RssSendCommitRequest(appId, 0);
    shuffleServerClient.sendCommit(rc1);
    RssFinishShuffleRequest rf1 = new RssFinishShuffleRequest(appId, 0);
    shuffleServerClient.finishShuffle(rf1);
    Map<Integer, List<Long>> partitionToBlockIds = Maps.newHashMap();
    partitionToBlockIds.put(0, new ArrayList<>(expectedBlock1));
    partitionToBlockIds.put(1, new ArrayList<>(expectedBlock2));
    RssReportShuffleResultRequest rrp1 = new RssReportShuffleResultRequest(
        appId, 0, 1L, partitionToBlockIds, 2);
    shuffleServerClient.reportShuffleResult(rrp1);

    DiskItem item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 0, 0);
    assertTrue(item.canWrite());
    assertEquals(3 * 25, item.getNotUploadedSize(appId + ""/"" + 0));
    item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 0, 1);
    assertTrue(item.canWrite());
    assertEquals(5 * 1024 * 1024, item.getNotUploadedSize(appId + ""/"" + 0));

    sendSinglePartitionToShuffleServer(appId, 0,2, 2L, blocks3);
    sendSinglePartitionToShuffleServer(appId, 0, 4, 3L, blocks4);

    item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 0, 2);
    assertTrue(item.canWrite());
    assertEquals(3 * 25 + 4 * 25, item.getNotUploadedSize(appId + ""/"" + 0));

    item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 0, 4);
    assertTrue(item.canWrite());
    assertEquals(5 * 1024 * 1024 + 1024 * 1024, item.getNotUploadedSize(appId + ""/"" + 0));


    RssGetShuffleResultRequest rg1 = new RssGetShuffleResultRequest(appId, 0, 0);
    shuffleServerClient.getShuffleResult(rg1);
    RssGetShuffleResultRequest rg2 = new RssGetShuffleResultRequest(appId, 0, 1);
    shuffleServerClient.getShuffleResult(rg2);
    RssGetShuffleResultRequest rg3 = new RssGetShuffleResultRequest(appId, 0, 2);
    shuffleServerClient.getShuffleResult(rg3);
    RssGetShuffleResultRequest rg4 = new RssGetShuffleResultRequest(appId, 0, 4);
    shuffleServerClient.getShuffleResult(rg4);

    readShuffleData(shuffleServerClient, appId, 0, 0, 1, 10, 100, 0);
    readShuffleData(shuffleServerClient, appId, 0, 1, 1, 10, 100, 0);


    wait(appId);

    item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 0, 0);
    assertTrue(item.canWrite());
    assertEquals(0, item.getNotUploadedSize(appId + ""/"" + 0));

    item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 0, 1);
    assertTrue(item.canWrite());
    assertEquals(0, item.getNotUploadedSize(appId + ""/"" + 0));

    boolean isException = false;
    try {
      ShuffleDataResult result = readShuffleData(shuffleServerClient, appId, 0, 0,
          1, 10, 1000,  0);
    } catch (RuntimeException re) {
      isException = true;
      assertTrue(re.getMessage().contains(""Can't get shuffle index""));
    }
    assertTrue(isException);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(""LOCALFILE_AND_HDFS"",
        appId, 0, 0, 100, 1, 10, 1000, HDFS_URI + ""rss/multi_storage"",
        blockIdBitmap1, Roaring64NavigableMap.bitmapOf(1), Lists.newArrayList(), conf);
    validateResult(readClient, expectedData, blockIdBitmap1);

    readClient = new ShuffleReadClientImpl(""LOCALFILE_AND_HDFS"",
        appId, 0, 1, 100, 1, 10, 1000, HDFS_URI + ""rss/multi_storage"",
        blockIdBitmap2, Roaring64NavigableMap.bitmapOf(1), Lists.newArrayList(), conf);
    validateResult(readClient, expectedData, blockIdBitmap2);

    readClient = new ShuffleReadClientImpl(""LOCALFILE_AND_HDFS"",
        appId, 0, 2, 100, 1, 10, 1000, HDFS_URI + ""rss/multi_storage"",
        blockIdBitmap3, Roaring64NavigableMap.bitmapOf(2), Lists.newArrayList(), conf);
    validateResult(readClient, expectedData, blockIdBitmap3);

    readClient = new ShuffleReadClientImpl(""LOCALFILE_AND_HDFS"",
        appId, 0, 4, 100, 1, 10, 1000, HDFS_URI + ""rss/multi_storage"",
        blockIdBitmap4, Roaring64NavigableMap.bitmapOf(3), Lists.newArrayList(), conf);
    validateResult(readClient, expectedData, blockIdBitmap4);
  }
",non-flaky,5
77010,Tencent_Firestorm,MultiStorageTest.readLocalDataTest,"  @Test
  public void readLocalDataTest() {
    String appId = ""app_read_not_uploaded_data"";
    Map<Long, byte[]> expectedData = Maps.newHashMap();
    RssRegisterShuffleRequest rr1 =  new RssRegisterShuffleRequest(appId, 1,
        Lists.newArrayList(new PartitionRange(0, 0)));
    RssRegisterShuffleRequest rr2 =  new RssRegisterShuffleRequest(appId, 1,
        Lists.newArrayList(new PartitionRange(1, 1)));
    RssRegisterShuffleRequest rr3 =  new RssRegisterShuffleRequest(appId, 1,
        Lists.newArrayList(new PartitionRange(2, 2)));
    RssRegisterShuffleRequest rr4 =  new RssRegisterShuffleRequest(appId, 1,
        Lists.newArrayList(new PartitionRange(3, 3)));
    shuffleServerClient.registerShuffle(rr1);
    shuffleServerClient.registerShuffle(rr2);
    shuffleServerClient.registerShuffle(rr3);
    shuffleServerClient.registerShuffle(rr4);

    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap2 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap3 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap4 = Roaring64NavigableMap.bitmapOf();

    List<ShuffleBlockInfo> blocks1 = createShuffleBlockList(
        1, 0, 1,3, 25, blockIdBitmap1, expectedData);
    List<ShuffleBlockInfo> blocks2 = createShuffleBlockList(
        1, 1, 2,5,1024 * 1024, blockIdBitmap2, expectedData);
    List<ShuffleBlockInfo> blocks3 = createShuffleBlockList(
        1, 2, 3,4, 25, blockIdBitmap3, expectedData);
    List<ShuffleBlockInfo> blocks4 = createShuffleBlockList(
        1, 3, 4,1, 1024 * 1024, blockIdBitmap4, expectedData);

    sendSinglePartitionToShuffleServer(appId, 1,0, 1L, blocks1);
    sendSinglePartitionToShuffleServer(appId, 1,1, 2L, blocks2);
    sendSinglePartitionToShuffleServer(appId, 1,2, 3L, blocks3);
    sendSinglePartitionToShuffleServer(appId, 1,3, 4L, blocks4);

    RssGetShuffleResultRequest rg1 = new RssGetShuffleResultRequest(appId, 1, 0);
    shuffleServerClient.getShuffleResult(rg1);
    RssGetShuffleResultRequest rg2 = new RssGetShuffleResultRequest(appId, 1, 1);
    shuffleServerClient.getShuffleResult(rg2);
    RssGetShuffleResultRequest rg3 = new RssGetShuffleResultRequest(appId, 1, 2);
    shuffleServerClient.getShuffleResult(rg3);
    RssGetShuffleResultRequest rg4 = new RssGetShuffleResultRequest(appId, 1, 3);
    shuffleServerClient.getShuffleResult(rg4);

    Uninterruptibles.sleepUninterruptibly(2, TimeUnit.SECONDS);
    validateResult(appId, 1, 0, expectedData, getExpectBlockIds(blocks1));
    Uninterruptibles.sleepUninterruptibly(2, TimeUnit.SECONDS);
    validateResult(appId, 1, 1, expectedData, getExpectBlockIds(blocks2));
    Uninterruptibles.sleepUninterruptibly(2, TimeUnit.SECONDS);
    validateResult(appId, 1, 2, expectedData, getExpectBlockIds(blocks3));
    Uninterruptibles.sleepUninterruptibly(2, TimeUnit.SECONDS);
    validateResult(appId, 1, 3, expectedData, getExpectBlockIds(blocks4));
    Uninterruptibles.sleepUninterruptibly(20, TimeUnit.SECONDS);
    boolean isException = false;
    try {
      readShuffleData(shuffleServerClient, appId, 1, 0,
          1, 10, 1000,  0);
    } catch (RuntimeException re) {
      isException = true;
      assertTrue(re.getMessage().contains(""Can't get shuffle index""));
    }
    assertTrue(isException);
  }
",non-flaky,5
77011,Tencent_Firestorm,MultiStorageTest.readMixedDataTest,"  @Test
  public void readMixedDataTest() {
    String appId = ""app_read_mix_data"";
    RssRegisterShuffleRequest rr1 =  new RssRegisterShuffleRequest(appId, 0,
        Lists.newArrayList(new PartitionRange(0, 0)));
    shuffleServerClient.registerShuffle(rr1);

    Map<Long, byte[]> expectedData = Maps.newHashMap();
    Set<Long> expectedBlock1 = Sets.newHashSet();

    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();

    List<ShuffleBlockInfo> blocks1 = createShuffleBlockList(
        0, 0, 1,15, 1024 * 1024, blockIdBitmap1, expectedData);

    blocks1.forEach(b -> expectedBlock1.add(b.getBlockId()));

    Map<Integer, List<ShuffleBlockInfo>> partitionToBlocks = Maps.newHashMap();
    partitionToBlocks.put(0, blocks1);
    Map<Integer, Map<Integer, List<ShuffleBlockInfo>>> shuffleToBlocks = Maps.newHashMap();
    shuffleToBlocks.put(0, partitionToBlocks);
    RssSendShuffleDataRequest rs1 = new RssSendShuffleDataRequest(appId, 3, 1000, shuffleToBlocks);
    shuffleServerClient.sendShuffleData(rs1);

    RssSendCommitRequest rc1 = new RssSendCommitRequest(appId, 0);
    shuffleServerClient.sendCommit(rc1);
    RssFinishShuffleRequest rf1 = new RssFinishShuffleRequest(appId, 0);
    shuffleServerClient.finishShuffle(rf1);
    Map<Integer, List<Long>> partitionToBlockIds = Maps.newHashMap();
    partitionToBlockIds.put(0, new ArrayList<>(expectedBlock1));
    RssReportShuffleResultRequest rrp1 = new RssReportShuffleResultRequest(
        appId, 0, 1L, partitionToBlockIds, 1);
    shuffleServerClient.reportShuffleResult(rrp1);

    RssGetShuffleResultRequest rg1 = new RssGetShuffleResultRequest(appId, 0, 0);
    shuffleServerClient.getShuffleResult(rg1);

    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(""LOCALFILE_AND_HDFS"",
        appId, 0, 0, 100, 1, 10, 1000, HDFS_URI + ""rss/multi_storage"",
        blockIdBitmap1, Roaring64NavigableMap.bitmapOf(1), Lists.newArrayList(new ShuffleServerInfo(""test"", LOCALHOST, SHUFFLE_SERVER_PORT)), conf);

    CompressedShuffleBlock csb = readClient.readShuffleBlockData();
    Roaring64NavigableMap matched = Roaring64NavigableMap.bitmapOf();
    assertNotNull(csb);
    assertNotNull(csb.getByteBuffer());
    for (Map.Entry<Long, byte[]> entry : expectedData.entrySet()) {
      if (compareByte(entry.getValue(), csb.getByteBuffer())) {
        matched.addLong(entry.getKey());
      }
    }
    wait(appId);

    csb = readClient.readShuffleBlockData();
    while (csb != null && csb.getByteBuffer() != null) {
      for (Map.Entry<Long, byte[]> entry : expectedData.entrySet()) {
        if (compareByte(entry.getValue(), csb.getByteBuffer())) {
          matched.addLong(entry.getKey());
          break;
        }
      }
      csb = readClient.readShuffleBlockData();
    }
    assertTrue(blockIdBitmap1.equals(matched));

    boolean isException = false;
    try {
      readShuffleData(shuffleServerClient, appId, 0, 0,
          1, 10, 1000, 0);
    } catch (RuntimeException re) {
      isException = true;
      assertTrue(re.getMessage().contains(""Can't get shuffle index""));
    }
    assertTrue(isException);

    List<ShuffleBlockInfo> blocks5 = createShuffleBlockList(
        0, 0, 1,15, 1024 * 1024, blockIdBitmap1, expectedData);
    partitionToBlocks.clear();
    shuffleToBlocks.clear();
    partitionToBlocks.put(0, blocks5);
    shuffleToBlocks.put(0, partitionToBlocks);
    RssSendShuffleDataRequest rs5 = new RssSendShuffleDataRequest(appId, 3, 1000, shuffleToBlocks);
    DiskItem diskItem = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 0, 0);
    String path = ShuffleStorageUtils.getFullShuffleDataFolder(diskItem.getBasePath(),
        ShuffleStorageUtils.getShuffleDataPath(appId, 0, 0, 0));
    File file = new File(path);
    assertFalse(file.exists());
    try {
      shuffleServerClient.sendShuffleData(rs5);
      shuffleServerClient.sendCommit(rc1);
      shuffleServerClient.finishShuffle(rf1);
      shuffleServerClient.reportShuffleResult(rrp1);
    } catch (Exception e) {
      fail();
    }
    assertFalse(file.exists());
  }
",non-flaky,5
77012,Tencent_Firestorm,MultiStorageTest.diskUsageTest,"  @Test
  public void diskUsageTest() {
    String appId = ""app_read_diskusage_data"";
    long originSize = shuffleServers.get(0).getShuffleBufferManager().getCapacity();
    Map<Long, byte[]> expectedData = Maps.newHashMap();

    RssRegisterShuffleRequest rr1 =  new RssRegisterShuffleRequest(appId, 2,
        Lists.newArrayList(new PartitionRange(0, 0)));
    shuffleServerClient.registerShuffle(rr1);

    RssRegisterShuffleRequest rr2 =  new RssRegisterShuffleRequest(appId, 3,
        Lists.newArrayList(new PartitionRange(1, 1)));
    shuffleServerClient.registerShuffle(rr2);

    RssRegisterShuffleRequest rr3 =  new RssRegisterShuffleRequest(appId, 2,
        Lists.newArrayList(new PartitionRange(1, 1)));
    shuffleServerClient.registerShuffle(rr3);

    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap2 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap3 = Roaring64NavigableMap.bitmapOf();

    List<ShuffleBlockInfo> blocks1 = createShuffleBlockList(
        2, 0, 1,30, 10 * 1024 * 1024, blockIdBitmap1, expectedData);

    List<ShuffleBlockInfo> blocks2 = createShuffleBlockList(
        3, 1, 2,9, 10 * 1024 * 1024, blockIdBitmap2, expectedData);

    List<ShuffleBlockInfo> blocks3 = createShuffleBlockList(
        2, 1, 2,9, 10 * 1024 * 1024, blockIdBitmap3, expectedData);

    DiskItem item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 2, 0);
    item.createMetadataIfNotExist(appId + ""/"" + 2);
    item.getLock(appId + ""/"" + 2).readLock().lock();
    sendSinglePartitionToShuffleServer(appId, 2, 0, 1, blocks1);
    assertFalse(item.canWrite());
    assertEquals(30 * 1024 * 1024 * 10, item.getNotUploadedSize(appId + ""/"" + 2));
    assertEquals(1, item.getNotUploadedPartitions(appId + ""/"" + 2).getCardinality());
    boolean isException = false;
    try {
      sendSinglePartitionToShuffleServer(appId, 2, 1, 2, blocks3);
    } catch (RuntimeException re) {
      isException = true;
      assertTrue(re.getMessage().contains(""Can't finish shuffle process""));
    }
    item.getLock(appId + ""/"" + 2).readLock().unlock();
    Uninterruptibles.sleepUninterruptibly(6, TimeUnit.SECONDS);
    assertEquals(originSize, shuffleServers.get(0).getShuffleBufferManager().getCapacity());
    assertTrue(isException);
    RssGetShuffleResultRequest rg1 = new RssGetShuffleResultRequest(appId, 2, 0);
    shuffleServerClient.getShuffleResult(rg1);
    validateResult(appId, 2, 0, expectedData, Sets.newHashSet());
    ShuffleReadClientImpl readClient = new ShuffleReadClientImpl(""LOCALFILE_AND_HDFS"",
        appId, 2, 0, 100, 1, 10, 1000, HDFS_URI + ""rss/multi_storage"",
        blockIdBitmap1, Roaring64NavigableMap.bitmapOf(1), Lists.newArrayList(new ShuffleServerInfo(""test"", LOCALHOST, SHUFFLE_SERVER_PORT)), conf);
    validateResult(readClient, expectedData, blockIdBitmap1);
    try {
      sendSinglePartitionToShuffleServer(appId, 3, 1,2, blocks2);
    } catch (RuntimeException re) {
      fail();
    }
    RssGetShuffleResultRequest rg2 = new RssGetShuffleResultRequest(appId, 3, 1);
    shuffleServerClient.getShuffleResult(rg2);
    validateResult(appId, 3, 1, expectedData,
        getExpectBlockIds(blocks2));

    Uninterruptibles.sleepUninterruptibly(5, TimeUnit.SECONDS);
  }
",non-flaky,5
77013,Tencent_Firestorm,MultiStorageTest.removeMetaTest,"  @Test
  public void removeMetaTest() {
    String appId = ""app_read_diskusage_data_without_report"";
    Map<Long, byte[]> expectedData = Maps.newHashMap();
    RssRegisterShuffleRequest rr1 =  new RssRegisterShuffleRequest(appId, 2,
        Lists.newArrayList(new PartitionRange(0, 0)));
    shuffleServerClient.registerShuffle(rr1);
    RssRegisterShuffleRequest rr2 =  new RssRegisterShuffleRequest(appId, 3,
        Lists.newArrayList(new PartitionRange(1, 1)));
    shuffleServerClient.registerShuffle(rr2);

    Roaring64NavigableMap blockIdBitmap1 = Roaring64NavigableMap.bitmapOf();
    Roaring64NavigableMap blockIdBitmap2 = Roaring64NavigableMap.bitmapOf();

    List<ShuffleBlockInfo> blocks1 = createShuffleBlockList(
        2, 0, 1,30, 10 * 1024 * 1024, blockIdBitmap1, expectedData);
    List<ShuffleBlockInfo> blocks2 = createShuffleBlockList(
        3, 1, 2,9, 10 * 1024 * 1024, blockIdBitmap2, expectedData);

    sendSinglePartitionToShuffleServerWithoutReport(appId, 2, 2, 2, blocks1);
    sendSinglePartitionToShuffleServerWithoutReport(appId, 3, 1,2, blocks2);
    shuffleServers.get(0).getShuffleTaskManager().removeResources(appId);
    DiskItem item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 2, 0);
    Uninterruptibles.sleepUninterruptibly(1500, TimeUnit.MILLISECONDS);
    Set<String> keys = item.getShuffleMetaSet();
    assertTrue(keys.isEmpty());
    item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 3, 1);
    keys = item.getShuffleMetaSet();
    assertTrue(keys.isEmpty());

    appId = ""app_read_diskusage_data_with_report"";
    rr1 =  new RssRegisterShuffleRequest(appId, 0, Lists.newArrayList(new PartitionRange(0, 0)));
    shuffleServerClient.registerShuffle(rr1);
    blocks1 = createShuffleBlockList(
        0, 0, 1,30, 10 * 1024, blockIdBitmap1, expectedData);
    sendSinglePartitionToShuffleServer(appId, 0, 0, 2, blocks1);
    shuffleServers.get(0).getShuffleTaskManager().removeResources(appId);
    item = shuffleServers.get(0).getMultiStorageManager().getDiskItem(appId, 0, 0);
    Uninterruptibles.sleepUninterruptibly(1500, TimeUnit.MILLISECONDS);
    keys = item.getShuffleMetaSet();
    assertTrue(keys.isEmpty());
  }
",non-flaky,5
77014,Tencent_Firestorm,AQESkewedJoinTest.resultCompareTest,"  @Test
  public void resultCompareTest() throws Exception {
    run();
  }
",non-flaky,5
77015,Tencent_Firestorm,AQERepartitionTest.resultCompareTest,"  @Test
  public void resultCompareTest() throws Exception {
    run();
  }
",non-flaky,5
77016,Tencent_Firestorm,ShuffleHdfsStorageUtilsTest.testUploadFile,"  @Test
  public void testUploadFile() {
    FileOutputStream fileOut = null;
    DataOutputStream dataOut = null;
    try {
      TemporaryFolder tmpDir = new TemporaryFolder();
      tmpDir.create();
      File file = tmpDir.newFile(""test"");
      fileOut = new FileOutputStream(file);
      dataOut = new DataOutputStream(fileOut);
      byte[] buf = new byte[2096];
      new Random().nextBytes(buf);
      dataOut.write(buf);
      dataOut.close();
      fileOut.close();
      String path = HDFS_URI + ""test"";
      HdfsFileWriter writer = new HdfsFileWriter(new Path(path), conf);
      long size = ShuffleStorageUtils.uploadFile(file, writer, 1024);
      assertEquals(2096, size);
      size = ShuffleStorageUtils.uploadFile(file, writer, 100);
      assertEquals(2096, size);
      writer.close();
      tmpDir.delete();
    } catch (Exception e) {
      e.printStackTrace();
      fail();
    }
  }
",non-flaky,5
77017,Tencent_Firestorm,ShuffleStorageUtilsTest.mergeSegmentsTest,"  @Test
  public void mergeSegmentsTest() {
    List<FileBasedShuffleSegment> segments = Lists.newArrayList(
        new FileBasedShuffleSegment(1, 0, 40, 0, 0, 0));
    List<DataFileSegment> fileSegments = ShuffleStorageUtils.mergeSegments(""path"", segments, 100);
    assertEquals(1, fileSegments.size());
    for (DataFileSegment seg : fileSegments) {
      assertEquals(0, seg.getOffset());
      assertEquals(40, seg.getLength());
      assertEquals(""path"", seg.getPath());
      List<BufferSegment> bufferSegments = seg.getBufferSegments();
      assertEquals(1, bufferSegments.size());
      assertEquals(new BufferSegment(1, 0, 40, 0, 0, 0), bufferSegments.get(0));
    }

    segments = Lists.newArrayList(
        new FileBasedShuffleSegment(1, 0, 40, 0, 0, 0),
        new FileBasedShuffleSegment(2, 40, 40, 0, 0, 0),
        new FileBasedShuffleSegment(3, 80, 20, 0, 0, 0));
    fileSegments = ShuffleStorageUtils.mergeSegments(""path"", segments, 100);
    assertEquals(1, fileSegments.size());
    for (DataFileSegment seg : fileSegments) {
      assertEquals(0, seg.getOffset());
      assertEquals(100, seg.getLength());
      assertEquals(""path"", seg.getPath());
      List<BufferSegment> bufferSegments = seg.getBufferSegments();
      assertEquals(3, bufferSegments.size());
      Set<Long> testedBlockIds = Sets.newHashSet();
      for (BufferSegment segment : bufferSegments) {
        if (segment.getBlockId() == 1) {
          assertTrue(segment.equals(new BufferSegment(1, 0, 40, 0, 0, 0)));
          testedBlockIds.add(1L);
        } else if (segment.getBlockId() == 2) {
          assertTrue(segment.equals(new BufferSegment(2, 40, 40, 0, 0, 0)));
          testedBlockIds.add(2L);
        } else if (segment.getBlockId() == 3) {
          assertTrue(segment.equals(new BufferSegment(3, 80, 20, 0, 0, 0)));
          testedBlockIds.add(3L);
        }
      }
      assertEquals(3, testedBlockIds.size());
    }

    segments = Lists.newArrayList(
        new FileBasedShuffleSegment(1, 0, 40, 0, 0, 0),
        new FileBasedShuffleSegment(2, 40, 40, 0, 0, 0),
        new FileBasedShuffleSegment(3, 80, 20, 0, 0, 0),
        new FileBasedShuffleSegment(4, 100, 20, 0, 0, 0));
    fileSegments = ShuffleStorageUtils.mergeSegments(""path"", segments, 100);
    assertEquals(2, fileSegments.size());
    boolean tested = false;
    for (DataFileSegment seg : fileSegments) {
      if (seg.getOffset() == 100) {
        tested = true;
        assertEquals(20, seg.getLength());
        assertEquals(""path"", seg.getPath());
        List<BufferSegment> bufferSegments = seg.getBufferSegments();
        assertEquals(1, bufferSegments.size());
        assertTrue(bufferSegments.get(0).equals(new BufferSegment(4, 0, 20, 0, 0, 0)));
      }
    }
    assertTrue(tested);

    segments = Lists.newArrayList(
        new FileBasedShuffleSegment(1, 0, 40, 0, 0, 0),
        new FileBasedShuffleSegment(2, 40, 40, 0, 0, 0),
        new FileBasedShuffleSegment(3, 80, 20, 0, 0, 0),
        new FileBasedShuffleSegment(4, 100, 20, 0, 0, 0),
        new FileBasedShuffleSegment(5, 120, 100, 0, 0, 0));
    fileSegments = ShuffleStorageUtils.mergeSegments(""path"", segments, 100);
    assertEquals(2, fileSegments.size());
    tested = false;
    for (DataFileSegment seg : fileSegments) {
      if (seg.getOffset() == 100) {
        tested = true;
        assertEquals(120, seg.getLength());
        assertEquals(""path"", seg.getPath());
        List<BufferSegment> bufferSegments = seg.getBufferSegments();
        assertEquals(2, bufferSegments.size());
        Set<Long> testedBlockIds = Sets.newHashSet();
        for (BufferSegment segment : bufferSegments) {
          if (segment.getBlockId() == 4) {
            assertTrue(segment.equals(new BufferSegment(4, 0, 20, 0, 0, 0)));
            testedBlockIds.add(4L);
          } else if (segment.getBlockId() == 5) {
            assertTrue(segment.equals(new BufferSegment(5, 20, 100, 0, 0, 0)));
            testedBlockIds.add(5L);
          }
        }
        assertEquals(2, testedBlockIds.size());
      }
    }
    assertTrue(tested);

    segments = Lists.newArrayList(
        new FileBasedShuffleSegment(1, 10, 40, 0, 0, 0),
        new FileBasedShuffleSegment(2, 80, 20, 0, 0, 0),
        new FileBasedShuffleSegment(3, 500, 120, 0, 0, 0),
        new FileBasedShuffleSegment(4, 700, 20, 0, 0, 0));
    fileSegments = ShuffleStorageUtils.mergeSegments(""path"", segments, 100);
    assertEquals(3, fileSegments.size());
    Set<Long> expectedOffset = Sets.newHashSet(10L, 500L, 700L);
    for (DataFileSegment seg : fileSegments) {
      if (seg.getOffset() == 10) {
        validResult(seg, 90, 1, 40, 2, 70);
        expectedOffset.remove(10L);
      }
      if (seg.getOffset() == 500) {
        assertEquals(120, seg.getLength());
        List<BufferSegment> bufferSegments = seg.getBufferSegments();
        assertEquals(1, bufferSegments.size());
        assertTrue(bufferSegments.get(0).equals(new BufferSegment(3, 0, 120, 0, 0, 0)));
        expectedOffset.remove(500L);
      }
      if (seg.getOffset() == 700) {
        assertEquals(20, seg.getLength());
        List<BufferSegment> bufferSegments = seg.getBufferSegments();
        assertEquals(1, bufferSegments.size());
        assertTrue(bufferSegments.get(0).equals(new BufferSegment(4, 0, 20, 0, 0, 0)));
        expectedOffset.remove(700L);
      }
    }
    assertTrue(expectedOffset.isEmpty());

    segments = Lists.newArrayList(
        new FileBasedShuffleSegment(5, 500, 120, 0, 0, 0),
        new FileBasedShuffleSegment(3, 630, 10, 0, 0, 0),
        new FileBasedShuffleSegment(2, 80, 20, 0, 0, 0),
        new FileBasedShuffleSegment(1, 10, 40, 0, 0, 0),
        new FileBasedShuffleSegment(6, 769, 20, 0, 0, 0),
        new FileBasedShuffleSegment(4, 700, 20, 0, 0, 0));
    fileSegments = ShuffleStorageUtils.mergeSegments(""path"", segments, 100);
    assertEquals(4, fileSegments.size());
    expectedOffset = Sets.newHashSet(10L, 500L, 630L, 700L);
    for (DataFileSegment seg : fileSegments) {
      if (seg.getOffset() == 10) {
        validResult(seg, 90, 1, 40, 2, 70);
        expectedOffset.remove(10L);
      }
      if (seg.getOffset() == 500) {
        assertEquals(120, seg.getLength());
        List<BufferSegment> bufferSegments = seg.getBufferSegments();
        assertEquals(1, bufferSegments.size());
        assertTrue(bufferSegments.get(0).equals(new BufferSegment(5, 0, 120, 0, 0, 0)));
        expectedOffset.remove(500L);
      }
      if (seg.getOffset() == 630) {
        assertEquals(10, seg.getLength());
        List<BufferSegment> bufferSegments = seg.getBufferSegments();
        assertEquals(1, bufferSegments.size());
        assertTrue(bufferSegments.get(0).equals(new BufferSegment(3, 0, 10, 0, 0, 0)));
        expectedOffset.remove(630L);
      }
      if (seg.getOffset() == 700) {
        validResult(seg, 89, 4, 20, 6, 69);
        expectedOffset.remove(700L);
      }
    }
    assertTrue(expectedOffset.isEmpty());
  }
",non-flaky,5
77018,Tencent_Firestorm,ShuffleStorageUtilsTest.getShuffleDataPathWithRangeTest,"  @Test
  public void getShuffleDataPathWithRangeTest() {
    String result = ShuffleStorageUtils.getShuffleDataPathWithRange(""appId"", 0, 1, 3, 6);
    assertEquals(""appId/0/0-2"", result);
    result = ShuffleStorageUtils.getShuffleDataPathWithRange(""appId"", 0, 2, 3, 6);
    assertEquals(""appId/0/0-2"", result);
    result = ShuffleStorageUtils.getShuffleDataPathWithRange(""appId"", 0, 3, 3, 6);
    assertEquals(""appId/0/3-5"", result);
    result = ShuffleStorageUtils.getShuffleDataPathWithRange(""appId"", 0, 5, 3, 6);
    assertEquals(""appId/0/3-5"", result);
    try {
      ShuffleStorageUtils.getShuffleDataPathWithRange(""appId"", 0, 6, 3, 6);
      fail(""shouldn't be here"");
    } catch (Exception e) {
      assertTrue(e.getMessage().startsWith(""Can't generate ShuffleData Path""));
    }
    result = ShuffleStorageUtils.getShuffleDataPathWithRange(""appId"", 0, 6, 3, 7);
    assertEquals(""appId/0/6-8"", result);
  }
",non-flaky,5
77019,Tencent_Firestorm,ShuffleStorageUtilsTest.getStorageIndexTest,"  @Test
  public void getStorageIndexTest() {
    int index = ShuffleStorageUtils.getStorageIndex(3, ""abcde"", 3, 1);
    assertEquals(2, index);
    index = ShuffleStorageUtils.getStorageIndex(3, ""abcde"", 3, 4);
    assertEquals(1, index);
  }
",non-flaky,5
343,OpenAPITools_openapi-generator,AbstractJavaCodegenTest.testAdditionalModelTypeAnnotationsSemiColon,"@Test
public void testAdditionalModelTypeAnnotationsSemiColon() throws Exception {
    OpenAPI openAPI = TestUtils.createOpenAPI();
    final AbstractJavaCodegen codegen = new P_AbstractJavaCodegen();
    codegen.additionalProperties().put(ADDITIONAL_MODEL_TYPE_ANNOTATIONS, ""@Foo;@Bar"");
    codegen.processOpts();
    codegen.preprocessOpenAPI(openAPI);
    final List<String> additionalModelTypeAnnotations = new ArrayList<String>();
    additionalModelTypeAnnotations.add(""@Foo"");
    additionalModelTypeAnnotations.add(""@Bar"");
    Assert.assertEquals(codegen.getAdditionalModelTypeAnnotations(), additionalModelTypeAnnotations);
}",unordered collections,3
162667,OpenAPITools_openapi-generator,StoreApiTest.deleteOrderTest,"    @Test
    public void deleteOrderTest() {
        String orderId = null;
        //api.deleteOrder(orderId);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162668,OpenAPITools_openapi-generator,StoreApiTest.getInventoryTest,"    @Test
    public void getInventoryTest() {
        //Map<String, Integer> response = api.getInventory();
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162669,OpenAPITools_openapi-generator,StoreApiTest.getOrderByIdTest,"    @Test
    public void getOrderByIdTest() {
        Long orderId = null;
        //Order response = api.getOrderById(orderId);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162670,OpenAPITools_openapi-generator,StoreApiTest.placeOrderTest,"    @Test
    public void placeOrderTest() {
        Order body = null;
        //Order response = api.placeOrder(body);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162671,OpenAPITools_openapi-generator,PetApiTest.addPetTest,"    @Test
    public void addPetTest() {
        Pet body = null;
        //api.addPet(body);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162672,OpenAPITools_openapi-generator,PetApiTest.deletePetTest,"    @Test
    public void deletePetTest() {
        Long petId = null;
        String apiKey = null;
        //api.deletePet(petId, apiKey);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162673,OpenAPITools_openapi-generator,PetApiTest.findPetsByStatusTest,"    @Test
    public void findPetsByStatusTest() {
        List<String> status = null;
        //List<Pet> response = api.findPetsByStatus(status);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162674,OpenAPITools_openapi-generator,PetApiTest.findPetsByTagsTest,"    @Test
    public void findPetsByTagsTest() {
        List<String> tags = null;
        //List<Pet> response = api.findPetsByTags(tags);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162675,OpenAPITools_openapi-generator,PetApiTest.getPetByIdTest,"    @Test
    public void getPetByIdTest() {
        Long petId = null;
        //Pet response = api.getPetById(petId);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162676,OpenAPITools_openapi-generator,PetApiTest.updatePetTest,"    @Test
    public void updatePetTest() {
        Pet body = null;
        //api.updatePet(body);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162677,OpenAPITools_openapi-generator,PetApiTest.updatePetWithFormTest,"    @Test
    public void updatePetWithFormTest() {
        Long petId = null;
        String name = null;
        String status = null;
        //api.updatePetWithForm(petId, name, status);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162678,OpenAPITools_openapi-generator,PetApiTest.uploadFileTest,"    @Test
    public void uploadFileTest() {
        Long petId = null;
        String additionalMetadata = null;
        org.apache.cxf.jaxrs.ext.multipart.Attachment file = null;
        //ModelApiResponse response = api.uploadFile(petId, additionalMetadata, file);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162679,OpenAPITools_openapi-generator,UserApiTest.createUserTest,"    @Test
    public void createUserTest() {
        User body = null;
        //api.createUser(body);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162680,OpenAPITools_openapi-generator,UserApiTest.createUsersWithArrayInputTest,"    @Test
    public void createUsersWithArrayInputTest() {
        List<User> body = null;
        //api.createUsersWithArrayInput(body);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162681,OpenAPITools_openapi-generator,UserApiTest.createUsersWithListInputTest,"    @Test
    public void createUsersWithListInputTest() {
        List<User> body = null;
        //api.createUsersWithListInput(body);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162682,OpenAPITools_openapi-generator,UserApiTest.deleteUserTest,"    @Test
    public void deleteUserTest() {
        String username = null;
        //api.deleteUser(username);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162683,OpenAPITools_openapi-generator,UserApiTest.getUserByNameTest,"    @Test
    public void getUserByNameTest() {
        String username = null;
        //User response = api.getUserByName(username);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162684,OpenAPITools_openapi-generator,UserApiTest.loginUserTest,"    @Test
    public void loginUserTest() {
        String username = null;
        String password = null;
        //String response = api.loginUser(username, password);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
",non-flaky,5
162685,OpenAPITools_openapi-generator,UserApiTest.logoutUserTest,"    @Test
    public void logoutUserTest() {
        //api.logoutUser();
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162686,OpenAPITools_openapi-generator,UserApiTest.updateUserTest,"    @Test
    public void updateUserTest() {
        String username = null;
        User body = null;
        //api.updateUser(username, body);
        
        // TODO: test validations
        
        
    }
",non-flaky,5
162687,OpenAPITools_openapi-generator,TypeHolderDefaultTest.testTypeHolderDefault,"    @Test
    public void testTypeHolderDefault() {
        // TODO: test TypeHolderDefault
    }
",non-flaky,5
162688,OpenAPITools_openapi-generator,TypeHolderDefaultTest.stringItemTest,"    @Test
    public void stringItemTest() {
        // TODO: test stringItem
    }
",non-flaky,5
162689,OpenAPITools_openapi-generator,TypeHolderDefaultTest.numberItemTest,"    @Test
    public void numberItemTest() {
        // TODO: test numberItem
    }
",non-flaky,5
162690,OpenAPITools_openapi-generator,TypeHolderDefaultTest.integerItemTest,"    @Test
    public void integerItemTest() {
        // TODO: test integerItem
    }
",non-flaky,5
162691,OpenAPITools_openapi-generator,TypeHolderDefaultTest.boolItemTest,"    @Test
    public void boolItemTest() {
        // TODO: test boolItem
    }
",non-flaky,5
162692,OpenAPITools_openapi-generator,TypeHolderDefaultTest.arrayItemTest,"    @Test
    public void arrayItemTest() {
        // TODO: test arrayItem
    }
",non-flaky,5
162693,OpenAPITools_openapi-generator,OuterEnumTest.testOuterEnum,"    @Test
    public void testOuterEnum() {
        // TODO: test OuterEnum
    }
",non-flaky,5
162694,OpenAPITools_openapi-generator,ClassModelTest.testClassModel,"    @Test
    public void testClassModel() {
        // TODO: test ClassModel
    }
",non-flaky,5
162695,OpenAPITools_openapi-generator,ClassModelTest.propertyClassTest,"    @Test
    public void propertyClassTest() {
        // TODO: test propertyClass
    }
",non-flaky,5
162696,OpenAPITools_openapi-generator,ArrayTestTest.testArrayTest,"    @Test
    public void testArrayTest() {
        // TODO: test ArrayTest
    }
",non-flaky,5
162697,OpenAPITools_openapi-generator,ArrayTestTest.arrayOfStringTest,"    @Test
    public void arrayOfStringTest() {
        // TODO: test arrayOfString
    }
",non-flaky,5
162698,OpenAPITools_openapi-generator,ArrayTestTest.arrayArrayOfIntegerTest,"    @Test
    public void arrayArrayOfIntegerTest() {
        // TODO: test arrayArrayOfInteger
    }
",non-flaky,5
162699,OpenAPITools_openapi-generator,ArrayTestTest.arrayArrayOfModelTest,"    @Test
    public void arrayArrayOfModelTest() {
        // TODO: test arrayArrayOfModel
    }
",non-flaky,5
162700,OpenAPITools_openapi-generator,CatTest.testCat,"    @Test
    public void testCat() {
        // TODO: test Cat
    }
",non-flaky,5
162701,OpenAPITools_openapi-generator,CatTest.classNameTest,"    @Test
    public void classNameTest() {
        // TODO: test className
    }
",non-flaky,5
162702,OpenAPITools_openapi-generator,CatTest.colorTest,"    @Test
    public void colorTest() {
        // TODO: test color
    }
",non-flaky,5
162703,OpenAPITools_openapi-generator,CatTest.declawedTest,"    @Test
    public void declawedTest() {
        // TODO: test declawed
    }
",non-flaky,5
162704,OpenAPITools_openapi-generator,XmlItemTest.testXmlItem,"    @Test
    public void testXmlItem() {
        // TODO: test XmlItem
    }
",non-flaky,5
162705,OpenAPITools_openapi-generator,XmlItemTest.attributeStringTest,"    @Test
    public void attributeStringTest() {
        // TODO: test attributeString
    }
",non-flaky,5
162706,OpenAPITools_openapi-generator,XmlItemTest.attributeNumberTest,"    @Test
    public void attributeNumberTest() {
        // TODO: test attributeNumber
    }
",non-flaky,5
162707,OpenAPITools_openapi-generator,XmlItemTest.attributeIntegerTest,"    @Test
    public void attributeIntegerTest() {
        // TODO: test attributeInteger
    }
",non-flaky,5
162708,OpenAPITools_openapi-generator,XmlItemTest.attributeBooleanTest,"    @Test
    public void attributeBooleanTest() {
        // TODO: test attributeBoolean
    }
",non-flaky,5
162709,OpenAPITools_openapi-generator,XmlItemTest.wrappedArrayTest,"    @Test
    public void wrappedArrayTest() {
        // TODO: test wrappedArray
    }
",non-flaky,5
162710,OpenAPITools_openapi-generator,XmlItemTest.nameStringTest,"    @Test
    public void nameStringTest() {
        // TODO: test nameString
    }
",non-flaky,5
162711,OpenAPITools_openapi-generator,XmlItemTest.nameNumberTest,"    @Test
    public void nameNumberTest() {
        // TODO: test nameNumber
    }
",non-flaky,5
162712,OpenAPITools_openapi-generator,XmlItemTest.nameIntegerTest,"    @Test
    public void nameIntegerTest() {
        // TODO: test nameInteger
    }
",non-flaky,5
162713,OpenAPITools_openapi-generator,XmlItemTest.nameBooleanTest,"    @Test
    public void nameBooleanTest() {
        // TODO: test nameBoolean
    }
",non-flaky,5
162714,OpenAPITools_openapi-generator,XmlItemTest.nameArrayTest,"    @Test
    public void nameArrayTest() {
        // TODO: test nameArray
    }
",non-flaky,5
162715,OpenAPITools_openapi-generator,XmlItemTest.nameWrappedArrayTest,"    @Test
    public void nameWrappedArrayTest() {
        // TODO: test nameWrappedArray
    }
",non-flaky,5
162716,OpenAPITools_openapi-generator,XmlItemTest.prefixStringTest,"    @Test
    public void prefixStringTest() {
        // TODO: test prefixString
    }
",non-flaky,5
162717,OpenAPITools_openapi-generator,XmlItemTest.prefixNumberTest,"    @Test
    public void prefixNumberTest() {
        // TODO: test prefixNumber
    }
",non-flaky,5
162718,OpenAPITools_openapi-generator,XmlItemTest.prefixIntegerTest,"    @Test
    public void prefixIntegerTest() {
        // TODO: test prefixInteger
    }
",non-flaky,5
162719,OpenAPITools_openapi-generator,XmlItemTest.prefixBooleanTest,"    @Test
    public void prefixBooleanTest() {
        // TODO: test prefixBoolean
    }
",non-flaky,5
162720,OpenAPITools_openapi-generator,XmlItemTest.prefixArrayTest,"    @Test
    public void prefixArrayTest() {
        // TODO: test prefixArray
    }
",non-flaky,5
162721,OpenAPITools_openapi-generator,XmlItemTest.prefixWrappedArrayTest,"    @Test
    public void prefixWrappedArrayTest() {
        // TODO: test prefixWrappedArray
    }
",non-flaky,5
162722,OpenAPITools_openapi-generator,XmlItemTest.namespaceStringTest,"    @Test
    public void namespaceStringTest() {
        // TODO: test namespaceString
    }
",non-flaky,5
162723,OpenAPITools_openapi-generator,XmlItemTest.namespaceNumberTest,"    @Test
    public void namespaceNumberTest() {
        // TODO: test namespaceNumber
    }
",non-flaky,5
162724,OpenAPITools_openapi-generator,XmlItemTest.namespaceIntegerTest,"    @Test
    public void namespaceIntegerTest() {
        // TODO: test namespaceInteger
    }
",non-flaky,5
162725,OpenAPITools_openapi-generator,XmlItemTest.namespaceBooleanTest,"    @Test
    public void namespaceBooleanTest() {
        // TODO: test namespaceBoolean
    }
",non-flaky,5
162726,OpenAPITools_openapi-generator,XmlItemTest.namespaceArrayTest,"    @Test
    public void namespaceArrayTest() {
        // TODO: test namespaceArray
    }
",non-flaky,5
162727,OpenAPITools_openapi-generator,XmlItemTest.namespaceWrappedArrayTest,"    @Test
    public void namespaceWrappedArrayTest() {
        // TODO: test namespaceWrappedArray
    }
",non-flaky,5
162728,OpenAPITools_openapi-generator,XmlItemTest.prefixNamespaceStringTest,"    @Test
    public void prefixNamespaceStringTest() {
        // TODO: test prefixNamespaceString
    }
",non-flaky,5
162729,OpenAPITools_openapi-generator,XmlItemTest.prefixNamespaceNumberTest,"    @Test
    public void prefixNamespaceNumberTest() {
        // TODO: test prefixNamespaceNumber
    }
",non-flaky,5
162730,OpenAPITools_openapi-generator,XmlItemTest.prefixNamespaceIntegerTest,"    @Test
    public void prefixNamespaceIntegerTest() {
        // TODO: test prefixNamespaceInteger
    }
",non-flaky,5
162731,OpenAPITools_openapi-generator,XmlItemTest.prefixNamespaceBooleanTest,"    @Test
    public void prefixNamespaceBooleanTest() {
        // TODO: test prefixNamespaceBoolean
    }
",non-flaky,5
162732,OpenAPITools_openapi-generator,XmlItemTest.prefixNamespaceArrayTest,"    @Test
    public void prefixNamespaceArrayTest() {
        // TODO: test prefixNamespaceArray
    }
",non-flaky,5
162733,OpenAPITools_openapi-generator,XmlItemTest.prefixNamespaceWrappedArrayTest,"    @Test
    public void prefixNamespaceWrappedArrayTest() {
        // TODO: test prefixNamespaceWrappedArray
    }
",non-flaky,5
162734,OpenAPITools_openapi-generator,OuterCompositeTest.testOuterComposite,"    @Test
    public void testOuterComposite() {
        // TODO: test OuterComposite
    }
",non-flaky,5
162735,OpenAPITools_openapi-generator,OuterCompositeTest.myNumberTest,"    @Test
    public void myNumberTest() {
        // TODO: test myNumber
    }
",non-flaky,5
162736,OpenAPITools_openapi-generator,OuterCompositeTest.myStringTest,"    @Test
    public void myStringTest() {
        // TODO: test myString
    }
",non-flaky,5
162737,OpenAPITools_openapi-generator,OuterCompositeTest.myBooleanTest,"    @Test
    public void myBooleanTest() {
        // TODO: test myBoolean
    }
",non-flaky,5
162738,OpenAPITools_openapi-generator,ModelApiResponseTest.testModelApiResponse,"    @Test
    public void testModelApiResponse() {
        // TODO: test ModelApiResponse
    }
",non-flaky,5
162739,OpenAPITools_openapi-generator,ModelApiResponseTest.codeTest,"    @Test
    public void codeTest() {
        // TODO: test code
    }
",non-flaky,5
162740,OpenAPITools_openapi-generator,ModelApiResponseTest.typeTest,"    @Test
    public void typeTest() {
        // TODO: test type
    }
",non-flaky,5
162741,OpenAPITools_openapi-generator,ModelApiResponseTest.messageTest,"    @Test
    public void messageTest() {
        // TODO: test message
    }
",non-flaky,5
162742,OpenAPITools_openapi-generator,CategoryTest.testCategory,"    @Test
    public void testCategory() {
        // TODO: test Category
    }
",non-flaky,5
162743,OpenAPITools_openapi-generator,CategoryTest.idTest,"    @Test
    public void idTest() {
        // TODO: test id
    }
",non-flaky,5
162744,OpenAPITools_openapi-generator,CategoryTest.nameTest,"    @Test
    public void nameTest() {
        // TODO: test name
    }
",non-flaky,5
162745,OpenAPITools_openapi-generator,NameTest.testName,"    @Test
    public void testName() {
        // TODO: test Name
    }
",non-flaky,5
162746,OpenAPITools_openapi-generator,NameTest.nameTest,"    @Test
    public void nameTest() {
        // TODO: test name
    }
",non-flaky,5
162747,OpenAPITools_openapi-generator,NameTest.snakeCaseTest,"    @Test
    public void snakeCaseTest() {
        // TODO: test snakeCase
    }
",non-flaky,5
162748,OpenAPITools_openapi-generator,NameTest.propertyTest,"    @Test
    public void propertyTest() {
        // TODO: test property
    }
",non-flaky,5
162749,OpenAPITools_openapi-generator,NameTest._123numberTest,"    @Test
    public void _123numberTest() {
        // TODO: test _123number
    }
",non-flaky,5
162750,OpenAPITools_openapi-generator,AdditionalPropertiesBooleanTest.testAdditionalPropertiesBoolean,"    @Test
    public void testAdditionalPropertiesBoolean() {
        // TODO: test AdditionalPropertiesBoolean
    }
",non-flaky,5
162751,OpenAPITools_openapi-generator,AdditionalPropertiesBooleanTest.nameTest,"    @Test
    public void nameTest() {
        // TODO: test name
    }
",non-flaky,5
162752,OpenAPITools_openapi-generator,CatAllOfTest.testCatAllOf,"    @Test
    public void testCatAllOf() {
        // TODO: test CatAllOf
    }
",non-flaky,5
162753,OpenAPITools_openapi-generator,CatAllOfTest.declawedTest,"    @Test
    public void declawedTest() {
        // TODO: test declawed
    }
",non-flaky,5
162754,OpenAPITools_openapi-generator,FileSchemaTestClassTest.testFileSchemaTestClass,"    @Test
    public void testFileSchemaTestClass() {
        // TODO: test FileSchemaTestClass
    }
",non-flaky,5
162755,OpenAPITools_openapi-generator,FileSchemaTestClassTest.fileTest,"    @Test
    public void fileTest() {
        // TODO: test file
    }
",non-flaky,5
162756,OpenAPITools_openapi-generator,FileSchemaTestClassTest.filesTest,"    @Test
    public void filesTest() {
        // TODO: test files
    }
",non-flaky,5
162757,OpenAPITools_openapi-generator,AdditionalPropertiesIntegerTest.testAdditionalPropertiesInteger,"    @Test
    public void testAdditionalPropertiesInteger() {
        // TODO: test AdditionalPropertiesInteger
    }
",non-flaky,5
162758,OpenAPITools_openapi-generator,AdditionalPropertiesIntegerTest.nameTest,"    @Test
    public void nameTest() {
        // TODO: test name
    }
",non-flaky,5
162759,OpenAPITools_openapi-generator,AdditionalPropertiesObjectTest.testAdditionalPropertiesObject,"    @Test
    public void testAdditionalPropertiesObject() {
        // TODO: test AdditionalPropertiesObject
    }
",non-flaky,5
162760,OpenAPITools_openapi-generator,AdditionalPropertiesObjectTest.nameTest,"    @Test
    public void nameTest() {
        // TODO: test name
    }
",non-flaky,5
162761,OpenAPITools_openapi-generator,ReadOnlyFirstTest.testReadOnlyFirst,"    @Test
    public void testReadOnlyFirst() {
        // TODO: test ReadOnlyFirst
    }
",non-flaky,5
162762,OpenAPITools_openapi-generator,ReadOnlyFirstTest.barTest,"    @Test
    public void barTest() {
        // TODO: test bar
    }
",non-flaky,5
162763,OpenAPITools_openapi-generator,ReadOnlyFirstTest.bazTest,"    @Test
    public void bazTest() {
        // TODO: test baz
    }
",non-flaky,5
162764,OpenAPITools_openapi-generator,EnumTestTest.testEnumTest,"    @Test
    public void testEnumTest() {
        // TODO: test EnumTest
    }
",non-flaky,5
162765,OpenAPITools_openapi-generator,EnumTestTest.enumStringTest,"    @Test
    public void enumStringTest() {
        // TODO: test enumString
    }
",non-flaky,5
162766,OpenAPITools_openapi-generator,EnumTestTest.enumStringRequiredTest,"    @Test
    public void enumStringRequiredTest() {
        // TODO: test enumStringRequired
    }
",non-flaky,5
163,pushtorefresh_storio,NotifyAboutChangesTest.notifyAboutChangesConcurrently,"@Test
public void notifyAboutChangesConcurrently() {
    final int numberOfThreads = 100;
    final TestSubscriber<Changes> testSubscriber = new TestSubscriber<Changes>();
    final Set<String> tables = new HashSet<String>();
    final List<Changes> expectedChanges = new ArrayList<Changes>();
    for (int i = 0; i < numberOfThreads; i++) {
        final String table = ""test_table"" + i;
        tables.add(table);
        expectedChanges.add(Changes.newInstance(table));
    }
    storIOSQLite.observeChanges(LATEST).subscribe(testSubscriber);
    final CountDownLatch startAllThreadsLock = new CountDownLatch(1);
    for (int i = 0; i < numberOfThreads; i++) {
        final int finalI = i;
        new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    startAllThreadsLock.await();
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
                storIOSQLite.lowLevel().notifyAboutChanges(Changes.newInstance(""test_table"" + finalI));
            }
        }).start();
    }
    startAllThreadsLock.countDown();
    final long startTime = SystemClock.elapsedRealtime();
    while ((testSubscriber.valueCount() != tables.size()) && ((SystemClock.elapsedRealtime() - startTime) < 20000)) {
        Thread.yield();
    }
    testSubscriber.assertNoErrors();
    testSubscriber.assertValueCount(expectedChanges.size());
    assertThat(expectedChanges.containsAll(testSubscriber.values())).isTrue();
}",concurrency,1
110836,pushtorefresh_storio,GetObjectObserveChangesTest.repeatsOperationWithQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, user);
    }
",non-flaky,5
110837,pushtorefresh_storio,GetObjectObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, user);
    }
",non-flaky,5
110838,pushtorefresh_storio,GetObjectObserveChangesTest.repeatsOperationWithQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, user);
    }
",non-flaky,5
110839,pushtorefresh_storio,GetObjectObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, user);
    }
",non-flaky,5
110840,pushtorefresh_storio,GetListOfObjectsObserveChangesTest.repeatsOperationWithQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, singletonList(user));
    }
",non-flaky,5
110841,pushtorefresh_storio,GetListOfObjectsObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tableChanges, singletonList(user));
    }
",non-flaky,5
110842,pushtorefresh_storio,GetListOfObjectsObserveChangesTest.repeatsOperationWithQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, singletonList(user));
    }
",non-flaky,5
110843,pushtorefresh_storio,GetListOfObjectsObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tagChanges, singletonList(user));
    }
",non-flaky,5
110844,pushtorefresh_storio,ObserveChangesOfTagTest.insertEmission,"    @Test
    public void insertEmission() {
        final List<User> users = TestFactory.newUsers(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        putUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110845,pushtorefresh_storio,ObserveChangesOfTagTest.updateEmission,"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);
        final List<User> updated = new ArrayList<User>(users.size());

        for (User user : users) {
            updated.add(User.newInstance(user.id(), user.email()));
        }

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        storIOSQLite
                .put()
                .objects(updated)
                .prepare()
                .executeAsBlocking();

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110846,pushtorefresh_storio,ObserveChangesOfTagTest.deleteEmission,"    @Test
    public void deleteEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        deleteUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110847,pushtorefresh_storio,QueryTest.queryAll,"    @Test
    public void queryAll() {
        final List<User> users = putUsersBlocking(3);
        final List<User> usersFromQuery = getAllUsersBlocking();
        assertThat(users.equals(usersFromQuery)).isTrue();
    }
",non-flaky,5
110848,pushtorefresh_storio,QueryTest.queryOneByField,"    @Test
    public void queryOneByField() {
        final List<User> users = putUsersBlocking(3);

        for (User user : users) {
            final List<User> usersFromQuery = storIOSQLite
                    .get()
                    .listOfObjects(User.class)
                    .withQuery(Query.builder()
                            .table(UserTableMeta.TABLE)
                            .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                            .whereArgs(user.email())
                            .build())
                    .prepare()
                    .executeAsBlocking();

            assertThat(usersFromQuery).isNotNull();
            assertThat(usersFromQuery).hasSize(1);
            assertThat(usersFromQuery.get(0)).isEqualTo(user);
        }
    }
",non-flaky,5
110849,pushtorefresh_storio,QueryTest.queryOrdered,"    @Test
    public void queryOrdered() {
        final List<User> users = TestFactory.newUsers(3);

        // Reverse sorting by email before inserting, for the purity of the experiment.
        Collections.reverse(users);

        putUsersBlocking(users);

        final List<User> usersFromQueryOrdered = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQueryOrdered).isNotNull();
        assertThat(usersFromQueryOrdered).hasSize(users.size());

        // Sorting by email for check ordering.
        Collections.sort(users);

        for (int i = 0; i < users.size(); i++) {
            assertThat(usersFromQueryOrdered.get(i)).isEqualTo(users.get(i));
        }
    }
",non-flaky,5
110850,pushtorefresh_storio,QueryTest.queryOrderedDesc,"    @Test
    public void queryOrderedDesc() {
        final List<User> users = TestFactory.newUsers(3);

        // Sorting by email before inserting, for the purity of the experiment.
        Collections.sort(users);

        putUsersBlocking(users);

        final List<User> usersFromQueryOrdered = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL + "" DESC"")
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQueryOrdered).isNotNull();
        assertThat(usersFromQueryOrdered).hasSize(users.size());

        // Reverse sorting by email for check ordering.
        Collections.reverse(users);

        for (int i = 0; i < users.size(); i++) {
            assertThat(usersFromQueryOrdered.get(i)).isEqualTo(users.get(i));
        }
    }
",non-flaky,5
110851,pushtorefresh_storio,QueryTest.querySingleLimit,"    @Test
    public void querySingleLimit() {
        putUsersBlocking(10);

        final int limit = 8;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .limit(String.valueOf(limit))
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(limit);
    }
",non-flaky,5
110852,pushtorefresh_storio,QueryTest.queryLimitOffset,"    @Test
    public void queryLimitOffset() {
        final List<User> users = putUsersBlocking(10);

        final int offset = 5;
        final int limit = 3;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .limit(offset + "", "" + limit)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(Math.min(limit, users.size() - offset));

        Collections.sort(users);

        int position = 0;
        for (int i = offset; i < offset + limit; i++) {
            assertThat(usersFromQuery.get(position++)).isEqualTo(users.get(i));
        }
    }
",non-flaky,5
110853,pushtorefresh_storio,QueryTest.queryIntegerLimit,"    @Test
    public void queryIntegerLimit() {
        putUsersBlocking(10);

        final int limit = 8;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .limit(limit)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(limit);
    }
",non-flaky,5
110854,pushtorefresh_storio,QueryTest.queryLimitOffsetQuantity,"    @Test
    public void queryLimitOffsetQuantity() {
        final List<User> users = putUsersBlocking(10);

        final int offset = 5;
        final int quantity = 3;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .limit(offset, quantity)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(Math.min(quantity, users.size() - offset));

        Collections.sort(users);

        int position = 0;
        for (int i = offset; i < offset + quantity; i++) {
            assertThat(usersFromQuery.get(position++)).isEqualTo(users.get(i));
        }
    }
",non-flaky,5
110855,pushtorefresh_storio,QueryTest.mapFromCursor,"    @Test
    public void queryGroupBy() {
        final List<User> users = TestFactory.newUsers(10);

        for (int i = 0; i < users.size(); i++) {
            final String commonEmail;
            if (i < 3) {
                commonEmail = ""first_group@gmail.com"";
            } else {
                commonEmail = ""second_group@gmail.com"";
            }

            users.set(i, User.newInstance(null, commonEmail));
        }

        putUsersBlocking(users);

        final List<User> groupsOfUsers = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .columns(UserTableMeta.COLUMN_EMAIL)
                        .groupBy(UserTableMeta.COLUMN_EMAIL)
                        .build())
                .withGetResolver(new DefaultGetResolver<User>() {
                    @NonNull
                    @Override
                    public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                        return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
                    }
",non-flaky,5
110856,pushtorefresh_storio,QueryTest.mapFromCursor,"    @Test
    public void queryHaving() {
        final List<User> users = TestFactory.newUsers(10);

        for (int i = 0; i < users.size(); i++) {
            final String commonEmail;
            if (i < 3) {
                commonEmail = ""first_group@gmail.com"";
            } else {
                commonEmail = ""second_group@gmail.com"";
            }

            users.set(i, User.newInstance(null, commonEmail));
        }

        putUsersBlocking(users);

        final int bigGroupThreshold = 5;

        final List<User> groupsOfUsers = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .columns(UserTableMeta.COLUMN_EMAIL)
                        .groupBy(UserTableMeta.COLUMN_EMAIL)
                        .having(""COUNT(*) >= "" + bigGroupThreshold)
                        .build())
                .withGetResolver(new DefaultGetResolver<User>() {
                    @NonNull
                    @Override
                    public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                        return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
                    }
",non-flaky,5
110857,pushtorefresh_storio,QueryTest.mapFromCursor,"    @Test
    public void queryDistinct() {
        final List<User> users = new ArrayList<User>();

        for (int i = 0; i < 10; i++) {
            users.add(User.newInstance((long) i, ""same@email.com""));
        }

        putUsersBlocking(users);

        final GetResolver<User> customGetResolver = new DefaultGetResolver<User>() {
            @NonNull
            @Override
            public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
            }
",non-flaky,5
110858,pushtorefresh_storio,QueryTest.queryWithRawQuery,"    @Test
    public void queryWithRawQuery() {
        final List<User> users = TestFactory.newUsers(20);

        int counter = 1;

        for (int i = 0; i < users.size(); i++) {
            char[] chars = new char[counter++];
            Arrays.fill(chars, '*'); // wtf is going on?
            users.set(i, User.newInstance(null, new String(chars)));
        }

        putUsersBlocking(users);

        final List<User> usersWithLongName = new ArrayList<User>(users.size());

        int lengthSum = 0;
        for (User user : users) {
            lengthSum += user.email().length();
        }

        final int avrLength = lengthSum / users.size();

        for (User user : users) {
            if (user.email().length() > avrLength) {
                usersWithLongName.add(user);
            }
        }

        final String query = ""Select * from "" + UserTableMeta.TABLE
                + "" where length("" + UserTableMeta.COLUMN_EMAIL + "") > ""
                + ""(select avg(length("" + UserTableMeta.COLUMN_EMAIL + "")) from users)"";

        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isEqualTo(usersWithLongName);
    }
",non-flaky,5
110859,pushtorefresh_storio,QueryTest.queryWithRawQueryAndArguments,"    @Test
    public void queryWithRawQueryAndArguments() {
        final User testUser = User.newInstance(null, ""testUserName"");

        final List<User> users = TestFactory.newUsers(10);
        users.add(testUser);
        putUsersBlocking(users);

        final String query = ""SELECT * FROM "" + UserTableMeta.TABLE
                + "" WHERE "" + UserTableMeta.COLUMN_EMAIL + "" LIKE ?"";

        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .args(testUser.email())
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(1);
        assertThat(usersFromQuery.get(0)).isEqualTo(testUser);
    }
",non-flaky,5
110860,pushtorefresh_storio,QueryTest.queryWithRawQuerySqlInjectionFail,"    @Test
    public void queryWithRawQuerySqlInjectionFail() {
        final List<User> users = putUsersBlocking(10);

        final String query = ""SELECT * FROM "" + UserTableMeta.TABLE
                + "" WHERE "" + UserTableMeta.COLUMN_EMAIL + "" LIKE ?"";

        final String arg = ""(DELETE FROM "" + UserTableMeta.TABLE + "")"";

        storIOSQLite.get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .args(arg)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(getAllUsersBlocking()).isEqualTo(users);
    }
",non-flaky,5
110861,pushtorefresh_storio,QueryTest.getNumberOfResults,"    @Test
    public void getNumberOfResults() {
        putUsersBlocking(8);

        Integer numberOfResults = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .executeAsBlocking();

        assertThat(numberOfResults).isEqualTo(8);
    }
",non-flaky,5
110862,pushtorefresh_storio,QueryTest.queryOneExistedObject,"    @Test
    public void queryOneExistedObject() {
        final List<User> users = putUsersBlocking(3);
        final User user = users.get(0);

        final User userFromQuery = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(user.email())
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(userFromQuery).isNotNull();
        assertThat(userFromQuery).isEqualTo(user);
    }
",non-flaky,5
110863,pushtorefresh_storio,QueryTest.queryOneNonExistedObject,"    @Test
    public void queryOneNonExistedObject() {
        putUsersBlocking(3);

        final User userFromQuery = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(userFromQuery).isNull();
    }
",non-flaky,5
110864,pushtorefresh_storio,DeleteTest.deleteOne,"    @Test
    public void deleteOne() {
        final User user = putUserBlocking();

        final Cursor cursorAfterInsert = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
        assertThat(cursorAfterInsert.getCount()).isEqualTo(1);
        cursorAfterInsert.close();

        deleteUserBlocking(user);

        final Cursor cursorAfterDelete = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
        assertThat(cursorAfterDelete.getCount()).isEqualTo(0);
        cursorAfterDelete.close();
    }
",non-flaky,5
110865,pushtorefresh_storio,DeleteTest.deleteCollection,"    @Test
    public void deleteCollection() {
        final List<User> allUsers = putUsersBlocking(10);

        final List<User> usersToDelete = new ArrayList<User>();

        for (int i = 0; i < allUsers.size(); i += 2) {  // Delete every second user
            usersToDelete.add(allUsers.get(i));
        }

        final DeleteResults<User> deleteResults = storIOSQLite
                .delete()
                .objects(usersToDelete)
                .prepare()
                .executeAsBlocking();

        final List<User> usersAfterDelete = getAllUsersBlocking();

        assertThat(usersAfterDelete).hasSize(allUsers.size() / 2);

        for (User user : allUsers) {
            final boolean shouldBeDeleted = usersToDelete.contains(user);

            // Check that we deleted what we going to.
            assertThat(deleteResults.wasDeleted(user)).isEqualTo(shouldBeDeleted);

            // Check that we didn't delete users that we didn't want to
            assertThat(usersAfterDelete.contains(user)).isEqualTo(!shouldBeDeleted);
        }
    }
",non-flaky,5
110866,pushtorefresh_storio,RxQueryTest.insertEmission,"    @Test
    public void insertEmission() {
        final List<User> initialUsers = putUsersBlocking(10);
        final List<User> usersForInsert = TestFactory.newUsers(10);
        final List<User> allUsers = new ArrayList<User>(initialUsers.size() + usersForInsert.size());

        allUsers.addAll(initialUsers);
        allUsers.addAll(usersForInsert);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();
        expectedUsers.add(initialUsers);
        expectedUsers.add(allUsers);

        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive initial users
        emissionChecker.awaitNextExpectedValue();

        putUsersBlocking(usersForInsert);

        // Should receive initial users + inserted users
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110867,pushtorefresh_storio,RxQueryTest.updateEmission,"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();

        final List<User> updatedList = new ArrayList<User>(users.size());

        int count = 1;
        for (User user : users) {
            updatedList.add(User.newInstance(user.id(), ""new_email"" + count++));
        }
        expectedUsers.add(users);
        expectedUsers.add(updatedList);
        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive all users
        emissionChecker.awaitNextExpectedValue();

        storIOSQLite
                .put()
                .objects(updatedList)
                .prepare()
                .executeAsBlocking();

        // Should receive updated users
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110868,pushtorefresh_storio,RxQueryTest.deleteEmission,"    @Test
    public void deleteEmission() {
        final List<User> usersThatShouldBeSaved = TestFactory.newUsers(10);
        final List<User> usersThatShouldBeDeleted = TestFactory.newUsers(10);
        final List<User> allUsers = new ArrayList<User>();

        allUsers.addAll(usersThatShouldBeSaved);
        allUsers.addAll(usersThatShouldBeDeleted);

        putUsersBlocking(allUsers);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();

        expectedUsers.add(allUsers);
        expectedUsers.add(usersThatShouldBeSaved);

        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive all users
        emissionChecker.awaitNextExpectedValue();

        deleteUsersBlocking(usersThatShouldBeDeleted);

        // Should receive users that should be saved
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110869,pushtorefresh_storio,RxQueryTest.run,"    @Test
    public void concurrentPutWithoutGlobalTransaction() throws InterruptedException {
        final int numberOfConcurrentPuts = ConcurrencyTesting.optimalTestThreadsCount();

        TestSubscriber<Changes> testSubscriber = new TestSubscriber<Changes>();

        storIOSQLite
                .observeChangesInTable(TweetTableMeta.TABLE)
                .subscribe(testSubscriber);

        final CountDownLatch concurrentPutLatch = new CountDownLatch(1);
        final CountDownLatch allPutsDoneLatch = new CountDownLatch(numberOfConcurrentPuts);

        for (int i = 0; i < numberOfConcurrentPuts; i++) {
            final int iCopy = i;

            new Thread(new Runnable() {
                @Override
                public void run() {
                    try {
                        concurrentPutLatch.await();
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }

                    storIOSQLite
                            .put()
                            .object(Tweet.newInstance(null, 1L, ""Some text: "" + iCopy))
                            .prepare()
                            .executeAsBlocking();

                    allPutsDoneLatch.countDown();
                }
",non-flaky,5
110870,pushtorefresh_storio,RxQueryTest.nestedTransaction,"    @Test
    public void nestedTransaction() {
        storIOSQLite.lowLevel().beginTransaction();

        storIOSQLite.lowLevel().beginTransaction();

        storIOSQLite.lowLevel().setTransactionSuccessful();
        storIOSQLite.lowLevel().endTransaction();

        storIOSQLite.lowLevel().setTransactionSuccessful();
        storIOSQLite.lowLevel().endTransaction();
    }
",non-flaky,5
110871,pushtorefresh_storio,RxQueryTest.queryOneExistedObjectObservable,"    @Test
    public void queryOneExistedObjectObservable() {
        final List<User> users = putUsersBlocking(3);
        final User expectedUser = users.get(0);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(expectedUser.email())
                        .build())
                .prepare()
                .asRxObservable()
                .take(1);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(expectedUser);
    }
",non-flaky,5
110872,pushtorefresh_storio,RxQueryTest.queryOneNonExistedObjectObservable,"    @Test
    public void queryOneNonExistedObjectObservable() {
        putUsersBlocking(3);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .asRxObservable()
                .take(1);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);
    }
",non-flaky,5
110873,pushtorefresh_storio,RxQueryTest.queryOneExistedObjectTableUpdate,"    @Test
    public void queryOneExistedObjectTableUpdate() {
        User expectedUser = User.newInstance(null, ""such@email.com"");
        putUsersBlocking(3);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(expectedUser.email())
                        .build())
                .prepare()
                .asRxObservable()
                .take(2);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);

        putUserBlocking(expectedUser);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(null, expectedUser);
    }
",non-flaky,5
110874,pushtorefresh_storio,RxQueryTest.queryOneNonexistedObjectTableUpdate,"    @Test
    public void queryOneNonexistedObjectTableUpdate() {
        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .asRxObservable()
                .take(2);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);

        putUserBlocking();

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(null, null);
    }
",non-flaky,5
110875,pushtorefresh_storio,RxQueryTest.queryListOfObjectsAsSingle,"    @Test
    public void queryListOfObjectsAsSingle() {
        final List<User> users = putUsersBlocking(10);

        final Single<List<User>> usersSingle = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<List<User>> testSubscriber = new TestSubscriber<List<User>>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(users);
        testSubscriber.assertCompleted();
    }
",non-flaky,5
110876,pushtorefresh_storio,RxQueryTest.queryObjectAsSingle,"    @Test
    public void queryObjectAsSingle() {
        final List<User> users = putUsersBlocking(3);

        final Single<User> usersSingle = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(users.get(0));
        testSubscriber.assertCompleted();
    }
",non-flaky,5
110877,pushtorefresh_storio,RxQueryTest.queryNumberOfResultsAsSingle,"    @Test
    public void queryNumberOfResultsAsSingle() {
        final List<User> users = putUsersBlocking(3);

        final Single<Integer> usersSingle = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<Integer> testSubscriber = new TestSubscriber<Integer>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(users.size());
        testSubscriber.assertCompleted();
    }
",non-flaky,5
110878,pushtorefresh_storio,InterceptorTest.deleteByQuery,"    @Test
    public void deleteByQuery() {
        storIOSQLite.delete()
                .byQuery(DeleteQuery.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110879,pushtorefresh_storio,InterceptorTest.deleteCollectionOfObjects,"    @Test
    public void deleteCollectionOfObjects() {
        storIOSQLite.delete()
                .objects(Collections.singleton(createTweet()))
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110880,pushtorefresh_storio,InterceptorTest.deleteObject,"    @Test
    public void deleteObject() {
        storIOSQLite.delete()
                .object(createTweet())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110881,pushtorefresh_storio,InterceptorTest.execSql,"    @Test
    public void execSql() {
        storIOSQLite.executeSQL()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110882,pushtorefresh_storio,InterceptorTest.getCursorWithRawQuery,"    @Test
    public void getCursorWithRawQuery() {
        storIOSQLite.get()
                .cursor()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110883,pushtorefresh_storio,InterceptorTest.getCursorWithQuery,"    @Test
    public void getCursorWithQuery() {
        storIOSQLite.get()
                .cursor()
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110884,pushtorefresh_storio,InterceptorTest.getListOfObjectsWithRawQuery,"    @Test
    public void getListOfObjectsWithRawQuery() {
        storIOSQLite.get()
                .listOfObjects(Tweet.class)
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110885,pushtorefresh_storio,InterceptorTest.getListOfObjectsWithQuery,"    @Test
    public void getListOfObjectsWithQuery() {
        storIOSQLite.get()
                .listOfObjects(Tweet.class)
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110886,pushtorefresh_storio,InterceptorTest.getNumberOfResultsWithRawQuery,"    @Test
    public void getNumberOfResultsWithRawQuery() {
        storIOSQLite.get()
                .numberOfResults()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110887,pushtorefresh_storio,InterceptorTest.getNumberOfResultsWithQuery,"    @Test
    public void getNumberOfResultsWithQuery() {
        storIOSQLite.get()
                .numberOfResults()
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110888,pushtorefresh_storio,InterceptorTest.getObjectWithRawQuery,"    @Test
    public void getObjectWithRawQuery() {
        storIOSQLite.get()
                .object(Tweet.class)
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110889,pushtorefresh_storio,InterceptorTest.getObjectWithQuery,"    @Test
    public void getObjectWithQuery() {
        storIOSQLite.get()
                .object(Tweet.class)
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110890,pushtorefresh_storio,InterceptorTest.putCollection,"    @Test
    public void putCollection() {
        storIOSQLite.put()
                .objects(Collections.singleton(createTweet()))
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110891,pushtorefresh_storio,InterceptorTest.putContentValues,"    @Test
    public void putContentValues() {
        storIOSQLite.put()
                .contentValues(createContentValues())
                .withPutResolver(createCVPutResolver())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110892,pushtorefresh_storio,InterceptorTest.putContentValuesIterable,"    @Test
    public void putContentValuesIterable() {
        storIOSQLite.put()
                .contentValues(createContentValues(), createContentValues())
                .withPutResolver(createCVPutResolver())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110893,pushtorefresh_storio,InterceptorTest.putObject,"    @Test
    public void putObject() {
        storIOSQLite.put()
                .object(createTweet())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110894,pushtorefresh_storio,InsertTest.insertOne,"    @Test
    public void insertOne() {
        final User user = putUserBlocking();

        // why we created StorIOSQLite: nobody loves nulls
        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        // asserting that values was really inserted to db
        assertThat(cursor.getCount()).isEqualTo(1);
        assertThat(cursor.moveToFirst()).isTrue();

        final User insertedUser = UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor);

        assertThat(insertedUser.id()).isNotNull();
        assertThat(user.equalsExceptId(insertedUser)).isTrue();

        cursor.close();
    }
",non-flaky,5
110895,pushtorefresh_storio,InsertTest.insertCollection,"    @Test
    public void insertCollection() {
        final List<User> users = putUsersBlocking(3);

        // asserting that values was really inserted to db
        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        assertThat(cursor.getCount()).isEqualTo(users.size());

        for (int i = 0; i < users.size(); i++) {
            assertThat(cursor.moveToNext()).isTrue();
            assertThat(UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor)).isEqualTo(users.get(i));
        }

        cursor.close();
    }
",non-flaky,5
110896,pushtorefresh_storio,InsertTest.insertAndDeleteTwice,"    @Test
    public void insertAndDeleteTwice() {
        final User user = TestFactory.newUser();

        for (int i = 0; i < 2; i++) {
            putUserBlocking(user);

            final List<User> existUsers = getAllUsersBlocking();

            assertThat(existUsers).isNotNull();
            assertThat(existUsers).hasSize(1);

            final Cursor cursorAfterPut = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
            assertThat(cursorAfterPut.getCount()).isEqualTo(1);
            cursorAfterPut.close();

            deleteUserBlocking(user);

            final Cursor cursorAfterDelete = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
            assertThat(cursorAfterDelete.getCount()).isEqualTo(0);
            cursorAfterDelete.close();
        }
    }
",non-flaky,5
110897,pushtorefresh_storio,InsertTest.insertOneWithNullField,"    @Test
    public void insertOneWithNullField() {
        User user = User.newInstance(null, ""user@example.com"", null); // phone is null
        putUserBlocking(user);

        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        // asserting that values was really inserted to db
        assertThat(cursor.getCount()).isEqualTo(1);
        assertThat(cursor.moveToFirst()).isTrue();

        final User insertedUser = UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor);

        assertThat(insertedUser.id()).isNotNull();
        assertThat(user.equalsExceptId(insertedUser)).isTrue();

        cursor.close();
    }
",non-flaky,5
110898,pushtorefresh_storio,UpdateTest.updateOne,"    @Test
    public void updateOne() {
        final User userForInsert = putUserBlocking();

        final User userForUpdate = User.newInstance(
                userForInsert.id(), // using id of inserted user
                ""new@email.com"" // new value
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);  // update should not add new rows!
    }
",non-flaky,5
110899,pushtorefresh_storio,UpdateTest.updateNullFieldToNotNull,"    @Test
    public void updateNullFieldToNotNull() {
        final User userForInsert = User.newInstance(null, ""user@email.com"", null); // phone is null

        putUserBlocking(userForInsert);

        final User userForUpdate = User.newInstance(
                userForInsert.id(),
                userForInsert.email(),
                ""1-999-547867""  // phone not null
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);
    }
",non-flaky,5
110900,pushtorefresh_storio,UpdateTest.updateNotNullFieldToNull,"    @Test
    public void updateNotNullFieldToNull() {
        final User userForInsert = User.newInstance(null, ""user@email.com"", ""1-999-547867""); // phone not null

        putUserBlocking(userForInsert);

        final User userForUpdate = User.newInstance(
                userForInsert.id(),
                userForInsert.email(),
                null    // phone is null
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);
    }
",non-flaky,5
110901,pushtorefresh_storio,UpdateTest.updateCollection,"    @Test
    public void updateCollection() {
        final List<User> usersForInsert = TestFactory.newUsers(3);

        final PutResults<User> insertResults = storIOSQLite
                .put()
                .objects(usersForInsert)
                .prepare()
                .executeAsBlocking();

        assertThat(insertResults.numberOfInserts()).isEqualTo(usersForInsert.size());

        final List<User> usersForUpdate = new ArrayList<User>(usersForInsert.size());

        for (int i = 0; i < usersForInsert.size(); i++) {
            usersForUpdate.add(User.newInstance(usersForInsert.get(i).id(), ""new"" + i + ""@email.com"" + i));
        }

        final PutResults<User> updateResults = storIOSQLite
                .put()
                .objects(usersForUpdate)
                .prepare()
                .executeAsBlocking();

        assertThat(updateResults.numberOfUpdates()).isEqualTo(usersForUpdate.size());

        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        assertThat(cursor.getCount()).isEqualTo(usersForUpdate.size()); // update should not add new rows!

        for (int i = 0; i < usersForUpdate.size(); i++) {
            assertThat(cursor.moveToNext()).isTrue();
            assertThat(UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor)).isEqualTo(usersForUpdate.get(i));
        }

        cursor.close();
    }
",non-flaky,5
110902,pushtorefresh_storio,ObserveChangesInTableTest.insertEmission,"    @Test
    public void insertEmission() {
        final List<User> users = TestFactory.newUsers(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        putUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110903,pushtorefresh_storio,ObserveChangesInTableTest.updateEmission,"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);
        final List<User> updated = new ArrayList<User>(users.size());

        for (User user : users) {
            updated.add(User.newInstance(user.id(), user.email()));
        }

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        storIOSQLite
                .put()
                .objects(updated)
                .prepare()
                .executeAsBlocking();

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110904,pushtorefresh_storio,ObserveChangesInTableTest.deleteEmission,"    @Test
    public void deleteEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        deleteUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110905,pushtorefresh_storio,ExecSQLTest.shouldReturnQueryInGetData,"    @Test
    public void shouldReturnQueryInGetData() {
        final RawQuery query = RawQuery.builder()
                .query(""DROP TABLE IF EXISTS no_such_table"") // we don't want to really delete table
                .build();
        final PreparedExecuteSQL operation = storIOSQLite
                .executeSQL()
                .withQuery(query)
                .prepare();

        assertThat(operation.getData()).isEqualTo(query);
    }
",non-flaky,5
110906,pushtorefresh_storio,ExecSQLTest.execSQLWithEmptyArgs,"    @Test
    public void execSQLWithEmptyArgs() {
        // Should not throw exceptions!
        storIOSQLite
                .executeSQL()
                .withQuery(RawQuery.builder()
                        .query(""DROP TABLE IF EXISTS no_such_table"") // we don't want to really delete table
                        .build())
                .prepare()
                .executeAsBlocking();
    }
",non-flaky,5
110907,pushtorefresh_storio,ExecSQLTest.shouldPassArgsAsObjects,"    @Test
    public void shouldPassArgsAsObjects() {
        final User user = putUserBlocking();

        assertThat(user.id()).isNotNull();
        //noinspection ConstantConditions
        final long uid = user.id();

        final String query = ""UPDATE "" + UserTableMeta.TABLE
                + "" SET "" + UserTableMeta.COLUMN_ID + "" = MIN("" + UserTableMeta.COLUMN_ID + "", ?)"";

        storIOSQLite
                .executeSQL()
                .withQuery(
                        RawQuery.builder()
                                .query(query)
                                .args(uid - 1)  // as integer is less, as string is greater
                                .build())
                .prepare()
                .executeAsBlocking();

        List<User> users = getAllUsersBlocking();

        assertThat(users.size()).isEqualTo(1);

        // Was updated, because (uid - 1) passed as object, not string, and (uid - 1) < uid.
        assertThat(users.get(0).id()).isEqualTo(uid - 1);
    }
",non-flaky,5
110908,pushtorefresh_storio,GetCursorObserveChangesTest.repeatsOperationWithQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(query)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tableChanges);

        testSubscriber.assertValueCount(2);
    }
",non-flaky,5
110909,pushtorefresh_storio,GetCursorObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(rawQuery)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tableChanges);

        testSubscriber.assertValueCount(2);
    }
",non-flaky,5
110910,pushtorefresh_storio,GetCursorObserveChangesTest.repeatsOperationWithQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(query)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tagChanges);

        testSubscriber.assertValueCount(2);
    }
",non-flaky,5
110911,pushtorefresh_storio,GetCursorObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(rawQuery)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tagChanges);

        testSubscriber.assertValueCount(2);
    }
",non-flaky,5
110912,pushtorefresh_storio,GetNumberOfResultsObserveChangesTest.repeatsOperationWithQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, 1);
    }
",non-flaky,5
110913,pushtorefresh_storio,GetNumberOfResultsObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tableChanges, 1);
    }
",non-flaky,5
110914,pushtorefresh_storio,GetNumberOfResultsObserveChangesTest.repeatsOperationWithQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, 1);
    }
",non-flaky,5
110915,pushtorefresh_storio,GetNumberOfResultsObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tagChanges, 1);
    }
",non-flaky,5
110916,pushtorefresh_storio,AutoParcelTest.insertObject,"    @Test
    public void insertObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult.wasInserted()).isTrue();

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(1);

        assertThat(storedBooks.get(0)).isEqualTo(book);
    }
",non-flaky,5
110917,pushtorefresh_storio,AutoParcelTest.updateObject,"    @Test
    public void updateObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult1 = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult1.wasInserted()).isTrue();

        final Book bookWithUpdatedInfo = Book.builder()
                .id(1) // Same id, should be updated
                .title(""Corrected title"")
                .author(""Corrected author"")
                .build();

        final PutResult putResult2 = storIOSQLite
                .put()
                .object(bookWithUpdatedInfo)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult2.wasUpdated()).isTrue();

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(1);

        assertThat(storedBooks.get(0)).isEqualTo(bookWithUpdatedInfo);
    }
",non-flaky,5
110918,pushtorefresh_storio,AutoParcelTest.deleteObject,"    @Test
    public void deleteObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult.wasInserted()).isTrue();

        final DeleteResult deleteResult = storIOSQLite
                .delete()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(deleteResult.numberOfRowsDeleted()).isEqualTo(1);

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(0);
    }
",non-flaky,5
110919,pushtorefresh_storio,SQLiteTypeMappingTest.nullPutResolver,"    @Test(expected = NullPointerException.class)
    public void nullPutResolver() {
        SQLiteTypeMapping.builder()
                .putResolver(null)
                .getResolver(mock(GetResolver.class))
                .deleteResolver(mock(DeleteResolver.class))
                .build();
    }
",non-flaky,5
110920,pushtorefresh_storio,SQLiteTypeMappingTest.nullMapFromCursor,"    @Test(expected = NullPointerException.class)
    public void nullMapFromCursor() {
        SQLiteTypeMapping.builder()
                .putResolver(mock(PutResolver.class))
                .getResolver(null)
                .deleteResolver(mock(DeleteResolver.class))
                .build();
    }
",non-flaky,5
110921,pushtorefresh_storio,SQLiteTypeMappingTest.nullMapToDeleteQuery,"    @Test(expected = NullPointerException.class)
    public void nullMapToDeleteQuery() {
        SQLiteTypeMapping.builder()
                .putResolver(mock(PutResolver.class))
                .getResolver(mock(GetResolver.class))
                .deleteResolver(null)
                .build();
    }
",non-flaky,5
110922,pushtorefresh_storio,SQLiteTypeMappingTest.build,"    @Test
    public void build() {
        class TestItem {

        }

        final PutResolver<TestItem> putResolver = mock(PutResolver.class);
        final GetResolver<TestItem> getResolver = mock(GetResolver.class);
        final DeleteResolver<TestItem> deleteResolver = mock(DeleteResolver.class);

        final SQLiteTypeMapping<TestItem> typeMapping = SQLiteTypeMapping.<TestItem>builder()
                .putResolver(putResolver)
                .getResolver(getResolver)
                .deleteResolver(deleteResolver)
                .build();

        assertThat(typeMapping.putResolver()).isSameAs(putResolver);
        assertThat(typeMapping.getResolver()).isSameAs(getResolver);
        assertThat(typeMapping.deleteResolver()).isSameAs(deleteResolver);
    }
",non-flaky,5
110923,pushtorefresh_storio,InsertQueryTest.shouldNotAllowNullTable,"    @Test
    public void shouldNotAllowNullTable() {
        expectedException.expect(NullPointerException.class);
        expectedException.expectMessage(equalTo(""Table name is null or empty""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder().table(null);
    }
",non-flaky,5
110924,pushtorefresh_storio,InsertQueryTest.shouldNotAllowEmptyTable,"    @Test
    public void shouldNotAllowEmptyTable() {
        expectedException.expect(IllegalStateException.class);
        expectedException.expectMessage(equalTo(""Table name is null or empty""));
        expectedException.expectCause(nullValue(Throwable.class));

        InsertQuery.builder().table("""");
    }
",non-flaky,5
110925,pushtorefresh_storio,InsertQueryTest.nullColumnHackShouldBeNullByDefault,"    @Test
    public void nullColumnHackShouldBeNullByDefault() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""test_table"")
                .build();

        assertThat(insertQuery.nullColumnHack()).isNull();
    }
",non-flaky,5
110926,pushtorefresh_storio,InsertQueryTest.completeBuilderShouldNotAllowNullTable,"    @Test
    public void completeBuilderShouldNotAllowNullTable() {
        try {
            //noinspection ConstantConditions
            InsertQuery.builder()
                    .table(""test_table"")
                    .table(null);
            failBecauseExceptionWasNotThrown(NullPointerException.class);
        } catch (NullPointerException expected) {
            assertThat(expected)
                    .hasMessage(""Table name is null or empty"")
                    .hasNoCause();
        }
    }
",non-flaky,5
110927,pushtorefresh_storio,InsertQueryTest.completeBuilderShouldNotAllowEmptyTable,"    @Test
    public void completeBuilderShouldNotAllowEmptyTable() {
        try {
            InsertQuery.builder()
                    .table(""test_table"")
                    .table("""");
            failBecauseExceptionWasNotThrown(IllegalStateException.class);
        } catch (IllegalStateException expected) {
            assertThat(expected)
                    .hasMessage(""Table name is null or empty"")
                    .hasNoCause();
        }
    }
",non-flaky,5
110928,pushtorefresh_storio,InsertQueryTest.completeBuilderShouldUpdateTable,"    @Test
    public void completeBuilderShouldUpdateTable() {
        InsertQuery query = InsertQuery.builder()
                .table(""old_table"")
                .table(""new_table"")
                .build();

        assertThat(query.table()).isEqualTo(""new_table"");
    }
",non-flaky,5
110929,pushtorefresh_storio,InsertQueryTest.createdThroughToBuilderQueryShouldBeEqual,"    @Test
    public void createdThroughToBuilderQueryShouldBeEqual() {
        final String table = ""test_table"";
        final String nullColumnHack = ""test_null_column_hack"";
        final String tag = ""test_tag"";

        final InsertQuery firstQuery = InsertQuery.builder()
                .table(table)
                .nullColumnHack(nullColumnHack)
                .affectsTags(tag)
                .build();

        final InsertQuery secondQuery = firstQuery.toBuilder().build();

        assertThat(secondQuery).isEqualTo(firstQuery);
    }
",non-flaky,5
110930,pushtorefresh_storio,InsertQueryTest.affectsTagsCollectionShouldRewrite,"    @Test
    public void affectsTagsCollectionShouldRewrite() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(new HashSet<String>((singletonList(""first_call_collection""))))
                .affectsTags(new HashSet<String>((singletonList(""second_call_collection""))))
                .build();

        assertThat(insertQuery.affectsTags()).isEqualTo(singleton(""second_call_collection""));
    }
",non-flaky,5
110931,pushtorefresh_storio,InsertQueryTest.affectsTagsVarargShouldRewrite,"    @Test
    public void affectsTagsVarargShouldRewrite() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(""first_call_vararg"")
                .affectsTags(""second_call_vararg"")
                .build();

        assertThat(insertQuery.affectsTags()).isEqualTo(singleton(""second_call_vararg""));
    }
",non-flaky,5
110932,pushtorefresh_storio,InsertQueryTest.affectsTagsCollectionAllowsNull,"    @Test
    public void affectsTagsCollectionAllowsNull() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(new HashSet<String>((singletonList(""first_call_collection""))))
                .affectsTags(null)
                .build();

        assertThat(insertQuery.affectsTags()).isEmpty();
    }
",non-flaky,5
110933,pushtorefresh_storio,InsertQueryTest.buildWithNormalValues,"    @Test
    public void buildWithNormalValues() {
        final String table = ""test_table"";
        final String nullColumnHack = ""test_null_column_hack"";
        final Set<String> tags = singleton(""tag"");

        final InsertQuery insertQuery = InsertQuery.builder()
                .table(table)
                .nullColumnHack(nullColumnHack)
                .affectsTags(tags)
                .build();

        assertThat(insertQuery.table()).isEqualTo(table);
        assertThat(insertQuery.nullColumnHack()).isEqualTo(nullColumnHack);
        assertThat(insertQuery.affectsTags()).isEqualTo(tags);
    }
",non-flaky,5
110934,pushtorefresh_storio,InsertQueryTest.shouldNotAllowNullTag,"    @Test
    public void shouldNotAllowNullTag() {
        expectedException.expect(NullPointerException.class);
        expectedException.expectMessage(startsWith(""affectsTag must not be null or empty, affectsTags = ""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder()
                .table(""table"")
                .affectsTags((String) null)
                .build();
    }
",non-flaky,5
110935,pushtorefresh_storio,InsertQueryTest.shouldNotAllowEmptyTag,"    @Test
    public void shouldNotAllowEmptyTag() {
        expectedException.expect(IllegalStateException.class);
        expectedException.expectMessage(startsWith(""affectsTag must not be null or empty, affectsTags = ""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder()
                .table(""table"")
                .affectsTags("""")
                .build();
    }
",non-flaky,5
145,apache_pinot,SegmentGenerationWithTimeColumnTest.testMinAllowedValue,"@Test
public void testMinAllowedValue() {
    long millis = _validMinTime;
    DateTime dateTime = new DateTime(millis, DateTimeZone.UTC);
    LocalDateTime localDateTime = dateTime.toLocalDateTime();
    int year = localDateTime.getYear();
    int month = localDateTime.getMonthOfYear();
    int day = localDateTime.getDayOfMonth();
    Assert.assertEquals(year, 1971);
    Assert.assertEquals(month, 1);
    Assert.assertEquals(day, 1);
}",time,2
104610,apache_pinot,AggregateMetricsClusterIntegrationTest.testQueries,"  @Test
  public void testQueries()
      throws Exception {
    String sql = ""SELECT SUM(AirTime), SUM(ArrDelay) FROM mytable"";
    testSqlQuery(sql, Collections.singletonList(sql));
    sql = ""SELECT SUM(AirTime), DaysSinceEpoch FROM mytable GROUP BY DaysSinceEpoch ORDER BY SUM(AirTime) DESC"";
    testSqlQuery(sql, Collections.singletonList(sql));
    sql = ""SELECT Origin, SUM(ArrDelay) FROM mytable WHERE Carrier = 'AA' GROUP BY Origin ORDER BY Origin"";
    testSqlQuery(sql, Collections.singletonList(sql));
  }
",non-flaky,5
104611,apache_pinot,SegmentPartitionLLCRealtimeClusterIntegrationTest.testPartitionMetadata,"  @Test
  public void testPartitionMetadata() {
    int[] numSegmentsForPartition = new int[2];
    String realtimeTableName = TableNameBuilder.REALTIME.tableNameWithType(getTableName());
    List<SegmentZKMetadata> segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(realtimeTableName);
    for (SegmentZKMetadata segmentZKMetadata : segmentsZKMetadata) {
      SegmentPartitionMetadata segmentPartitionMetadata = segmentZKMetadata.getPartitionMetadata();
      assertNotNull(segmentPartitionMetadata);
      Map<String, ColumnPartitionMetadata> columnPartitionMetadataMap =
          segmentPartitionMetadata.getColumnPartitionMap();
      assertEquals(columnPartitionMetadataMap.size(), 1);
      ColumnPartitionMetadata columnPartitionMetadata = columnPartitionMetadataMap.get(PARTITION_COLUMN);
      assertNotNull(columnPartitionMetadata);
      assertTrue(columnPartitionMetadata.getFunctionName().equalsIgnoreCase(""murmur""));
      assertEquals(columnPartitionMetadata.getNumPartitions(), 2);
      int partitionGroupId = new LLCSegmentName(segmentZKMetadata.getSegmentName()).getPartitionGroupId();
      assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
      numSegmentsForPartition[partitionGroupId]++;
    }

    // There should be 2 segments for partition 0, 2 segments for partition 1
    assertEquals(numSegmentsForPartition[0], 2);
    assertEquals(numSegmentsForPartition[1], 2);
  }
",non-flaky,5
104612,apache_pinot,SegmentPartitionLLCRealtimeClusterIntegrationTest.testPartitionRouting,"  @Test(dependsOnMethods = ""testPartitionMetadata"")
  public void testPartitionRouting()
      throws Exception {
    // Query partition 0
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'CA'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'CA' AND 'CA'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should only query the segments for partition 0
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), 2);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), 4);

      assertEquals(response.get(""aggregationResults"").get(0).get(""value"").asInt(),
          responseToCompare.get(""aggregationResults"").get(0).get(""value"").asInt());
    }

    // Query partition 1
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'FL'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'FL' AND 'FL'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should only query the segments for partition 1
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), 2);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), 4);

      assertEquals(response.get(""aggregationResults"").get(0).get(""value"").asInt(),
          responseToCompare.get(""aggregationResults"").get(0).get(""value"").asInt());
    }
  }
",non-flaky,5
104613,apache_pinot,SegmentPartitionLLCRealtimeClusterIntegrationTest.testNonPartitionedStream,"  @Test(dependsOnMethods = ""testPartitionRouting"")
  public void testNonPartitionedStream()
      throws Exception {
    // Push the second Avro file into Kafka without partitioning
    _partitionColumn = null;
    pushAvroIntoKafka(Collections.singletonList(_avroFiles.get(1)));

    // Wait for all documents loaded
    _countStarResult += NUM_DOCS_IN_SECOND_AVRO_FILE;
    waitForAllDocsLoaded(600_000L);

    // Check partition metadata
    int[] numSegmentsForPartition = new int[2];
    String realtimeTableName = TableNameBuilder.REALTIME.tableNameWithType(getTableName());
    List<SegmentZKMetadata> segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(realtimeTableName);
    for (SegmentZKMetadata segmentZKMetadata : segmentsZKMetadata) {
      SegmentPartitionMetadata segmentPartitionMetadata = segmentZKMetadata.getPartitionMetadata();
      assertNotNull(segmentPartitionMetadata);
      Map<String, ColumnPartitionMetadata> columnPartitionMetadataMap =
          segmentPartitionMetadata.getColumnPartitionMap();
      assertEquals(columnPartitionMetadataMap.size(), 1);
      ColumnPartitionMetadata columnPartitionMetadata = columnPartitionMetadataMap.get(PARTITION_COLUMN);
      assertNotNull(columnPartitionMetadata);
      assertTrue(columnPartitionMetadata.getFunctionName().equalsIgnoreCase(""murmur""));
      assertEquals(columnPartitionMetadata.getNumPartitions(), 2);
      int partitionGroupId = new LLCSegmentName(segmentZKMetadata.getSegmentName()).getPartitionGroupId();
      numSegmentsForPartition[partitionGroupId]++;

      if (segmentZKMetadata.getStatus() == Status.IN_PROGRESS) {
        // For consuming segment, the partition metadata should only contain the stream partition
        assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
      } else {
        LLCSegmentName llcSegmentName = new LLCSegmentName(segmentZKMetadata.getSegmentName());
        int sequenceNumber = llcSegmentName.getSequenceNumber();
        if (sequenceNumber == 0) {
          // The partition metadata for the first completed segment should only contain the stream partition
          assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
        } else {
          // The partition metadata for the new completed segments should contain both partitions
          assertEquals(columnPartitionMetadata.getPartitions(), new HashSet<>(Arrays.asList(0, 1)));
        }
      }
    }

    // There should be 4 segments for partition 0, 4 segments for partition 1
    assertEquals(numSegmentsForPartition[0], 4);
    assertEquals(numSegmentsForPartition[1], 4);

    // Check partition routing
    int numSegments = segmentsZKMetadata.size();

    // Query partition 0
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'CA'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'CA' AND 'CA'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should skip the first completed segments and the consuming segment for partition 1
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments - 2);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments);

      // The result won't match because the consuming segment for partition 1 is pruned out
    }

    // Query partition 1
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'FL'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'FL' AND 'FL'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should skip the first completed segments and the consuming segment for partition 0
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments - 2);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments);

      // The result won't match because the consuming segment for partition 0 is pruned out
    }

    // Push the third Avro file into Kafka with partitioning
    _partitionColumn = PARTITION_COLUMN;
    pushAvroIntoKafka(Collections.singletonList(_avroFiles.get(2)));

    // Wait for all documents loaded
    _countStarResult += NUM_DOCS_IN_THIRD_AVRO_FILE;
    waitForAllDocsLoaded(600_000L);

    // Check partition metadata
    numSegmentsForPartition = new int[2];
    segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(realtimeTableName);
    for (SegmentZKMetadata segmentZKMetadata : segmentsZKMetadata) {
      SegmentPartitionMetadata segmentPartitionMetadata = segmentZKMetadata.getPartitionMetadata();
      assertNotNull(segmentPartitionMetadata);
      Map<String, ColumnPartitionMetadata> columnPartitionMetadataMap =
          segmentPartitionMetadata.getColumnPartitionMap();
      assertEquals(columnPartitionMetadataMap.size(), 1);
      ColumnPartitionMetadata columnPartitionMetadata = columnPartitionMetadataMap.get(PARTITION_COLUMN);
      assertNotNull(columnPartitionMetadata);
      assertTrue(columnPartitionMetadata.getFunctionName().equalsIgnoreCase(""murmur""));
      assertEquals(columnPartitionMetadata.getNumPartitions(), 2);
      int partitionGroupId = new LLCSegmentName(segmentZKMetadata.getSegmentName()).getPartitionGroupId();
      numSegmentsForPartition[partitionGroupId]++;

      if (segmentZKMetadata.getStatus() == Status.IN_PROGRESS) {
        // For consuming segment, the partition metadata should only contain the stream partition
        assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
      } else {
        // The partition metadata for the new completed segments should only contain the stream partition
        LLCSegmentName llcSegmentName = new LLCSegmentName(segmentZKMetadata.getSegmentName());
        int sequenceNumber = llcSegmentName.getSequenceNumber();
        if (sequenceNumber == 0 || sequenceNumber >= 4) {
          // The partition metadata for the first and new completed segments should only contain the stream partition
          assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
        } else {
          // The partition metadata for the completed segments containing records from the second Avro file should
          // contain both partitions
          assertEquals(columnPartitionMetadata.getPartitions(), new HashSet<>(Arrays.asList(0, 1)));
        }
      }
    }

    // There should be 6 segments for partition 0, 6 segments for partition 1
    assertEquals(numSegmentsForPartition[0], 6);
    assertEquals(numSegmentsForPartition[1], 6);

    // Check partition routing
    numSegments = segmentsZKMetadata.size();

    // Query partition 0
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'CA'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'CA' AND 'CA'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should skip 2 completed segments and the consuming segment for partition 1
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments - 3);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments);

      // The result should match again after all the segments with the non-partitioning records are committed
      assertEquals(response.get(""aggregationResults"").get(0).get(""value"").asInt(),
          responseToCompare.get(""aggregationResults"").get(0).get(""value"").asInt());
    }

    // Query partition 1
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'FL'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'FL' AND 'FL'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should skip 2 completed segments and the consuming segment for partition 0
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments - 3);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments);

      // The result should match again after all the segments with the non-partitioning records are committed
      assertEquals(response.get(""aggregationResults"").get(0).get(""value"").asInt(),
          responseToCompare.get(""aggregationResults"").get(0).get(""value"").asInt());
    }
  }
",non-flaky,5
104614,apache_pinot,NullHandlingIntegrationTest.testTotalCount,"  @Test
  public void testTotalCount()
      throws Exception {
    String query = ""SELECT count(*) FROM "" + getTableName();
    testQuery(query, Collections.singletonList(query));
  }
",non-flaky,5
104615,apache_pinot,NullHandlingIntegrationTest.testCountWithNullDescription,"  @Test
  public void testCountWithNullDescription()
      throws Exception {
    String query = ""SELECT count(*) FROM "" + getTableName() + "" where description IS NOT NULL"";
    testQuery(query, Collections.singletonList(query));
  }
",non-flaky,5
104616,apache_pinot,NullHandlingIntegrationTest.testCountWithNullDescriptionAndSalary,"  @Test
  public void testCountWithNullDescriptionAndSalary()
      throws Exception {
    String query = ""SELECT count(*) FROM "" + getTableName() + "" where description IS NOT NULL AND salary IS NOT NULL"";
    testQuery(query, Collections.singletonList(query));
  }
",non-flaky,5
104617,apache_pinot,ChaosMonkeyIntegrationTest.testShortZookeeperFreeze,"  @Test(enabled = false)
  public void testShortZookeeperFreeze()
      throws Exception {
    testFreezeZookeeper(10000L);
  }
",non-flaky,5
104618,apache_pinot,ChaosMonkeyIntegrationTest.testLongZookeeperFreeze,"  @Test(enabled = false)
  public void testLongZookeeperFreeze()
      throws Exception {
    testFreezeZookeeper(60000L);
  }
",non-flaky,5
104619,apache_pinot,RealtimeClusterIntegrationTest.testQueriesFromQueryFile,"  @Test
  public void testQueriesFromQueryFile()
      throws Exception {
    super.testQueriesFromQueryFile();
  }
",non-flaky,5
104620,apache_pinot,RealtimeClusterIntegrationTest.testGeneratedQueriesWithMultiValues,"  @Test
  public void testGeneratedQueriesWithMultiValues()
      throws Exception {
    super.testGeneratedQueriesWithMultiValues();
  }
",non-flaky,5
104621,apache_pinot,RealtimeClusterIntegrationTest.testDictionaryBasedQueries,"  @Test
  public void testDictionaryBasedQueries()
      throws Exception {

    // Dictionary columns
    // int
    testDictionaryBasedFunctions(""NASDelay"");

    // long
    testDictionaryBasedFunctions(""AirlineID"");

    // double
    testDictionaryBasedFunctions(""ArrDelayMinutes"");

    // float
    testDictionaryBasedFunctions(""DepDelayMinutes"");

    // Non Dictionary columns
    // int
    testDictionaryBasedFunctions(""ActualElapsedTime"");

    // double
    testDictionaryBasedFunctions(""DepDelay"");

    // float
    testDictionaryBasedFunctions(""ArrDelay"");
  }
",non-flaky,5
104622,apache_pinot,RealtimeClusterIntegrationTest.testQueryExceptions,"  @Test
  public void testQueryExceptions()
      throws Exception {
    super.testQueryExceptions();
  }
",non-flaky,5
104623,apache_pinot,RealtimeClusterIntegrationTest.testInstanceShutdown,"  @Test
  public void testInstanceShutdown()
      throws Exception {
    super.testInstanceShutdown();
  }
",non-flaky,5
104624,apache_pinot,RealtimeClusterIntegrationTest.testHardcodedSqlQueries,"  @Test
  public void testHardcodedSqlQueries()
      throws Exception {
    super.testHardcodedSqlQueries();
  }
",non-flaky,5
104625,apache_pinot,RealtimeClusterIntegrationTest.testSqlQueriesFromQueryFile,"  @Test
  public void testSqlQueriesFromQueryFile()
      throws Exception {
    super.testSqlQueriesFromQueryFile();
  }
",non-flaky,5
104626,apache_pinot,MergeRollupMinionClusterIntegrationTest.testSingleLevelConcat,"  @Test
  public void testSingleLevelConcat()
      throws Exception {
    // The original segments are time partitioned by month:
    // segmentName (totalDocs)
    // myTable1_16071_16101_3 (9746)
    // myTable1_16102_16129_4 (8690)
    // myTable1_16130_16159_5 (9621)
    // myTable1_16160_16189_6 (9454)
    // myTable1_16190_16220_7 (10329)
    // myTable1_16221_16250_8 (10468)
    // myTable1_16251_16281_9 (10499)
    // myTable1_16282_16312_10 (10196)
    // myTable1_16313_16342_11 (9136)
    // myTable1_16343_16373_0 (9292)
    // myTable1_16374_16404_1 (8736)
    // myTable1_16405_16435_2 (9378)

    // Expected merge tasks and result segments:
    // 1.
    //    {myTable1_16071_16101_3}
    //      -> {merged_100days_T1_0_myTable1_16071_16099_0, merged_100days_T1_0_myTable1_16100_16101_1}
    // 2.
    //    {merged_100days_T1_0_myTable1_16100_16101_1, myTable1_16102_16129_4, myTable1_16130_16159_5}
    //      -> {merged_100days_T2_0_myTable1_16100_???_0(15000), merged_100days_T2_0_myTable1_???_16159_1}
    //    {myTable1_16160_16189_6, myTable1_16190_16220_7}
    //      -> {merged_100days_T2_1_myTable1_16160_16199_0, merged_100days_T2_1_myTable1_16200_16220_1}
    // 3.
    //    {merged_100days_T2_1_myTable1_16200_16220_1, myTable1_16221_16250_8}
    //      -> {merged_100days_T3_0_myTable1_16200_???_0(15000), merged_100days_T3_0_myTable1_???_16250_1}
    //    {myTable1_16251_16281_9, myTable1_16282_16312_10}
    //      -> {merged_100days_T3_1_myTable1_16251_???_0(15000), merged_100days_T3_1_myTable1_???_16299_1,
    //      merged_100days_T3_1_myTable1_16300_16312_2}
    // 4.
    //    {merged_100days_T3_1_myTable1_16300_16312_2, myTable1_16313_16342_11, myTable1_16343_16373_0}
    //      -> {merged_100days_T4_0_myTable1_16300_???_0(15000), merged_100days_T4_0_myTable1_???_16373_1}
    //    {myTable1_16374_16404_1}
    //      -> {merged_100days_T4_1_16374_16399_0, merged_100days_T4_1_16400_16404_1}
    // 5.
    //    {merged_100days_T4_1_16400_16404_1, myTable1_16405_16435_2}
    //      -> {merged_100days_T5_0_myTable1_16400_16435_0}

    String sqlQuery = ""SELECT count(*) FROM myTable1""; // 115545 rows for the test table
    JsonNode expectedJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    int[] expectedNumSubTasks = {1, 2, 2, 2, 1};
    int[] expectedNumSegmentsQueried = {13, 12, 13, 13, 12};
    long expectedWatermark = 16000 * 86_400_000L;
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(SINGLE_LEVEL_CONCAT_TEST_TABLE);
    int numTasks = 0;
    for (String tasks = _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE);
        tasks != null; tasks =
        _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE), numTasks++) {
      assertEquals(_helixTaskResourceManager.getTaskConfigs(tasks).size(), expectedNumSubTasks[numTasks]);
      assertTrue(_helixTaskResourceManager.getTaskQueues()
          .contains(PinotHelixTaskResourceManager.getHelixJobQueueName(MinionConstants.MergeRollupTask.TASK_TYPE)));
      // Will not schedule task if there's incomplete task
      assertNull(
          _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));
      waitForTaskToComplete();

      // Check watermark
      MergeRollupTaskMetadata minionTaskMetadata = MergeRollupTaskMetadata
          .fromZNRecord(_taskManager.getClusterInfoAccessor().getMinionMergeRollupTaskZNRecord(offlineTableName));
      assertNotNull(minionTaskMetadata);
      assertEquals((long) minionTaskMetadata.getWatermarkMap().get(""100days""), expectedWatermark);
      expectedWatermark += 100 * 86_400_000L;

      // Check metadata of merged segments
      for (SegmentZKMetadata metadata : _pinotHelixResourceManager.getSegmentsZKMetadata(offlineTableName)) {
        if (metadata.getSegmentName().startsWith(""merged"")) {
          // Check merged segment zk metadata
          assertNotNull(metadata.getCustomMap());
          assertEquals(""100days"",
              metadata.getCustomMap().get(MinionConstants.MergeRollupTask.SEGMENT_ZK_METADATA_MERGE_LEVEL_KEY));
          // Check merged segments are time partitioned
          assertEquals(metadata.getEndTimeMs() / (86_400_000L * 100), metadata.getStartTimeMs() / (86_400_000L * 100));
        }
      }

      // Check num total doc of merged segments are the same as the original segments
      JsonNode actualJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
      SqlResultComparator.areEqual(actualJson, expectedJson, sqlQuery);
      // Check query routing
      int numSegmentsQueried = actualJson.get(""numSegmentsQueried"").asInt();
      assertEquals(numSegmentsQueried, expectedNumSegmentsQueried[numTasks]);
    }
    // Check total tasks
    assertEquals(numTasks, 5);

    assertTrue(_controllerStarter.getControllerMetrics()
        .containsGauge(""mergeRollupTaskDelayInNumBuckets.myTable1_OFFLINE.100days""));

    // Drop the table
    dropOfflineTable(SINGLE_LEVEL_CONCAT_TEST_TABLE);

    // Check if the task metadata is cleaned up on table deletion
    verifyTableDelete(offlineTableName);
  }
",non-flaky,5
104627,apache_pinot,MergeRollupMinionClusterIntegrationTest.testSingleLevelRollup,"  @Test
  public void testSingleLevelRollup()
      throws Exception {
    // The original segments are time partitioned by month:
    // segmentName (totalDocs)
    // myTable2_16071_16101_3_1, myTable2_16071_16101_3_2 (9746)
    // myTable2_16102_16129_4_1, myTable2_16102_16129_4_2 (8690)
    // myTable2_16130_16159_5_1, myTable2_16130_16159_5_2 (9621)
    // myTable2_16160_16189_6_1, myTable2_16160_16189_6_2 (9454)
    // myTable2_16190_16220_7_1, myTable2_16190_16220_7_2 (10329)
    // myTable2_16221_16250_8_1, myTable2_16221_16250_8_2 (10468)
    // myTable2_16251_16281_9_1, myTable2_16251_16281_9_2 (10499)
    // myTable2_16282_16312_10_1, myTable2_16282_16312_10_2 (10196)
    // myTable2_16313_16342_11_1, myTable2_16313_16342_11_2 (9136)
    // myTable2_16343_16373_0_1, myTable2_16343_16373_0_2 (9292)
    // myTable2_16374_16404_1_1, myTable2_16374_16404_1_2 (8736)
    // myTable2_16405_16435_2_1, myTable2_16405_16435_2_2 (9378)

    // Expected merge tasks and result segments:
    // 1.
    //    {myTable2_16071_16101_3_1, myTable2_16071_16101_3_2, myTable2_16102_16129_4_1, myTable2_16102_16129_4_2,
    //     myTable2_16130_16159_5_1, myTable2_16130_16159_5_2, myTable2_16160_16189_6_1, myTable2_16160_16189_6_2
    //     myTable2_16190_16220_7}
    //      -> {merged_150days_T1_0_myTable2_16065_16198_0, merged_150days_T1_0_myTable2_16205_16219_1}
    // 2.
    //    {merged_150days_T1_0_myTable2_16205_16219_1, myTable2_16221_16250_8_1, myTable2_16221_16250_8_2,
    //     myTable2_16251_16281_9_1, myTable2_16251_16281_9_2, myTable2_16282_16312_10_1
    //     myTable2_16282_16312_10_2, myTable2_16313_16342_11_1, myTable2_16313_16342_11_2,
    //     myTable2_16343_16373_0_1, myTable2_16343_16373_0_2}
    //      -> {merged_150days_1628644088146_0_myTable2_16205_16345_0,
    //          merged_150days_1628644088146_0_myTable2_16352_16373_1}
    // 3.
    //    {merged_150days_1628644088146_0_myTable2_16352_16373_1, myTable2_16374_16404_1_1, myTable2_16374_16404_1_2
    //     myTable2_16405_16435_2_1, myTable2_16405_16435_2_2}
    //      -> {merged_150days_1628644105127_0_myTable2_16352_16429_0}

    String sqlQuery = ""SELECT count(*) FROM myTable2""; // 115545 rows for the test table
    JsonNode expectedJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    int[] expectedNumSegmentsQueried = {16, 7, 3};
    long expectedWatermark = 16050 * 86_400_000L;
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(SINGLE_LEVEL_ROLLUP_TEST_TABLE);
    int numTasks = 0;
    for (String tasks = _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE);
        tasks != null; tasks =
        _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE), numTasks++) {
      assertEquals(_helixTaskResourceManager.getTaskConfigs(tasks).size(), 1);
      assertTrue(_helixTaskResourceManager.getTaskQueues()
          .contains(PinotHelixTaskResourceManager.getHelixJobQueueName(MinionConstants.MergeRollupTask.TASK_TYPE)));
      // Will not schedule task if there's incomplete task
      assertNull(
          _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));
      waitForTaskToComplete();

      // Check watermark
      MergeRollupTaskMetadata minionTaskMetadata = MergeRollupTaskMetadata
          .fromZNRecord(_taskManager.getClusterInfoAccessor().getMinionMergeRollupTaskZNRecord(offlineTableName));
      assertNotNull(minionTaskMetadata);
      assertEquals((long) minionTaskMetadata.getWatermarkMap().get(""150days""), expectedWatermark);
      expectedWatermark += 150 * 86_400_000L;

      // Check metadata of merged segments
      for (SegmentZKMetadata metadata : _pinotHelixResourceManager.getSegmentsZKMetadata(offlineTableName)) {
        if (metadata.getSegmentName().startsWith(""merged"")) {
          // Check merged segment zk metadata
          assertNotNull(metadata.getCustomMap());
          assertEquals(""150days"",
              metadata.getCustomMap().get(MinionConstants.MergeRollupTask.SEGMENT_ZK_METADATA_MERGE_LEVEL_KEY));
          // Check merged segments are time partitioned
          assertEquals(metadata.getEndTimeMs() / (86_400_000L * 150), metadata.getStartTimeMs() / (86_400_000L * 150));
        }
      }

      // Check total doc of merged segments are less than the original segments
      JsonNode actualJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
      assertTrue(
          actualJson.get(""resultTable"").get(""rows"").get(0).get(0).asInt() < expectedJson.get(""resultTable"").get(""rows"")
              .get(0).get(0).asInt());
      // Check query routing
      int numSegmentsQueried = actualJson.get(""numSegmentsQueried"").asInt();
      assertEquals(numSegmentsQueried, expectedNumSegmentsQueried[numTasks]);
    }

    // Check total doc is half of the original after all merge tasks are finished
    JsonNode actualJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    assertEquals(actualJson.get(""resultTable"").get(""rows"").get(0).get(0).asInt(),
        expectedJson.get(""resultTable"").get(""rows"").get(0).get(0).asInt() / 2);
    // Check time column is rounded
    JsonNode responseJson =
        postSqlQuery(""SELECT count(*), DaysSinceEpoch FROM myTable2 GROUP BY DaysSinceEpoch ORDER BY DaysSinceEpoch"");
    for (int i = 0; i < responseJson.get(""resultTable"").get(""rows"").size(); i++) {
      int daysSinceEpoch = responseJson.get(""resultTable"").get(""rows"").get(i).get(1).asInt();
      assertTrue(daysSinceEpoch % 7 == 0);
    }
    // Check total tasks
    assertEquals(numTasks, 3);

    assertTrue(_controllerStarter.getControllerMetrics()
        .containsGauge(""mergeRollupTaskDelayInNumBuckets.myTable2_OFFLINE.150days""));
  }
",non-flaky,5
104628,apache_pinot,MergeRollupMinionClusterIntegrationTest.testMultiLevelConcat,"  @Test
  public void testMultiLevelConcat()
      throws Exception {
    // The original segments are time partitioned by month:
    // segmentName (totalDocs)
    // myTable3_16071_16101_3 (9746)
    // myTable3_16102_16129_4 (8690)
    // myTable3_16130_16159_5 (9621)
    // myTable3_16160_16189_6 (9454)
    // myTable3_16190_16220_7 (10329)
    // myTable3_16221_16250_8 (10468)
    // myTable3_16251_16281_9 (10499)
    // myTable3_16282_16312_10 (10196)
    // myTable3_16313_16342_11 (9136)
    // myTable3_16343_16373_0 (9292)
    // myTable3_16374_16404_1 (8736)
    // myTable3_16405_16435_2 (9378)

    // Expected merge tasks and results:
    // 1.
    //    45days: {myTable3_16071_16101_3, myTable3_16102_16129_4}
    //      -> {merged_45days_T1_0_myTable3_16071_16109_0, merged_45days_T1_0_myTable3_16110_16129_1}
    //    watermark: {45days: 16065, 90days: null}
    // 2.
    //    45days: {merged_45days_T1_0_myTable3_16110_16129_1, myTable3_16130_16159_5}
    //      -> {merged_45days_T2_0_myTable3_16110_16154_0, merged_45days_T2_0_myTable3_16155_16159_1}
    //    90days: {merged_45days_T1_0_myTable3_16071_16109_0}
    //      -> {merged_90days_T2_0_myTable3_16071_16109_0}
    //    watermark: {45days: 16110, 90days: 16020}
    // 3.
    //    45days: {merged_45days_T2_0_myTable3_16155_16159_1, myTable3_16160_16189_6, myTable3_16190_16220_7}
    //      -> {merged_45days_T3_0_myTable3_16155_16199_0, merged_45days_T3_0_myTable3_16200_16220_1}
    //    watermark: {45days: 16155, 90days: 16020}
    // 4.
    //    45days: {merged_45days_T3_-_myTable3_16200_16220_1, myTable3_16221_16250_8}
    //      -> {merged_45days_T4_0_myTable3_16200_16244_0, merged_45days_T4_0_myTable3_16245_16250_1}
    //    90days: {merged_45days_T2_0_myTable3_16110_16154_0, merged_45days_T3_0_myTable3_16155_16199_0}
    //      -> {merged_90days_T4_0_myTable3_16110_16199_0}
    //    watermark: {45days: 16200, 90days: 16110}
    // 5.
    //    45days: {merged_45days_T4_0_myTable3_16245_16250_1, myTable3_16251_16281_9, myTable3_16282_16312_10}
    //      -> {merged_45days_T5_0_myTable3_16245_16289_0, merged_45days_T5_0_myTable3_16290_16312_1}
    //    watermark: {45days: 16245, 90days: 16110}
    // 6.
    //    45days: {merged_45days_T5_0_myTable3_16290_16312_1, myTable3_16313_16342_11}
    //      -> {merged_45days_T6_0_myTable3_16290_16334_0, merged_45days_T6_0_myTable3_16335_16342_1}
    //    90days: {merged_45days_T4_0_myTable3_16200_16244_0, merged_45days_T5_0_myTable3_16245_16289_0}
    //      -> {merged_90days_T6_0_myTable3_16200_16289_0}
    //    watermark: {45days: 16290, 90days: 16200}
    // 7.
    //    45days: {merged_45days_T6_0_myTable3_16335_16342_1, myTable_16343_16373_0, myTable_16374_16404_1}
    //      -> {merged_45days_T7_0_myTable3_16335_16379_0, merged_45days_T7_0_myTable3_16380_16404_1}
    //    watermark: {45days: 16335, 90days: 16200}
    // 8.
    //    45days: {merged_45days_T7_0_myTable3_16380_16404_1, myTable3_16405_16435_2}
    //      -> {merged_45days_T8_0_myTable3_16380_16424_0, merged_45days_T8_1_myTable3_16425_16435_1}
    //    90days: {merged_45days_T6_0_myTable3_16290_16334_0, merged_45days_T7_0_myTable3_16335_16379_0}
    //      -> {merged_90days_T8_0_myTable3_16290_16379_0}
    //    watermark: {45days:16380, 90days: 16290}
    // 9.
    //    45days: no segment left, not scheduling
    //    90days: [16380, 16470) is not a valid merge window because windowEndTime > 45days watermark, not scheduling

    String sqlQuery = ""SELECT count(*) FROM myTable3""; // 115545 rows for the test table
    JsonNode expectedJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    int[] expectedNumSubTasks = {1, 2, 1, 2, 1, 2, 1, 2, 1};
    int[] expectedNumSegmentsQueried = {12, 12, 11, 10, 9, 8, 7, 6, 5};
    Long[] expectedWatermarks45Days = {16065L, 16110L, 16155L, 16200L, 16245L, 16290L, 16335L, 16380L};
    Long[] expectedWatermarks90Days = {null, 16020L, 16020L, 16110L, 16110L, 16200L, 16200L, 16290L};
    for (int i = 0; i < expectedWatermarks45Days.length; i++) {
      expectedWatermarks45Days[i] *= 86_400_000L;
    }
    for (int i = 1; i < expectedWatermarks90Days.length; i++) {
      expectedWatermarks90Days[i] *= 86_400_000L;
    }

    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(MULTI_LEVEL_CONCAT_TEST_TABLE);
    int numTasks = 0;
    for (String tasks = _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE);
        tasks != null; tasks =
        _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE), numTasks++) {
      assertEquals(_helixTaskResourceManager.getTaskConfigs(tasks).size(), expectedNumSubTasks[numTasks]);
      assertTrue(_helixTaskResourceManager.getTaskQueues()
          .contains(PinotHelixTaskResourceManager.getHelixJobQueueName(MinionConstants.MergeRollupTask.TASK_TYPE)));
      // Will not schedule task if there's incomplete task
      assertNull(
          _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));
      waitForTaskToComplete();

      // Check watermark
      MergeRollupTaskMetadata minionTaskMetadata = MergeRollupTaskMetadata
          .fromZNRecord(_taskManager.getClusterInfoAccessor().getMinionMergeRollupTaskZNRecord(offlineTableName));
      assertNotNull(minionTaskMetadata);
      assertEquals(minionTaskMetadata.getWatermarkMap().get(""45days""), expectedWatermarks45Days[numTasks]);
      assertEquals(minionTaskMetadata.getWatermarkMap().get(""90days""), expectedWatermarks90Days[numTasks]);

      // Check metadata of merged segments
      for (SegmentZKMetadata metadata : _pinotHelixResourceManager.getSegmentsZKMetadata(offlineTableName)) {
        if (metadata.getSegmentName().startsWith(""merged"")) {
          // Check merged segment zk metadata
          assertNotNull(metadata.getCustomMap());
          if (metadata.getSegmentName().startsWith(""merged_45days"")) {
            assertEquals(""45days"",
                metadata.getCustomMap().get(MinionConstants.MergeRollupTask.SEGMENT_ZK_METADATA_MERGE_LEVEL_KEY));
            assertEquals(metadata.getEndTimeMs() / (86_400_000L * 45), metadata.getStartTimeMs() / (86_400_000L * 45));
          }
          if (metadata.getSegmentName().startsWith(""merged_90days"")) {
            assertEquals(""90days"",
                metadata.getCustomMap().get(MinionConstants.MergeRollupTask.SEGMENT_ZK_METADATA_MERGE_LEVEL_KEY));
            assertEquals(metadata.getEndTimeMs() / (86_400_000L * 90), metadata.getStartTimeMs() / (86_400_000L * 90));
          }
        }
      }

      // Check total doc of merged segments are the same as the original segments
      JsonNode actualJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
      SqlResultComparator.areEqual(actualJson, expectedJson, sqlQuery);
      // Check query routing
      int numSegmentsQueried = actualJson.get(""numSegmentsQueried"").asInt();
      assertEquals(numSegmentsQueried, expectedNumSegmentsQueried[numTasks]);
    }
    // Check total tasks
    assertEquals(numTasks, 8);

    assertTrue(_controllerStarter.getControllerMetrics()
        .containsGauge(""mergeRollupTaskDelayInNumBuckets.myTable3_OFFLINE.45days""));
    assertTrue(_controllerStarter.getControllerMetrics()
        .containsGauge(""mergeRollupTaskDelayInNumBuckets.myTable3_OFFLINE.90days""));
  }
",non-flaky,5
104629,apache_pinot,OfflineClusterIntegrationTest.testInstancesStarted,"  @Test
  public void testInstancesStarted() {
    assertEquals(_serviceStatusCallbacks.size(), getNumBrokers() + getNumServers());
    for (ServiceStatus.ServiceStatusCallback serviceStatusCallback : _serviceStatusCallbacks) {
      assertEquals(serviceStatusCallback.getServiceStatus(), ServiceStatus.Status.GOOD);
    }
  }
",non-flaky,5
104630,apache_pinot,OfflineClusterIntegrationTest.testInvalidTableConfig,"  @Test
  public void testInvalidTableConfig() {
    TableConfig tableConfig = new TableConfigBuilder(TableType.OFFLINE).setTableName(""badTable"").build();
    ObjectNode tableConfigJson = (ObjectNode) tableConfig.toJsonNode();
    // Remove a mandatory field
    tableConfigJson.remove(TableConfig.VALIDATION_CONFIG_KEY);
    try {
      sendPostRequest(_controllerRequestURLBuilder.forTableCreate(), tableConfigJson.toString());
      fail();
    } catch (IOException e) {
      // Should get response code 400 (BAD_REQUEST)
      assertTrue(e.getMessage().startsWith(""Server returned HTTP response code: 400""));
    }
  }
",non-flaky,5
104631,apache_pinot,OfflineClusterIntegrationTest.testRefreshTableConfigAndQueryTimeout,"  @Test
  public void testRefreshTableConfigAndQueryTimeout()
      throws Exception {
    // Set timeout as 5ms so that query will timeout
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.setQueryConfig(new QueryConfig(5L));
    updateTableConfig(tableConfig);

    // Wait for at most 1 minute for broker to receive and process the table config refresh message
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_TIMEOUT_QUERY);
        JsonNode exceptions = queryResponse.get(""exceptions"");
        if (exceptions.isEmpty()) {
          return false;
        }
        int errorCode = exceptions.get(0).get(""errorCode"").asInt();
        if (errorCode == QueryException.BROKER_TIMEOUT_ERROR_CODE) {
          // Timed out on broker side
          return true;
        }
        if (errorCode == QueryException.SERVER_NOT_RESPONDING_ERROR_CODE) {
          // Timed out on server side
          int numServersQueried = queryResponse.get(""numServersQueried"").asInt();
          int numServersResponded = queryResponse.get(""numServersResponded"").asInt();
          int numDocsScanned = queryResponse.get(""numDocsScanned"").asInt();
          return numServersQueried == getNumServers() && numServersResponded == 0 && numDocsScanned == 0;
        }
        return false;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 60_000L, ""Failed to refresh table config"");

    // Remove timeout so that query will finish
    tableConfig.setQueryConfig(null);
    updateTableConfig(tableConfig);

    // Wait for at most 1 minute for broker to receive and process the table config refresh message
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_TIMEOUT_QUERY);
        JsonNode exceptions = queryResponse.get(""exceptions"");
        if (!exceptions.isEmpty()) {
          return false;
        }
        int numServersQueried = queryResponse.get(""numServersQueried"").asInt();
        int numServersResponded = queryResponse.get(""numServersResponded"").asInt();
        int numDocsScanned = queryResponse.get(""numDocsScanned"").asInt();
        return numServersQueried == getNumServers() && numServersResponded == getNumServers()
            && numDocsScanned == getCountStarResult();
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 60_000L, ""Failed to refresh table config"");
  }
",non-flaky,5
104632,apache_pinot,OfflineClusterIntegrationTest.testUploadSameSegments,"  @Test
  public void testUploadSameSegments()
      throws Exception {
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(getTableName());
    SegmentZKMetadata segmentZKMetadata = _helixResourceManager.getSegmentsZKMetadata(offlineTableName).get(0);
    String segmentName = segmentZKMetadata.getSegmentName();
    long crc = segmentZKMetadata.getCrc();
    // Creation time is when the segment gets created
    long creationTime = segmentZKMetadata.getCreationTime();
    // Push time is when the segment gets first pushed (new segment)
    long pushTime = segmentZKMetadata.getPushTime();
    // Refresh time is when the segment gets refreshed (existing segment)
    long refreshTime = segmentZKMetadata.getRefreshTime();

    uploadSegments(offlineTableName, _tarDir);
    for (SegmentZKMetadata segmentZKMetadataAfterUpload : _helixResourceManager
        .getSegmentsZKMetadata(offlineTableName)) {
      // Only check one segment
      if (segmentZKMetadataAfterUpload.getSegmentName().equals(segmentName)) {
        assertEquals(segmentZKMetadataAfterUpload.getCrc(), crc);
        assertEquals(segmentZKMetadataAfterUpload.getCreationTime(), creationTime);
        assertEquals(segmentZKMetadataAfterUpload.getPushTime(), pushTime);
        // Refresh time should change
        assertTrue(segmentZKMetadataAfterUpload.getRefreshTime() > refreshTime);
        return;
      }
    }
  }
",non-flaky,5
104633,apache_pinot,OfflineClusterIntegrationTest.testUploadSegmentRefreshOnly,"  @Test
  public void testUploadSegmentRefreshOnly()
      throws Exception {
    TableConfig segmentUploadTestTableConfig =
        new TableConfigBuilder(TableType.OFFLINE).setTableName(SEGMENT_UPLOAD_TEST_TABLE).setSchemaName(getSchemaName())
            .setTimeColumnName(getTimeColumnName()).setSortedColumn(getSortedColumn())
            .setInvertedIndexColumns(getInvertedIndexColumns()).setNoDictionaryColumns(getNoDictionaryColumns())
            .setRangeIndexColumns(getRangeIndexColumns()).setBloomFilterColumns(getBloomFilterColumns())
            .setFieldConfigList(getFieldConfigs()).setNumReplicas(getNumReplicas())
            .setSegmentVersion(getSegmentVersion())
            .setLoadMode(getLoadMode()).setTaskConfig(getTaskConfig()).setBrokerTenant(getBrokerTenant())
            .setServerTenant(getServerTenant()).setIngestionConfig(getIngestionConfig())
            .setNullHandlingEnabled(getNullHandlingEnabled()).build();
    addTableConfig(segmentUploadTestTableConfig);
    String offlineTableName = segmentUploadTestTableConfig.getTableName();
    File[] segmentTarFiles = _tarDir.listFiles();
    assertNotNull(segmentTarFiles);
    int numSegments = segmentTarFiles.length;
    assertTrue(numSegments > 0);
    List<Header> headers = new ArrayList<>();
    headers.add(new BasicHeader(FileUploadDownloadClient.CustomHeaders.REFRESH_ONLY, ""true""));
    List<NameValuePair> parameters = new ArrayList<>();
    NameValuePair tableNameParameter = new BasicNameValuePair(FileUploadDownloadClient.QueryParameters.TABLE_NAME,
        TableNameBuilder.extractRawTableName(offlineTableName));
    parameters.add(tableNameParameter);

    URI uploadSegmentHttpURI = FileUploadDownloadClient.getUploadSegmentHttpURI(LOCAL_HOST, _controllerPort);
    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {
      // Refresh non-existing segment
      File segmentTarFile = segmentTarFiles[0];
      try {
        fileUploadDownloadClient
            .uploadSegment(uploadSegmentHttpURI, segmentTarFile.getName(), segmentTarFile, headers, parameters,
                FileUploadDownloadClient.DEFAULT_SOCKET_TIMEOUT_MS);
        fail();
      } catch (HttpErrorStatusException e) {
        assertEquals(e.getStatusCode(), HttpStatus.SC_GONE);
        assertTrue(_helixResourceManager.getSegmentsZKMetadata(SEGMENT_UPLOAD_TEST_TABLE).isEmpty());
      }

      // Upload segment
      SimpleHttpResponse response = fileUploadDownloadClient
          .uploadSegment(uploadSegmentHttpURI, segmentTarFile.getName(), segmentTarFile, null, parameters,
              FileUploadDownloadClient.DEFAULT_SOCKET_TIMEOUT_MS);
      assertEquals(response.getStatusCode(), HttpStatus.SC_OK);
      System.out.println(response.getResponse());
      List<SegmentZKMetadata> segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(offlineTableName);
      assertEquals(segmentsZKMetadata.size(), 1);

      // Refresh existing segment
      response = fileUploadDownloadClient
          .uploadSegment(uploadSegmentHttpURI, segmentTarFile.getName(), segmentTarFile, headers, parameters,
              FileUploadDownloadClient.DEFAULT_SOCKET_TIMEOUT_MS);
      assertEquals(response.getStatusCode(), HttpStatus.SC_OK);
      segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(offlineTableName);
      assertEquals(segmentsZKMetadata.size(), 1);
      assertNotEquals(segmentsZKMetadata.get(0).getRefreshTime(), Long.MIN_VALUE);
    }
    dropOfflineTable(SEGMENT_UPLOAD_TEST_TABLE);
  }
",non-flaky,5
104634,apache_pinot,OfflineClusterIntegrationTest.testInvertedIndexTriggering,"  @Test(dependsOnMethods = ""testRangeIndexTriggering"")
  public void testInvertedIndexTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();

    // Without index on DivActualElapsedTime, all docs are scanned at filtering stage.
    assertEquals(postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY).get(""numEntriesScannedInFilter"").asLong(), numTotalDocs);

    addInvertedIndex();
    long tableSizeWithNewIndex = getTableSize(getTableName());

    // Update table config to remove the new inverted index, and
    // reload table to clean the new inverted indices physically.
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setInvertedIndexColumns(getInvertedIndexColumns());
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
        // Total docs should not change during reload, but num entries scanned
        // gets back to total number of documents as the index is removed.
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numEntriesScannedInFilter"").asLong() == numTotalDocs;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to cleanup obsolete index"");
    assertEquals(getTableSize(getTableName()), _tableSizeAfterRemovingIndex);

    // Add the inverted index back to test index removal via force download.
    addInvertedIndex();
    long tableSizeAfterAddingIndexAgain = getTableSize(getTableName());
    assertEquals(tableSizeAfterAddingIndexAgain, tableSizeWithNewIndex);

    // Update table config to remove the new inverted index.
    tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setInvertedIndexColumns(getInvertedIndexColumns());
    updateTableConfig(tableConfig);

    // Force to download a single segment, and disk usage should drop a bit.
    SegmentZKMetadata segmentZKMetadata =
        _helixResourceManager.getSegmentsZKMetadata(TableNameBuilder.OFFLINE.tableNameWithType(getTableName())).get(0);
    String segmentName = segmentZKMetadata.getSegmentName();
    reloadOfflineSegment(getTableName(), segmentName, true);
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return getTableSize(getTableName()) < tableSizeAfterAddingIndexAgain;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to clean up obsolete index in segment"");

    // Force to download the whole table and expect disk usage drops further.
    reloadOfflineTable(getTableName(), true);
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
        // Total docs should not change during reload, but num entries scanned
        // gets back to total number of documents as the index is removed.
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numEntriesScannedInFilter"").asLong() == numTotalDocs;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to cleanup obsolete index in table"");
    // With force download, the table size gets back to the initial value.
    assertEquals(getTableSize(getTableName()), DISK_SIZE_IN_BYTES);
  }
",non-flaky,5
104635,apache_pinot,OfflineClusterIntegrationTest.testTimeFunc,"  @Test
  public void testTimeFunc()
      throws Exception {
    String sqlQuery = ""SELECT toDateTime(now(), 'yyyy-MM-dd z'), toDateTime(ago('PT1H'), 'yyyy-MM-dd z') FROM mytable"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    String todayStr = response.get(""resultTable"").get(""rows"").get(0).get(0).asText();
    String expectedTodayStr =
        Instant.now().atZone(ZoneId.of(""UTC"")).format(DateTimeFormatter.ofPattern(""yyyy-MM-dd z""));
    assertEquals(todayStr, expectedTodayStr);

    String oneHourAgoTodayStr = response.get(""resultTable"").get(""rows"").get(0).get(1).asText();
    String expectedOneHourAgoTodayStr = Instant.now().minus(Duration.parse(""PT1H"")).atZone(ZoneId.of(""UTC""))
        .format(DateTimeFormatter.ofPattern(""yyyy-MM-dd z""));
    assertEquals(oneHourAgoTodayStr, expectedOneHourAgoTodayStr);
  }
",non-flaky,5
104636,apache_pinot,OfflineClusterIntegrationTest.testLiteralOnlyFunc,"  @Test
  public void testLiteralOnlyFunc()
      throws Exception {
    long currentTsMin = System.currentTimeMillis();
    long oneHourAgoTsMin = currentTsMin - ONE_HOUR_IN_MS;
    String sqlQuery =
        ""SELECT 1, now() as currentTs, ago('PT1H') as oneHourAgoTs, 'abc', toDateTime(now(), 'yyyy-MM-dd z') as ""
            + ""today, now(), ago('PT1H')"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    long currentTsMax = System.currentTimeMillis();
    long oneHourAgoTsMax = currentTsMax - ONE_HOUR_IN_MS;

    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(0).asText(), ""1"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(1).asText(), ""currentTs"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(2).asText(), ""oneHourAgoTs"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(3).asText(), ""abc"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(4).asText(), ""today"");
    String nowColumnName = response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(5).asText();
    String oneHourAgoColumnName = response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(6).asText();
    assertTrue(Long.parseLong(nowColumnName) > currentTsMin);
    assertTrue(Long.parseLong(nowColumnName) < currentTsMax);
    assertTrue(Long.parseLong(oneHourAgoColumnName) > oneHourAgoTsMin);
    assertTrue(Long.parseLong(oneHourAgoColumnName) < oneHourAgoTsMax);

    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(0).asText(), ""LONG"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(1).asText(), ""LONG"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(2).asText(), ""LONG"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(3).asText(), ""STRING"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(4).asText(), ""STRING"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(5).asText(), ""LONG"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(6).asText(), ""LONG"");

    int first = response.get(""resultTable"").get(""rows"").get(0).get(0).asInt();
    long second = response.get(""resultTable"").get(""rows"").get(0).get(1).asLong();
    long third = response.get(""resultTable"").get(""rows"").get(0).get(2).asLong();
    String fourth = response.get(""resultTable"").get(""rows"").get(0).get(3).asText();
    assertEquals(first, 1);
    assertTrue(second > currentTsMin);
    assertTrue(second < currentTsMax);
    assertTrue(third > oneHourAgoTsMin);
    assertTrue(third < oneHourAgoTsMax);
    assertEquals(fourth, ""abc"");
    String todayStr = response.get(""resultTable"").get(""rows"").get(0).get(4).asText();
    String expectedTodayStr =
        Instant.now().atZone(ZoneId.of(""UTC"")).format(DateTimeFormatter.ofPattern(""yyyy-MM-dd z""));
    assertEquals(todayStr, expectedTodayStr);
    long nowValue = response.get(""resultTable"").get(""rows"").get(0).get(5).asLong();
    assertEquals(nowValue, Long.parseLong(nowColumnName));
    long oneHourAgoValue = response.get(""resultTable"").get(""rows"").get(0).get(6).asLong();
    assertEquals(oneHourAgoValue, Long.parseLong(oneHourAgoColumnName));
  }
",non-flaky,5
104637,apache_pinot,OfflineClusterIntegrationTest.testRangeIndexTriggering,"  @Test(dependsOnMethods = ""testBloomFilterTriggering"")
  public void testRangeIndexTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();
    assertEquals(postQuery(TEST_UPDATED_RANGE_INDEX_QUERY).get(""numEntriesScannedInFilter"").asLong(), numTotalDocs);

    // Update table config and trigger reload
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setRangeIndexColumns(UPDATED_RANGE_INDEX_COLUMNS);
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_RANGE_INDEX_QUERY);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numEntriesScannedInFilter"").asLong() < numTotalDocs;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to generate range index"");

    // Update table config to remove the new range index, and
    // reload table to clean the new range index physically.
    tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setRangeIndexColumns(getRangeIndexColumns());
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_RANGE_INDEX_QUERY);
        // Total docs should not change during reload, but num entries scanned
        // gets back to total number of documents as the index is removed.
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numEntriesScannedInFilter"").asLong() == numTotalDocs;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to cleanup obsolete index"");

    assertEquals(getTableSize(getTableName()), _tableSizeAfterRemovingIndex);
  }
",non-flaky,5
104638,apache_pinot,OfflineClusterIntegrationTest.testBloomFilterTriggering,"  @Test(dependsOnMethods = ""testDefaultColumns"")
  public void testBloomFilterTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();
    assertEquals(postQuery(TEST_UPDATED_BLOOM_FILTER_QUERY).get(""numSegmentsProcessed"").asLong(), NUM_SEGMENTS);

    // Update table config and trigger reload
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setBloomFilterColumns(UPDATED_BLOOM_FILTER_COLUMNS);
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_BLOOM_FILTER_QUERY);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numSegmentsProcessed"").asLong() == 0L;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to generate bloom filter"");

    // Update table config to remove the new bloom filter, and
    // reload table to clean the new bloom filter physically.
    tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setBloomFilterColumns(getBloomFilterColumns());
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_BLOOM_FILTER_QUERY);
        // Total docs should not change during reload, but num entries scanned
        // gets back to total number of documents as bloom filter is removed.
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numSegmentsProcessed"").asLong() == NUM_SEGMENTS;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to cleanup obsolete index"");
    assertEquals(getTableSize(getTableName()), _tableSizeAfterRemovingIndex);
  }
",non-flaky,5
104639,apache_pinot,OfflineClusterIntegrationTest.testServerErrorWithBrokerTimeout,"  @Test
  public void testServerErrorWithBrokerTimeout()
      throws Exception {
    // Set query timeout
    long queryTimeout = 5000;
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.setQueryConfig(new QueryConfig(queryTimeout));
    updateTableConfig(tableConfig);

    long startTime = System.currentTimeMillis();
    // The query below will fail execution due to JSON_MATCH on column without json index
    JsonNode queryResponse = postSqlQuery(""SELECT count(*) FROM mytable WHERE JSON_MATCH(Dest, '$=123')"");

    assertTrue(System.currentTimeMillis() - startTime < queryTimeout);
    assertTrue(queryResponse.get(""exceptions"").get(0).get(""message"").toString().startsWith(""\""QueryExecutionError""));

    // Remove timeout
    tableConfig.setQueryConfig(null);
    updateTableConfig(tableConfig);
  }
",non-flaky,5
104640,apache_pinot,OfflineClusterIntegrationTest.testStarTreeTriggering,"  @Test
  public void testStarTreeTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();
    long tableSizeWithDefaultIndex = getTableSize(getTableName());

    // Test the first query
    JsonNode firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    int firstQueryResult = firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt();
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    // Initially 'numDocsScanned' should be the same as 'COUNT(*)' result
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);

    // Update table config and trigger reload
    TableConfig tableConfig = getOfflineTableConfig();
    IndexingConfig indexingConfig = tableConfig.getIndexingConfig();
    indexingConfig.setStarTreeIndexConfigs(Collections.singletonList(STAR_TREE_INDEX_CONFIG_1));
    indexingConfig.setEnableDynamicStarTreeCreation(true);
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());

    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
        // Result should not change during reload
        assertEquals(queryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        // With star-tree, 'numDocsScanned' should be the same as number of segments (1 per segment)
        return queryResponse.get(""numDocsScanned"").asInt() == NUM_SEGMENTS;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to add first star-tree index"");

    // Reload again should have no effect
    reloadOfflineTable(getTableName());
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), NUM_SEGMENTS);

    // Should be able to use the star-tree with an additional match-all predicate on another dimension
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1 + "" AND DaysSinceEpoch > 16070"");
    assertEquals(firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), NUM_SEGMENTS);

    // Test the second query
    JsonNode secondQueryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
    int secondQueryResult = secondQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt();
    assertEquals(secondQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    // Initially 'numDocsScanned' should be the same as 'COUNT(*)' result
    assertEquals(secondQueryResponse.get(""numDocsScanned"").asInt(), secondQueryResult);

    // Update table config with a different star-tree index config and trigger reload
    indexingConfig.setStarTreeIndexConfigs(Collections.singletonList(STAR_TREE_INDEX_CONFIG_2));
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());

    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
        // Result should not change during reload
        assertEquals(queryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        // With star-tree, 'numDocsScanned' should be the same as number of segments (1 per segment)
        return queryResponse.get(""numDocsScanned"").asInt() == NUM_SEGMENTS;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to change to second star-tree index"");

    // First query should not be able to use the star-tree
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);

    // Reload again should have no effect
    reloadOfflineTable(getTableName());
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);
    secondQueryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
    assertEquals(secondQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
    assertEquals(secondQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(secondQueryResponse.get(""numDocsScanned"").asInt(), NUM_SEGMENTS);

    // Should be able to use the star-tree with an additional match-all predicate on another dimension
    secondQueryResponse = postQuery(TEST_STAR_TREE_QUERY_2 + "" AND DaysSinceEpoch > 16070"");
    assertEquals(secondQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
    assertEquals(secondQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(secondQueryResponse.get(""numDocsScanned"").asInt(), NUM_SEGMENTS);

    // Remove the star-tree index config and trigger reload
    indexingConfig.setStarTreeIndexConfigs(null);
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());

    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
        // Result should not change during reload
        assertEquals(queryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        // Without star-tree, 'numDocsScanned' should be the same as the 'COUNT(*)' result
        return queryResponse.get(""numDocsScanned"").asInt() == secondQueryResult;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to remove star-tree index"");
    assertEquals(getTableSize(getTableName()), tableSizeWithDefaultIndex);

    // First query should not be able to use the star-tree
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);

    // Reload again should have no effect
    reloadOfflineTable(getTableName());
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);
    secondQueryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
    assertEquals(secondQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
    assertEquals(secondQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(secondQueryResponse.get(""numDocsScanned"").asInt(), secondQueryResult);
  }
",non-flaky,5
104641,apache_pinot,OfflineClusterIntegrationTest.testDefaultColumns,"  @Test(dependsOnMethods = ""testAggregateMetadataAPI"")
  public void testDefaultColumns()
      throws Exception {
    long numTotalDocs = getCountStarResult();

    reloadWithExtraColumns();
    JsonNode queryResponse = postQuery(SELECT_STAR_QUERY);
    assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(queryResponse.get(""selectionResults"").get(""columns"").size(), 91);

    testNewAddedColumns();

    reloadWithMissingColumns();
    queryResponse = postQuery(SELECT_STAR_QUERY);
    assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(queryResponse.get(""selectionResults"").get(""columns"").size(), 75);

    reloadWithRegularColumns();
    queryResponse = postQuery(SELECT_STAR_QUERY);
    assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(queryResponse.get(""selectionResults"").get(""columns"").size(), 79);

    _tableSizeAfterRemovingIndex = getTableSize(getTableName());
  }
",non-flaky,5
104642,apache_pinot,OfflineClusterIntegrationTest.testBrokerResponseMetadata,"  @Test
  public void testBrokerResponseMetadata()
      throws Exception {
    super.testBrokerResponseMetadata();
  }
",non-flaky,5
104643,apache_pinot,OfflineClusterIntegrationTest.testGroupByUDF,"  @Test
  public void testGroupByUDF()
      throws Exception {
    String pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY timeConvert(DaysSinceEpoch,'DAYS','SECONDS')"";
    JsonNode response = postQuery(pqlQuery);
    JsonNode groupByResult = response.get(""aggregationResults"").get(0);
    JsonNode groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asInt(), 16138 * 24 * 3600);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""timeconvert(DaysSinceEpoch,'DAYS','SECONDS')"");

    pqlQuery =
        ""SELECT COUNT(*) FROM mytable GROUP BY dateTimeConvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH',""
            + ""'1:HOURS')"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asInt(), 16138 * 24);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(),
        ""datetimeconvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH','1:HOURS')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY add(DaysSinceEpoch,DaysSinceEpoch,15)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asDouble(), 16138.0 + 16138 + 15);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""add(DaysSinceEpoch,DaysSinceEpoch,'15')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY sub(DaysSinceEpoch,25)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asDouble(), 16138.0 - 25);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""sub(DaysSinceEpoch,'25')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY mult(DaysSinceEpoch,24,3600)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asDouble(), 16138.0 * 24 * 3600);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""mult(DaysSinceEpoch,'24','3600')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY div(DaysSinceEpoch,2)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asDouble(), 16138.0 / 2);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""div(DaysSinceEpoch,'2')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY arrayLength(DivAirports)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 115545.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""5"");
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""arraylength(DivAirports)"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY arrayLength(valueIn(DivAirports,'DFW','ORD'))"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 114895.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""0"");
    groupByEntry = groupByResult.get(""groupByResult"").get(1);
    assertEquals(groupByEntry.get(""value"").asDouble(), 648.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""1"");
    groupByEntry = groupByResult.get(""groupByResult"").get(2);
    assertEquals(groupByEntry.get(""value"").asDouble(), 2.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""2"");
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""arraylength(valuein(DivAirports,'DFW','ORD'))"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY valueIn(DivAirports,'DFW','ORD')"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 336.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""ORD"");
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""valuein(DivAirports,'DFW','ORD')"");

    pqlQuery = ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"";
    response = postQuery(pqlQuery);
    JsonNode aggregationResult = response.get(""aggregationResults"").get(0);
    assertEquals(aggregationResult.get(""function"").asText(), ""max_timeconvert(DaysSinceEpoch,'DAYS','SECONDS')"");
    assertEquals(aggregationResult.get(""value"").asDouble(), 16435.0 * 24 * 3600);

    pqlQuery = ""SELECT MIN(div(DaysSinceEpoch,2)) FROM mytable"";
    response = postQuery(pqlQuery);
    aggregationResult = response.get(""aggregationResults"").get(0);
    assertEquals(aggregationResult.get(""function"").asText(), ""min_div(DaysSinceEpoch,'2')"");
    assertEquals(aggregationResult.get(""value"").asDouble(), 16071.0 / 2);
  }
",non-flaky,5
104644,apache_pinot,OfflineClusterIntegrationTest.testAggregationUDF,"  @Test
  public void testAggregationUDF()
      throws Exception {

    String pqlQuery = ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"";
    JsonNode response = postQuery(pqlQuery);
    JsonNode aggregationResult = response.get(""aggregationResults"").get(0);
    assertEquals(aggregationResult.get(""function"").asText(), ""max_timeconvert(DaysSinceEpoch,'DAYS','SECONDS')"");
    assertEquals(aggregationResult.get(""value"").asDouble(), 16435.0 * 24 * 3600);

    pqlQuery = ""SELECT MIN(div(DaysSinceEpoch,2)) FROM mytable"";
    response = postQuery(pqlQuery);
    aggregationResult = response.get(""aggregationResults"").get(0);
    assertEquals(aggregationResult.get(""function"").asText(), ""min_div(DaysSinceEpoch,'2')"");
    assertEquals(aggregationResult.get(""value"").asDouble(), 16071.0 / 2);
  }
",non-flaky,5
104645,apache_pinot,OfflineClusterIntegrationTest.testSelectionUDF,"  @Test
  public void testSelectionUDF()
      throws Exception {
    String pqlQuery = ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable"";
    JsonNode response = postQuery(pqlQuery);
    ArrayNode selectionResults = (ArrayNode) response.get(""selectionResults"").get(""results"");
    assertNotNull(selectionResults);
    assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      long daysSinceEpoch = selectionResults.get(i).get(0).asLong();
      long secondsSinceEpoch = selectionResults.get(i).get(1).asLong();
      assertEquals(daysSinceEpoch * 24 * 60 * 60, secondsSinceEpoch);
    }

    pqlQuery =
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by DaysSinceEpoch ""
            + ""limit 10000"";
    response = postQuery(pqlQuery);
    selectionResults = (ArrayNode) response.get(""selectionResults"").get(""results"");
    assertNotNull(selectionResults);
    assertFalse(selectionResults.isEmpty());
    long prevValue = -1;
    for (int i = 0; i < selectionResults.size(); i++) {
      long daysSinceEpoch = selectionResults.get(i).get(0).asLong();
      long secondsSinceEpoch = selectionResults.get(i).get(1).asLong();
      assertEquals(daysSinceEpoch * 24 * 60 * 60, secondsSinceEpoch);
      assertTrue(daysSinceEpoch >= prevValue);
      prevValue = daysSinceEpoch;
    }

    pqlQuery =
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by timeConvert""
            + ""(DaysSinceEpoch,'DAYS','SECONDS') DESC limit 10000"";
    response = postQuery(pqlQuery);
    selectionResults = (ArrayNode) response.get(""selectionResults"").get(""results"");
    assertNotNull(selectionResults);
    assertFalse(selectionResults.isEmpty());
    prevValue = Long.MAX_VALUE;
    for (int i = 0; i < selectionResults.size(); i++) {
      long daysSinceEpoch = selectionResults.get(i).get(0).asLong();
      long secondsSinceEpoch = selectionResults.get(i).get(1).asLong();
      assertEquals(daysSinceEpoch * 24 * 60 * 60, secondsSinceEpoch);
      assertTrue(secondsSinceEpoch <= prevValue);
      prevValue = secondsSinceEpoch;
    }
  }
",non-flaky,5
104646,apache_pinot,OfflineClusterIntegrationTest.testFilterUDF,"  @Test
  public void testFilterUDF()
      throws Exception {
    int daysSinceEpoch = 16138;
    long secondsSinceEpoch = 16138 * 24 * 60 * 60;

    String pqlQuery;
    pqlQuery = ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch;
    long expectedResult = postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong();

    pqlQuery = ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch;
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery = ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch
        + "" OR timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch;
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery = ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch
        + "" AND timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch;
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery =
        ""SELECT count(*) FROM mytable WHERE DIV(timeConvert(DaysSinceEpoch,'DAYS','SECONDS'),1) = "" + secondsSinceEpoch;
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery = String
        .format(""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') IN (%d, %d)"",
            secondsSinceEpoch - 100, secondsSinceEpoch);
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery = String
        .format(""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') BETWEEN %d AND %d"",
            secondsSinceEpoch - 100, secondsSinceEpoch);
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);
  }
",non-flaky,5
104647,apache_pinot,OfflineClusterIntegrationTest.testCaseStatementInSelection,"  @Test
  public void testCaseStatementInSelection()
      throws Exception {
    List<String> origins = Arrays
        .asList(""ATL"", ""ORD"", ""DFW"", ""DEN"", ""LAX"", ""IAH"", ""SFO"", ""PHX"", ""LAS"", ""EWR"", ""MCO"", ""BOS"", ""SLC"", ""SEA"", ""MSP"",
            ""CLT"", ""LGA"", ""DTW"", ""JFK"", ""BWI"");
    StringBuilder caseStatementBuilder = new StringBuilder(""CASE "");
    for (int i = 0; i < origins.size(); i++) {
      // WHEN origin = 'ATL' THEN 1
      // WHEN origin = 'ORD' THEN 2
      // WHEN origin = 'DFW' THEN 3
      // ....
      caseStatementBuilder.append(String.format(""WHEN origin = '%s' THEN %d "", origins.get(i), i + 1));
    }
    caseStatementBuilder.append(""ELSE 0 END"");
    String sqlQuery = ""SELECT origin, "" + caseStatementBuilder + "" AS origin_code FROM mytable LIMIT 1000"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    JsonNode rows = response.get(""resultTable"").get(""rows"");
    assertEquals(response.get(""exceptions"").size(), 0);
    for (int i = 0; i < rows.size(); i++) {
      String origin = rows.get(i).get(0).asText();
      int originCode = rows.get(i).get(1).asInt();
      if (originCode > 0) {
        assertEquals(origin, origins.get(originCode - 1));
      } else {
        assertFalse(origins.contains(origin));
      }
    }
  }
",non-flaky,5
104648,apache_pinot,OfflineClusterIntegrationTest.testCaseStatementInSelectionWithTransformFunctionInThen,"  @Test
  public void testCaseStatementInSelectionWithTransformFunctionInThen()
      throws Exception {
    String sqlQuery =
        ""SELECT ArrDelay, CASE WHEN ArrDelay > 0 THEN ArrDelay WHEN ArrDelay < 0 THEN ArrDelay * -1 ELSE 0 END AS ""
            + ""ArrTimeDiff FROM mytable LIMIT 1000"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    JsonNode rows = response.get(""resultTable"").get(""rows"");
    assertEquals(response.get(""exceptions"").size(), 0);
    for (int i = 0; i < rows.size(); i++) {
      int arrDelay = rows.get(i).get(0).asInt();
      int arrDelayDiff = rows.get(i).get(1).asInt();
      if (arrDelay > 0) {
        assertEquals(arrDelay, arrDelayDiff);
      } else {
        assertEquals(arrDelay, arrDelayDiff * -1);
      }
    }
  }
",non-flaky,5
104649,apache_pinot,OfflineClusterIntegrationTest.testCaseStatementWithLogicalTransformFunction,"  @Test
  public void testCaseStatementWithLogicalTransformFunction()
      throws Exception {
    String sqlQuery = ""SELECT ArrDelay"" + "", CASE WHEN ArrDelay > 50 OR ArrDelay < 10 THEN 10 ELSE 0 END""
        + "", CASE WHEN ArrDelay < 50 AND ArrDelay >= 10 THEN 10 ELSE 0 END"" + "" FROM mytable LIMIT 1000"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    JsonNode rows = response.get(""resultTable"").get(""rows"");
    assertEquals(response.get(""exceptions"").size(), 0);
    for (int i = 0; i < rows.size(); i++) {
      int row0 = rows.get(i).get(0).asInt();
      int row1 = rows.get(i).get(1).asInt();
      int row2 = rows.get(i).get(2).asInt();
      if (row0 > 50 || row0 < 10) {
        assertEquals(row1, 10);
      } else {
        assertEquals(row1, 0);
      }
      if (row0 < 50 && row0 >= 10) {
        assertEquals(row2, 10);
      } else {
        assertEquals(row2, 0);
      }
    }
  }
",non-flaky,5
104650,apache_pinot,OfflineClusterIntegrationTest.testCaseStatementWithInAggregation,"  @Test
  public void testCaseStatementWithInAggregation()
      throws Exception {
    testCountVsCaseQuery(""origin = 'ATL'"");
    testCountVsCaseQuery(""origin <> 'ATL'"");

    testCountVsCaseQuery(""DaysSinceEpoch > 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch >= 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch < 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch <= 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch = 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch <> 16312"");
  }
",non-flaky,5
104651,apache_pinot,OfflineClusterIntegrationTest.testFilterWithInvertedIndexUDF,"  @Test
  public void testFilterWithInvertedIndexUDF()
      throws Exception {
    int daysSinceEpoch = 16138;
    long secondsSinceEpoch = 16138 * 24 * 60 * 60;

    String[] origins = new String[]{
        ""ATL"", ""ORD"", ""DFW"", ""DEN"", ""LAX"", ""IAH"", ""SFO"", ""PHX"", ""LAS"", ""EWR"", ""MCO"", ""BOS"", ""SLC"", ""SEA"", ""MSP"", ""CLT"",
        ""LGA"", ""DTW"", ""JFK"", ""BWI""
    };
    String pqlQuery;
    for (String origin : origins) {
      pqlQuery =
          ""SELECT count(*) FROM mytable WHERE Origin = \"""" + origin + ""\"" AND DaysSinceEpoch = "" + daysSinceEpoch;
      JsonNode response1 = postQuery(pqlQuery);
      pqlQuery = ""SELECT count(*) FROM mytable WHERE Origin = \"""" + origin
          + ""\"" AND timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch;
      JsonNode response2 = postQuery(pqlQuery);
      double val1 = response1.get(""aggregationResults"").get(0).get(""value"").asDouble();
      double val2 = response2.get(""aggregationResults"").get(0).get(""value"").asDouble();
      assertEquals(val1, val2);
    }
  }
",non-flaky,5
104652,apache_pinot,OfflineClusterIntegrationTest.testQueryWithRepeatedColumns,"  @Test
  public void testQueryWithRepeatedColumns()
      throws Exception {
    //test repeated columns in selection query
    String query = ""SELECT ArrTime, ArrTime FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));

    //test repeated columns in selection query with order by
    query = ""SELECT ArrTime, ArrTime FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL' order by ArrTime"";
    testQuery(query, Collections.singletonList(query));

    //test repeated columns in agg query
    query = ""SELECT count(*), count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'"";
    testQuery(query, Arrays.asList(""SELECT count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'"",
        ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'""));

    //test repeated columns in agg group by query
    query =
        ""SELECT ArrTime, ArrTime, count(*), count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL' ""
            + ""group by ArrTime, ArrTime"";
    testQuery(query, Arrays.asList(
        ""SELECT ArrTime, ArrTime, count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL' group by ""
            + ""ArrTime, ArrTime"",
        ""SELECT ArrTime, ArrTime, count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL' group by ""
            + ""ArrTime, ArrTime""));
  }
",non-flaky,5
104653,apache_pinot,OfflineClusterIntegrationTest.testQueryWithOrderby,"  @Test
  public void testQueryWithOrderby()
      throws Exception {
    //test repeated columns in selection query
    String query = ""SELECT ArrTime, Carrier, DaysSinceEpoch FROM mytable ORDER BY DaysSinceEpoch DESC"";
    testQuery(query, Collections.singletonList(query));

    //test repeated columns in selection query
    query = ""SELECT ArrTime, DaysSinceEpoch, Carrier FROM mytable ORDER BY Carrier DESC"";
    testQuery(query, Collections.singletonList(query));

    //test repeated columns in selection query
    query = ""SELECT ArrTime, DaysSinceEpoch, Carrier FROM mytable ORDER BY Carrier DESC, ArrTime DESC"";
    testQuery(query, Collections.singletonList(query));
  }
",non-flaky,5
104654,apache_pinot,OfflineClusterIntegrationTest.testQueryWithAlias,"  @Test
  public void testQueryWithAlias()
      throws Exception {
    {
      //test same alias name with column name
      String query =
          ""SELECT ArrTime AS ArrTime, Carrier AS Carrier, DaysSinceEpoch AS DaysSinceEpoch FROM mytable ORDER BY ""
              + ""DaysSinceEpoch DESC"";
      testSqlQuery(query, Collections.singletonList(query));

      query =
          ""SELECT ArrTime AS ArrTime, DaysSinceEpoch AS DaysSinceEpoch, Carrier AS Carrier FROM mytable ORDER BY ""
              + ""Carrier DESC"";
      testSqlQuery(query, Collections.singletonList(query));

      query =
          ""SELECT ArrTime AS ArrTime, DaysSinceEpoch AS DaysSinceEpoch, Carrier AS Carrier FROM mytable ORDER BY ""
              + ""Carrier DESC, ArrTime DESC"";
      testSqlQuery(query, Collections.singletonList(query));
    }
    {
      //test single alias
      String query = ""SELECT ArrTime, Carrier AS CarrierName, DaysSinceEpoch FROM mytable ORDER BY DaysSinceEpoch DESC"";
      testSqlQuery(query, Collections.singletonList(query));

      query = ""SELECT count(*) AS cnt, max(ArrTime) as maxArrTime FROM mytable"";
      testSqlQuery(query, Collections.singletonList(query));

      query = ""SELECT count(*) AS cnt, Carrier AS CarrierName FROM mytable GROUP BY CarrierName ORDER BY cnt"";
      testSqlQuery(query, Collections.singletonList(query));
    }
    {
      //test multiple alias
      String query =
          ""SELECT ArrTime, Carrier, Carrier AS CarrierName1, Carrier AS CarrierName2, DaysSinceEpoch FROM mytable ""
              + ""ORDER BY DaysSinceEpoch DESC"";
      testSqlQuery(query, Collections.singletonList(query));

      query = ""SELECT count(*) AS cnt, max(ArrTime) as maxArrTime1, max(ArrTime) as maxArrTime2 FROM mytable"";
      testSqlQuery(query, Collections.singletonList(query));

      query =
          ""SELECT count(*), count(*) AS cnt1, count(*) AS cnt2, Carrier AS CarrierName FROM mytable GROUP BY ""
              + ""CarrierName ORDER BY cnt2"";
      testSqlQuery(query, Collections.singletonList(query));
    }
  }
",non-flaky,5
104655,apache_pinot,OfflineClusterIntegrationTest.testDistinctQuery,"  @Test
  public void testDistinctQuery()
      throws Exception {
    // by default 10 rows will be returned, so use high limit
    String pql = ""SELECT DISTINCT(Carrier) FROM mytable LIMIT 1000000"";
    String sql = ""SELECT DISTINCT Carrier FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier FROM mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT DISTINCT(Carrier, DestAirportID) FROM mytable LIMIT 1000000"";
    sql = ""SELECT DISTINCT Carrier, DestAirportID FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier, DestAirportID FROM mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT DISTINCT(Carrier, DestAirportID, DestStateName) FROM mytable LIMIT 1000000"";
    sql = ""SELECT DISTINCT Carrier, DestAirportID, DestStateName FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier, DestAirportID, DestStateName FROM mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT DISTINCT(Carrier, DestAirportID, DestCityName) FROM mytable LIMIT 1000000"";
    sql = ""SELECT DISTINCT Carrier, DestAirportID, DestCityName FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier, DestAirportID, DestCityName FROM mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));
  }
",non-flaky,5
104656,apache_pinot,OfflineClusterIntegrationTest.testNonAggregationGroupByQuery,"  @Test
  public void testNonAggregationGroupByQuery()
      throws Exception {
    // by default 10 rows will be returned, so use high limit
    String pql = ""SELECT Carrier FROM mytable GROUP BY Carrier LIMIT 1000000"";
    String sql = ""SELECT Carrier FROM mytable GROUP BY Carrier"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT Carrier, DestAirportID FROM mytable GROUP BY Carrier, DestAirportID LIMIT 1000000"";
    sql = ""SELECT Carrier, DestAirportID FROM mytable GROUP BY Carrier, DestAirportID"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql =
        ""SELECT Carrier, DestAirportID, DestStateName FROM mytable GROUP BY Carrier, DestAirportID, DestStateName ""
            + ""LIMIT 1000000"";
    sql = ""SELECT Carrier, DestAirportID, DestStateName FROM mytable GROUP BY Carrier, DestAirportID, DestStateName"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql =
        ""SELECT Carrier, DestAirportID, DestCityName FROM mytable GROUP BY Carrier, DestAirportID, DestCityName LIMIT""
            + "" 1000000"";
    sql = ""SELECT Carrier, DestAirportID, DestCityName FROM mytable GROUP BY Carrier, DestAirportID, DestCityName"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT ArrTime-DepTime FROM mytable GROUP BY ArrTime, DepTime LIMIT 1000000"";
    sql = ""SELECT ArrTime-DepTime FROM mytable GROUP BY ArrTime, DepTime"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT ArrTime-DepTime,ArrTime/3,DepTime*2 FROM mytable GROUP BY ArrTime, DepTime LIMIT 1000000"";
    sql = ""SELECT ArrTime-DepTime,ArrTime/3,DepTime*2 FROM mytable GROUP BY ArrTime, DepTime"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT ArrTime+DepTime FROM mytable GROUP BY ArrTime + DepTime LIMIT 1000000"";
    sql = ""SELECT ArrTime+DepTime FROM mytable GROUP BY ArrTime + DepTime"";
    testSqlQuery(pql, Collections.singletonList(sql));
  }
",non-flaky,5
104657,apache_pinot,OfflineClusterIntegrationTest.testCaseInsensitivity,"  @Test
  public void testCaseInsensitivity() {
    int daysSinceEpoch = 16138;
    int hoursSinceEpoch = 16138 * 24;
    int secondsSinceEpoch = 16138 * 24 * 60 * 60;
    List<String> baseQueries = Arrays.asList(""SELECT * FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by DaysSinceEpoch ""
            + ""limit 10000"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by timeConvert""
            + ""(DaysSinceEpoch,'DAYS','SECONDS') DESC limit 10000"",
        ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','HOURS') = "" + hoursSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch,
        ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"",
        ""SELECT COUNT(*) FROM mytable GROUP BY dateTimeConvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH',""
            + ""'1:HOURS')"");
    List<String> queries = new ArrayList<>();
    baseQueries.forEach(q -> queries.add(q.replace(""mytable"", ""MYTABLE"").replace(""DaysSinceEpoch"", ""DAYSSinceEpOch"")));
    baseQueries
        .forEach(q -> queries.add(q.replace(""mytable"", ""MYDB.MYTABLE"").replace(""DaysSinceEpoch"", ""DAYSSinceEpOch"")));

    for (String query : queries) {
      try {
        JsonNode response = postQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""PQL: "" + query + "" failed"");

        response = postSqlQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""SQL: "" + query + "" failed"");
      } catch (Exception e) {
        // Fail the test when exception caught
        throw new RuntimeException(""Got Exceptions from query - "" + query);
      }
    }
  }
",non-flaky,5
104658,apache_pinot,OfflineClusterIntegrationTest.testColumnNameContainsTableName,"  @Test
  public void testColumnNameContainsTableName() {
    int daysSinceEpoch = 16138;
    int hoursSinceEpoch = 16138 * 24;
    int secondsSinceEpoch = 16138 * 24 * 60 * 60;
    List<String> baseQueries = Arrays.asList(""SELECT * FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by DaysSinceEpoch ""
            + ""limit 10000"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by timeConvert""
            + ""(DaysSinceEpoch,'DAYS','SECONDS') DESC limit 10000"",
        ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','HOURS') = "" + hoursSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch,
        ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"",
        ""SELECT COUNT(*) FROM mytable GROUP BY dateTimeConvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH',""
            + ""'1:HOURS')"");
    List<String> queries = new ArrayList<>();
    baseQueries.forEach(q -> queries.add(q.replace(""DaysSinceEpoch"", ""mytable.DAYSSinceEpOch"")));
    baseQueries.forEach(q -> queries.add(q.replace(""DaysSinceEpoch"", ""mytable.DAYSSinceEpOch"")));

    for (String query : queries) {
      try {
        JsonNode response = postQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""PQL: "" + query + "" failed"");

        response = postSqlQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""SQL: "" + query + "" failed"");
      } catch (Exception e) {
        // Fail the test when exception caught
        throw new RuntimeException(""Got Exceptions from query - "" + query);
      }
    }
  }
",non-flaky,5
104659,apache_pinot,OfflineClusterIntegrationTest.testCaseInsensitivityWithColumnNameContainsTableName,"  @Test
  public void testCaseInsensitivityWithColumnNameContainsTableName() {
    int daysSinceEpoch = 16138;
    int hoursSinceEpoch = 16138 * 24;
    int secondsSinceEpoch = 16138 * 24 * 60 * 60;
    List<String> baseQueries = Arrays.asList(""SELECT * FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by DaysSinceEpoch ""
            + ""limit 10000"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by timeConvert""
            + ""(DaysSinceEpoch,'DAYS','SECONDS') DESC limit 10000"",
        ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','HOURS') = "" + hoursSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch,
        ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"",
        ""SELECT COUNT(*) FROM mytable GROUP BY dateTimeConvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH',""
            + ""'1:HOURS')"");
    List<String> queries = new ArrayList<>();
    baseQueries
        .forEach(q -> queries.add(q.replace(""mytable"", ""MYTABLE"").replace(""DaysSinceEpoch"", ""MYTABLE.DAYSSinceEpOch"")));
    baseQueries.forEach(
        q -> queries.add(q.replace(""mytable"", ""MYDB.MYTABLE"").replace(""DaysSinceEpoch"", ""MYTABLE.DAYSSinceEpOch"")));

    for (String query : queries) {
      try {
        JsonNode response = postQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""PQL: "" + query + "" failed"");

        response = postSqlQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""SQL: "" + query + "" failed"");
      } catch (Exception e) {
        // Fail the test when exception caught
        throw new RuntimeException(""Got Exceptions from query - "" + query);
      }
    }
  }
",non-flaky,5
104660,apache_pinot,OfflineClusterIntegrationTest.testQuerySourceWithDatabaseName,"  @Test
  public void testQuerySourceWithDatabaseName()
      throws Exception {
    // by default 10 rows will be returned, so use high limit
    String pql = ""SELECT DISTINCT(Carrier) FROM mytable LIMIT 1000000"";
    String sql = ""SELECT DISTINCT Carrier FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier FROM db.mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));
  }
",non-flaky,5
104661,apache_pinot,OfflineClusterIntegrationTest.testDistinctCountHll,"  @Test
  public void testDistinctCountHll()
      throws Exception {
    String query;

    // The Accurate value is 6538.
    query = ""SELECT distinctCount(FlightNum) FROM mytable "";
    assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), 6538);
    assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(), 6538);

    // Expected distinctCountHll with different log2m value from 2 to 19. The Accurate value is 6538.
    long[] expectedResults = new long[]{
        3504, 6347, 8877, 9729, 9046, 7672, 7538, 6993, 6649, 6651, 6553, 6525, 6459, 6523, 6532, 6544, 6538, 6539
    };

    for (int i = 2; i < 20; i++) {
      query = String.format(""SELECT distinctCountHLL(FlightNum, %d) FROM mytable "", i);
      assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResults[i - 2]);
      assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(),
          expectedResults[i - 2]);
    }

    // Default HLL is set as log2m=12
    query = ""SELECT distinctCountHLL(FlightNum) FROM mytable "";
    assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResults[10]);
    assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(),
        expectedResults[10]);
  }
",non-flaky,5
104662,apache_pinot,OfflineClusterIntegrationTest.testAggregationFunctionsWithUnderscore,"  @Test
  public void testAggregationFunctionsWithUnderscore()
      throws Exception {
    String query;

    // The Accurate value is 6538.
    query = ""SELECT distinct_count(FlightNum) FROM mytable "";
    assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), 6538);
    assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(), 6538);

    // The Accurate value is 6538.
    query = ""SELECT c_o_u_n_t(FlightNum) FROM mytable "";
    assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), 115545);
    assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(), 115545);
  }
",non-flaky,5
104663,apache_pinot,OfflineClusterIntegrationTest.testGrpcQueryServer,"  @Test
  public void testGrpcQueryServer()
      throws Exception {
    GrpcQueryClient queryClient = new GrpcQueryClient(""localhost"", CommonConstants.Server.DEFAULT_GRPC_PORT);
    String sql = ""SELECT * FROM mytable_OFFLINE LIMIT 1000000"";
    BrokerRequest brokerRequest = new Pql2Compiler().compileToBrokerRequest(sql);
    List<String> segments = _helixResourceManager.getSegmentsFor(""mytable_OFFLINE"");

    GrpcRequestBuilder requestBuilder = new GrpcRequestBuilder().setSegments(segments);
    testNonStreamingRequest(queryClient.submit(requestBuilder.setSql(sql).build()));
    testNonStreamingRequest(queryClient.submit(requestBuilder.setBrokerRequest(brokerRequest).build()));

    requestBuilder.setEnableStreaming(true);
    testStreamingRequest(queryClient.submit(requestBuilder.setSql(sql).build()));
    testStreamingRequest(queryClient.submit(requestBuilder.setBrokerRequest(brokerRequest).build()));
  }
",non-flaky,5
104664,apache_pinot,OfflineClusterIntegrationTest.testHardcodedServerPartitionedSqlQueries,"  @Test
  public void testHardcodedServerPartitionedSqlQueries()
      throws Exception {
    super.testHardcodedServerPartitionedSqlQueries();
  }
",non-flaky,5
104665,apache_pinot,OfflineClusterIntegrationTest.testAggregateMetadataAPI,"  @Test
  public void testAggregateMetadataAPI()
      throws IOException {
    JsonNode oneColumnResponse = JsonUtils
        .stringToJsonNode(sendGetRequest(_controllerBaseApiUrl + ""/tables/mytable/metadata?columns=DestCityMarketID""));
    assertEquals(oneColumnResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(oneColumnResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(oneColumnResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(oneColumnResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 1);
    assertEquals(oneColumnResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 1);

    JsonNode threeColumnsResponse = JsonUtils.stringToJsonNode(sendGetRequest(_controllerBaseApiUrl
        + ""/tables/mytable/metadata?columns=DivActualElapsedTime&columns=CRSElapsedTime&columns=OriginStateName""));
    assertEquals(threeColumnsResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(threeColumnsResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(threeColumnsResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(threeColumnsResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 3);
    assertEquals(threeColumnsResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 3);

    JsonNode zeroColumnResponse =
        JsonUtils.stringToJsonNode(sendGetRequest(_controllerBaseApiUrl + ""/tables/mytable/metadata""));
    assertEquals(zeroColumnResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(zeroColumnResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(zeroColumnResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(zeroColumnResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 0);
    assertEquals(zeroColumnResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 0);

    JsonNode allColumnResponse =
        JsonUtils.stringToJsonNode(sendGetRequest(_controllerBaseApiUrl + ""/tables/mytable/metadata?columns=*""));
    assertEquals(allColumnResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(allColumnResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(allColumnResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(allColumnResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 82);
    assertEquals(allColumnResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 82);

    allColumnResponse = JsonUtils.stringToJsonNode(sendGetRequest(
        _controllerBaseApiUrl + ""/tables/mytable/metadata?columns=CRSElapsedTime&columns=*&columns=OriginStateName""));
    assertEquals(allColumnResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(allColumnResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(allColumnResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(allColumnResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 82);
    assertEquals(allColumnResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 82);
  }
",non-flaky,5
104666,apache_pinot,RealtimeKinesisIntegrationTest.testRecords,"  @Test
  public void testRecords()
      throws Exception {
    Assert.assertNotEquals(_totalRecordsPushedInStream, 0);

    ResultSet pinotResultSet = getPinotConnection()
        .execute(new Request(""sql"", ""SELECT * FROM "" + getTableName() + "" ORDER BY Origin LIMIT 10000""))
        .getResultSet(0);

    Assert.assertNotEquals(pinotResultSet.getRowCount(), 0);

    Statement h2statement =
        _h2Connection.createStatement(java.sql.ResultSet.TYPE_FORWARD_ONLY, java.sql.ResultSet.CONCUR_READ_ONLY);
    h2statement.execute(""SELECT * FROM "" + getTableName() + "" ORDER BY Origin"");
    java.sql.ResultSet h2ResultSet = h2statement.getResultSet();

    Assert.assertFalse(h2ResultSet.isLast());

    h2ResultSet.beforeFirst();
    int row = 0;
    Map<String, Integer> columnToIndex = new HashMap<>();
    for (int i = 0; i < _h2FieldNameAndTypes.size(); i++) {
      columnToIndex.put(pinotResultSet.getColumnName(i), i);
    }

    while (h2ResultSet.next()) {

      for (String fieldNameAndDatatype : _h2FieldNameAndTypes) {
        String[] fieldNameAndDatatypeList = fieldNameAndDatatype.split("" "");
        String fieldName = fieldNameAndDatatypeList[0];
        String h2DataType = fieldNameAndDatatypeList[1];
        switch (h2DataType) {
          case ""int"": {
            int expectedValue = h2ResultSet.getInt(fieldName);
            int actualValue = pinotResultSet.getInt(row, columnToIndex.get(fieldName));
            Assert.assertEquals(expectedValue, actualValue);
            break;
          }
          case ""varchar(128)"": {
            String expectedValue = h2ResultSet.getString(fieldName);
            String actualValue = pinotResultSet.getString(row, columnToIndex.get(fieldName));
            Assert.assertEquals(expectedValue, actualValue);
            break;
          }
          default:
            break;
        }
      }

      row++;

      if (row >= pinotResultSet.getRowCount()) {
        int cnt = 0;
        while (h2ResultSet.next()) {
          cnt++;
        }
        Assert.assertEquals(cnt, 0);
        break;
      }
    }
  }
",non-flaky,5
104667,apache_pinot,RealtimeKinesisIntegrationTest.testCountRecords,"  @Test
  public void testCountRecords() {
    long count =
        getPinotConnection().execute(new Request(""sql"", ""SELECT COUNT(*) FROM "" + getTableName())).getResultSet(0)
            .getLong(0);

    Assert.assertEquals(count, _totalRecordsPushedInStream);
  }
",non-flaky,5
104668,apache_pinot,StarTreeClusterIntegrationTest.testGeneratedQueries,"  @Test
  public void testGeneratedQueries()
      throws Exception {
    for (int i = 0; i < NUM_QUERIES_TO_GENERATE; i += 2) {
      testStarQuery(_starTree1QueryGenerator.nextQuery());
      testStarQuery(_starTree2QueryGenerator.nextQuery());
    }
  }
",non-flaky,5
104669,apache_pinot,StarTreeClusterIntegrationTest.testPredicateOnMetrics,"  @Test
  public void testPredicateOnMetrics()
      throws Exception {
    String starQuery;

    // Query containing predicate on one metric only
    starQuery = ""SELECT SUM(DepDelayMinutes) FROM myStarTable WHERE DepDelay > 0"";
    testStarQuery(starQuery);
    starQuery = ""SELECT SUM(DepDelayMinutes) FROM myStarTable WHERE DepDelay BETWEEN 0 and 10000"";
    testStarQuery(starQuery);

    // Query containing predicate on multiple metrics
    starQuery = ""SELECT SUM(DepDelayMinutes) FROM myStarTable WHERE DepDelay > 0 AND ArrDelay > 0"";
    testStarQuery(starQuery);

    // Query containing predicate on multiple metrics and dimensions
    starQuery =
        ""SELECT SUM(DepDelayMinutes) FROM myStarTable WHERE DepDelay > 0 AND ArrDelay > 0 AND OriginStateName = ""
            + ""'Massachusetts'"";
    testStarQuery(starQuery);
  }
",non-flaky,5
104670,apache_pinot,LuceneRealtimeClusterIntegrationTest.testTextSearchCountQuery,"  @Test
  public void testTextSearchCountQuery()
      throws Exception {
    // Keep posting queries until all records are consumed
    long previousResult = 0;
    while (getCurrentCountStarResult() < NUM_RECORDS) {
      long result = getTextColumnQueryResult();
      assertTrue(result >= previousResult);
      previousResult = result;
      Thread.sleep(100);
    }

    //Lucene index on consuming segments to update the latest records
    TestUtils.waitForCondition(aVoid -> {
      try {
        return getTextColumnQueryResult() == NUM_MATCHING_RECORDS;
      } catch (Exception e) {
        fail(""Caught exception while getting text column query result"");
        return false;
      }
    }, 10_000L, ""Failed to reach expected number of matching records"");
  }
",non-flaky,5
104671,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testRealtimeToOfflineSegmentsTask,"  @Test
  public void testRealtimeToOfflineSegmentsTask()
      throws IOException {
    List<SegmentZKMetadata> segmentsZKMetadata = _pinotHelixResourceManager.getSegmentsZKMetadata(_offlineTableName);
    Assert.assertTrue(segmentsZKMetadata.isEmpty());

    long expectedWatermark = _dataSmallestTimeMs + 86400000;
    int numOfflineSegments = 0;
    for (int i = 0; i < 3; i++) {
      // Schedule task
      Assert.assertNotNull(_taskManager.scheduleTasks().get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));
      Assert.assertTrue(_helixTaskResourceManager.getTaskQueues().contains(
          PinotHelixTaskResourceManager.getHelixJobQueueName(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE)));
      // Should not generate more tasks
      Assert.assertNull(_taskManager.scheduleTasks().get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));

      // Wait at most 600 seconds for all tasks COMPLETED
      waitForTaskToComplete(expectedWatermark);
      // check segment is in offline
      segmentsZKMetadata = _pinotHelixResourceManager.getSegmentsZKMetadata(_offlineTableName);
      numOfflineSegments++;
      Assert.assertEquals(segmentsZKMetadata.size(), numOfflineSegments);
      long expectedOfflineSegmentTimeMs = expectedWatermark - 86400000;
      Assert.assertEquals(segmentsZKMetadata.get(i).getStartTimeMs(), expectedOfflineSegmentTimeMs);
      Assert.assertEquals(segmentsZKMetadata.get(i).getEndTimeMs(), expectedOfflineSegmentTimeMs);

      expectedWatermark += 86400000;
    }
    testHardcodedSqlQueries();

    // Delete the table
    dropRealtimeTable(_realtimeTableName);

    // Check if the metadata is cleaned up on table deletion
    verifyTableDelete(_realtimeTableName);
  }
",non-flaky,5
104672,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testSegmentListApi,"  @Test(enabled = false)
  public void testSegmentListApi() {
  }
",non-flaky,5
104673,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testBrokerDebugOutput,"  @Test(enabled = false)
  public void testBrokerDebugOutput() {
  }
",non-flaky,5
104674,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testBrokerDebugRoutingTableSQL,"  @Test(enabled = false)
  public void testBrokerDebugRoutingTableSQL() {
  }
",non-flaky,5
104675,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testBrokerResponseMetadata,"  @Test(enabled = false)
  public void testBrokerResponseMetadata() {
  }
",non-flaky,5
104676,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testDictionaryBasedQueries,"  @Test(enabled = false)
  public void testDictionaryBasedQueries() {
  }
",non-flaky,5
104677,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testGeneratedQueriesWithMultiValues,"  @Test(enabled = false)
  public void testGeneratedQueriesWithMultiValues() {
  }
",non-flaky,5
104678,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testGeneratedQueriesWithoutMultiValues,"  @Test(enabled = false)
  public void testGeneratedQueriesWithoutMultiValues() {
  }
",non-flaky,5
104679,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testHardcodedQueries,"  @Test(enabled = false)
  public void testHardcodedQueries() {
  }
",non-flaky,5
104680,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testHardcodedSqlQueries,"  @Test(enabled = false)
  public void testHardcodedSqlQueries() {
  }
",non-flaky,5
104681,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testInstanceShutdown,"  @Test(enabled = false)
  public void testInstanceShutdown() {
  }
",non-flaky,5
104682,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testQueriesFromQueryFile,"  @Test(enabled = false)
  public void testQueriesFromQueryFile() {
  }
",non-flaky,5
104683,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testQueryExceptions,"  @Test(enabled = false)
  public void testQueryExceptions() {
  }
",non-flaky,5
104684,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testReload,"  @Test(enabled = false)
  public void testReload(boolean includeOfflineTable) {
  }
",non-flaky,5
104685,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testSqlQueriesFromQueryFile,"  @Test(enabled = false)
  public void testSqlQueriesFromQueryFile() {
  }
",non-flaky,5
104686,apache_pinot,RealtimeToOfflineSegmentsMinionClusterIntegrationTest.testVirtualColumnQueries,"  @Test(enabled = false)
  public void testVirtualColumnQueries() {
  }
",non-flaky,5
104687,apache_pinot,MapTypeClusterIntegrationTest.testJsonPathQueries,"  @Test
  public void testJsonPathQueries()
      throws Exception {
    // Selection only
    String query = ""SELECT stringKeyMapStr FROM "" + getTableName();
    JsonNode pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(selectionResults.get(i).get(0).textValue(), String.format(""{\""k1\"":%d,\""k2\"":100%d}"", i, i));
    }
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), i);
    }
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.95', 'INT') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), i);
    }

    // Selection order-by
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k2', 'INT') FROM "" + getTableName()
        + "" ORDER BY jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT')"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), NUM_DOCS + i);
    }
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.717', 'INT') FROM "" + getTableName()
        + "" ORDER BY jsonExtractScalar(intKeyMapStr, '$.95', 'INT')"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), NUM_DOCS + i);
    }

    // Aggregation only
    query = ""SELECT MAX(jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT')) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode aggregationResult = pinotResponse.get(""aggregationResults"").get(0).get(""value"");
    assertEquals((int) Double.parseDouble(aggregationResult.textValue()), NUM_DOCS - 1);
    query = ""SELECT MAX(jsonExtractScalar(intKeyMapStr, '$.95', 'INT')) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    aggregationResult = pinotResponse.get(""aggregationResults"").get(0).get(""value"");
    assertEquals((int) Double.parseDouble(aggregationResult.textValue()), NUM_DOCS - 1);

    // Aggregation group-by
    query = ""SELECT MIN(jsonExtractScalar(stringKeyMapStr, '$.k2', 'INT')) FROM "" + getTableName()
        + "" GROUP BY jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT')"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode groupByResults = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    assertEquals(groupByResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      JsonNode groupByResult = groupByResults.get(i);
      assertEquals(Integer.parseInt(groupByResult.get(""group"").get(0).asText()), i);
      assertEquals((int) Double.parseDouble(groupByResult.get(""value"").asText()), NUM_DOCS + i);
    }
    query = ""SELECT MIN(jsonExtractScalar(intKeyMapStr, '$.717', 'INT')) FROM "" + getTableName()
        + "" GROUP BY jsonExtractScalar(intKeyMapStr, '$.95', 'INT')"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    groupByResults = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    assertEquals(groupByResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      JsonNode groupByResult = groupByResults.get(i);
      assertEquals(Integer.parseInt(groupByResult.get(""group"").get(0).asText()), i);
      assertEquals((int) Double.parseDouble(groupByResult.get(""value"").asText()), NUM_DOCS + i);
    }

    // Filter
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k2', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT') = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 1);
    assertEquals(Integer.parseInt(selectionResults.get(0).get(0).textValue()), NUM_DOCS + 25);
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.717', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractScalar(intKeyMapStr, '$.95', 'INT') = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 1);
    assertEquals(Integer.parseInt(selectionResults.get(0).get(0).textValue()), NUM_DOCS + 25);

    // Filter on non-existing key
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k2', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractScalar(stringKeyMapStr, '$.k3', 'INT_ARRAY') = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.717', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractScalar(intKeyMapStr, '$.123', 'INT_ARRAY') = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);

    // Select non-existing key (illegal query)
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k3', 'INT') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertNotEquals(pinotResponse.get(""exceptions"").size(), 0);
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.123', 'INT') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertNotEquals(pinotResponse.get(""exceptions"").size(), 0);

    // Select non-existing key with default value
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k3', 'INT', '0') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.123', 'INT', '0') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);

    // Select non-existing key with proper filter
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.123', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractKey(intKeyMapStr, '$.*') = \""$['123']\"""";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k3', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractKey(stringKeyMapStr, '$.*') = \""$['k3']\"""";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
  }
",non-flaky,5
104688,apache_pinot,MapTypeClusterIntegrationTest.testQueries,"  @Test
  public void testQueries()
      throws Exception {
    // Selection only
    String query = ""SELECT mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES) FROM "" + getTableName();
    JsonNode pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), i);
    }
    query = ""SELECT mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), i);
    }

    // Selection order-by
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k2', stringKeyMap__VALUES) FROM "" + getTableName()
        + "" ORDER BY mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES)"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), NUM_DOCS + i);
    }
    query = ""SELECT mapValue(intKeyMap__KEYS, 717, intKeyMap__VALUES) FROM "" + getTableName()
        + "" ORDER BY mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES)"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), NUM_DOCS + i);
    }

    // Aggregation only
    query = ""SELECT MAX(mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES)) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode aggregationResult = pinotResponse.get(""aggregationResults"").get(0).get(""value"");
    assertEquals((int) Double.parseDouble(aggregationResult.textValue()), NUM_DOCS - 1);
    query = ""SELECT MAX(mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES)) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    aggregationResult = pinotResponse.get(""aggregationResults"").get(0).get(""value"");
    assertEquals((int) Double.parseDouble(aggregationResult.textValue()), NUM_DOCS - 1);

    // Aggregation group-by
    query = ""SELECT MIN(mapValue(stringKeyMap__KEYS, 'k2', stringKeyMap__VALUES)) FROM "" + getTableName()
        + "" GROUP BY mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES)"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode groupByResults = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    assertEquals(groupByResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      JsonNode groupByResult = groupByResults.get(i);
      assertEquals(Integer.parseInt(groupByResult.get(""group"").get(0).asText()), i);
      assertEquals((int) Double.parseDouble(groupByResult.get(""value"").asText()), NUM_DOCS + i);
    }
    query = ""SELECT MIN(mapValue(intKeyMap__KEYS, 717, intKeyMap__VALUES)) FROM "" + getTableName()
        + "" GROUP BY mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES)"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    groupByResults = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    assertEquals(groupByResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      JsonNode groupByResult = groupByResults.get(i);
      assertEquals(Integer.parseInt(groupByResult.get(""group"").get(0).asText()), i);
      assertEquals((int) Double.parseDouble(groupByResult.get(""value"").asText()), NUM_DOCS + i);
    }

    // Filter
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k2', stringKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES) = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 1);
    assertEquals(Integer.parseInt(selectionResults.get(0).get(0).textValue()), NUM_DOCS + 25);
    query = ""SELECT mapValue(intKeyMap__KEYS, 717, intKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES) = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 1);
    assertEquals(Integer.parseInt(selectionResults.get(0).get(0).textValue()), NUM_DOCS + 25);

    // Filter on non-existing key
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k2', stringKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE mapValue(stringKeyMap__KEYS, 'k3', stringKeyMap__VALUES) = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
    query = ""SELECT mapValue(intKeyMap__KEYS, 717, intKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE mapValue(intKeyMap__KEYS, 123, intKeyMap__VALUES) = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);

    // Select non-existing key (illegal query)
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k3', stringKeyMap__VALUES) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertNotEquals(pinotResponse.get(""exceptions"").size(), 0);
    query = ""SELECT mapValue(stringKeyMap__KEYS, 123, stringKeyMap__VALUES) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertNotEquals(pinotResponse.get(""exceptions"").size(), 0);

    // Select non-existing key with proper filter
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k3', stringKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE stringKeyMap__KEYS = 'k3'"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
    query = ""SELECT mapValue(intKeyMap__KEYS, 123, intKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE stringKeyMap__KEYS = 123"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
  }
",non-flaky,5
104689,apache_pinot,UpsertTableSegmentUploadIntegrationTest.testSegmentAssignment,"  @Test
  public void testSegmentAssignment()
      throws Exception {
    IdealState idealState = HelixHelper.getTableIdealState(_helixManager, TABLE_NAME_WITH_TYPE);
    Assert.assertEquals(getCurrentCountStarResult(), getCountStarResult());
    verifyTableIdealStates(idealState);
    // Wait 3 seconds to let the realtime validation thread to run.
    Thread.sleep(3000);
    // Verify the result again.
    Assert.assertEquals(getCurrentCountStarResult(), getCountStarResult());
    verifyTableIdealStates(idealState);
  }
",non-flaky,5
104690,apache_pinot,BasicAuthTlsRealtimeIntegrationTest.testSegmentUploadDownload,"  @Test
  public void testSegmentUploadDownload()
      throws Exception {
    final Request query = new Request(""sql"", ""SELECT count(*) FROM "" + getTableName());

    ResultSetGroup resultBeforeOffline = getPinotConnection().execute(query);
    Assert.assertTrue(resultBeforeOffline.getResultSet(0).getLong(0) > 0);

    // schedule offline segment generation
    Assert.assertNotNull(_controllerStarter.getTaskManager().scheduleTasks());

    // wait for offline segments
    JsonNode offlineSegments = TestUtils.waitForResult(() -> {
      JsonNode segmentSets = JsonUtils.stringToJsonNode(
          sendGetRequest(_controllerRequestURLBuilder.forSegmentListAPI(getTableName()), AUTH_HEADER));
      JsonNode currentOfflineSegments =
          new IntRange(0, segmentSets.size()).stream().map(segmentSets::get).filter(s -> s.has(""OFFLINE""))
              .map(s -> s.get(""OFFLINE"")).findFirst().get();
      Assert.assertFalse(currentOfflineSegments.isEmpty());
      return currentOfflineSegments;
    }, 30000);

    // Verify constant row count
    ResultSetGroup resultAfterOffline = getPinotConnection().execute(query);
    Assert.assertEquals(resultBeforeOffline.getResultSet(0).getLong(0), resultAfterOffline.getResultSet(0).getLong(0));

    // download and sanity-check size of offline segment(s)
    for (int i = 0; i < offlineSegments.size(); i++) {
      String segment = offlineSegments.get(i).asText();
      Assert.assertTrue(
          sendGetRequest(_controllerRequestURLBuilder.forSegmentDownload(getTableName(), segment), AUTH_HEADER).length()
              > 200000); // download segment
    }
  }
",non-flaky,5
104691,apache_pinot,ConvertToRawIndexMinionClusterIntegrationTest.testConvertToRawIndexTask,"  @Test
  public void testConvertToRawIndexTask()
      throws Exception {
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(getTableName());

    File testDataDir = new File(CommonConstants.Server.DEFAULT_INSTANCE_DATA_DIR + ""-0"", offlineTableName);
    if (!testDataDir.isDirectory()) {
      testDataDir = new File(CommonConstants.Server.DEFAULT_INSTANCE_DATA_DIR + ""-1"", offlineTableName);
    }
    Assert.assertTrue(testDataDir.isDirectory());
    File tableDataDir = testDataDir;

    // Check that all columns have dictionary
    File[] indexDirs = tableDataDir.listFiles();
    Assert.assertNotNull(indexDirs);
    for (File indexDir : indexDirs) {
      SegmentMetadata segmentMetadata = new SegmentMetadataImpl(indexDir);
      for (String columnName : segmentMetadata.getSchema().getColumnNames()) {
        Assert.assertTrue(segmentMetadata.getColumnMetadataFor(columnName).hasDictionary());
      }
    }

    // Should create the task queues and generate a ConvertToRawIndexTask task with 5 child tasks
    Assert.assertNotNull(_taskManager.scheduleTasks().get(ConvertToRawIndexTask.TASK_TYPE));
    Assert.assertTrue(_helixTaskResourceManager.getTaskQueues()
        .contains(PinotHelixTaskResourceManager.getHelixJobQueueName(ConvertToRawIndexTask.TASK_TYPE)));

    // Should generate one more ConvertToRawIndexTask task with 3 child tasks
    Assert.assertNotNull(_taskManager.scheduleTasks().get(ConvertToRawIndexTask.TASK_TYPE));

    // Should not generate more tasks
    Assert.assertNull(_taskManager.scheduleTasks().get(ConvertToRawIndexTask.TASK_TYPE));

    // Wait at most 600 seconds for all tasks COMPLETED and new segments refreshed
    TestUtils.waitForCondition(input -> {
      // Check task state
      for (TaskState taskState : _helixTaskResourceManager.getTaskStates(ConvertToRawIndexTask.TASK_TYPE).values()) {
        if (taskState != TaskState.COMPLETED) {
          return false;
        }
      }

      // Check segment ZK metadata
      for (SegmentZKMetadata segmentZKMetadata : _helixResourceManager.getSegmentsZKMetadata(offlineTableName)) {
        Map<String, String> customMap = segmentZKMetadata.getCustomMap();
        if (customMap == null || customMap.size() != 1 || !customMap
            .containsKey(ConvertToRawIndexTask.TASK_TYPE + MinionConstants.TASK_TIME_SUFFIX)) {
          return false;
        }
      }

      // Check segment metadata
      File[] indexDirs1 = tableDataDir.listFiles();
      Assert.assertNotNull(indexDirs1);
      for (File indexDir : indexDirs1) {
        SegmentMetadata segmentMetadata;

        // Segment metadata file might not exist if the segment is refreshing
        try {
          segmentMetadata = new SegmentMetadataImpl(indexDir);
        } catch (Exception e) {
          return false;
        }

        // The columns in COLUMNS_TO_CONVERT should have raw index
        List<String> rawIndexColumns = Arrays.asList(StringUtils.split(COLUMNS_TO_CONVERT, ','));
        for (String columnName : segmentMetadata.getSchema().getColumnNames()) {
          if (rawIndexColumns.contains(columnName)) {
            if (segmentMetadata.getColumnMetadataFor(columnName).hasDictionary()) {
              return false;
            }
          } else {
            if (!segmentMetadata.getColumnMetadataFor(columnName).hasDictionary()) {
              return false;
            }
          }
        }
      }

      return true;
    }, 600_000L, ""Failed to get all tasks COMPLETED and new segments refreshed"");
  }
",non-flaky,5
104692,apache_pinot,ConvertToRawIndexMinionClusterIntegrationTest.testPinotHelixResourceManagerAPIs,"  @Test
  public void testPinotHelixResourceManagerAPIs() {
    // Instance APIs
    Assert.assertEquals(_helixResourceManager.getAllInstances().size(), 5);
    Assert.assertEquals(_helixResourceManager.getOnlineInstanceList().size(), 5);
    Assert.assertEquals(_helixResourceManager.getOnlineUnTaggedBrokerInstanceList().size(), 0);
    Assert.assertEquals(_helixResourceManager.getOnlineUnTaggedServerInstanceList().size(), 0);

    // Table APIs
    String rawTableName = getTableName();
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(rawTableName);
    String realtimeTableName = TableNameBuilder.REALTIME.tableNameWithType(rawTableName);
    List<String> tableNames = _helixResourceManager.getAllTables();
    Assert.assertEquals(tableNames.size(), 2);
    Assert.assertTrue(tableNames.contains(offlineTableName));
    Assert.assertTrue(tableNames.contains(realtimeTableName));
    Assert.assertEquals(_helixResourceManager.getAllRawTables(), Collections.singletonList(rawTableName));
    Assert.assertEquals(_helixResourceManager.getAllRealtimeTables(), Collections.singletonList(realtimeTableName));

    // Tenant APIs
    Assert.assertEquals(_helixResourceManager.getAllBrokerTenantNames(), Collections.singleton(""TestTenant""));
    Assert.assertEquals(_helixResourceManager.getAllServerTenantNames(), Collections.singleton(""TestTenant""));
  }
",non-flaky,5
104693,apache_pinot,BaseClusterIntegrationTestSet.testHardcodedQueries," * <p>To enable the test, override it and add @Test annotation.
  public void testHardcodedQueries()
      throws Exception {
    // Here are some sample queries.
    String query;
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch = 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch <> 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch > 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch >= 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch < 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT MAX(ArrTime), MIN(ArrTime) FROM mytable WHERE DaysSinceEpoch >= 16312"";
    testQuery(query, Arrays.asList(""SELECT MAX(ArrTime) FROM mytable WHERE DaysSinceEpoch >= 15312"",
        ""SELECT MIN(ArrTime) FROM mytable WHERE DaysSinceEpoch >= 15312""));
    query =
        ""SELECT SUM(TotalAddGTime) FROM mytable WHERE DivArrDelay NOT IN (67, 260) AND Carrier IN ('F9', 'B6') OR ""
            + ""DepTime BETWEEN 2144 AND 1926"";
    testQuery(query, Collections.singletonList(query));
  }
",non-flaky,5
104694,apache_pinot,LLCRealtimeClusterIntegrationTest.testConsumerDirectoryExists,"  @Test
  public void testConsumerDirectoryExists() {
    File consumerDirectory = new File(CONSUMER_DIRECTORY, ""mytable_REALTIME"");
    assertEquals(consumerDirectory.exists(), _isConsumerDirConfigured,
        ""The off heap consumer directory does not exist"");
  }
",non-flaky,5
104695,apache_pinot,LLCRealtimeClusterIntegrationTest.testSegmentFlushSize,"  @Test
  public void testSegmentFlushSize() {
    String realtimeTableName = TableNameBuilder.REALTIME.tableNameWithType(getTableName());
    List<SegmentZKMetadata> segmentsZKMetadata =
        ZKMetadataProvider.getSegmentsZKMetadata(_propertyStore, realtimeTableName);
    for (SegmentZKMetadata segmentZKMetadata : segmentsZKMetadata) {
      assertEquals(segmentZKMetadata.getSizeThresholdToFlushSegment(),
          getRealtimeSegmentFlushSize() / getNumKafkaPartitions());
    }
  }
",non-flaky,5
104696,apache_pinot,LLCRealtimeClusterIntegrationTest.testInvertedIndexTriggering,"  @Test
  public void testInvertedIndexTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();

    JsonNode queryResponse = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
    assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertTrue(queryResponse.get(""numEntriesScannedInFilter"").asLong() > 0L);

    TableConfig tableConfig = getRealtimeTableConfig();
    tableConfig.getIndexingConfig().setInvertedIndexColumns(UPDATED_INVERTED_INDEX_COLUMNS);
    updateTableConfig(tableConfig);
    reloadRealtimeTable(getTableName());

    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse1 = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
        // Total docs should not change during reload
        assertEquals(queryResponse1.get(""totalDocs"").asLong(), numTotalDocs);
        assertEquals(queryResponse1.get(""numConsumingSegmentsQueried"").asLong(), 2);
        assertTrue(queryResponse1.get(""minConsumingFreshnessTimeMs"").asLong() > _startTime);
        assertTrue(queryResponse1.get(""minConsumingFreshnessTimeMs"").asLong() < System.currentTimeMillis());
        return queryResponse1.get(""numEntriesScannedInFilter"").asLong() == 0;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to generate inverted index"");
  }
",non-flaky,5
104697,apache_pinot,LLCRealtimeClusterIntegrationTest.testAddHLCTableShouldFail,"  @Test(expectedExceptions = IOException.class)
  public void testAddHLCTableShouldFail()
      throws IOException {
    TableConfig tableConfig = new TableConfigBuilder(TableType.REALTIME).setTableName(""testTable"")
        .setStreamConfigs(Collections.singletonMap(""stream.kafka.consumer.type"", ""HIGHLEVEL"")).build();
    sendPostRequest(_controllerRequestURLBuilder.forTableCreate(), tableConfig.toJsonString());
  }
",non-flaky,5
104698,apache_pinot,LLCRealtimeClusterIntegrationTest.testReload,"  @Test
  public void testReload()
      throws Exception {
    testReload(false);
  }
",non-flaky,5
104699,apache_pinot,LLCRealtimeClusterIntegrationTest.testHardcodedServerPartitionedSqlQueries,"  @Test
  public void testHardcodedServerPartitionedSqlQueries()
      throws Exception {
    super.testHardcodedServerPartitionedSqlQueries();
  }
",non-flaky,5
104700,apache_pinot,SegmentWriterUploaderIntegrationTest.testFileBasedSegmentWriterAndDefaultUploader,"  @Test
  public void testFileBasedSegmentWriterAndDefaultUploader()
      throws Exception {

    TableConfig offlineTableConfig = createOfflineTableConfig();
    addTableConfig(offlineTableConfig);

    SegmentWriter segmentWriter = new FileBasedSegmentWriter();
    segmentWriter.init(offlineTableConfig, _schema);
    SegmentUploader segmentUploader = new SegmentUploaderDefault();
    segmentUploader.init(offlineTableConfig);

    GenericRow reuse = new GenericRow();
    long totalDocs = 0;
    for (int i = 0; i < 3; i++) {
      AvroRecordReader avroRecordReader = new AvroRecordReader();
      avroRecordReader.init(_avroFiles.get(i), null, null);

      long numDocsInSegment = 0;
      while (avroRecordReader.hasNext()) {
        avroRecordReader.next(reuse);
        segmentWriter.collect(reuse);
        numDocsInSegment++;
        totalDocs++;
      }
      // flush to segment
      URI segmentTarURI = segmentWriter.flush();
      // upload
      segmentUploader.uploadSegment(segmentTarURI, null);

      // check num segments
      Assert.assertEquals(getNumSegments(), i + 1);
      // check numDocs in latest segment
      Assert.assertEquals(getNumDocsInLatestSegment(), numDocsInSegment);
      // check totalDocs in query
      checkTotalDocsInQuery(totalDocs);
    }
    segmentWriter.close();

    dropAllSegments(_tableNameWithType, TableType.OFFLINE);
    checkNumSegments(0);

    // upload all together using dir
    segmentUploader.uploadSegmentsFromDir(_tarDir.toURI(), null);
    // check num segments
    Assert.assertEquals(getNumSegments(), 3);
    // check totalDocs in query
    checkTotalDocsInQuery(totalDocs);

    dropOfflineTable(_tableNameWithType);
  }
",non-flaky,5
104701,apache_pinot,BasicAuthRealtimeIntegrationTest.testSegmentUploadDownload,"  @Test
  public void testSegmentUploadDownload()
      throws Exception {
    final Request query = new Request(""sql"", ""SELECT count(*) FROM "" + getTableName());

    ResultSetGroup resultBeforeOffline = getPinotConnection().execute(query);
    Assert.assertTrue(resultBeforeOffline.getResultSet(0).getLong(0) > 0);

    // schedule offline segment generation
    Assert.assertNotNull(_controllerStarter.getTaskManager().scheduleTasks());

    // wait for offline segments
    JsonNode offlineSegments = TestUtils.waitForResult(() -> {
      JsonNode segmentSets = JsonUtils.stringToJsonNode(
          sendGetRequest(_controllerRequestURLBuilder.forSegmentListAPI(getTableName()), AUTH_HEADER));
      JsonNode currentOfflineSegments =
          new IntRange(0, segmentSets.size()).stream().map(segmentSets::get).filter(s -> s.has(""OFFLINE""))
              .map(s -> s.get(""OFFLINE"")).findFirst().get();
      Assert.assertFalse(currentOfflineSegments.isEmpty());
      return currentOfflineSegments;
    }, 30000);

    // Verify constant row count
    ResultSetGroup resultAfterOffline = getPinotConnection().execute(query);
    Assert.assertEquals(resultBeforeOffline.getResultSet(0).getLong(0), resultAfterOffline.getResultSet(0).getLong(0));

    // download and sanity-check size of offline segment(s)
    for (int i = 0; i < offlineSegments.size(); i++) {
      String segment = offlineSegments.get(i).asText();
      Assert.assertTrue(
          sendGetRequest(_controllerRequestURLBuilder.forSegmentDownload(getTableName(), segment), AUTH_HEADER).length()
              > 200000); // download segment
    }
  }
",non-flaky,5
104702,apache_pinot,ServerStarterIntegrationTest.testDefaultServerConf,"  @Test
  public void testDefaultServerConf()
      throws Exception {
    String expectedHost = NetUtils.getHostAddress();
    String expectedInstanceId = PREFIX_OF_SERVER_INSTANCE + expectedHost + ""_"" + DEFAULT_SERVER_NETTY_PORT;

    verifyInstanceConfig(new PinotConfiguration(), expectedInstanceId, expectedHost, DEFAULT_SERVER_NETTY_PORT);
  }
",non-flaky,5
104703,apache_pinot,ServerStarterIntegrationTest.testSetInstanceIdToHostname,"  @Test
  public void testSetInstanceIdToHostname()
      throws Exception {
    String expectedHost = NetUtils.getHostnameOrAddress();
    String expectedInstanceId = PREFIX_OF_SERVER_INSTANCE + expectedHost + ""_"" + DEFAULT_SERVER_NETTY_PORT;

    Map<String, Object> properties = new HashMap<>();
    properties.put(SET_INSTANCE_ID_TO_HOSTNAME_KEY, true);

    verifyInstanceConfig(new PinotConfiguration(properties), expectedInstanceId, expectedHost,
        DEFAULT_SERVER_NETTY_PORT);
  }
",non-flaky,5
104704,apache_pinot,ServerStarterIntegrationTest.testCustomInstanceId,"  @Test
  public void testCustomInstanceId()
      throws Exception {
    Map<String, Object> properties = new HashMap<>();
    properties.put(CONFIG_OF_INSTANCE_ID, CUSTOM_INSTANCE_ID);

    verifyInstanceConfig(new PinotConfiguration(properties), CUSTOM_INSTANCE_ID, NetUtils.getHostAddress(),
        DEFAULT_SERVER_NETTY_PORT);
  }
",non-flaky,5
104705,apache_pinot,ServerStarterIntegrationTest.testCustomHost,"  @Test
  public void testCustomHost()
      throws Exception {
    String expectedInstanceId = PREFIX_OF_SERVER_INSTANCE + CUSTOM_HOST + ""_"" + DEFAULT_SERVER_NETTY_PORT;

    Map<String, Object> properties = new HashMap<>();
    properties.put(KEY_OF_SERVER_NETTY_HOST, CUSTOM_HOST);

    verifyInstanceConfig(new PinotConfiguration(properties), expectedInstanceId, CUSTOM_HOST,
        DEFAULT_SERVER_NETTY_PORT);
  }
",non-flaky,5
104706,apache_pinot,ServerStarterIntegrationTest.testCustomPort,"  @Test
  public void testCustomPort()
      throws Exception {
    String expectedHost = NetUtils.getHostAddress();
    String expectedInstanceId = PREFIX_OF_SERVER_INSTANCE + expectedHost + ""_"" + CUSTOM_PORT;

    Map<String, Object> properties = new HashMap<>();
    properties.put(KEY_OF_SERVER_NETTY_PORT, CUSTOM_PORT);

    verifyInstanceConfig(new PinotConfiguration(properties), expectedInstanceId, expectedHost, CUSTOM_PORT);
  }
",non-flaky,5
104707,apache_pinot,ServerStarterIntegrationTest.testAllCustomServerConf,"  @Test
  public void testAllCustomServerConf()
      throws Exception {
    Map<String, Object> properties = new HashMap<>();
    properties.put(CONFIG_OF_INSTANCE_ID, CUSTOM_INSTANCE_ID);
    properties.put(KEY_OF_SERVER_NETTY_HOST, CUSTOM_HOST);
    properties.put(KEY_OF_SERVER_NETTY_PORT, CUSTOM_PORT);
    verifyInstanceConfig(new PinotConfiguration(properties), CUSTOM_INSTANCE_ID, CUSTOM_HOST, CUSTOM_PORT);
  }
",non-flaky,5
104708,apache_pinot,JsonPathClusterIntegrationTest.testPqlQueries,"  @Test
  public void testPqlQueries()
      throws Exception {

    //Selection Query
    String pqlQuery = ""Select "" + MY_MAP_STR_FIELD_NAME + "" from "" + DEFAULT_TABLE_NAME;
    JsonNode pinotResponse = postQuery(pqlQuery);
    ArrayNode selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }

    //Filter Query
    pqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING') from "" + DEFAULT_TABLE_NAME
        + ""  where jsonExtractScalar(myMapStr,'$.k1','STRING') = 'value-k1-0'"";
    pinotResponse = postQuery(pqlQuery);
    selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertEquals(value, ""value-k1-0"");
    }
    pqlQuery =
        ""Select "" + MY_MAP_STR_K1_FIELD_NAME + "" from "" + DEFAULT_TABLE_NAME + ""  where "" + MY_MAP_STR_K1_FIELD_NAME
            + "" = 'value-k1-0'"";
    pinotResponse = postQuery(pqlQuery);
    selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertEquals(value, ""value-k1-0"");
    }

    //selection order by
    pqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING') from "" + DEFAULT_TABLE_NAME
        + "" order by jsonExtractScalar(myMapStr,'$.k1','STRING')"";
    pinotResponse = postQuery(pqlQuery);
    selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }
    pqlQuery =
        ""Select "" + MY_MAP_STR_K1_FIELD_NAME + "" from "" + DEFAULT_TABLE_NAME + "" order by "" + MY_MAP_STR_K1_FIELD_NAME;
    pinotResponse = postQuery(pqlQuery);
    selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }

    //Group By Query
    pqlQuery = ""Select count(*) from "" + DEFAULT_TABLE_NAME + "" group by jsonExtractScalar(myMapStr,'$.k1','STRING')"";
    pinotResponse = postQuery(pqlQuery);
    Assert.assertNotNull(pinotResponse.get(""aggregationResults""));
    JsonNode groupByResult = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    Assert.assertNotNull(groupByResult);
    Assert.assertTrue(groupByResult.isArray());
    Assert.assertFalse(groupByResult.isEmpty());

    pqlQuery = ""Select count(*) from "" + DEFAULT_TABLE_NAME + "" group by "" + MY_MAP_STR_K1_FIELD_NAME;
    pinotResponse = postQuery(pqlQuery);
    Assert.assertNotNull(pinotResponse.get(""aggregationResults""));
    groupByResult = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    Assert.assertNotNull(groupByResult);
    Assert.assertTrue(groupByResult.isArray());
    Assert.assertFalse(groupByResult.isEmpty());
  }
",non-flaky,5
104709,apache_pinot,JsonPathClusterIntegrationTest.testSqlQueries,"  @Test
  public void testSqlQueries()
      throws Exception {
    //Selection Query
    String sqlQuery = ""Select myMapStr from "" + DEFAULT_TABLE_NAME;
    JsonNode pinotResponse = postSqlQuery(sqlQuery);
    ArrayNode rows = (ArrayNode) pinotResponse.get(""resultTable"").get(""rows"");
    Assert.assertNotNull(rows);
    Assert.assertFalse(rows.isEmpty());
    for (int i = 0; i < rows.size(); i++) {
      String value = rows.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }

    //Filter Query
    sqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING') from "" + DEFAULT_TABLE_NAME
        + ""  where jsonExtractScalar(myMapStr,'$.k1','STRING') = 'value-k1-0'"";
    pinotResponse = postSqlQuery(sqlQuery);
    rows = (ArrayNode) pinotResponse.get(""resultTable"").get(""rows"");
    Assert.assertNotNull(rows);
    Assert.assertFalse(rows.isEmpty());
    for (int i = 0; i < rows.size(); i++) {
      String value = rows.get(i).get(0).textValue();
      Assert.assertEquals(value, ""value-k1-0"");
    }

    //selection order by
    sqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING') from "" + DEFAULT_TABLE_NAME
        + "" order by jsonExtractScalar(myMapStr,'$.k1','STRING')"";
    pinotResponse = postSqlQuery(sqlQuery);
    rows = (ArrayNode) pinotResponse.get(""resultTable"").get(""rows"");
    Assert.assertNotNull(rows);
    Assert.assertFalse(rows.isEmpty());
    for (int i = 0; i < rows.size(); i++) {
      String value = rows.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }

    //Group By Query
    sqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING'), count(*) from "" + DEFAULT_TABLE_NAME
        + "" group by jsonExtractScalar(myMapStr,'$.k1','STRING')"";
    pinotResponse = postSqlQuery(sqlQuery);
    Assert.assertNotNull(pinotResponse.get(""resultTable""));
    rows = (ArrayNode) pinotResponse.get(""resultTable"").get(""rows"");
    for (int i = 0; i < rows.size(); i++) {
      String value = rows.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }
  }
",non-flaky,5
119,vert-x3_vertx-mongo-client,MongoClientTest.testWatch,"@Test
public void testWatch() throws Exception {
    final JsonArray operationTypes = new JsonArray(Arrays.asList(""insert"", ""update"", ""replace"", ""delete""));
    final JsonObject match = new JsonObject().put(""operationType"", new JsonObject().put(""$in"", operationTypes));
    final JsonArray pipeline = new JsonArray().add(new JsonObject().put(""$match"", match));
    final JsonObject fields = new JsonObject().put(""operationType"", true).put(""namespaceDocument"", true).put(""destinationNamespaceDocument"", true).put(""documentKey"", true).put(""updateDescription"", true).put(""fullDocument"", true);
    pipeline.add(new JsonObject().put(""$project"", fields));
    final String collection = randomCollection();
    final JsonObject doc = createDoc();
    final CountDownLatch latch = new CountDownLatch(4);
    final AtomicReference<ReadStream<ChangeStreamDocument<JsonObject>>> streamReference = new AtomicReference<>();
    mongoClient.createCollection(collection, onSuccess(( res) -> {
        ReadStream<ChangeStreamDocument<JsonObject>> stream = mongoClient.watch(collection, pipeline, true, 1).handler(( changeStreamDocument) -> {
            OperationType operationType = changeStreamDocument.getOperationType();
            assertNotNull(operationType);
            JsonObject fullDocument = changeStreamDocument.getFullDocument();
            switch (operationType.getValue()) {
                case ""insert"" :
                assertNotNull(fullDocument);
                assertNotNull(fullDocument.getString(MongoClientUpdateResult.ID_FIELD));
                assertEquals(""bar"", fullDocument.getString(""foo""));
                break;
                case ""update"" :
                assertNotNull(fullDocument);
                assertEquals(""updatedValue"", fullDocument.getString(""fieldToUpdate""));
                break;
                case ""replace"" :
                assertNotNull(fullDocument);
                assertEquals(""replacedValue"", fullDocument.getString(""fieldToReplace""));
                break;
                case ""delete"" :
                assertNull(fullDocument);
                break;
                default :
            }
            latch.countDown();
            if (latch.getCount() == 1) {
                mongoClient.removeDocuments(collection, new JsonObject());
            }
        }).endHandler(( v) -> assertEquals(0, latch.getCount())).exceptionHandler(this::fail).fetch(1);
        streamReference.set(stream);
        vertx.setTimer(50, ( v) -> {
            mongoClient.insert(collection, doc).compose(( idString) -> {
                doc.put(MongoClientUpdateResult.ID_FIELD, idString);
                doc.put(""fieldToUpdate"", ""updatedValue"");
                final JsonObject query = new JsonObject().put(MongoClientUpdateResult.ID_FIELD, idString);
                final JsonObject updateField = new JsonObject().put(""fieldToUpdate"", ""updatedValue"");
                return CompositeFuture.all(mongoClient.updateCollection(collection, query, new JsonObject().put(""$set"", updateField)), mongoClient.save(collection, doc.put(""fieldToReplace"", ""replacedValue"")));
            });
        });
    }));
    awaitLatch(latch);
    streamReference.get().handler(null);
}",time,2
98010,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testSavePreexistingLongID,"  @Test
  public void testSavePreexistingLongID() throws Exception {
    //Override this test as it does not make sense for useObjectId = true
    assertTrue(true);
    testComplete();
    await();
  }
",non-flaky,5
98011,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testFindOneReturnsStringId,"  @Test
  public void testFindOneReturnsStringId() throws Exception {
    String collection = randomCollection();
    mongoClient.createCollection(collection, onSuccess(res -> {
      JsonObject orig = createDoc();
      JsonObject doc = orig.copy();
      mongoClient.insert(collection, doc, onSuccess(id -> {
        assertNotNull(id);
        mongoClient.findOne(collection, new JsonObject().put(""foo"", ""bar""), null, onSuccess(obj -> {
          assertTrue(obj.containsKey(""_id""));
          assertTrue(obj.getValue(""_id"") instanceof String);
          obj.remove(""_id"");
          assertEquals(orig, obj);
          testComplete();
        }));
      }));
    }));
    await();
  }
",non-flaky,5
98012,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testFindOneReturnsNothing,"  @Test
  public void testFindOneReturnsNothing() throws Exception {
    String collection = randomCollection();
    mongoClient.createCollection(collection, onSuccess(res -> {
      JsonObject orig = createDoc();
      JsonObject doc = orig.copy();
      mongoClient.insert(collection, doc, onSuccess(id -> {
        assertNotNull(id);
        mongoClient.findOne(collection, new JsonObject().put(""nothing"", ""xxrandomxx""), null, onSuccess(obj -> {
          assertNull(obj);
          testComplete();
        }));
      }));
    }));
    await();
  }
",non-flaky,5
98013,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testFindReturnsStringId,"  @Test
  public void testFindReturnsStringId() throws Exception {
    String collection = randomCollection();
    mongoClient.createCollection(collection, onSuccess(res -> {
      JsonObject orig = createDoc();
      JsonObject doc = orig.copy();
      mongoClient.insert(collection, doc, onSuccess(id -> {
        assertNotNull(id);
        mongoClient.find(collection, new JsonObject().put(""foo"", ""bar""), onSuccess(list -> {
          assertTrue(list.size() == 1);
          JsonObject obj = list.get(0);
          assertTrue(obj.containsKey(""_id""));
          assertTrue(obj.getValue(""_id"") instanceof String);
          obj.remove(""_id"");
          assertEquals(orig, obj);
          testComplete();
        }));
      }));
    }));
    await();
  }
",non-flaky,5
98014,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testInsertPreexistingObjectID,"  @Test
  public void testInsertPreexistingObjectID() throws Exception {
    String collection = randomCollection();
    mongoClient.createCollection(collection, onSuccess(res -> {
      JsonObject doc = createDoc();
      //Changed to hex string as a random string will not be valid for useObjectId = true
      doc.put(""_id"", new ObjectId().toHexString());
      mongoClient.insertWithOptions(collection, doc, ACKNOWLEDGED, onSuccess(id -> {
        assertNull(id);
        testComplete();
      }));
    }));
    await();
  }
",non-flaky,5
98015,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testInsertPreexistingID,"  @Test
  public void testInsertPreexistingID() throws Exception {
    String collection = randomCollection();
    mongoClient.createCollection(collection, onSuccess(res -> {
      JsonObject doc = createDoc();
      //Changed to hex string as a random string will not be valid for useObjectId = true
      doc.put(""_id"", new ObjectId().toHexString());
      mongoClient.insert(collection, doc, onSuccess(id -> {
        assertNull(id);
        testComplete();
      }));
    }));
    await();
  }
",non-flaky,5
98016,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testInsertRetrieve,"  @Test
  public void testInsertRetrieve() throws Exception {
    String collection = randomCollection();
    mongoClient.createCollection(collection, onSuccess(res -> {
      JsonObject doc = createDoc();
      doc.put(""_id"", new ObjectId().toHexString());
      mongoClient.insert(collection, doc, onSuccess(id -> {
        assertNull(id);
        mongoClient.findOne(collection, new JsonObject(), null, onSuccess(retrieved -> {
          assertEquals(doc, retrieved);
          testComplete();
        }));
      }));
    }));
    await();
  }
",non-flaky,5
98017,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testSavePreexistingObjectID,"  @Test
  public void testSavePreexistingObjectID() throws Exception {
    String collection = randomCollection();
    mongoClient.createCollection(collection, onSuccess(res -> {
      JsonObject doc = createDoc();
      //Changed to hex string as a random string will not be valid for useObjectId = true
      doc.put(""_id"", new ObjectId().toHexString());
      mongoClient.saveWithOptions(collection, doc, ACKNOWLEDGED, onSuccess(id -> {
        assertNull(id);
        testComplete();
      }));
    }));
    await();
  }
",non-flaky,5
98018,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testInsertAlreadyExists,"  @Test
  public void testInsertAlreadyExists() throws Exception {
    String collection = randomCollection();
    mongoClient.createCollection(collection, onSuccess(res -> {
      JsonObject doc = createDoc();
      mongoClient.insert(collection, doc, onSuccess(id -> {
        assertNotNull(id);
        doc.put(""_id"", id);
        mongoClient.insert(collection, doc, response -> {
          assertFalse(response.succeeded());
          testComplete();
        });
      }));
    }));
    await();
  }
",non-flaky,5
98019,vert-x3_vertx-mongo-client,MongoClientWithObjectIdTest.testReplaceUpsert,"  @Test
  public void testReplaceUpsert() {
    String collection = randomCollection();
    JsonObject doc = createDoc();
    mongoClient.insert(collection, doc, onSuccess(id -> {
      assertNotNull(id);
      JsonObject replacement = createDoc();
      replacement.put(""replacement"", true);
      mongoClient.replaceDocumentsWithOptions(collection, new JsonObject().put(""_id"", new ObjectId().toHexString()), replacement, new UpdateOptions(true), onSuccess(v -> {
        mongoClient.find(collection, new JsonObject(), onSuccess(list -> {
          assertNotNull(list);
          assertEquals(2, list.size());
          JsonObject result = null;
          for (JsonObject o : list) {
            if (o.containsKey(""replacement"")) {
              result = o;
            }
          }
          assertNotNull(result);
          testComplete();
        }));
      }));
    }));

    await();
  }
",non-flaky,5
98020,vert-x3_vertx-mongo-client,UpdateOptionsTest.testOptions,"  @Test
  public void testOptions() {
    UpdateOptions options = new UpdateOptions();

    WriteOption writeOption = ACKNOWLEDGED;
    assertEquals(options, options.setWriteOption(writeOption));
    assertEquals(writeOption, options.getWriteOption());

    boolean multi = TestUtils.randomBoolean();
    assertEquals(options, options.setMulti(multi));
    assertEquals(multi, options.isMulti());

    boolean upsert = TestUtils.randomBoolean();
    assertEquals(options, options.setUpsert(upsert));
    assertEquals(upsert, options.isUpsert());

    JsonArray arrayFilters = new JsonArray().add(new JsonObject().put(TestUtils.randomAlphaString(5), TestUtils.randomAlphaString(5)));
    assertEquals(options, options.setArrayFilters(arrayFilters));
    assertEquals(arrayFilters, options.getArrayFilters());
  }
",non-flaky,5
98021,vert-x3_vertx-mongo-client,UpdateOptionsTest.testDefaultOptions,"  @Test
  public void testDefaultOptions() {
    UpdateOptions options = new UpdateOptions();
    assertNull(options.getWriteOption());
    assertFalse(options.isMulti());
    assertFalse(options.isUpsert());
    assertNull(options.getArrayFilters());
  }
",non-flaky,5
98022,vert-x3_vertx-mongo-client,UpdateOptionsTest.testOptionsJson,"  @Test
  public void testOptionsJson() {
    JsonObject json = new JsonObject();

    WriteOption writeOption = JOURNALED;
    json.put(""writeOption"", writeOption.name());

    boolean multi = TestUtils.randomBoolean();
    json.put(""multi"", multi);

    boolean upsert = TestUtils.randomBoolean();
    json.put(""upsert"", upsert);

    JsonArray arrayFilters = new JsonArray().add(new JsonObject().put(TestUtils.randomAlphaString(5), TestUtils.randomAlphaString(5)));
    json.put(""arrayFilters"", arrayFilters);

    UpdateOptions options = new UpdateOptions(json);
    assertEquals(writeOption, options.getWriteOption());
    assertEquals(multi, options.isMulti());
    assertEquals(upsert, options.isUpsert());
    assertEquals(arrayFilters, options.getArrayFilters());
  }
",non-flaky,5
98023,vert-x3_vertx-mongo-client,UpdateOptionsTest.testDefaultOptionsJson,"  @Test
  public void testDefaultOptionsJson() {
    UpdateOptions options = new UpdateOptions(new JsonObject());
    UpdateOptions def = new UpdateOptions();
    assertEquals(def.getWriteOption(), options.getWriteOption());
    assertEquals(def.isMulti(), options.isMulti());
    assertEquals(def.isUpsert(), options.isUpsert());
    assertEquals(def.getArrayFilters(), options.getArrayFilters());
  }
",non-flaky,5
98024,vert-x3_vertx-mongo-client,UpdateOptionsTest.testCopyOptions,"  @Test
  public void testCopyOptions() {
    UpdateOptions options = new UpdateOptions();
    WriteOption writeOption = REPLICA_ACKNOWLEDGED;
    boolean multi = TestUtils.randomBoolean();
    boolean upsert = TestUtils.randomBoolean();
    JsonArray arrayFilters = new JsonArray().add(new JsonObject().put(TestUtils.randomAlphaString(5), TestUtils.randomAlphaString(5)));

    options.setWriteOption(writeOption);
    options.setMulti(multi);
    options.setUpsert(upsert);
    options.setArrayFilters(arrayFilters);

    UpdateOptions copy = new UpdateOptions(options);
    assertEquals(options.getWriteOption(), copy.getWriteOption());
    assertEquals(options.isMulti(), copy.isMulti());
    assertEquals(options.isUpsert(), copy.isUpsert());
    assertEquals(options.getArrayFilters(), copy.getArrayFilters());
  }
",non-flaky,5
98025,vert-x3_vertx-mongo-client,UpdateOptionsTest.testToJson,"  @Test
  public void testToJson() {
    UpdateOptions options = new UpdateOptions();
    WriteOption writeOption = MAJORITY;
    boolean multi = TestUtils.randomBoolean();
    boolean upsert = TestUtils.randomBoolean();
    JsonArray arrayFilters = new JsonArray().add(new JsonObject().put(TestUtils.randomAlphaString(5), TestUtils.randomAlphaString(5)));

    options.setWriteOption(writeOption);
    options.setMulti(multi);
    options.setUpsert(upsert);
    options.setArrayFilters(arrayFilters);

    assertEquals(options, new UpdateOptions(options.toJson()));
  }
",non-flaky,5
98026,vert-x3_vertx-mongo-client,MongoClientUpdateResultTest.testMongoClientUpdateResultStatuses,"  @Test
  public void testMongoClientUpdateResultStatuses() {
    long randomMatched = TestUtils.randomLong();
    JsonObject randomUpsertedId = randomUpsertId();
    long randomModified = TestUtils.randomLong();

    MongoClientUpdateResult mongoClientUpdateResult = new MongoClientUpdateResult(randomMatched, randomUpsertedId, randomModified);

    assertEquals(randomMatched, mongoClientUpdateResult.getDocMatched());
    assertEquals(randomUpsertedId, mongoClientUpdateResult.getDocUpsertedId());
    assertEquals(randomModified, mongoClientUpdateResult.getDocModified());
  }
",non-flaky,5
98027,vert-x3_vertx-mongo-client,MongoClientUpdateResultTest.testDefaultMongoClientUpdateResult,"  @Test
  public void testDefaultMongoClientUpdateResult() {
    MongoClientUpdateResult mongoClientUpdateResult = new MongoClientUpdateResult();

    assertEquals(MongoClientUpdateResult.DEFAULT_DOCMATCHED, mongoClientUpdateResult.getDocMatched());
    assertNull(mongoClientUpdateResult.getDocUpsertedId());
    assertEquals(MongoClientUpdateResult.DEFAULT_DOCMODIFIED, mongoClientUpdateResult.getDocModified());
  }
",non-flaky,5
98028,vert-x3_vertx-mongo-client,MongoClientUpdateResultTest.testCopyMongoClientUpdateResult,"  @Test
  public void testCopyMongoClientUpdateResult() {
    MongoClientUpdateResult mongoClientUpdateResultOrigin = new MongoClientUpdateResult(TestUtils.randomLong(),
      randomUpsertId(), TestUtils.randomLong());
    MongoClientUpdateResult mongoClientUpdateResultCopy = new MongoClientUpdateResult(mongoClientUpdateResultOrigin);

    assertEquals(mongoClientUpdateResultOrigin.getDocMatched(), mongoClientUpdateResultCopy.getDocMatched());
    assertEquals(mongoClientUpdateResultOrigin.getDocUpsertedId(), mongoClientUpdateResultCopy.getDocUpsertedId());
    assertEquals(mongoClientUpdateResultOrigin.getDocModified(), mongoClientUpdateResultCopy.getDocModified());
  }
",non-flaky,5
98029,vert-x3_vertx-mongo-client,MongoClientUpdateResultTest.testJsonMongoClientUpdateResult,"  @Test
  public void testJsonMongoClientUpdateResult() {
    properJson();

    jsonWithoutRequiredFields();
  }
",non-flaky,5
98030,vert-x3_vertx-mongo-client,MongoClientUpdateResultTest.testToJsonMongoClientUpdateResult,"  @Test
  public void testToJsonMongoClientUpdateResult() {
    JsonObject mongoClientUpdateResultJson = randomMongoClientUpdateResultJson();
    MongoClientUpdateResult mongoClientUpdateResult = new MongoClientUpdateResult(mongoClientUpdateResultJson);

    assertEquals(mongoClientUpdateResultJson, mongoClientUpdateResult.toJson());
  }
",non-flaky,5
98031,vert-x3_vertx-mongo-client,MongoClientUpdateResultTest.testMongoUpdateResultEquality,"  @Test
  public void testMongoUpdateResultEquality() {
    logicallyUnequal();

    logicallyEqual();
  }
",non-flaky,5
98032,vert-x3_vertx-mongo-client,MongoClientBulkWriteResultTest.testMongoClientBulkWriteStatuses,"  @Test
  public void testMongoClientBulkWriteStatuses() {
    long randomMatched = TestUtils.randomLong();
    long randomModified = TestUtils.randomLong();
    long randomInserted = TestUtils.randomLong();
    long randomDeleted = TestUtils.randomLong();
    List<JsonObject> upserts = randomUpsertIds();

    MongoClientBulkWriteResult mongoClientBulkWriteResult = new MongoClientBulkWriteResult(randomInserted,
        randomMatched, randomDeleted, randomModified, upserts);

    assertEquals(randomMatched, mongoClientBulkWriteResult.getMatchedCount());
    assertEquals(randomModified, mongoClientBulkWriteResult.getModifiedCount());
    assertEquals(randomInserted, mongoClientBulkWriteResult.getInsertedCount());
    assertEquals(randomDeleted, mongoClientBulkWriteResult.getDeletedCount());
    assertEquals(upserts, mongoClientBulkWriteResult.getUpserts());
  }
",non-flaky,5
98033,vert-x3_vertx-mongo-client,MongoClientBulkWriteResultTest.testDefaultMongoClientBulkWriteResult,"  @Test
  public void testDefaultMongoClientBulkWriteResult() {
    MongoClientBulkWriteResult mongoClientBulkWriteResult = new MongoClientBulkWriteResult();

    assertEquals(MongoClientBulkWriteResult.DEFAULT_MATCHED_COUNT, mongoClientBulkWriteResult.getMatchedCount());
    assertEquals(MongoClientBulkWriteResult.DEFAULT_MODIFIED_COUNT, mongoClientBulkWriteResult.getModifiedCount());
    assertEquals(MongoClientBulkWriteResult.DEFAULT_INSERTED_COUNT, mongoClientBulkWriteResult.getInsertedCount());
    assertEquals(MongoClientBulkWriteResult.DEFAULT_DELETED_COUNT, mongoClientBulkWriteResult.getDeletedCount());
    assertNull(mongoClientBulkWriteResult.getUpserts());
  }
",non-flaky,5
98034,vert-x3_vertx-mongo-client,MongoClientBulkWriteResultTest.testCopyMongoClientBulkWriteResult,"  @Test
  public void testCopyMongoClientBulkWriteResult() {
    MongoClientBulkWriteResult mongoClientBulkWriteResultOrigin = new MongoClientBulkWriteResult(TestUtils.randomLong(),
        TestUtils.randomLong(), TestUtils.randomLong(), TestUtils.randomLong(), randomUpsertIds());

    MongoClientBulkWriteResult mongoClientBulkWriteResultCopy = new MongoClientBulkWriteResult(
        mongoClientBulkWriteResultOrigin);

    assertEquals(mongoClientBulkWriteResultCopy.getMatchedCount(), mongoClientBulkWriteResultOrigin.getMatchedCount());
    assertEquals(mongoClientBulkWriteResultCopy.getModifiedCount(),
        mongoClientBulkWriteResultOrigin.getModifiedCount());
    assertEquals(mongoClientBulkWriteResultCopy.getInsertedCount(),
        mongoClientBulkWriteResultOrigin.getInsertedCount());
    assertEquals(mongoClientBulkWriteResultCopy.getDeletedCount(), mongoClientBulkWriteResultOrigin.getDeletedCount());
    assertEquals(mongoClientBulkWriteResultCopy.getUpserts(), mongoClientBulkWriteResultOrigin.getUpserts());
  }
",non-flaky,5
98035,vert-x3_vertx-mongo-client,MongoClientBulkWriteResultTest.testJsonMongoClientBulkWriteResult,"  @Test
  public void testJsonMongoClientBulkWriteResult() {
    properJson();

    jsonWithoutRequiredFields();
  }
",non-flaky,5
98036,vert-x3_vertx-mongo-client,MongoClientBulkWriteResultTest.testToJsonMongoClientBulkWriteResult,"  @Test
  public void testToJsonMongoClientBulkWriteResult() {
    JsonObject mongoClientBulkWriteResultJson = randomMongoClientBulkWriteResultJson();
    MongoClientBulkWriteResult mongoClientBulkWriteResult = new MongoClientBulkWriteResult(
        mongoClientBulkWriteResultJson);

    assertEquals(mongoClientBulkWriteResultJson, mongoClientBulkWriteResult.toJson());
  }
",non-flaky,5
98037,vert-x3_vertx-mongo-client,MongoClientBulkWriteResultTest.testMongoBulkWriteResultEquality,"  @Test
  public void testMongoBulkWriteResultEquality() {
    logicallyUnequal();

    logicallyEqual();
  }
",non-flaky,5
98038,vert-x3_vertx-mongo-client,AggregateOptionsTest.testOptions,"  @Test
  public void testOptions() {
    AggregateOptions options = new AggregateOptions();

    long maxTime = TestUtils.randomLong();
    assertEquals(options, options.setMaxTime(maxTime));
    assertEquals(maxTime, options.getMaxTime());
  }
",non-flaky,5
98039,vert-x3_vertx-mongo-client,AggregateOptionsTest.testDefaultOptions,"  @Test
  public void testDefaultOptions() {
    AggregateOptions options = new AggregateOptions();
    assertEquals(AggregateOptions.DEFAULT_MAX_TIME, options.getMaxTime());
  }
",non-flaky,5
98040,vert-x3_vertx-mongo-client,AggregateOptionsTest.testOptionsJson,"  @Test
  public void testOptionsJson() {
    JsonObject json = new JsonObject();

    long maxAwaitTime = TestUtils.randomLong();
    json.put(""maxAwaitTime"", maxAwaitTime);

    long maxTime = TestUtils.randomLong();
    json.put(""maxTime"", maxTime);

    AggregateOptions options = new AggregateOptions(json);
    assertEquals(maxTime, options.getMaxTime());
  }
",non-flaky,5
98041,vert-x3_vertx-mongo-client,AggregateOptionsTest.testDefaultOptionsJson,"  @Test
  public void testDefaultOptionsJson() {
    AggregateOptions options = new AggregateOptions(new JsonObject());
    AggregateOptions def = new AggregateOptions();
    assertEquals(def.getMaxTime(), options.getMaxTime());
  }
",non-flaky,5
98042,vert-x3_vertx-mongo-client,AggregateOptionsTest.testCopyOptions,"  @Test
  public void testCopyOptions() {
    AggregateOptions options = new AggregateOptions();
    options.setMaxTime(TestUtils.randomLong());

    AggregateOptions copy = new AggregateOptions(options);
    assertEquals(options.getMaxTime(), copy.getMaxTime());
  }
",non-flaky,5
98043,vert-x3_vertx-mongo-client,AggregateOptionsTest.testToJson,"  @Test
  public void testToJson() {
    AggregateOptions options = new AggregateOptions();
    long maxTime = TestUtils.randomPositiveLong();
    options.setMaxTime(maxTime);

    assertEquals(options, new AggregateOptions(options.toJson()));
  }
",non-flaky,5
98044,vert-x3_vertx-mongo-client,RefCountTest.testNonShared,"  @Test
  public void testNonShared() {
    LocalMap<String, Object> map = getLocalMap();
    JsonObject config = getConfig();
    MongoClient client1 = MongoClient.create(vertx, config);
    assertEquals(1, map.size());
    MongoClient client2 = MongoClient.create(vertx, config);
    assertEquals(2, map.size());
    MongoClient client3 = MongoClient.create(vertx, config);
    assertEquals(3, map.size());
    client1.close();
    assertEquals(2, map.size());
    client2.close();
    assertEquals(1, map.size());
    client3.close();
    assertWaitUntil(() -> map.size() == 0);
    assertWaitUntil(() -> getLocalMap().size() == 0);
    assertWaitUntil(() -> map != getLocalMap()); // Map has been closed
  }
",non-flaky,5
98045,vert-x3_vertx-mongo-client,RefCountTest.testSharedDefault,"  @Test
  public void testSharedDefault() throws Exception {
    LocalMap<String, Object> map = getLocalMap();
    JsonObject config = getConfig();
    MongoClient client1 = MongoClient.createShared(vertx, config);
    assertEquals(1, map.size());
    MongoClient client2 = MongoClient.createShared(vertx, config);
    assertEquals(1, map.size());
    MongoClient client3 = MongoClient.createShared(vertx, config);
    assertEquals(1, map.size());
    client1.close();
    assertEquals(1, map.size());
    client2.close();
    assertEquals(1, map.size());
    client3.close();
    assertEquals(0, map.size());
    assertNotSame(map, getLocalMap());
  }
",non-flaky,5
98046,vert-x3_vertx-mongo-client,RefCountTest.testSharedNamed,"  @Test
  public void testSharedNamed() throws Exception {
    LocalMap<String, Object> map = getLocalMap();
    JsonObject config = getConfig();
    MongoClient client1 = MongoClient.createShared(vertx, config, ""ds1"");
    assertEquals(1, map.size());
    MongoClient client2 = MongoClient.createShared(vertx, config, ""ds1"");
    assertEquals(1, map.size());
    MongoClient client3 = MongoClient.createShared(vertx, config, ""ds1"");
    assertEquals(1, map.size());

    MongoClient client4 = MongoClient.createShared(vertx, config, ""ds2"");
    assertEquals(2, map.size());
    MongoClient client5 = MongoClient.createShared(vertx, config, ""ds2"");
    assertEquals(2, map.size());
    MongoClient client6 = MongoClient.createShared(vertx, config, ""ds2"");
    assertEquals(2, map.size());

    client1.close();
    assertEquals(2, map.size());
    client2.close();
    assertEquals(2, map.size());
    client3.close();
    assertEquals(1, map.size());

    client4.close();
    assertEquals(1, map.size());
    client5.close();
    assertEquals(1, map.size());
    client6.close();
    assertEquals(0, map.size());
    assertNotSame(map, getLocalMap());
  }
",non-flaky,5
98047,vert-x3_vertx-mongo-client,GridFsTest.testDelete,"  @Test
  public void testDelete() {
    String fileName = createTempFileWithContent((1024 * 3) + 70);

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> mongoGridFsPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", mongoGridFsPromise);

    mongoGridFsPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      Promise<Void> deletePromise = Promise.promise();
      gridFsClient.get().delete(id, deletePromise);
      return deletePromise.future();
    }).onComplete(event -> {
      if (event.succeeded()) {
        testComplete();
      } else {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98048,vert-x3_vertx-mongo-client,GridFsTest.testFileUpload,"  @Test
  public void testFileUpload() {

    long fileLength = (1024 * 3) + 70;
    String fileName = createTempFileWithContent(fileLength);
    String downloadFileName = createTempFile();

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      Promise<Long> downloadPromise = Promise.promise();
      gridFsClient.get().downloadFileAs(fileName, downloadFileName, downloadPromise);
      return downloadPromise.future();
    }).compose(length -> {
      assertEquals((long)length, fileLength);
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      } else {
        testComplete();
      }
    });
    await();
  }
",non-flaky,5
98049,vert-x3_vertx-mongo-client,GridFsTest.testBigFileUpload,"  @Test
  public void testBigFileUpload() {
    String originalFileName = createTempFileWithContent((1024 * 50) + 16);
    long originalLength = new File(originalFileName).length();
    String copiedFileName = createTempFile();

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(originalFileName, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      Promise<Long> downloadPromise = Promise.promise();
      gridFsClient.get().downloadFileAs(originalFileName, copiedFileName, downloadPromise);
      return downloadPromise.future();
    }).compose(length -> {
      assertEquals(originalLength, length.longValue());
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      } else {
        testComplete();
      }
    });
    await();
  }
",non-flaky,5
98050,vert-x3_vertx-mongo-client,GridFsTest.testFileUploadWithOptions,"  @Test
  public void testFileUploadWithOptions() {

    String fileName = createTempFileWithContent((1027) + 7000);

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    JsonObject meta = new JsonObject();
    meta.put(""nick_name"", ""Puhi the eel"");

    GridFsUploadOptions options = new GridFsUploadOptions();
    options.setMetadata(meta);

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFileWithOptions(fileName, options, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98051,vert-x3_vertx-mongo-client,GridFsTest.testFindWithMetadata,"  @Test
  public void testFindWithMetadata() {
    String fileName = createTempFileWithContent((1024 * 3) + 70);

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    JsonObject meta = new JsonObject();
    meta.put(""nick_name"", ""Puhi the eel"");

    GridFsUploadOptions options = new GridFsUploadOptions();
    options.setMetadata(meta);

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFileWithOptions(fileName, options, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      Promise<List<String>> findPromise = Promise.promise();
      JsonObject query = new JsonObject().put(""metadata.nick_name"", ""Puhi the eel"");
      gridFsClient.get().findIds(query, findPromise);
      return findPromise.future();
    }).compose(list -> {
      assertTrue(list.size() > 0);
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98052,vert-x3_vertx-mongo-client,GridFsTest.testFindAllIds,"  @Test
  public void testFindAllIds() {

    String fileName = createTempFileWithContent((1024 * 3) + 70);

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      Promise<List<String>> findPromise = Promise.promise();
      gridFsClient.get().findAllIds(findPromise);
      return findPromise.future();
    }).compose(list -> {
      assertTrue(list.size() == 1);
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98053,vert-x3_vertx-mongo-client,GridFsTest.testDrop,"  @Test
  public void testDrop() {
    createTempFileWithContent((1024 * 3) + 70);

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();

  }
",non-flaky,5
98054,vert-x3_vertx-mongo-client,GridFsTest.testDownloadStream,"  @Test
  public void testDownloadStream() {
    long fileLength = (1024 * 3) + 70;
    String fileName = createTempFileWithContent(fileLength);
    String downloadFileName = createTempFile();

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createDefaultGridFsBucketService(gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      Promise<AsyncFile> openPromise = Promise.promise();
      vertx.fileSystem().open(downloadFileName, new OpenOptions().setWrite(true), openPromise);
      return openPromise.future();
    }).compose(asyncFile -> {
      Promise<Long> downloadedPromise = Promise.promise();
      gridFsClient.get().downloadByFileName(asyncFile, fileName, downloadedPromise);
      return downloadedPromise.future();
    }).compose(length -> {
      assertTrue(fileLength == length);
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();

  }
",non-flaky,5
98055,vert-x3_vertx-mongo-client,GridFsTest.testDownloadStreamById,"  @Test
  public void testDownloadStreamById() {
    long fileLength = (1027) + 7000;
    String fileName = createTempFileWithContent(fileLength);
    String downloadFileName = createTempFile();

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();
    AtomicReference<String> idCreated = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createDefaultGridFsBucketService(gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      idCreated.set(id);
      Promise<AsyncFile> openPromise = Promise.promise();
      vertx.fileSystem().open(downloadFileName, new OpenOptions().setWrite(true), openPromise);
      return openPromise.future();
    }).compose(asyncFile -> {
      Promise<Long> downloadedPromise = Promise.promise();
      gridFsClient.get().downloadById(asyncFile, idCreated.get(), downloadedPromise);
      return downloadedPromise.future();
    }).compose(length -> {
      assertTrue(fileLength == length);
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98056,vert-x3_vertx-mongo-client,GridFsTest.testDownloadStreamWithOptions,"  @Test
  public void testDownloadStreamWithOptions() {
    long fileLength = (1024 * 3) + 70;
    String fileName = createTempFileWithContent(fileLength);
    String downloadFileName = createTempFile();
    GridFsDownloadOptions options = new GridFsDownloadOptions();
    options.setRevision(GridFsDownloadOptions.DEFAULT_REVISION);

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createDefaultGridFsBucketService(gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      Promise<AsyncFile> openPromise = Promise.promise();
      vertx.fileSystem().open(downloadFileName, new OpenOptions().setWrite(true), openPromise);
      return openPromise.future();
    }).compose(asyncFile -> {
      Promise<Long> downloadedPromise = Promise.promise();
      gridFsClient.get().downloadByFileNameWithOptions(asyncFile, fileName, options, downloadedPromise);
      return downloadedPromise.future();
    }).compose(length -> {
      assertTrue(fileLength == length);
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98057,vert-x3_vertx-mongo-client,GridFsTest.testFileDownload,"  @Test
  public void testFileDownload() {
    String fileName = createTempFileWithContent(1024);

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(uploaded -> {
      Promise<Long> downloadPromise = Promise.promise();
      gridFsClient.get().downloadFile(fileName, downloadPromise);
      return downloadPromise.future();
    }).compose(length -> {
      assertEquals(1024L, length.longValue());
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();

  }
",non-flaky,5
98058,vert-x3_vertx-mongo-client,GridFsTest.testStreamUpload,"  @Test
  public void testStreamUpload() {
    String fileName = createTempFileWithContent(1024);

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<AsyncFile> openPromise = Promise.promise();
      vertx.fileSystem().open(fileName, new OpenOptions(), openPromise);
      return openPromise.future();
    }).compose(asyncFile -> {
      Promise<String> uploadedPromise = Promise.promise();
      gridFsClient.get().uploadByFileName(asyncFile, fileName, uploadedPromise);
      return uploadedPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();

  }
",non-flaky,5
98059,vert-x3_vertx-mongo-client,GridFsTest.testStreamUploadWithOptions,"  @Test
  public void testStreamUploadWithOptions() {
    String fileName = createTempFileWithContent(1024);
    GridFsUploadOptions options = new GridFsUploadOptions();
    options.setChunkSizeBytes(1024);
    options.setMetadata(new JsonObject().put(""meta_test"", ""Kamapua`a""));

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<AsyncFile> openPromise = Promise.promise();
      vertx.fileSystem().open(fileName, new OpenOptions(), openPromise);
      return openPromise.future();
    }).compose(asyncFile -> {
      Promise<String> uploadedPromise = Promise.promise();
      gridFsClient.get().uploadByFileNameWithOptions(asyncFile, fileName, options, uploadedPromise);
      return uploadedPromise.future();
    }).compose(id -> {
      assertNotNull(id);
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98060,vert-x3_vertx-mongo-client,GridFsTest.testFileDownloadAs,"  @Test
  public void testFileDownloadAs() {
    String fileName = createTempFileWithContent(1024);
    String asFileName = createTempFile();

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(uploaded -> {
      Promise<Long> downloadPromise = Promise.promise();
      gridFsClient.get().downloadFileAs(fileName, asFileName, downloadPromise);
      return downloadPromise.future();
    }).compose(length -> {
      assertEquals(1024L, length.longValue());
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98061,vert-x3_vertx-mongo-client,GridFsTest.testFileDownloadById,"  @Test
  public void testFileDownloadById() {
    String fileName = createTempFileWithContent(1024);
    String asFileName = createTempFile();

    AtomicReference<MongoGridFsClient> gridFsClient = new AtomicReference<>();

    Promise<MongoGridFsClient> gridFsClientPromise = Promise.promise();

    mongoClient.createGridFsBucketService(""fs"", gridFsClientPromise);

    gridFsClientPromise.future().compose(mongoGridFsClient -> {
      assertNotNull(mongoGridFsClient);
      gridFsClient.set(mongoGridFsClient);
      Promise<Void> dropPromise = Promise.promise();
      mongoGridFsClient.drop(dropPromise);
      return dropPromise.future();
    }).compose(dropped -> {
      Promise<String> uploadPromise = Promise.promise();
      gridFsClient.get().uploadFile(fileName, uploadPromise);
      return uploadPromise.future();
    }).compose(id -> {
      Promise<Long> downloadPromise = Promise.promise();
      gridFsClient.get().downloadFileByID(id, asFileName, downloadPromise);
      return downloadPromise.future();
    }).compose(length -> {
      assertEquals(1024L, length.longValue());
      testComplete();
      return Future.succeededFuture();
    }).onComplete(event -> {
      if (event.failed()) {
        fail(event.cause());
      }
    });
    await();
  }
",non-flaky,5
98062,vert-x3_vertx-mongo-client,MongoClientAggregateUpdateTest.testAggregateUpdateCollection,"  @Test
  public void testAggregateUpdateCollection() {
    String collection = randomCollection();
    mongoClient.insert(collection, new JsonObject().put(""price"", 10).put(""quantity"", 1), onSuccess(id -> {
      mongoClient.insert(collection, new JsonObject().put(""price"", 20).put(""quantity"", 2), onSuccess(id2 -> {
        mongoClient.insert(collection, new JsonObject().put(""price"", 30).put(""quantity"", 10), onSuccess(id3 -> {
          mongoClient.updateCollection(collection,
            // reduce price of low quantity items
            new JsonObject().put(""quantity"", new JsonObject().put(""$lte"", 2)),
            new JsonArray().add(new JsonObject().put(""$set"", new JsonObject().put(""price"", new JsonObject().put(""$subtract"", new JsonArray().add(""$price"").add(2))))),
            onSuccess(res -> {
              assertEquals(2, res.getDocModified());
              assertEquals(2, res.getDocMatched());
              testComplete();
            }));
        }));
      }));
    }));
    await();
  }
",non-flaky,5
98063,vert-x3_vertx-mongo-client,MongoClientAggregateUpdateTest.testAggregateUpdateCollectionWithOptions,"  @Test
  public void testAggregateUpdateCollectionWithOptions() {
    String collection = randomCollection();
    mongoClient.insert(collection, new JsonObject().put(""price"", 10).put(""quantity"", 1), onSuccess(id -> {
      mongoClient.insert(collection, new JsonObject().put(""price"", 20).put(""quantity"", 2), onSuccess(id2 -> {
        mongoClient.insert(collection, new JsonObject().put(""price"", 30).put(""quantity"", 10), onSuccess(id3 -> {
          mongoClient.updateCollectionWithOptions(collection,
            // reduce price of low quantity items
            new JsonObject().put(""quantity"", new JsonObject().put(""$lte"", 2)),
            new JsonArray().add(new JsonObject().put(""$set"", new JsonObject().put(""price"", new JsonObject().put(""$subtract"", new JsonArray().add(""$price"").add(2))))),
            new UpdateOptions(),onSuccess(res -> {
              assertEquals(2, res.getDocModified());
              assertEquals(2, res.getDocMatched());
              testComplete();
            }));
        }));
      }));
    }));
    await();
  }
",non-flaky,5
98064,vert-x3_vertx-mongo-client,BulkWriteOptionsTest.testEquals,"  @Test
  public void testEquals() {
    BulkWriteOptions a = new BulkWriteOptions();
    BulkWriteOptions b = new BulkWriteOptions();
    assertEquals(a, b);

    a.setWriteOption(WriteOption.ACKNOWLEDGED);
    b.setWriteOption(WriteOption.JOURNALED);
    assertNotEquals(a, b);

    a.setWriteOption(WriteOption.MAJORITY);
    b.setWriteOption(WriteOption.MAJORITY);
    assertEquals(a, b);

    a.setOrdered(true);
    b.setOrdered(false);
    assertNotEquals(a, b);

    assertNotEquals(a, null);
  }
",non-flaky,5
98065,vert-x3_vertx-mongo-client,BulkWriteOptionsTest.testHashCode,"  @Test
  public void testHashCode() {
    BulkWriteOptions a = new BulkWriteOptions()
      .setWriteOption(WriteOption.JOURNALED)
      .setOrdered(false);
    int hash = a.hashCode();

    a.setWriteOption(WriteOption.ACKNOWLEDGED);
    assertNotEquals(hash, a.hashCode());

    a.setWriteOption(WriteOption.JOURNALED);
    a.setOrdered(true);
    assertNotEquals(hash, a.hashCode());

    a.setWriteOption(WriteOption.JOURNALED);
    a.setOrdered(false);
    assertEquals(hash, a.hashCode());
  }
",non-flaky,5
98066,vert-x3_vertx-mongo-client,FindOptionsTest.testOptions,"  @Test
  public void testOptions() {
    FindOptions options = new FindOptions();

    JsonObject fields = randomJsonObject();
    assertEquals(options, options.setFields(fields));
    assertEquals(fields, options.getFields());

    JsonObject sort = randomJsonObject();
    assertEquals(options, options.setSort(sort));
    assertEquals(sort, options.getSort());

    int limit = TestUtils.randomInt();
    assertEquals(options, options.setLimit(limit));
    assertEquals(limit, options.getLimit());

    int skip = TestUtils.randomInt();
    assertEquals(options, options.setSkip(skip));
    assertEquals(skip, options.getSkip());
  }
",non-flaky,5
98067,vert-x3_vertx-mongo-client,FindOptionsTest.testDefaultOptions,"  @Test
  public void testDefaultOptions() {
    FindOptions options = new FindOptions();
    assertNotNull(options.getFields());
    assertTrue(options.getFields().isEmpty());
    assertNotNull(options.getSort());
    assertTrue(options.getSort().isEmpty());
    assertEquals(FindOptions.DEFAULT_LIMIT, options.getLimit());
    assertEquals(FindOptions.DEFAULT_SKIP, options.getSkip());
  }
",non-flaky,5
98068,vert-x3_vertx-mongo-client,FindOptionsTest.testOptionsJson,"  @Test
  public void testOptionsJson() {
    JsonObject json = new JsonObject();

    JsonObject fields = randomJsonObject();
    json.put(""fields"", fields);

    JsonObject sort = randomJsonObject();
    json.put(""sort"", sort);

    int limit = TestUtils.randomInt();
    json.put(""limit"", limit);

    int skip = TestUtils.randomInt();
    json.put(""skip"", skip);

    FindOptions options = new FindOptions(json);
    assertEquals(fields, options.getFields());
    assertEquals(sort, options.getSort());
    assertEquals(limit, options.getLimit());
    assertEquals(skip, options.getSkip());
  }
",non-flaky,5
98069,vert-x3_vertx-mongo-client,FindOptionsTest.testDefaultOptionsJson,"  @Test
  public void testDefaultOptionsJson() {
    FindOptions options = new FindOptions(new JsonObject());
    FindOptions def = new FindOptions();
    assertEquals(def.getFields(), options.getFields());
    assertEquals(def.getSort(), options.getSort());
    assertEquals(def.getLimit(), options.getLimit());
    assertEquals(def.getSkip(), options.getSkip());
  }
",non-flaky,5
98070,vert-x3_vertx-mongo-client,FindOptionsTest.testCopyOptions,"  @Test
  public void testCopyOptions() {
    FindOptions options = new FindOptions();
    JsonObject fields = randomJsonObject();
    JsonObject sort = randomJsonObject();
    int limit = TestUtils.randomInt();
    int skip = TestUtils.randomInt();
    options.setFields(fields);
    options.setSort(sort);
    options.setLimit(limit);
    options.setSkip(skip);

    FindOptions copy = new FindOptions(options);
    assertEquals(options.getFields(), copy.getFields());
    assertEquals(options.getSort(), copy.getSort());
    assertEquals(options.getLimit(), copy.getLimit());
    assertEquals(options.getSkip(), copy.getSkip());
  }
",non-flaky,5
98071,vert-x3_vertx-mongo-client,FindOptionsTest.testToJson,"  @Test
  public void testToJson() {
    FindOptions options = new FindOptions();
    JsonObject fields = randomJsonObject();
    JsonObject sort = randomJsonObject();
    int limit = TestUtils.randomPositiveInt();
    int skip = TestUtils.randomPositiveInt();
    options.setFields(fields);
    options.setSort(sort);
    options.setLimit(limit);
    options.setSkip(skip);

    assertEquals(options, new FindOptions(options.toJson()));
  }
",non-flaky,5
98072,vert-x3_vertx-mongo-client,ServerSettingsParserTest.testServerSettings,"  @Test
  public void testServerSettings() {
    long heartbeatFrequencyMS = 1234;
    long minHeartbeatFrequencyMS = heartbeatFrequencyMS / 2;
    JsonObject config = new JsonObject();
    config.put(""heartbeatFrequencyMS"", heartbeatFrequencyMS);
    config.put(""minHeartbeatFrequencyMS"", minHeartbeatFrequencyMS);

    ServerSettings settings = new ServerSettingsParser(config).settings();
    assertEquals(heartbeatFrequencyMS, settings.getHeartbeatFrequency(TimeUnit.MILLISECONDS));
    assertEquals(minHeartbeatFrequencyMS, settings.getMinHeartbeatFrequency(TimeUnit.MILLISECONDS));
  }
",non-flaky,5
98073,vert-x3_vertx-mongo-client,WriteConcernParserTest.testNoWriteConcern,"  @Test
  public void testNoWriteConcern() {
    WriteConcern wc = new WriteConcernParser(null, new JsonObject()).writeConcern();
    assertNull(wc);
  }
",non-flaky,5
98074,vert-x3_vertx-mongo-client,WriteConcernParserTest.testWriteConcern,"  @Test
  public void testWriteConcern() {
    JsonObject config = new JsonObject();
    config.put(""writeConcern"", ""ACKNOWLEDGED"");

    WriteConcern wc = new WriteConcernParser(null, config).writeConcern();
    assertNotNull(wc);
    assertEquals(WriteConcern.ACKNOWLEDGED, wc);
  }
",non-flaky,5
98075,vert-x3_vertx-mongo-client,WriteConcernParserTest.testWriteConcernCaseInsensitive,"  @Test
  public void testWriteConcernCaseInsensitive() {
    JsonObject config = new JsonObject();
    config.put(""writeConcern"", ""acknowledged"");

    WriteConcern wc = new WriteConcernParser(null, config).writeConcern();
    assertNotNull(wc);
    assertEquals(WriteConcern.ACKNOWLEDGED, wc);
  }
",non-flaky,5
98076,vert-x3_vertx-mongo-client,WriteConcernParserTest.testInvalidWriteConcern,"  @Test(expected = IllegalArgumentException.class)
  public void testInvalidWriteConcern() {
    JsonObject config = new JsonObject();
    config.put(""writeConcern"", ""foo"");

    new WriteConcernParser(null, config).writeConcern();
  }
",non-flaky,5
98077,vert-x3_vertx-mongo-client,WriteConcernParserTest.testInvalidTypeWriteConcern,"  @Test(expected = IllegalArgumentException.class)
  public void testInvalidTypeWriteConcern() {
    JsonObject config = new JsonObject();
    config.put(""writeConcern"", 123);

    new WriteConcernParser(null, config);
  }
",non-flaky,5
98078,vert-x3_vertx-mongo-client,WriteConcernParserTest.testAdvancedWriteConcern_w_int,"  @Test
  public void testAdvancedWriteConcern_w_int() {
    WriteConcern expected = new WriteConcern(3).withWTimeout(25, TimeUnit.MILLISECONDS).withJournal(true);
    JsonObject config = new JsonObject();
    config.put(""w"", 3);
    config.put(""wtimeoutMS"", 25);
    config.put(""j"", true);

    WriteConcern wc = new WriteConcernParser(null, config).writeConcern();
    assertNotNull(wc);
    assertEquals(expected, wc);
  }
",non-flaky,5
98079,vert-x3_vertx-mongo-client,WriteConcernParserTest.testAdvancedWriteConcern_w_string,"  @Test
  public void testAdvancedWriteConcern_w_string() {
    WriteConcern expected = WriteConcern.MAJORITY.withWTimeout(1, TimeUnit.MILLISECONDS).withJournal(true);
    JsonObject config = new JsonObject();
    config.put(""w"", ""majority"");
    config.put(""wtimeoutMS"", 1);
    config.put(""j"", true);

    WriteConcern wc = new WriteConcernParser(null, config).writeConcern();
    assertNotNull(wc);
    assertEquals(expected, wc);
  }
",non-flaky,5
98080,vert-x3_vertx-mongo-client,WriteConcernParserTest.testAdvancedWriteConcern_w_int_only,"  @Test
  public void testAdvancedWriteConcern_w_int_only() {
    WriteConcern expected = new WriteConcern(123);
    JsonObject config = new JsonObject();
    config.put(""w"", 123);

    WriteConcern wc = new WriteConcernParser(null, config).writeConcern();
    assertNotNull(wc);
    assertEquals(expected, wc);
  }
",non-flaky,5
98081,vert-x3_vertx-mongo-client,WriteConcernParserTest.testAdvancedWriteConcern_w_string_only,"  @Test
  public void testAdvancedWriteConcern_w_string_only() {
    WriteConcern expected = new WriteConcern(""foo"");
    JsonObject config = new JsonObject();
    config.put(""w"", ""foo"");

    WriteConcern wc = new WriteConcernParser(null, config).writeConcern();
    assertNotNull(wc);
    assertEquals(expected, wc);
  }
",non-flaky,5
98082,vert-x3_vertx-mongo-client,WriteConcernParserTest.testSimpleAndAdvancedWriteConcern,"  @Test
  public void testSimpleAndAdvancedWriteConcern() {
    WriteConcern expected = WriteConcern.JOURNALED;
    JsonObject config = new JsonObject();
    config.put(""w"", ""majority"");
    config.put(""wtimeoutMS"", 1);
    config.put(""j"", true);
    // this overwrites the other options
    config.put(""writeConcern"", ""journaled"");

    WriteConcern wc = new WriteConcernParser(null, config).writeConcern();
    assertNotNull(wc);
    assertEquals(expected, wc);
  }
",non-flaky,5
98083,vert-x3_vertx-mongo-client,WriteConcernParserTest.testInvalidWriteConcern_w_boolean,"  @Test(expected = IllegalArgumentException.class)
  public void testInvalidWriteConcern_w_boolean() {
    JsonObject config = new JsonObject();
    config.put(""w"", true);

    new WriteConcernParser(null, config).writeConcern();
  }
",non-flaky,5
98084,vert-x3_vertx-mongo-client,WriteConcernParserTest.testConnStringNoWriteConcern,"  @Test
  public void testConnStringNoWriteConcern() {
    final ConnectionString connString = new ConnectionString(""mongodb://localhost:27017/mydb?replicaSet=myapp"");
    WriteConcern rp = new WriteConcernParser(connString, new JsonObject()).writeConcern();
    assertNull(rp);
  }
",non-flaky,5
98085,vert-x3_vertx-mongo-client,WriteConcernParserTest.testConnStringWriteConcern,"  @Test
  public void testConnStringWriteConcern() {
    final ConnectionString connString = new ConnectionString(""mongodb://localhost:27017/mydb?replicaSet=myapp&safe=true"");
    WriteConcern wc = new WriteConcernParser(connString, new JsonObject()).writeConcern();

    assertNotNull(wc);
    assertEquals(WriteConcern.ACKNOWLEDGED, wc);
  }
",non-flaky,5
98086,vert-x3_vertx-mongo-client,WriteConcernParserTest.testConnStringSimpleAndAdvancedWriteConcern,"  @Test
  public void testConnStringSimpleAndAdvancedWriteConcern() {
    final ConnectionString connString = new ConnectionString(""mongodb://localhost:27017/mydb?replicaSet=myapp"" +
      ""&w=majority&wtimeoutms=20&journal=false"");
    WriteConcern expected = new WriteConcern(""majority"").withWTimeout(20, TimeUnit.MILLISECONDS).withJournal(false);
    WriteConcern wc = new WriteConcernParser(connString, new JsonObject()).writeConcern();
    assertNotNull(wc);
    assertEquals(expected, wc);
  }
",non-flaky,5
98087,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.ssl_should_be_disabled_by_default,"  @Test
  public void ssl_should_be_disabled_by_default() {
    // given
    final JsonObject configWithoutSSLInfo = new JsonObject().put(
      ""connection_string"", ""mongodb://localhost:27017/mydb?replicaSet=myRs""
    );

    // when
    final MongoClientSettings parsedSettings = new MongoClientOptionsParser(vertx, configWithoutSSLInfo).settings();

    // then
    assertFalse(parsedSettings.getSslSettings().isEnabled());
    assertFalse(parsedSettings.getSslSettings().isInvalidHostNameAllowed());
  }
",non-flaky,5
98088,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.one_should_be_able_to_enable_ssl_support_via_connection_string,"  @Test
  public void one_should_be_able_to_enable_ssl_support_via_connection_string() {
    // given
    final JsonObject withSSLEnabled = new JsonObject().put(
      ""connection_string"", ""mongodb://localhost:27017/mydb?replicaSet=myRs&ssl=true""
    );

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLEnabled).settings().getSslSettings();

    // then
    assertTrue(sslSettings.isEnabled());
  }
",non-flaky,5
98089,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.one_should_be_able_to_enable_ssl_support_via_config_property,"  @Test
  public void one_should_be_able_to_enable_ssl_support_via_config_property() {
    // given
    final JsonObject withSSLEnabled = new JsonObject().put(""ssl"", true);

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLEnabled).settings().getSslSettings();

    // then
    assertTrue(sslSettings.isEnabled());
  }
",non-flaky,5
98090,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.one_should_be_able_to_allow_invalid_host_names_via_connection_string,"  @Test
  public void one_should_be_able_to_allow_invalid_host_names_via_connection_string() {
    // given
    final JsonObject withSSLAndInvalidHostnameEnabled = new JsonObject().put(
      ""connection_string"", ""mongodb://localhost:27017/mydb?replicaSet=myRs&ssl=true&sslInvalidHostNameAllowed=true""
    );

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLAndInvalidHostnameEnabled)
      .settings()
      .getSslSettings();

    // then
    assertTrue(sslSettings.isInvalidHostNameAllowed());
  }
",non-flaky,5
98091,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.one_should_be_able_to_allow_invalid_host_names_via_config_property,"  @Test
  public void one_should_be_able_to_allow_invalid_host_names_via_config_property() {
    // given
    final JsonObject withSSLAndInvalidHostnameEnabled = new JsonObject()
      .put(""ssl"", true)
      .put(""sslInvalidHostNameAllowed"", true);

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLAndInvalidHostnameEnabled)
      .settings()
      .getSslSettings();

    // then
    assertTrue(sslSettings.isInvalidHostNameAllowed());
  }
",non-flaky,5
98092,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testTrustAllProperty,"  @Test
  public void testTrustAllProperty() {
    // given
    final JsonObject withSSLAndTrustAllEnabled = new JsonObject()
      .put(""ssl"", true)
      .put(""trustAll"", true);

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLAndTrustAllEnabled)
      .settings()
      .getSslSettings();

    // then
    assertNotNull(sslSettings.getContext());
  }
",non-flaky,5
98093,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testEmptyCaPathProperty,"  @Test
  public void testEmptyCaPathProperty() {
    // given
    final JsonObject withSSLwithoutCaPath = new JsonObject().put(""ssl"", true);

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLwithoutCaPath)
      .settings()
      .getSslSettings();

    // then
    assertNotNull(sslSettings.getContext());
  }
",non-flaky,5
98094,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testInvalidCaPathProperty,"  @Test(expected = IllegalArgumentException.class)
  public void testInvalidCaPathProperty() {
    // given
    final JsonObject withSSLAndCaPath = new JsonObject()
      .put(""ssl"", true)
      .put(""caPath"", ""notExisting.pem"");

    // then
    new MongoClientOptionsParser(vertx, withSSLAndCaPath);
  }
",non-flaky,5
98095,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testEmptyCaPemCertificate,"  @Test(expected = IllegalArgumentException.class)
  public void testEmptyCaPemCertificate() throws IOException {
    // given
    final File tmpFile = tmpFolder.newFile(""invalidCa.pem"");
    final JsonObject withSSLAndCaPath = new JsonObject()
      .put(""ssl"", true)
      .put(""caPath"", tmpFile.getAbsolutePath());

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLAndCaPath)
      .settings()
      .getSslSettings();

    // then
    assertNull(sslSettings.getContext());
  }
",non-flaky,5
98096,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testValidCaPemCertificate,"  @Test
  public void testValidCaPemCertificate() throws IOException {
    // given
    final File tmpFile = tmpFolder.newFile(""validCa.pem"");
    try (final FileWriter tmpWriter = new FileWriter(tmpFile)) {
      tmpWriter.write(""-----BEGIN CERTIFICATE-----\n"" +
        ""MIICljCCAfigAwIBAgIJAK0oe+f4DaojMAoGCCqGSM49BAMEMFkxCzAJBgNVBAYT\n"" +
        ""AkFUMQ8wDQYDVQQIDAZWaWVubmExDjAMBgNVBAoMBU5vRW52MSkwJwYDVQQLDCBO\n"" +
        ""b0VudiBSb290IENlcnRpZmljYXRlIEF1dGhvcml0eTAeFw0xNjEwMjcxNTAwNTFa\n"" +
        ""Fw00NjEwMjAxNTAwNTFaMFkxCzAJBgNVBAYTAkFUMQ8wDQYDVQQIDAZWaWVubmEx\n"" +
        ""DjAMBgNVBAoMBU5vRW52MSkwJwYDVQQLDCBOb0VudiBSb290IENlcnRpZmljYXRl\n"" +
        ""IEF1dGhvcml0eTCBmzAQBgcqhkjOPQIBBgUrgQQAIwOBhgAEAHpsMQth12N0d+aE\n"" +
        ""FIFRd8in4MTYZNSQEyQ4fuPDNq0Zb+4TXpUmedLZQJKkAQxorak8ESC/tXuQJDUL\n"" +
        ""OoKa+R6NAT4EKR1aaVVd7clC9rfGqVwGYslppycy9zsN6O4XLUiripamQF78FzRF\n"" +
        ""8wRZvkwYhzud+jpV6shgEMw3zmcwDSYKo2YwZDAdBgNVHQ4EFgQUD96n//91CReu\n"" +
        ""Cz1K0qics6aNFV0wHwYDVR0jBBgwFoAUD96n//91CReuCz1K0qics6aNFV0wEgYD\n"" +
        ""VR0TAQH/BAgwBgEB/wIBATAOBgNVHQ8BAf8EBAMCAYYwCgYIKoZIzj0EAwQDgYsA\n"" +
        ""MIGHAkFOxsApSB7fn8ZnYG/EUscn/uAkjxHsvdEkPKCC+XYCKMssW4YP2kR6gZjo\n"" +
        ""J8vaOAJZwNevBe/R9J8zMvsAWRJmWgJCAKLedGLnBuJOK9jjnKBwbVm5OIQfApMA\n"" +
        ""I2mJVnNXvS12w4DTZlP0K1t63WxsykBBTOIVXnYdPkdZvvnoAIcfA7iM\n"" +
        ""-----END CERTIFICATE-----"");
    }
    final JsonObject withSSLAndCaPath = new JsonObject()
      .put(""ssl"", true)
      .put(""caPath"", tmpFile.getAbsolutePath());

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLAndCaPath)
      .settings()
      .getSslSettings();

    // then
    assertNotNull(sslSettings.getContext());
  }
",non-flaky,5
98097,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testValidCaPemCertificateChain,"  @Test
  public void testValidCaPemCertificateChain() throws IOException {
    // given
    final File tmpFile = tmpFolder.newFile(""validCa.pem"");
    try (final FileWriter tmpWriter = new FileWriter(tmpFile)) {
      tmpWriter.write(""-----BEGIN CERTIFICATE-----\n"" +
        ""MIICljCCAfigAwIBAgIJAK0oe+f4DaojMAoGCCqGSM49BAMEMFkxCzAJBgNVBAYT\n"" +
        ""AkFUMQ8wDQYDVQQIDAZWaWVubmExDjAMBgNVBAoMBU5vRW52MSkwJwYDVQQLDCBO\n"" +
        ""b0VudiBSb290IENlcnRpZmljYXRlIEF1dGhvcml0eTAeFw0xNjEwMjcxNTAwNTFa\n"" +
        ""Fw00NjEwMjAxNTAwNTFaMFkxCzAJBgNVBAYTAkFUMQ8wDQYDVQQIDAZWaWVubmEx\n"" +
        ""DjAMBgNVBAoMBU5vRW52MSkwJwYDVQQLDCBOb0VudiBSb290IENlcnRpZmljYXRl\n"" +
        ""IEF1dGhvcml0eTCBmzAQBgcqhkjOPQIBBgUrgQQAIwOBhgAEAHpsMQth12N0d+aE\n"" +
        ""FIFRd8in4MTYZNSQEyQ4fuPDNq0Zb+4TXpUmedLZQJKkAQxorak8ESC/tXuQJDUL\n"" +
        ""OoKa+R6NAT4EKR1aaVVd7clC9rfGqVwGYslppycy9zsN6O4XLUiripamQF78FzRF\n"" +
        ""8wRZvkwYhzud+jpV6shgEMw3zmcwDSYKo2YwZDAdBgNVHQ4EFgQUD96n//91CReu\n"" +
        ""Cz1K0qics6aNFV0wHwYDVR0jBBgwFoAUD96n//91CReuCz1K0qics6aNFV0wEgYD\n"" +
        ""VR0TAQH/BAgwBgEB/wIBATAOBgNVHQ8BAf8EBAMCAYYwCgYIKoZIzj0EAwQDgYsA\n"" +
        ""MIGHAkFOxsApSB7fn8ZnYG/EUscn/uAkjxHsvdEkPKCC+XYCKMssW4YP2kR6gZjo\n"" +
        ""J8vaOAJZwNevBe/R9J8zMvsAWRJmWgJCAKLedGLnBuJOK9jjnKBwbVm5OIQfApMA\n"" +
        ""I2mJVnNXvS12w4DTZlP0K1t63WxsykBBTOIVXnYdPkdZvvnoAIcfA7iM\n"" +
        ""-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n"" +
        ""MIIE0zCCA7ugAwIBAgIQGNrRniZ96LtKIVjNzGs7SjANBgkqhkiG9w0BAQUFADCB\n"" +
        ""yjELMAkGA1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMR8wHQYDVQQL\n"" +
        ""ExZWZXJpU2lnbiBUcnVzdCBOZXR3b3JrMTowOAYDVQQLEzEoYykgMjAwNiBWZXJp\n"" +
        ""U2lnbiwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVzZSBvbmx5MUUwQwYDVQQDEzxW\n"" +
        ""ZXJpU2lnbiBDbGFzcyAzIFB1YmxpYyBQcmltYXJ5IENlcnRpZmljYXRpb24gQXV0\n"" +
        ""aG9yaXR5IC0gRzUwHhcNMDYxMTA4MDAwMDAwWhcNMzYwNzE2MjM1OTU5WjCByjEL\n"" +
        ""MAkGA1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMR8wHQYDVQQLExZW\n"" +
        ""ZXJpU2lnbiBUcnVzdCBOZXR3b3JrMTowOAYDVQQLEzEoYykgMjAwNiBWZXJpU2ln\n"" +
        ""biwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVzZSBvbmx5MUUwQwYDVQQDEzxWZXJp\n"" +
        ""U2lnbiBDbGFzcyAzIFB1YmxpYyBQcmltYXJ5IENlcnRpZmljYXRpb24gQXV0aG9y\n"" +
        ""aXR5IC0gRzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvJAgIKXo1\n"" +
        ""nmAMqudLO07cfLw8RRy7K+D+KQL5VwijZIUVJ/XxrcgxiV0i6CqqpkKzj/i5Vbex\n"" +
        ""t0uz/o9+B1fs70PbZmIVYc9gDaTY3vjgw2IIPVQT60nKWVSFJuUrjxuf6/WhkcIz\n"" +
        ""SdhDY2pSS9KP6HBRTdGJaXvHcPaz3BJ023tdS1bTlr8Vd6Gw9KIl8q8ckmcY5fQG\n"" +
        ""BO+QueQA5N06tRn/Arr0PO7gi+s3i+z016zy9vA9r911kTMZHRxAy3QkGSGT2RT+\n"" +
        ""rCpSx4/VBEnkjWNHiDxpg8v+R70rfk/Fla4OndTRQ8Bnc+MUCH7lP59zuDMKz10/\n"" +
        ""NIeWiu5T6CUVAgMBAAGjgbIwga8wDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E\n"" +
        ""BAMCAQYwbQYIKwYBBQUHAQwEYTBfoV2gWzBZMFcwVRYJaW1hZ2UvZ2lmMCEwHzAH\n"" +
        ""BgUrDgMCGgQUj+XTGoasjY5rw8+AatRIGCx7GS4wJRYjaHR0cDovL2xvZ28udmVy\n"" +
        ""aXNpZ24uY29tL3ZzbG9nby5naWYwHQYDVR0OBBYEFH/TZafC3ey78DAJ80M5+gKv\n"" +
        ""MzEzMA0GCSqGSIb3DQEBBQUAA4IBAQCTJEowX2LP2BqYLz3q3JktvXf2pXkiOOzE\n"" +
        ""p6B4Eq1iDkVwZMXnl2YtmAl+X6/WzChl8gGqCBpH3vn5fJJaCGkgDdk+bW48DW7Y\n"" +
        ""5gaRQBi5+MHt39tBquCWIMnNZBU4gcmU7qKEKQsTb47bDN0lAtukixlE0kF6BWlK\n"" +
        ""WE9gyn6CagsCqiUXObXbf+eEZSqVir2G3l6BFoMtEMze/aiCKm0oHw0LxOXnGiYZ\n"" +
        ""4fQRbxC1lfznQgUy286dUV4otp6F01vvpX1FQHKOtw5rDgb7MzVIcbidJ4vEZV8N\n"" +
        ""hnacRHr2lVz2XTIIM6RUthg/aFzyQkqFOFSDX9HoLPKsEdao7WNq\n"" +
        ""-----END CERTIFICATE-----\n"");
    }
    final JsonObject withSSLAndCaPath = new JsonObject()
      .put(""ssl"", true)
      .put(""caPath"", tmpFile.getAbsolutePath());

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLAndCaPath)
      .settings()
      .getSslSettings();

    // then
    assertNotNull(sslSettings.getContext());
  }
",non-flaky,5
98098,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testInvalidPemCertificate,"  @Test(expected = IllegalArgumentException.class)
  public void testInvalidPemCertificate() throws IOException {
    // given
    final File tmpFile = tmpFolder.newFile(""brokenCa.pem"");
    try (final FileWriter tmpWriter = new FileWriter(tmpFile)) {
      tmpWriter.write(""-----BEGIN CERTIFICATE-----\n"" +
        ""MIICljCCAfigAwIBAgI...BROKEN...xsykBBTOIVXnYdPkdZvvnoAIcfA7iM\n"" +
        ""-----END CERTIFICATE-----"");
    }
    final JsonObject withSSLAndCaPath = new JsonObject()
      .put(""ssl"", true)
      .put(""caPath"", tmpFile.getAbsolutePath());

    // then
    new MongoClientOptionsParser(vertx, withSSLAndCaPath);
  }
",non-flaky,5
98099,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testValidKeyAndCertificate,"  @Test
  public void testValidKeyAndCertificate() throws IOException {
    // given
    final File tmpKeyFile = tmpFolder.newFile(""validKey.pem"");
    try (final FileWriter tmpKeyWriter = new FileWriter(tmpKeyFile)) {
      tmpKeyWriter.write(""-----BEGIN PRIVATE KEY-----\n"" +
        ""MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDVmCecLdUZU917\n"" +
        ""hweVz4JqvZ9vZEi1rH+BG98HYfRR/h3QaobxPImZu3hzKHZ+MPbm94HunLPAVA9y\n"" +
        ""ZhvZMToNfOuD4TUPBPloBuNzwBfZk2O4CaXeG4ailVWUfm5t/l+RD/55zYKuhw1/\n"" +
        ""Vl9lcOryF2XAmPQ2F1gwEKK7wt1Ak8zw8/yeYgBv1/F+ibCMvR6FVj9ABBEfTM+o\n"" +
        ""Os4oy51otUv0h63GqYgXMJyLX7q+AGWdC3srwwLQROtkzi7y00g/YryXUoIqdXEI\n"" +
        ""7CrNL35rZXcZ5LfGRwFX9evX11PpT3OShYlsJBcFE9KMatRoIWd6xUKlxTk0yLjo\n"" +
        ""OUE2tsMJAgMBAAECggEAdewZAjqzidYpU0eLQoRcBj5GRaNiGRrxEgCnM1Y7IwFe\n"" +
        ""yG/nrEu11DASIdHXCXhS99Tx4SCWhLpkBM6m1VQ+LrAm/ppZRr+CSpJzBLaq9C5R\n"" +
        ""QYviDSu5Ow2jP+ZFZWiorlfcMLbrTRu2sfSnmkOrEpkkTh6jxTFCONcWYP8GU93D\n"" +
        ""YCA3hSH0li7CueS+GYJ1JB2Cd7buu+tOhl36AhBD96miExlgNn0YGpTJJ3I0Hb+O\n"" +
        ""lKIIQy+KK8f9TXrSeZC3OYlTtJaIr9ejspTXxIYN11EIit5MFEwnnkCglcsePjsx\n"" +
        ""qeOFRumJ5Nj5H8qyCNZ5MtzwbLkyktJzlumvnyr+AQKBgQDv/QfGKZJFeoCEWpoj\n"" +
        ""f+078JxSYyPVNXxbbr2NuN/V79hJBol87ukycz2CZkDCubIKfubc50eXDmhWCp4p\n"" +
        ""aJgl6BMhnovftYrIrGWJLwqXnwFwsKJSrJJqHlHDJDRGfUSQEWNclNeaB3Mr8W46\n"" +
        ""Zcaadeikstvka9xKA1LOCG3oIQKBgQDj2FFOxZK27KhY/9Oz1dUsPtAYYbLOor/P\n"" +
        ""Rbne3jICQStH3dnUEmWKIKrdYV1u2saw5djn3ujwB0xEXydRvRgiSF0qxYjbm9CG\n"" +
        ""TJaiHhTsQDjWkYMZaxk3gc7Yfh8DHF0wlvWpu1wMXNsCJ6jxqW2e+jSRioZICPK6\n"" +
        ""McWWmArd6QKBgDWjoHEyKXdOAhuTBJCarzOOe+IONpwY8EqfXc6nW6A9k2H/DAvY\n"" +
        ""elbEWyMiJ6deSeT+qCsHpoCkv707ck5fCmKulFgXT7wYn4Rqw+b9lKh+6Zt+X0mL\n"" +
        ""OM5vKGctWGHI7eIlgMfYnLfYom1X8QMsbE9puy3UrEFJulrwkzlpuOcBAoGAVRNV\n"" +
        ""sNsXIFSXu7uyueizU3UU0LXSRVQB2QxJDg3bkHnzBj+xcX15Cq2N/2G2uIjaPf1l\n"" +
        ""E5dpVQ70jGcXUG8SDuMEXs8pfg7dOvhoGpqu51RHpN7qm9ggr1g5+x6Ex+2UYmtL\n"" +
        ""yZfbFAasBE74x1ujQgRdEqct4sHsmFezVrro+9kCgYEAgl70mKk9yK/f7515OaO0\n"" +
        ""Y39tgVzpAG6RN1NKnY6NR5VNNemZx5jhKfk5byaYxX4XBjygD0sQ5KTpaZmoQIIX\n"" +
        ""FxuwhLRRMn6vtsEf1HexJAtRd82aL5wKS62l0AXG/CVLAygn4aSSqLrgTyFFVUR3\n"" +
        ""cASPpPIdZaKZG6q4Hmcpl58=\n"" +
        ""-----END PRIVATE KEY-----"");
    }
    final File tmpCertFile = tmpFolder.newFile(""validCert.pem"");
    try (final FileWriter tmpCertWriter = new FileWriter(tmpCertFile)) {
      tmpCertWriter.write(""-----BEGIN CERTIFICATE-----\n"" +
        ""MIICwTCCAamgAwIBAgIEBeVm4jANBgkqhkiG9w0BAQsFADARMQ8wDQYDVQQDEwZj\n"" +
        ""bGllbnQwHhcNMTgwNTI2MTEzNjUxWhcNMjEwNTI1MTEzNjUxWjARMQ8wDQYDVQQD\n"" +
        ""EwZjbGllbnQwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDVmCecLdUZ\n"" +
        ""U917hweVz4JqvZ9vZEi1rH+BG98HYfRR/h3QaobxPImZu3hzKHZ+MPbm94HunLPA\n"" +
        ""VA9yZhvZMToNfOuD4TUPBPloBuNzwBfZk2O4CaXeG4ailVWUfm5t/l+RD/55zYKu\n"" +
        ""hw1/Vl9lcOryF2XAmPQ2F1gwEKK7wt1Ak8zw8/yeYgBv1/F+ibCMvR6FVj9ABBEf\n"" +
        ""TM+oOs4oy51otUv0h63GqYgXMJyLX7q+AGWdC3srwwLQROtkzi7y00g/YryXUoIq\n"" +
        ""dXEI7CrNL35rZXcZ5LfGRwFX9evX11PpT3OShYlsJBcFE9KMatRoIWd6xUKlxTk0\n"" +
        ""yLjoOUE2tsMJAgMBAAGjITAfMB0GA1UdDgQWBBQ6xJBQsJCJdj/u0iTLYYD2qQsB\n"" +
        ""DDANBgkqhkiG9w0BAQsFAAOCAQEAfoquV375+eAGmfnlLxB30v9VhsFckrxFVpYs\n"" +
        ""XXC6h2G8MtXLpIEpgJo+4SZ4YjNwf/8m9J5j/duU8RukYanyzJdgkFFqKDBYCX7U\n"" +
        ""SD1nQP7729KnQgxtbR/+i3zkNgo7FATdkLq+HOxklNOEE24Ldenya39bsG779B9n\n"" +
        ""Sskcbq++7rMM+onDYBv6PbUKCm6nfqPspq809CLxSaUJg9+9ykut6hiyke/i7GEP\n"" +
        ""XIZHrM+mEvG00ES/zBIdV6TE0AIBP7q2MN7ylT509Ko9sUBMOZdEzikYp5GaRdiv\n"" +
        ""zG9q6rqK5COK614BwJFOD1DKV1BoDFsgugvfvm/mrc3QfIUPDA==\n"" +
        ""-----END CERTIFICATE-----"");
    }
    final JsonObject withSSLAndCertKeyPath = new JsonObject()
      .put(""ssl"", true)
      .put(""keyPath"", tmpKeyFile.getAbsolutePath())
      .put(""certPath"", tmpCertFile.getAbsolutePath());

    // when
    final SslSettings sslSettings = new MongoClientOptionsParser(vertx, withSSLAndCertKeyPath)
      .settings()
      .getSslSettings();

    // then
    assertNotNull(sslSettings.getContext());
  }
",non-flaky,5
98100,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testInvalidKey,"  @Test(expected = IllegalArgumentException.class)
  public void testInvalidKey() throws IOException {
    // given
    final File tmpKeyFile = tmpFolder.newFile(""brokenKey.pem"");
    try (final FileWriter tmpKeyWriter = new FileWriter(tmpKeyFile)) {
      tmpKeyWriter.write(""-----BEGIN CERTIFICATE-----\n"" +
        ""MIICljCCAfigAwIBAgI...BROKEN...xsykBBTOIVXnYdPkdZvvnoAIcfA7iM\n"" +
        ""-----END CERTIFICATE-----"");
    }
    final File tmpCertFile = tmpFolder.newFile(""validCert.pem"");
    try (final FileWriter tmpCertWriter = new FileWriter(tmpCertFile)) {
      tmpCertWriter.write(""-----BEGIN CERTIFICATE-----\n"" +
        ""MIICwTCCAamgAwIBAgIEBeVm4jANBgkqhkiG9w0BAQsFADARMQ8wDQYDVQQDEwZj\n"" +
        ""bGllbnQwHhcNMTgwNTI2MTEzNjUxWhcNMjEwNTI1MTEzNjUxWjARMQ8wDQYDVQQD\n"" +
        ""EwZjbGllbnQwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDVmCecLdUZ\n"" +
        ""U917hweVz4JqvZ9vZEi1rH+BG98HYfRR/h3QaobxPImZu3hzKHZ+MPbm94HunLPA\n"" +
        ""VA9yZhvZMToNfOuD4TUPBPloBuNzwBfZk2O4CaXeG4ailVWUfm5t/l+RD/55zYKu\n"" +
        ""hw1/Vl9lcOryF2XAmPQ2F1gwEKK7wt1Ak8zw8/yeYgBv1/F+ibCMvR6FVj9ABBEf\n"" +
        ""TM+oOs4oy51otUv0h63GqYgXMJyLX7q+AGWdC3srwwLQROtkzi7y00g/YryXUoIq\n"" +
        ""dXEI7CrNL35rZXcZ5LfGRwFX9evX11PpT3OShYlsJBcFE9KMatRoIWd6xUKlxTk0\n"" +
        ""yLjoOUE2tsMJAgMBAAGjITAfMB0GA1UdDgQWBBQ6xJBQsJCJdj/u0iTLYYD2qQsB\n"" +
        ""DDANBgkqhkiG9w0BAQsFAAOCAQEAfoquV375+eAGmfnlLxB30v9VhsFckrxFVpYs\n"" +
        ""XXC6h2G8MtXLpIEpgJo+4SZ4YjNwf/8m9J5j/duU8RukYanyzJdgkFFqKDBYCX7U\n"" +
        ""SD1nQP7729KnQgxtbR/+i3zkNgo7FATdkLq+HOxklNOEE24Ldenya39bsG779B9n\n"" +
        ""Sskcbq++7rMM+onDYBv6PbUKCm6nfqPspq809CLxSaUJg9+9ykut6hiyke/i7GEP\n"" +
        ""XIZHrM+mEvG00ES/zBIdV6TE0AIBP7q2MN7ylT509Ko9sUBMOZdEzikYp5GaRdiv\n"" +
        ""zG9q6rqK5COK614BwJFOD1DKV1BoDFsgugvfvm/mrc3QfIUPDA==\n"" +
        ""-----END CERTIFICATE-----"");
    }
    final JsonObject withSSLAndCertKeyPath = new JsonObject()
      .put(""ssl"", true)
      .put(""keyPath"", tmpKeyFile.getAbsolutePath())
      .put(""certPath"", tmpCertFile.getAbsolutePath());

    // then
    new MongoClientOptionsParser(vertx, withSSLAndCertKeyPath);
  }
",non-flaky,5
98101,vert-x3_vertx-mongo-client,ParsingSSLOptionsTest.testInvalidCertificate,"  @Test(expected = IllegalArgumentException.class)
  public void testInvalidCertificate() throws IOException {
    // given
    final File tmpKeyFile = tmpFolder.newFile(""validKey.pem"");
    try (final FileWriter tmpKeyWriter = new FileWriter(tmpKeyFile)) {
      tmpKeyWriter.write(""-----BEGIN PRIVATE KEY-----\n"" +
        ""MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDVmCecLdUZU917\n"" +
        ""hweVz4JqvZ9vZEi1rH+BG98HYfRR/h3QaobxPImZu3hzKHZ+MPbm94HunLPAVA9y\n"" +
        ""ZhvZMToNfOuD4TUPBPloBuNzwBfZk2O4CaXeG4ailVWUfm5t/l+RD/55zYKuhw1/\n"" +
        ""Vl9lcOryF2XAmPQ2F1gwEKK7wt1Ak8zw8/yeYgBv1/F+ibCMvR6FVj9ABBEfTM+o\n"" +
        ""Os4oy51otUv0h63GqYgXMJyLX7q+AGWdC3srwwLQROtkzi7y00g/YryXUoIqdXEI\n"" +
        ""7CrNL35rZXcZ5LfGRwFX9evX11PpT3OShYlsJBcFE9KMatRoIWd6xUKlxTk0yLjo\n"" +
        ""OUE2tsMJAgMBAAECggEAdewZAjqzidYpU0eLQoRcBj5GRaNiGRrxEgCnM1Y7IwFe\n"" +
        ""yG/nrEu11DASIdHXCXhS99Tx4SCWhLpkBM6m1VQ+LrAm/ppZRr+CSpJzBLaq9C5R\n"" +
        ""QYviDSu5Ow2jP+ZFZWiorlfcMLbrTRu2sfSnmkOrEpkkTh6jxTFCONcWYP8GU93D\n"" +
        ""YCA3hSH0li7CueS+GYJ1JB2Cd7buu+tOhl36AhBD96miExlgNn0YGpTJJ3I0Hb+O\n"" +
        ""lKIIQy+KK8f9TXrSeZC3OYlTtJaIr9ejspTXxIYN11EIit5MFEwnnkCglcsePjsx\n"" +
        ""qeOFRumJ5Nj5H8qyCNZ5MtzwbLkyktJzlumvnyr+AQKBgQDv/QfGKZJFeoCEWpoj\n"" +
        ""f+078JxSYyPVNXxbbr2NuN/V79hJBol87ukycz2CZkDCubIKfubc50eXDmhWCp4p\n"" +
        ""aJgl6BMhnovftYrIrGWJLwqXnwFwsKJSrJJqHlHDJDRGfUSQEWNclNeaB3Mr8W46\n"" +
        ""Zcaadeikstvka9xKA1LOCG3oIQKBgQDj2FFOxZK27KhY/9Oz1dUsPtAYYbLOor/P\n"" +
        ""Rbne3jICQStH3dnUEmWKIKrdYV1u2saw5djn3ujwB0xEXydRvRgiSF0qxYjbm9CG\n"" +
        ""TJaiHhTsQDjWkYMZaxk3gc7Yfh8DHF0wlvWpu1wMXNsCJ6jxqW2e+jSRioZICPK6\n"" +
        ""McWWmArd6QKBgDWjoHEyKXdOAhuTBJCarzOOe+IONpwY8EqfXc6nW6A9k2H/DAvY\n"" +
        ""elbEWyMiJ6deSeT+qCsHpoCkv707ck5fCmKulFgXT7wYn4Rqw+b9lKh+6Zt+X0mL\n"" +
        ""OM5vKGctWGHI7eIlgMfYnLfYom1X8QMsbE9puy3UrEFJulrwkzlpuOcBAoGAVRNV\n"" +
        ""sNsXIFSXu7uyueizU3UU0LXSRVQB2QxJDg3bkHnzBj+xcX15Cq2N/2G2uIjaPf1l\n"" +
        ""E5dpVQ70jGcXUG8SDuMEXs8pfg7dOvhoGpqu51RHpN7qm9ggr1g5+x6Ex+2UYmtL\n"" +
        ""yZfbFAasBE74x1ujQgRdEqct4sHsmFezVrro+9kCgYEAgl70mKk9yK/f7515OaO0\n"" +
        ""Y39tgVzpAG6RN1NKnY6NR5VNNemZx5jhKfk5byaYxX4XBjygD0sQ5KTpaZmoQIIX\n"" +
        ""FxuwhLRRMn6vtsEf1HexJAtRd82aL5wKS62l0AXG/CVLAygn4aSSqLrgTyFFVUR3\n"" +
        ""cASPpPIdZaKZG6q4Hmcpl58=\n"" +
        ""-----END PRIVATE KEY-----"");
    }
    final File tmpCertFile = tmpFolder.newFile(""brokenCert.pem"");
    try (final FileWriter tmpCertWriter = new FileWriter(tmpCertFile)) {
      tmpCertWriter.write(""-----BEGIN CERTIFICATE-----\n"" +
        ""MIICwTCCAamgAwIBA...BROKEN...FOD1DKV1BoDFsgugvfvm/mrc3QfIUPDA==\n"" +
        ""-----END CERTIFICATE-----"");
    }
    final JsonObject withSSLAndCertKeyPath = new JsonObject()
      .put(""ssl"", true)
      .put(""keyPath"", tmpKeyFile.getAbsolutePath())
      .put(""certPath"", tmpCertFile.getAbsolutePath());

    // then
    new MongoClientOptionsParser(vertx, withSSLAndCertKeyPath);
  }
",non-flaky,5
98102,vert-x3_vertx-mongo-client,SocketSettingsParserTest.testSocketSettings,"  @Test
  public void testSocketSettings() {
    int connectTimeoutMS = Math.abs(TestUtils.randomInt());
    int socketTimeoutMS = Math.abs(TestUtils.randomInt());
    int receiveBufferSize = Math.abs(TestUtils.randomInt());
    int sendBufferSize = Math.abs(TestUtils.randomInt());

    JsonObject config = new JsonObject();
    config.put(""connectTimeoutMS"", connectTimeoutMS);
    config.put(""socketTimeoutMS"", socketTimeoutMS);
    config.put(""receiveBufferSize"", receiveBufferSize);
    config.put(""sendBufferSize"", sendBufferSize);

    SocketSettings settings = new SocketSettingsParser(null, config).settings();
    assertEquals(connectTimeoutMS, settings.getConnectTimeout(TimeUnit.MILLISECONDS));
    assertEquals(socketTimeoutMS, settings.getReadTimeout(TimeUnit.MILLISECONDS));
    assertEquals(receiveBufferSize, settings.getReceiveBufferSize());
    assertEquals(sendBufferSize, settings.getSendBufferSize());
  }
",non-flaky,5
98103,vert-x3_vertx-mongo-client,ParsingStreamTypeTest.should_not_include_any_stream_type_by_default_for_backwards_compatibility,"  @Test
  public void should_not_include_any_stream_type_by_default_for_backwards_compatibility() {
    // given
    final JsonObject noStreamTypeProvided = new JsonObject().put(
      ""connection_string"", ""mongodb://localhost:27017/mydb?replicaSet=myRs""
    );

    // when
    final MongoClientSettings parsedSettings = new MongoClientOptionsParser(vertx, noStreamTypeProvided).settings();

    // then
    assertNull(parsedSettings.getStreamFactoryFactory());
  }
",non-flaky,5
98104,vert-x3_vertx-mongo-client,ParsingStreamTypeTest.should_parse_stream_type_from_config_property,"  @Test
  public void should_parse_stream_type_from_config_property(String streamTypeString, Class<StreamFactoryFactory> streamType) {
    // given
    final JsonObject cfgWithStreamTypeProvided = new JsonObject().put(""streamType"", streamTypeString);

    // when
    final MongoClientSettings parsedSettings = new MongoClientOptionsParser(vertx, cfgWithStreamTypeProvided).settings();

    // then
    assertThat(parsedSettings.getStreamFactoryFactory(), instanceOf(streamType));
  }
",non-flaky,5
98105,vert-x3_vertx-mongo-client,ParsingStreamTypeTest.only_valid_stream_type_values_allowed_as_config_property,"  @Test(expected = IllegalArgumentException.class)
  public void only_valid_stream_type_values_allowed_as_config_property() {
    // given
    final JsonObject withInvalidStreamType = new JsonObject().put(""streamType"", ""unrecognized"");

    // expect thrown
    new MongoClientOptionsParser(vertx, withInvalidStreamType).settings();
  }
",non-flaky,5
98106,vert-x3_vertx-mongo-client,CredentialListParserTest.testConnectionString,"  @Test
  public void testConnectionString() {
    String username = TestUtils.randomAlphaString(8);
    String password = TestUtils.randomAlphaString(20);

    ConnectionString connectionString = new ConnectionString(
      String.format(
        ""mongodb://%s:%s@%s/%s"",
        username,
        password,
        ""localhost:27017"",
        ""my-datasource""));

    List<MongoCredential> credentials = new CredentialListParser(connectionString, null).credentials();
    assertEquals(1, credentials.size());
    MongoCredential credential = credentials.get(0);
    assertEquals(username, credential.getUserName());
    assertArrayEquals(password.toCharArray(), credential.getPassword());
    assertEquals(""my-datasource"", credential.getSource());
  }
",non-flaky,5
98107,vert-x3_vertx-mongo-client,CredentialListParserTest.testSimpleAuth,"  @Test
  public void testSimpleAuth() {
    JsonObject config = new JsonObject().put(""db_name"", ""my-datasource"");
    String username = TestUtils.randomAlphaString(8);
    String password = TestUtils.randomAlphaString(20);
    config.put(""username"", username);
    config.put(""password"", password);


    List<MongoCredential> credentials = new CredentialListParser(null, config).credentials();
    assertEquals(1, credentials.size());
    MongoCredential credential = credentials.get(0);
    assertEquals(username, credential.getUserName());
    assertArrayEquals(password.toCharArray(), credential.getPassword());
    // default source should be the database name - see https://github.com/vert-x3/vertx-mongo-client/issues/46.
    assertEquals(""my-datasource"", credential.getSource());
  }
",non-flaky,5
98108,vert-x3_vertx-mongo-client,CredentialListParserTest.testSimpleAuthWithSource,"  @Test
  public void testSimpleAuthWithSource() {
    JsonObject config = new JsonObject();
    String username = TestUtils.randomAlphaString(8);
    String password = TestUtils.randomAlphaString(20);
    String authSource = TestUtils.randomAlphaString(10);
    config.put(""username"", username);
    config.put(""password"", password);
    config.put(""authSource"", authSource);

    List<MongoCredential> credentials = new CredentialListParser(null, config).credentials();
    assertEquals(1, credentials.size());
    MongoCredential credential = credentials.get(0);
    assertEquals(username, credential.getUserName());
    assertArrayEquals(password.toCharArray(), credential.getPassword());
    assertEquals(authSource, credential.getSource());
  }
",non-flaky,5
98109,vert-x3_vertx-mongo-client,CredentialListParserTest.testAuth_GSSAPI,"  @Test
  public void testAuth_GSSAPI() {
    JsonObject config = new JsonObject();
    String username = TestUtils.randomAlphaString(8);
    String authSource = TestUtils.randomAlphaString(10);
    config.put(""username"", username);
    config.put(""authSource"", authSource);
    config.put(""authMechanism"", ""GSSAPI"");

    List<MongoCredential> credentials = new CredentialListParser(null, config).credentials();
    assertEquals(1, credentials.size());
    MongoCredential credential = credentials.get(0);
    assertEquals(username, credential.getUserName());
    assertNotEquals(authSource, credential.getSource()); // It should ignore the source we pass in

    assertEquals(AuthenticationMechanism.GSSAPI, credential.getAuthenticationMechanism());
  }
",non-flaky,5
72,strapdata_elassandra,testTokenExpiry,"@Test
public void testTokenExpiry() throws Exception {
    ClockMock clock = ClockMock.frozen();
    TokenService tokenService = createTokenService(tokenServiceEnabledSettings, clock);
    Authentication authentication = new Authentication(new User(""joe"", ""admin""), new RealmRef(""native_realm"", ""native"", ""node1""), null);
    PlainActionFuture<Tuple<UserToken, String>> tokenFuture = new PlainActionFuture<>();
    tokenService.createUserToken(authentication, authentication, tokenFuture, Collections.emptyMap(), true);
    final UserToken token = tokenFuture.get().v1();
    mockGetTokenFromId(token);
    mockCheckTokenInvalidationFromId(token);
    authentication = token.getAuthentication();
    ThreadContext requestContext = new ThreadContext(Settings.EMPTY);
    storeTokenHeader(requestContext, tokenService.getUserTokenString(token));
    try (ThreadContext.StoredContext ignore = requestContext.newStoredContext(true)) {
        PlainActionFuture<UserToken> future = new PlainActionFuture<>();
        tokenService.getAndValidateToken(requestContext, future);
        assertAuthenticationEquals(authentication, future.get().getAuthentication());
    }
    final TimeValue defaultExpiration = TokenService.TOKEN_EXPIRATION.get(Settings.EMPTY);
    final int fastForwardAmount = randomIntBetween(1, Math.toIntExact(defaultExpiration.getSeconds()) - 5);
    try (ThreadContext.StoredContext ignore = requestContext.newStoredContext(true)) {
        clock.fastForwardSeconds(Math.toIntExact(defaultExpiration.getSeconds()) - fastForwardAmount);
        clock.rewind(TimeValue.timeValueNanos(clock.instant().getNano()));
        PlainActionFuture<UserToken> future = new PlainActionFuture<>();
        tokenService.getAndValidateToken(requestContext, future);
        assertAuthenticationEquals(authentication, future.get().getAuthentication());
    }
    assertSettingDeprecationsAndWarnings(new Setting[] { TokenService.BWC_ENABLED });
}",time,2
91412,strapdata_elassandra,PreBuiltXPackTransportClientTests.testPluginInstalled,"    @Test
    public void testPluginInstalled() {
        try (TransportClient client = new PreBuiltXPackTransportClient(Settings.EMPTY)) {
            Settings settings = client.settings();
            assertEquals(SecurityField.NAME4, NetworkModule.TRANSPORT_TYPE_SETTING.get(settings));
        }
    }
",non-flaky,5
91413,strapdata_elassandra,MonitoringWithWatcherRestIT.cleanExporters,"@TestLogging(""org.elasticsearch.client:TRACE,tracer:TRACE"")
    public void cleanExporters() throws Exception {
        Request request = new Request(""PUT"", ""/_cluster/settings"");
        request.setJsonEntity(Strings.toString(jsonBuilder().startObject()
                .startObject(""transient"")
                    .nullField(""xpack.monitoring.exporters.*"")
                .endObject().endObject()));
        adminClient().performRequest(request);
        adminClient().performRequest(new Request(""DELETE"", ""/.watch*""));
    }
",non-flaky,5
91414,strapdata_elassandra,WatchBackwardsCompatibilityIT.waitForSecuritySetup,"@TestLogging(""org.elasticsearch.client:TRACE"")
    public void waitForSecuritySetup() throws Exception {

        String masterNode = null;
        String catNodesResponse = EntityUtils.toString(
                client().performRequest(""GET"", ""/_cat/nodes?h=id,master"").getEntity(),
                StandardCharsets.UTF_8
        );
        for (String line : catNodesResponse.split(""\n"")) {
            int indexOfStar = line.indexOf('*'); // * in the node's output denotes it is master
            if (indexOfStar != -1) {
                masterNode = line.substring(0, indexOfStar).trim();
                break;
            }
        }
        assertNotNull(masterNode);
        final String masterNodeId = masterNode;

        assertBusy(() -> {
            try {
                Response nodeDetailsResponse = client().performRequest(""GET"", ""/_nodes"");
                ObjectPath path = ObjectPath.createFromResponse(nodeDetailsResponse);
                Map<String, Object> nodes = path.evaluate(""nodes"");
                assertThat(nodes.size(), greaterThanOrEqualTo(2));
                String masterVersion = null;
                for (String key : nodes.keySet()) {
                    // get the ES version number master is on
                    if (key.startsWith(masterNodeId)) {
                        masterVersion = path.evaluate(""nodes."" + key + "".version"");
                        break;
                    }
                }
                assertNotNull(masterVersion);
                final String masterTemplateVersion = masterVersion;

                Response response = client().performRequest(""GET"", ""/_cluster/state/metadata"");
                ObjectPath objectPath = ObjectPath.createFromResponse(response);
                final String mappingsPath = ""metadata.templates.security-index-template.mappings"";
                Map<String, Object> mappings = objectPath.evaluate(mappingsPath);
                assertNotNull(mappings);
                assertThat(mappings.size(), greaterThanOrEqualTo(1));
                for (String key : mappings.keySet()) {
                    String templateVersion = objectPath.evaluate(mappingsPath + ""."" + key + """" +
                            ""._meta.security-version"");
                    final Version mVersion = Version.fromString(masterTemplateVersion);
                    final Version tVersion = Version.fromString(templateVersion);
                    assertEquals(mVersion, tVersion);
                }
            } catch (Exception e) {
                throw new AssertionError(""failed to get cluster state"", e);
            }
        });

        nodes = buildNodeAndVersions();
        logger.info(""Nodes in cluster before test: bwc [{}], new [{}], master [{}]"", nodes.getBWCNodes(), nodes.getNewNodes(),
                nodes.getMaster());

        Map<String, String> params = Collections.singletonMap(""error_trace"", ""true"");
        executeAgainstMasterNode(client -> {
            // create a watch before each test, most of the time this is just overwriting...
            assertOK(client.performRequest(""PUT"", ""/_xpack/watcher/watch/my-watch"", params, entity));
            // just a check to see if we can execute a watch, purely optional
            if (randomBoolean()) {
                assertOK(client.performRequest(""POST"", ""/_xpack/watcher/watch/my-watch/_execute"", params,
                        new StringEntity(""{ \""record_execution\"" : true }"", ContentType.APPLICATION_JSON)));
            }
            if (randomBoolean()) {
                Map<String, String> ignore404Params = MapBuilder.newMapBuilder(params).put(""ignore"", ""404"").immutableMap();
                Response indexExistsResponse = client.performRequest(""HEAD"", ""/.triggered_watches"", ignore404Params);
                if (indexExistsResponse.getStatusLine().getStatusCode() == 404) {
                    logger.info(""Created triggered watches index to ensure it gets upgraded"");
                    client.performRequest(""PUT"", ""/.triggered_watches"");
                }
            }
        });

        // helping debugging output
        executeAgainstMasterNode(client -> {
            Map<String, String> filterPathParams = MapBuilder.newMapBuilder(params)
                    .put(""filter_path"", ""*.template,*.index_patterns"").immutableMap();
            Response r = client.performRequest(""GET"", ""_template/*watch*"", filterPathParams);
            logger.info(""existing watcher templates response [{}]"", EntityUtils.toString(r.getEntity(), StandardCharsets.UTF_8));
        });

        // set logging to debug
//        executeAgainstMasterNode(client -> {
//            StringEntity entity = new StringEntity(""{ \""transient\"" : { \""logger.org.elasticsearch.xpack.watcher\"" : \""TRACE\"" } }"",
//                    ContentType.APPLICATION_JSON);
//            Response response = client.performRequest(""PUT"", ""_cluster/settings"", params, entity);
//            logger.info(""cluster update settings response [{}]"", EntityUtils.toString(response.getEntity(), StandardCharsets.UTF_8));
//        });
    }
",non-flaky,5
91415,strapdata_elassandra,OpenLdapUserSearchSessionFactoryTests.init,"@TestLogging(""org.elasticsearch.xpack.core.ssl.SSLService:TRACE"")
    public void init() throws Exception {
        Path caPath = getDataPath(LDAPCACERT_PATH);
        /*
         * Prior to each test we reinitialize the socket factory with a new SSLService so that we get a new SSLContext.
         * If we re-use a SSLContext, previously connected sessions can get re-established which breaks hostname
         * verification tests since a re-established connection does not perform hostname verification.
         */
        globalSettings = Settings.builder()
            .put(""path.home"", createTempDir())
            .put(""xpack.ssl.certificate_authorities"", caPath)
            .build();
        threadPool = new TestThreadPool(""LdapUserSearchSessionFactoryTests"");
    }
",non-flaky,5
91416,strapdata_elassandra,TribeRestTestCase.compare,"    @TestGroup(enabled = true, sysProperty = ESRestTestCase.TESTS_REST)
            public int compare(RestTestCandidate o1, RestTestCandidate o2) {
                return o1.getTestPath().compareTo(o2.getTestPath());
            }
",non-flaky,5
91417,strapdata_elassandra,SessionFactoryLoadBalancingTests.init,"@TestLogging(""org.elasticsearch.xpack.security.authc.ldap.support:DEBUG"")
    public void init() throws Exception {
        threadPool = new TestThreadPool(""SessionFactoryLoadBalancingTests thread pool"");
    }
",non-flaky,5
91418,strapdata_elassandra,TokenAuthIntegTests.testExpiredTokensDeletedAfterExpiration,"    @TestLogging(""org.elasticsearch.xpack.security.authc:DEBUG"")
    public void testExpiredTokensDeletedAfterExpiration() throws Exception {
        final Client client = client().filterWithHeader(Collections.singletonMap(""Authorization"",
                UsernamePasswordToken.basicAuthHeaderValue(SecuritySettingsSource.TEST_SUPERUSER,
                        SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING)));
        SecurityClient securityClient = new SecurityClient(client);
        CreateTokenResponse response = securityClient.prepareCreateToken()
                .setGrantType(""password"")
                .setUsername(SecuritySettingsSource.TEST_USER_NAME)
                .setPassword(new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()))
                .get();

        Instant created = Instant.now();

        InvalidateTokenResponse invalidateResponse = securityClient
                .prepareInvalidateToken(response.getTokenString())
                .setType(InvalidateTokenRequest.Type.ACCESS_TOKEN)
                .get();
        assertTrue(invalidateResponse.isCreated());
        AtomicReference<String> docId = new AtomicReference<>();
        assertBusy(() -> {
            SearchResponse searchResponse = client.prepareSearch(SecurityIndexManager.SECURITY_INDEX_NAME)
                    .setSource(SearchSourceBuilder.searchSource()
                            .query(QueryBuilders.termQuery(""doc_type"", TokenService.INVALIDATED_TOKEN_DOC_TYPE)))
                    .setSize(1)
                    .setTerminateAfter(1)
                    .get();
            assertThat(searchResponse.getHits().getTotalHits(), equalTo(1L));
            docId.set(searchResponse.getHits().getAt(0).getId());
        });

        // hack doc to modify the time to the day before
        Instant dayBefore = created.minus(1L, ChronoUnit.DAYS);
        assertTrue(Instant.now().isAfter(dayBefore));
        client.prepareUpdate(SecurityIndexManager.SECURITY_INDEX_NAME, ""doc"", docId.get())
                .setDoc(""expiration_time"", dayBefore.toEpochMilli())
                .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE)
                .get();

        AtomicBoolean deleteTriggered = new AtomicBoolean(false);
        assertBusy(() -> {
            if (deleteTriggered.compareAndSet(false, true)) {
                // invalidate a invalid token... doesn't matter that it is bad... we just want this action to trigger the deletion
                try {
                    securityClient.prepareInvalidateToken(""fooobar"")
                            .setType(randomFrom(InvalidateTokenRequest.Type.values()))
                            .execute()
                            .actionGet();
                } catch (ElasticsearchSecurityException e) {
                    assertEquals(""token malformed"", e.getMessage());
                }
            }
            client.admin().indices().prepareRefresh(SecurityIndexManager.SECURITY_INDEX_NAME).get();
            SearchResponse searchResponse = client.prepareSearch(SecurityIndexManager.SECURITY_INDEX_NAME)
                    .setSource(SearchSourceBuilder.searchSource()
                            .query(QueryBuilders.termQuery(""doc_type"", TokenService.INVALIDATED_TOKEN_DOC_TYPE)))
                    .setSize(0)
                    .setTerminateAfter(1)
                    .get();
            assertThat(searchResponse.getHits().getTotalHits(), equalTo(0L));
        }, 30, TimeUnit.SECONDS);
    }
",non-flaky,5
91419,strapdata_elassandra,NativePrivilegeStoreTests.setup,"@TestLogging(""org.elasticsearch.xpack.security.authz.store.NativePrivilegeStore:TRACE"")
    public void setup() {
        requests = new ArrayList<>();
        listener = new AtomicReference<>();
        client = new NoOpClient(getTestName()) {
            @Override
            protected <Request extends ActionRequest,
                Response extends ActionResponse,
                RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>>
            void doExecute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener) {
                NativePrivilegeStoreTests.this.requests.add(request);
                NativePrivilegeStoreTests.this.listener.set(listener);
            }
        };
        final SecurityIndexManager securityIndex = mock(SecurityIndexManager.class);
        when(securityIndex.freeze()).thenReturn(securityIndex);
        when(securityIndex.indexExists()).thenReturn(true);
        when(securityIndex.isAvailable()).thenReturn(true);
        Mockito.doAnswer(invocationOnMock -> {
            assertThat(invocationOnMock.getArguments().length, equalTo(2));
            assertThat(invocationOnMock.getArguments()[1], instanceOf(Runnable.class));
            ((Runnable) invocationOnMock.getArguments()[1]).run();
            return null;
        }).when(securityIndex).prepareIndexIfNeededThenExecute(any(Consumer.class), any(Runnable.class));
        Mockito.doAnswer(invocationOnMock -> {
            assertThat(invocationOnMock.getArguments().length, equalTo(2));
            assertThat(invocationOnMock.getArguments()[1], instanceOf(Runnable.class));
            ((Runnable) invocationOnMock.getArguments()[1]).run();
            return null;
        }).when(securityIndex).checkIndexVersionThenExecute(any(Consumer.class), any(Runnable.class));
        store = new NativePrivilegeStore(Settings.EMPTY, client, securityIndex);
    }
",non-flaky,5
91420,strapdata_elassandra,TransportHasPrivilegesActionTests.setup,"@TestLogging(""org.elasticsearch.xpack.security.action.user.TransportHasPrivilegesAction:TRACE,"" +
    public void setup() {
        final Settings settings = Settings.builder().build();
        user = new User(randomAlphaOfLengthBetween(4, 12));
        final ThreadPool threadPool = mock(ThreadPool.class);
        final ThreadContext threadContext = new ThreadContext(Settings.EMPTY);
        final TransportService transportService = new TransportService(Settings.EMPTY, mock(Transport.class), null,
            TransportService.NOOP_TRANSPORT_INTERCEPTOR, x -> null, null, Collections.emptySet());

        final Authentication authentication = mock(Authentication.class);
        threadContext.putTransient(AuthenticationField.AUTHENTICATION_KEY, authentication);
        when(threadPool.getThreadContext()).thenReturn(threadContext);

        when(authentication.getUser()).thenReturn(user);

        AuthorizationService authorizationService = mock(AuthorizationService.class);
        Mockito.doAnswer(invocationOnMock -> {
            ActionListener<Role> listener = (ActionListener<Role>) invocationOnMock.getArguments()[1];
            listener.onResponse(role);
            return null;
        }).when(authorizationService).roles(eq(user), any(ActionListener.class));

        applicationPrivileges = new ArrayList<>();
        NativePrivilegeStore privilegeStore = mock(NativePrivilegeStore.class);
        Mockito.doAnswer(inv -> {
            assertThat(inv.getArguments(), arrayWithSize(3));
            ActionListener<List<ApplicationPrivilegeDescriptor>> listener
                = (ActionListener<List<ApplicationPrivilegeDescriptor>>) inv.getArguments()[2];
            logger.info(""Privileges for ({}) are {}"", Arrays.toString(inv.getArguments()), applicationPrivileges);
            listener.onResponse(applicationPrivileges);
            return null;
        }).when(privilegeStore).getPrivileges(any(Collection.class), any(Collection.class), any(ActionListener.class));

        action = new TransportHasPrivilegesAction(settings, threadPool, transportService, mock(ActionFilters.class),
            mock(IndexNameExpressionResolver.class), authorizationService, privilegeStore);
    }
",non-flaky,5
91421,strapdata_elassandra,RemoteIndexAuditTrailStartingTests.transportSSLEnabled,"@TestLogging(""org.elasticsearch.xpack.security.audit.index:TRACE"")
    public boolean transportSSLEnabled() {
        return sslEnabled;
    }
",non-flaky,5
91422,strapdata_elassandra,SSLTrustRestrictionsTests.nodeSettings,"@TestLogging(""org.elasticsearch.xpack.ssl.RestrictedTrustManager:DEBUG"")
    public Settings nodeSettings(int nodeOrdinal) {

        Settings parentSettings = super.nodeSettings(nodeOrdinal);
        Settings.Builder builder = Settings.builder()
                .put(parentSettings.filter((s) -> s.startsWith(""xpack.ssl."") == false))
                .put(nodeSSL);

        restrictionsPath = configPath.resolve(""trust_restrictions.yml"");
        restrictionsTmpPath = configPath.resolve(""trust_restrictions.tmp"");

        writeRestrictions(""*.trusted"");
        builder.put(""xpack.ssl.trust_restrictions.path"", restrictionsPath);
        builder.put(""resource.reload.interval.high"", RESOURCE_RELOAD_MILLIS + ""ms"");

        return builder.build();
    }
",non-flaky,5
91423,strapdata_elassandra,LicensingTests.nodeSettings,"@TestLogging(""org.elasticsearch.cluster.service:TRACE,org.elasticsearch.discovery.zen:TRACE,org.elasticsearch.action.search:TRACE,"" +
    public Settings nodeSettings(int nodeOrdinal) {
        return Settings.builder().put(super.nodeSettings(nodeOrdinal))
                .put(NetworkModule.HTTP_ENABLED.getKey(), true)
            .put(TestZenDiscovery.USE_MOCK_PINGS.getKey(), false)
                .build();
    }
",non-flaky,5
91424,strapdata_elassandra,BasicDistributedJobsIT.testDedicatedMlNode,"    @TestLogging(""org.elasticsearch.xpack.persistent:TRACE,org.elasticsearch.cluster.service:DEBUG,org.elasticsearch.xpack.ml.action:DEBUG"")
    public void testDedicatedMlNode() throws Exception {
        internalCluster().ensureAtMostNumDataNodes(0);
        // start 2 non ml node that will never get a job allocated. (but ml apis are accessible from this node)
        internalCluster().startNode(Settings.builder().put(MachineLearning.ML_ENABLED.getKey(), false));
        internalCluster().startNode(Settings.builder().put(MachineLearning.ML_ENABLED.getKey(), false));
        // start ml node
        if (randomBoolean()) {
            internalCluster().startNode(Settings.builder().put(MachineLearning.ML_ENABLED.getKey(), true));
        } else {
            // the default is based on 'xpack.ml.enabled', which is enabled in base test class.
            internalCluster().startNode();
        }
        ensureStableCluster(3);

        String jobId = ""dedicated-ml-node-job"";
        Job.Builder job = createJob(jobId, new ByteSizeValue(2, ByteSizeUnit.MB));
        PutJobAction.Request putJobRequest = new PutJobAction.Request(job);
        client().execute(PutJobAction.INSTANCE, putJobRequest).actionGet();

        OpenJobAction.Request openJobRequest = new OpenJobAction.Request(job.getId());
        client().execute(OpenJobAction.INSTANCE, openJobRequest).actionGet();
        assertBusy(() -> {
            ClusterState clusterState = client().admin().cluster().prepareState().get().getState();
            PersistentTasksCustomMetaData tasks = clusterState.getMetaData().custom(PersistentTasksCustomMetaData.TYPE);
            PersistentTask<?> task = tasks.getTask(MlTasks.jobTaskId(jobId));
            DiscoveryNode node = clusterState.nodes().resolveNode(task.getExecutorNode());
            assertThat(node.getAttributes(), hasEntry(MachineLearning.ML_ENABLED_NODE_ATTR, ""true""));
            assertThat(node.getAttributes(), hasEntry(MachineLearning.MAX_OPEN_JOBS_NODE_ATTR, ""20""));
            JobTaskState jobTaskState = (JobTaskState) task.getState();
            assertNotNull(jobTaskState);
            assertEquals(JobState.OPENED, jobTaskState.getState());
        });

        logger.info(""stop the only running ml node"");
        internalCluster().stopRandomNode(settings -> settings.getAsBoolean(MachineLearning.ML_ENABLED.getKey(), true));
        ensureStableCluster(2);
        assertBusy(() -> {
            // job should get and remain in a failed state and
            // the status remains to be opened as from ml we didn't had the chance to set the status to failed:
            assertJobTask(jobId, JobState.OPENED, false);
        });

        logger.info(""start ml node"");
        internalCluster().startNode(Settings.builder().put(MachineLearning.ML_ENABLED.getKey(), true));
        ensureStableCluster(3);
        assertBusy(() -> {
            // job should be re-opened:
            assertJobTask(jobId, JobState.OPENED, true);
        });
    }
",non-flaky,5
91425,strapdata_elassandra,MlDistributedFailureIT.testLoseDedicatedMasterNode,"    @TestLogging(""org.elasticsearch.xpack.ml.action:DEBUG,org.elasticsearch.xpack.persistent:TRACE,"" +
    public void testLoseDedicatedMasterNode() throws Exception {
        internalCluster().ensureAtMostNumDataNodes(0);
        logger.info(""Starting dedicated master node..."");
        internalCluster().startNode(Settings.builder()
                .put(""node.master"", true)
                .put(""node.data"", false)
                .put(""node.ml"", false)
                .build());
        logger.info(""Starting ml and data node..."");
        String mlAndDataNode = internalCluster().startNode(Settings.builder()
                .put(""node.master"", false)
                .build());
        ensureStableClusterOnAllNodes(2);
        run(""lose-dedicated-master-node-job"", () -> {
            logger.info(""Stopping dedicated master node"");
            internalCluster().stopRandomNode(settings -> settings.getAsBoolean(""node.master"", false));
            assertBusy(() -> {
                ClusterState state = client(mlAndDataNode).admin().cluster().prepareState()
                        .setLocal(true).get().getState();
                assertNull(state.nodes().getMasterNodeId());
            });
            logger.info(""Restarting dedicated master node"");
            internalCluster().startNode(Settings.builder()
                    .put(""node.master"", true)
                    .put(""node.data"", false)
                    .put(""node.ml"", false)
                    .build());
            ensureStableClusterOnAllNodes(2);
        });
    }
",non-flaky,5
91426,strapdata_elassandra,AutodetectProcessManagerTests.testCanCloseClosingJob,"    @TestLogging(""org.elasticsearch.xpack.ml.job.process.autodetect:DEBUG"")
    public void testCanCloseClosingJob() throws Exception {
        AutodetectCommunicator communicator = mock(AutodetectCommunicator.class);
        AtomicInteger numberOfCommunicatorCloses = new AtomicInteger(0);
        doAnswer(invocationOnMock -> {
            numberOfCommunicatorCloses.incrementAndGet();
            // This increases the chance of the two threads both getting into
            // the middle of the AutodetectProcessManager.close() method
            Thread.yield();
            return null;
        }).when(communicator).close(anyBoolean(), anyString());
        AutodetectProcessManager manager = createManager(communicator);
        assertEquals(0, manager.numberOfOpenJobs());

        JobTask jobTask = mock(JobTask.class);
        when(jobTask.getJobId()).thenReturn(""foo"");
        manager.openJob(jobTask, e -> {});
        manager.processData(jobTask, analysisRegistry, createInputStream(""""), randomFrom(XContentType.values()),
                mock(DataLoadParams.class), (dataCounts1, e) -> {});

        assertEquals(1, manager.numberOfOpenJobs());

        // Close the job in a separate thread
        Thread closeThread = new Thread(() -> manager.closeJob(jobTask, false, ""in separate thread""));
        closeThread.start();
        Thread.yield();

        // Also close the job in the current thread, so that we have two simultaneous close requests
        manager.closeJob(jobTask, false, ""in main test thread"");

        // The 10 second timeout here is usually far in excess of what is required.  In the vast
        // majority of cases the other thread will exit within a few milliseconds.  However, it
        // has been observed that on some VMs the test can fail because the VM stalls at the
        // wrong moment.  A 10 second timeout is on a par with the length of time assertBusy()
        // would wait under these circumstances.
        closeThread.join(10000);
        assertFalse(closeThread.isAlive());

        // Only one of the threads should have called AutodetectCommunicator.close()
        assertEquals(1, numberOfCommunicatorCloses.get());
        assertEquals(0, manager.numberOfOpenJobs());
    }
",non-flaky,5
91427,strapdata_elassandra,MachineLearningLicensingTests.resetLicensing,"@TestLogging(""org.elasticsearch.xpack.ml.action:DEBUG"")
    public void resetLicensing() {
        enableLicensing();

        ensureStableCluster(1);
        ensureYellow();
    }
",non-flaky,5
91428,strapdata_elassandra,HistoryActionConditionTests.testActionConditionWithHardFailures,"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,org.elasticsearch.xpack.watcher.WatcherIndexingListener:TRACE"")
    public void testActionConditionWithHardFailures() throws Exception {
        final String id = ""testActionConditionWithHardFailures"";

        final ExecutableCondition scriptConditionFailsHard = mockScriptCondition(""throw new IllegalStateException('failed');"");
        final List<ExecutableCondition> actionConditionsWithFailure =
                Arrays.asList(scriptConditionFailsHard, conditionPasses, InternalAlwaysCondition.INSTANCE);

        Collections.shuffle(actionConditionsWithFailure, random());

        final int failedIndex = actionConditionsWithFailure.indexOf(scriptConditionFailsHard);

        putAndTriggerWatch(id, input, actionConditionsWithFailure.toArray(new Condition[actionConditionsWithFailure.size()]));

        flush();

        assertWatchWithMinimumActionsCount(id, ExecutionState.EXECUTED, 1);

        // only one action should have failed via condition
        final SearchResponse response = searchHistory(SearchSourceBuilder.searchSource().query(termQuery(""watch_id"", id)));
        assertThat(response.getHits().getTotalHits(), is(1L));

        final SearchHit hit = response.getHits().getAt(0);
        final List<Object> actions = getActionsFromHit(hit.getSourceAsMap());

        for (int i = 0; i < actionConditionsWithFailure.size(); ++i) {
            final Map<String, Object> action = (Map<String, Object>)actions.get(i);
            final Map<String, Object> condition = (Map<String, Object>)action.get(""condition"");
            final Map<String, Object> logging = (Map<String, Object>)action.get(""logging"");

            assertThat(action.get(""id""), is(""action"" + i));

            if (i == failedIndex) {
                assertThat(action.get(""status""), is(""condition_failed""));
                assertThat(action.get(""reason""), is(""condition failed. skipping: [expected] failed hard""));
                assertThat(condition, nullValue());
                assertThat(logging, nullValue());
            } else {
                assertThat(condition.get(""type""), is(actionConditionsWithFailure.get(i).type()));

                assertThat(action.get(""status""), is(""success""));
                assertThat(condition.get(""met""), is(true));
                assertThat(action.get(""reason""), nullValue());
                assertThat(logging.get(""logged_text""), is(Integer.toString(i)));
            }
        }
    }
",non-flaky,5
91429,strapdata_elassandra,HistoryTemplateEmailMappingsTests.setUp,"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,"" +
    public void setUp() throws Exception {
        super.setUp();
        server = EmailServer.localhost(logger);
    }
",non-flaky,5
91430,strapdata_elassandra,HttpInputIntegrationTests.testHttpInput,"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,org.elasticsearch.xpack.watcher.WatcherIndexingListener:TRACE"")
    public void testHttpInput() throws Exception {
        createIndex(""index"");
        client().prepareIndex(""index"", ""type"", ""id"").setSource(""{}"", XContentType.JSON).setRefreshPolicy(IMMEDIATE).get();

        InetSocketAddress address = internalCluster().httpAddresses()[0];
        watcherClient().preparePutWatch(""_name"")
                .setSource(watchBuilder()
                        .trigger(schedule(interval(""5s"")))
                        .input(httpInput(HttpRequestTemplate.builder(address.getHostString(), address.getPort())
                                .path(""/index/_search"")
                                .body(Strings.toString(jsonBuilder().startObject().field(""size"", 1).endObject()))
                                .putHeader(""Content-Type"", new TextTemplate(""application/json""))))
                        .condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 1L))
                        .addAction(""_id"", loggingAction(""anything"")))
                .get();

        timeWarp().trigger(""_name"");
        refresh();
        assertWatchWithMinimumPerformedActionsCount(""_name"", 1, false);
    }
",non-flaky,5
91431,strapdata_elassandra,BasicWatcherTests.testIndexWatch,"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,"" +
    public void testIndexWatch() throws Exception {
        WatcherClient watcherClient = watcherClient();
        createIndex(""idx"");
        // Have a sample document in the index, the watch is going to evaluate
        client().prepareIndex(""idx"", ""type"").setSource(""field"", ""foo"").get();
        refresh();
        WatcherSearchTemplateRequest request = templateRequest(searchSource().query(termQuery(""field"", ""foo"")), ""idx"");
        watcherClient.preparePutWatch(""_name"")
                .setSource(watchBuilder()
                        .trigger(schedule(interval(5, IntervalSchedule.Interval.Unit.SECONDS)))
                        .input(searchInput(request))
                        .condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 1L))
                        .addAction(""_logger"", loggingAction(""_logging"")
                                .setCategory(""_category"")))
                .get();

        timeWarp().trigger(""_name"");
        assertWatchWithMinimumPerformedActionsCount(""_name"", 1);

        GetWatchResponse getWatchResponse = watcherClient().prepareGetWatch().setId(""_name"").get();
        assertThat(getWatchResponse.isFound(), is(true));
        assertThat(getWatchResponse.getSource(), notNullValue());
    }
",non-flaky,5
91432,strapdata_elassandra,BasicWatcherTests.testModifyWatches,"    @TestLogging(""org.elasticsearch.xpack.watcher:DEBUG"")
    public void testModifyWatches() throws Exception {
        createIndex(""idx"");
        WatcherSearchTemplateRequest searchRequest = templateRequest(searchSource().query(matchAllQuery()), ""idx"");

        WatchSourceBuilder source = watchBuilder()
                .trigger(schedule(interval(""5s"")))
                .input(searchInput(searchRequest))
                .addAction(""_id"", indexAction(""idx"", ""action""));

        watcherClient().preparePutWatch(""_name"")
                .setSource(source.condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 1L)))
                .get();

        timeWarp().clock().fastForwardSeconds(5);
        timeWarp().trigger(""_name"");
        assertWatchWithMinimumPerformedActionsCount(""_name"", 0, false);

        watcherClient().preparePutWatch(""_name"")
                .setSource(source.condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 0L)))
                .get();

        timeWarp().clock().fastForwardSeconds(5);
        timeWarp().trigger(""_name"");
        refresh();
        assertWatchWithMinimumPerformedActionsCount(""_name"", 1, false);

        watcherClient().preparePutWatch(""_name"")
                .setSource(source
                        .trigger(schedule(Schedules.cron(""0/1 * * * * ? 2020"")))
                        .condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 0L)))
                .get();

        timeWarp().clock().fastForwardSeconds(5);
        timeWarp().trigger(""_name"");
        long count = findNumberOfPerformedActions(""_name"");

        timeWarp().clock().fastForwardSeconds(5);
        timeWarp().trigger(""_name"");
        assertThat(count, equalTo(findNumberOfPerformedActions(""_name"")));
    }
",non-flaky,5
91433,strapdata_elassandra,WatchAckTests.indexTestDocument,"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG"")
    public void indexTestDocument() {
        IndexResponse eventIndexResponse = client().prepareIndex(""events"", ""event"", id)
                .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE)
                .setSource(""level"", ""error"")
                .get();
        assertEquals(DocWriteResponse.Result.CREATED, eventIndexResponse.getResult());
    }
",non-flaky,5
91434,strapdata_elassandra,TimeThrottleIntegrationTests.testTimeThrottle,"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,"" +
    public void testTimeThrottle(){
        String id = randomAlphaOfLength(20);
        PutWatchResponse putWatchResponse = watcherClient().preparePutWatch()
                .setId(id)
                .setSource(watchBuilder()
                        .trigger(schedule(interval(""5s"")))
                        .input(simpleInput())
                        .addAction(""my-logging-action"", loggingAction(""foo""))
                        .defaultThrottlePeriod(TimeValue.timeValueSeconds(30)))
                .get();
        assertThat(putWatchResponse.isCreated(), is(true));

        timeWarp().trigger(id);
        assertHistoryEntryExecuted(id);

        timeWarp().clock().fastForward(TimeValue.timeValueMillis(4000));
        timeWarp().trigger(id);
        assertHistoryEntryThrottled(id);

        timeWarp().clock().fastForwardSeconds(30);
        timeWarp().trigger(id);
        assertHistoryEntryExecuted(id);

        assertTotalHistoryEntries(id, 3);
    }
",non-flaky,5
91435,strapdata_elassandra,ActivateWatchTests.testDeactivateAndActivate,"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,org.elasticsearch.xpack.watcher.WatcherIndexingListener:TRACE"")
    public void testDeactivateAndActivate() throws Exception {
        PutWatchResponse putWatchResponse = watcherClient().preparePutWatch()
                .setId(""_id"")
                .setSource(watchBuilder()
                        .trigger(schedule(interval(""1s"")))
                        .input(simpleInput(""foo"", ""bar""))
                        .addAction(""_a1"", indexAction(""actions"", ""action1""))
                        .defaultThrottlePeriod(new TimeValue(0, TimeUnit.SECONDS)))
                .get();

        assertThat(putWatchResponse.isCreated(), is(true));

        GetWatchResponse getWatchResponse = watcherClient().prepareGetWatch(""_id"").get();
        assertThat(getWatchResponse, notNullValue());
        assertThat(getWatchResponse.getStatus().state().isActive(), is(true));

        logger.info(""Waiting for watch to be executed at least once"");
        assertWatchWithMinimumActionsCount(""_id"", ExecutionState.EXECUTED, 1);

        // we now know the watch is executing... lets deactivate it
        ActivateWatchResponse activateWatchResponse = watcherClient().prepareActivateWatch(""_id"", false).get();
        assertThat(activateWatchResponse, notNullValue());
        assertThat(activateWatchResponse.getStatus().state().isActive(), is(false));

        getWatchResponse = watcherClient().prepareGetWatch(""_id"").get();
        assertThat(getWatchResponse, notNullValue());
        assertThat(getWatchResponse.getStatus().state().isActive(), is(false));

        // wait until no watch is executing
        assertBusy(() -> {
            WatcherStatsResponse statsResponse = watcherClient().prepareWatcherStats().setIncludeCurrentWatches(true).get();
            int sum = statsResponse.getNodes().stream().map(WatcherStatsResponse.Node::getSnapshots).mapToInt(List::size).sum();
            assertThat(sum, is(0));
        });

        logger.info(""Ensured no more watches are being executed"");
        refresh();
        long count1 = docCount("".watcher-history*"", ""doc"", matchAllQuery());

        logger.info(""Sleeping for 5 seconds, watch history count [{}]"", count1);
        Thread.sleep(5000);

        refresh();
        long count2 = docCount("".watcher-history*"", ""doc"", matchAllQuery());

        assertThat(count2, is(count1));

        // lets activate it again
        logger.info(""Activating watch again"");

        activateWatchResponse = watcherClient().prepareActivateWatch(""_id"", true).get();
        assertThat(activateWatchResponse, notNullValue());
        assertThat(activateWatchResponse.getStatus().state().isActive(), is(true));

        getWatchResponse = watcherClient().prepareGetWatch(""_id"").get();
        assertThat(getWatchResponse, notNullValue());
        assertThat(getWatchResponse.getStatus().state().isActive(), is(true));

        logger.info(""Sleeping for another five seconds, ensuring that watch is executed"");
        Thread.sleep(5000);
        refresh();
        long count3 = docCount("".watcher-history*"", ""doc"", matchAllQuery());
        assertThat(count3, greaterThan(count1));
    }
",non-flaky,5
91436,strapdata_elassandra,PreBuiltTransportClientTests.testPluginInstalled,"    @Test
    public void testPluginInstalled() {
        try (TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)) {
            Settings settings = client.settings();
            assertEquals(Netty4Plugin.NETTY_TRANSPORT_NAME, NetworkModule.HTTP_DEFAULT_TYPE_SETTING.get(settings));
            assertEquals(Netty4Plugin.NETTY_TRANSPORT_NAME, NetworkModule.TRANSPORT_DEFAULT_TYPE_SETTING.get(settings));
        }
    }
",non-flaky,5
91437,strapdata_elassandra,PreBuiltTransportClientTests.testInstallPluginTwice,"    @Test
    public void testInstallPluginTwice() {
        for (Class<? extends Plugin> plugin :
                Arrays.asList(ParentJoinPlugin.class, ReindexPlugin.class, PercolatorPlugin.class,
                    MustachePlugin.class)) {
            try {
                new PreBuiltTransportClient(Settings.EMPTY, plugin);
                fail(""exception expected"");
            } catch (IllegalArgumentException ex) {
                assertTrue(""Expected message to start with [plugin already exists: ] but was instead ["" + ex.getMessage() + ""]"",
                        ex.getMessage().startsWith(""plugin already exists: ""));
            }
        }
    }
",non-flaky,5
91438,strapdata_elassandra,PackagingTestCase.logTestNameBefore,"@TestMethodProviders({
    public void logTestNameBefore() {
        logger.info(""["" + testNameRule.getMethodName() + ""]: before test"");
    }
",non-flaky,5
91439,strapdata_elassandra,RpmPreservationTestCase.onlyCompatibleDistributions,"@TestCaseOrdering(TestCaseOrdering.AlphabeticOrder.class)
    public void onlyCompatibleDistributions() {
        assumeTrue(""only rpm platforms"", isRPM());
        assumeTrue(""only compatible distributions"", distribution().packaging.compatible);
    }
",non-flaky,5
91440,strapdata_elassandra,ArchiveTestCase.onlyCompatibleDistributions,"@TestCaseOrdering(TestCaseOrdering.AlphabeticOrder.class)
    public void onlyCompatibleDistributions() {
        assumeTrue(""only compatible distributions"", distribution().packaging.compatible);
    }
",non-flaky,5
91441,strapdata_elassandra,PackageTestCase.onlyCompatibleDistributions,"@TestCaseOrdering(TestCaseOrdering.AlphabeticOrder.class)
    public void onlyCompatibleDistributions() {
        assumeTrue(""only compatible distributions"", distribution().packaging.compatible);
    }
",non-flaky,5
91442,strapdata_elassandra,DebPreservationTestCase.onlyCompatibleDistributions,"@TestCaseOrdering(TestCaseOrdering.AlphabeticOrder.class)
    public void onlyCompatibleDistributions() {
        assumeTrue(""only dpkg platforms"", isDPKG());
        assumeTrue(""only compatible distributions"", distribution().packaging.compatible);
    }
",non-flaky,5
91443,strapdata_elassandra,WildflyIT.testTransportClient,"@TestRuleLimitSysouts.Limit(bytes = 14000)
    public void testTransportClient() throws URISyntaxException, IOException {
        try (CloseableHttpClient client = HttpClientBuilder.create().build()) {
            final String str = String.format(
                    Locale.ROOT,
                    ""http://localhost:%d/wildfly-%s%s/transport/employees/1"",
                    Integer.parseInt(System.getProperty(""tests.jboss.http.port"")),
                    Version.CURRENT,
                    Build.CURRENT.isSnapshot() ? ""-SNAPSHOT"" : """");
            final HttpPut put = new HttpPut(new URI(str));
            final String body;
            try (XContentBuilder builder = jsonBuilder()) {
                builder.startObject();
                {
                    builder.field(""first_name"", ""John"");
                    builder.field(""last_name"", ""Smith"");
                    builder.field(""age"", 25);
                    builder.field(""about"", ""I love to go rock climbing"");
                    builder.startArray(""interests"");
                    {
                        builder.value(""sports"");
                        builder.value(""music"");
                    }
                    builder.endArray();
                }
                builder.endObject();
                body = Strings.toString(builder);
            }
            put.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
            try (CloseableHttpResponse response = client.execute(put)) {
                int status = response.getStatusLine().getStatusCode();
                assertThat(""expected a 201 response but got: "" + status + "" - body: "" + EntityUtils.toString(response.getEntity()),
                        status, equalTo(201));
            }

            final HttpGet get = new HttpGet(new URI(str));
            try (
                    CloseableHttpResponse response = client.execute(get);
                    XContentParser parser =
                            JsonXContent.jsonXContent.createParser(
                                    new NamedXContentRegistry(ClusterModule.getNamedXWriteables()),
                                    DeprecationHandler.THROW_UNSUPPORTED_OPERATION,
                                    response.getEntity().getContent())) {
                final Map<String, Object> map = parser.map();
                assertThat(map.get(""first_name""), equalTo(""John""));
                assertThat(map.get(""last_name""), equalTo(""Smith""));
                assertThat(map.get(""age""), equalTo(25));
                assertThat(map.get(""about""), equalTo(""I love to go rock climbing""));
                final Object interests = map.get(""interests"");
                assertThat(interests, instanceOf(List.class));
                @SuppressWarnings(""unchecked"") final List<String> interestsAsList = (List<String>) interests;
                assertThat(interestsAsList, containsInAnyOrder(""sports"", ""music""));
            }
        }
    }
",non-flaky,5
91444,strapdata_elassandra,ESIntegTestCase.randomIndexTemplate,"    @TestGroup(enabled = false, sysProperty = ESIntegTestCase.SYSPROP_THIRDPARTY)
    public void randomIndexTemplate() throws IOException {

        // TODO move settings for random directory etc here into the index based randomized settings.
        if (cluster().size() > 0) {
            Settings.Builder randomSettingsBuilder =
                setRandomIndexSettings(random(), Settings.builder());
            if (isInternalCluster()) {
                // this is only used by mock plugins and if the cluster is not internal we just can't set it
                randomSettingsBuilder.put(INDEX_TEST_SEED_SETTING.getKey(), random().nextLong());
            }

            randomSettingsBuilder.put(SETTING_NUMBER_OF_SHARDS, numberOfShards())
                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas());

            // if the test class is annotated with SuppressCodecs(""*""), it means don't use lucene's codec randomization
            // otherwise, use it, it has assertions and so on that can find bugs.
            SuppressCodecs annotation = getClass().getAnnotation(SuppressCodecs.class);
            if (annotation != null && annotation.value().length == 1 && ""*"".equals(annotation.value()[0])) {
                randomSettingsBuilder.put(""index.codec"", randomFrom(CodecService.DEFAULT_CODEC, CodecService.BEST_COMPRESSION_CODEC));
            } else {
                randomSettingsBuilder.put(""index.codec"", CodecService.LUCENE_DEFAULT_CODEC);
            }

            for (String setting : randomSettingsBuilder.keys()) {
                assertThat(""non index. prefix setting set on index template, its a node setting..."", setting, startsWith(""index.""));
            }
            // always default delayed allocation to 0 to make sure we have tests are not delayed
            randomSettingsBuilder.put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), 0);
            if (randomBoolean()) {
                randomSettingsBuilder.put(IndexModule.INDEX_QUERY_CACHE_ENABLED_SETTING.getKey(), randomBoolean());
            }
            PutIndexTemplateRequestBuilder putTemplate = client().admin().indices()
                .preparePutTemplate(""random_index_template"")
                .setPatterns(Collections.singletonList(""*""))
                .setOrder(0)
                .setSettings(randomSettingsBuilder);
            assertAcked(putTemplate.execute().actionGet());
        }
    }
",non-flaky,5
91445,strapdata_elassandra,SuiteScopeClusterIT.testReproducible,"    @Test
    public void testReproducible() throws IOException {
        if (ITER++ == 0) {
            CLUSTER_SEED = cluster().seed();
            for (int i = 0; i < SEQUENCE.length; i++) {
                SEQUENCE[i] = randomLong();
            }
        } else {
            assertEquals(CLUSTER_SEED, Long.valueOf(cluster().seed()));
            for (int i = 0; i < SEQUENCE.length; i++) {
                assertThat(SEQUENCE[i], equalTo(randomLong()));
            }
        }
    }
",non-flaky,5
91446,strapdata_elassandra,LoggingListenerTests.annotatedTestMethod,"        @TestLogging(""xyz:TRACE,foo:WARN,foo.bar:ERROR"")
        public void annotatedTestMethod() {

        }
",non-flaky,5
91447,strapdata_elassandra,LoggingListenerTests.annotatedTestMethod2,"        @TestLogging(""abc:TRACE,xyz:DEBUG"")
        public void annotatedTestMethod2() {

        }
",non-flaky,5
91448,strapdata_elassandra,LoggingListenerTests.invalidMethod,"        @TestLogging(""abc:INFO:WARN"")
        public void invalidMethod() {

        }
",non-flaky,5
91449,strapdata_elassandra,IndexShardIT.testStressMaybeFlushOrRollTranslogGeneration,"    @TestLogging(""org.elasticsearch.index.shard:TRACE,org.elasticsearch.index.engine:TRACE"")
    public void testStressMaybeFlushOrRollTranslogGeneration() throws Exception {
        createIndex(""test"");
        ensureGreen();
        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
        IndexService test = indicesService.indexService(resolveIndex(""test""));
        final IndexShard shard = test.getShardOrNull(0);
        assertFalse(shard.shouldPeriodicallyFlush());
        final boolean flush = randomBoolean();
        final Settings settings;
        if (flush) {
            // size of the operation plus two generations of overhead.
            settings = Settings.builder().put(""index.translog.flush_threshold_size"", ""180b"").build();
        } else {
            // size of the operation plus header and footer
            settings = Settings.builder().put(""index.translog.generation_threshold_size"", ""117b"").build();
        }
        client().admin().indices().prepareUpdateSettings(""test"").setSettings(settings).get();
        client().prepareIndex(""test"", ""test"", ""0"")
                .setSource(""{}"", XContentType.JSON)
                .setRefreshPolicy(randomBoolean() ? IMMEDIATE : NONE)
                .get();
        assertFalse(shard.shouldPeriodicallyFlush());
        final AtomicBoolean running = new AtomicBoolean(true);
        final int numThreads = randomIntBetween(2, 4);
        final Thread[] threads = new Thread[numThreads];
        final CyclicBarrier barrier = new CyclicBarrier(numThreads + 1);
        for (int i = 0; i < threads.length; i++) {
            threads[i] = new Thread(() -> {
                try {
                    barrier.await();
                } catch (final InterruptedException | BrokenBarrierException e) {
                    throw new RuntimeException(e);
                }
                while (running.get()) {
                    shard.afterWriteOperation();
                }
            });
            threads[i].start();
        }
        barrier.await();
        final CheckedRunnable<Exception> check;
        if (flush) {
            final FlushStats initialStats = shard.flushStats();
            client().prepareIndex(""test"", ""test"", ""1"").setSource(""{}"", XContentType.JSON).get();
            check = () -> {
                final FlushStats currentStats = shard.flushStats();
                String msg = String.format(Locale.ROOT, ""flush stats: total=[%d vs %d], periodic=[%d vs %d]"",
                    initialStats.getTotal(), currentStats.getTotal(), initialStats.getPeriodic(), currentStats.getPeriodic());
                assertThat(msg, currentStats.getPeriodic(), equalTo(initialStats.getPeriodic() + 1));
                assertThat(msg, currentStats.getTotal(), equalTo(initialStats.getTotal() + 1));
            };
        } else {
            final long generation = getTranslog(shard).currentFileGeneration();
            client().prepareIndex(""test"", ""test"", ""1"").setSource(""{}"", XContentType.JSON).get();
            check = () -> assertEquals(
                    generation + 1,
                    getTranslog(shard).currentFileGeneration());
        }
        assertBusy(check);
        running.set(false);
        for (int i = 0; i < threads.length; i++) {
            threads[i].join();
        }
        check.run();
    }
",non-flaky,5
91450,strapdata_elassandra,RecoveryDuringReplicationTests.testRecoveryAfterPrimaryPromotion,"    @TestLogging(""org.elasticsearch.index.shard:TRACE,org.elasticsearch.indices.recovery:TRACE"")
    public void testRecoveryAfterPrimaryPromotion() throws Exception {
        try (ReplicationGroup shards = createGroup(2)) {
            shards.startAll();
            int totalDocs = shards.indexDocs(randomInt(10));
            int committedDocs = 0;
            if (randomBoolean()) {
                shards.flush();
                committedDocs = totalDocs;
            }

            final IndexShard oldPrimary = shards.getPrimary();
            final IndexShard newPrimary = shards.getReplicas().get(0);
            final IndexShard replica = shards.getReplicas().get(1);
            if (randomBoolean()) {
                // simulate docs that were inflight when primary failed, these will be rolled back
                final int rollbackDocs = randomIntBetween(1, 5);
                logger.info(""--> indexing {} rollback docs"", rollbackDocs);
                for (int i = 0; i < rollbackDocs; i++) {
                    final IndexRequest indexRequest = new IndexRequest(index.getName(), ""type"", ""rollback_"" + i)
                            .source(""{}"", XContentType.JSON);
                    final BulkShardRequest bulkShardRequest = indexOnPrimary(indexRequest, oldPrimary);
                    indexOnReplica(bulkShardRequest, shards, replica);
                }
                if (randomBoolean()) {
                    oldPrimary.flush(new FlushRequest(index.getName()));
                }
            }

            shards.promoteReplicaToPrimary(newPrimary).get();

            // check that local checkpoint of new primary is properly tracked after primary promotion
            assertThat(newPrimary.getLocalCheckpoint(), equalTo(totalDocs - 1L));
            assertThat(IndexShardTestCase.getReplicationTracker(newPrimary)
                .getTrackedLocalCheckpointForShard(newPrimary.routingEntry().allocationId().getId()).getLocalCheckpoint(),
                equalTo(totalDocs - 1L));

            // index some more
            int moreDocs = shards.indexDocs(randomIntBetween(0, 5));
            totalDocs += moreDocs;

            // As a replica keeps a safe commit, the file-based recovery only happens if the required translog
            // for the sequence based recovery are not fully retained and extra documents were added to the primary.
            boolean expectSeqNoRecovery = (moreDocs == 0 || randomBoolean());
            int uncommittedOpsOnPrimary = 0;
            if (expectSeqNoRecovery == false) {
                IndexMetaData.Builder builder = IndexMetaData.builder(newPrimary.indexSettings().getIndexMetaData());
                builder.settings(Settings.builder().put(newPrimary.indexSettings().getSettings())
                    .put(IndexSettings.INDEX_TRANSLOG_RETENTION_AGE_SETTING.getKey(), ""-1"")
                    .put(IndexSettings.INDEX_TRANSLOG_RETENTION_SIZE_SETTING.getKey(), ""-1"")
                    .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0)
                );
                newPrimary.indexSettings().updateIndexMetaData(builder.build());
                newPrimary.onSettingsChanged();
                // Make sure the global checkpoint on the new primary is persisted properly,
                // otherwise the deletion policy won't trim translog
                assertBusy(() -> {
                    shards.syncGlobalCheckpoint();
                    assertThat(newPrimary.getLastSyncedGlobalCheckpoint(), equalTo(newPrimary.seqNoStats().getMaxSeqNo()));
                });
                newPrimary.flush(new FlushRequest().force(true));
                if (replica.indexSettings().isSoftDeleteEnabled()) {
                    // We need an extra flush to advance the min_retained_seqno on the new primary so ops-based won't happen.
                    // The min_retained_seqno only advances when a merge asks for the retention query.
                    newPrimary.flush(new FlushRequest().force(true));
                }
                uncommittedOpsOnPrimary = shards.indexDocs(randomIntBetween(0, 10));
                totalDocs += uncommittedOpsOnPrimary;
            }

            if (randomBoolean()) {
                uncommittedOpsOnPrimary = 0;
                shards.syncGlobalCheckpoint();
                newPrimary.flush(new FlushRequest());
            }

            oldPrimary.close(""demoted"", false);
            oldPrimary.store().close();

            IndexShard newReplica = shards.addReplicaWithExistingPath(oldPrimary.shardPath(), oldPrimary.routingEntry().currentNodeId());
            shards.recoverReplica(newReplica);

            if (expectSeqNoRecovery) {
                assertThat(newReplica.recoveryState().getIndex().fileDetails(), empty());
                assertThat(newReplica.recoveryState().getTranslog().recoveredOperations(), equalTo(totalDocs - committedDocs));
            } else {
                assertThat(newReplica.recoveryState().getIndex().fileDetails(), not(empty()));
                assertThat(newReplica.recoveryState().getTranslog().recoveredOperations(), equalTo(uncommittedOpsOnPrimary));
            }
            // Make sure that flushing on a recovering shard is ok.
            shards.flush();
            shards.assertAllEqual(totalDocs);
        }
    }
",non-flaky,5
91451,strapdata_elassandra,RecoveryDuringReplicationTests.testResyncAfterPrimaryPromotion,"    @TestLogging(""org.elasticsearch.index.shard:TRACE,org.elasticsearch.action.resync:TRACE"")
    public void testResyncAfterPrimaryPromotion() throws Exception {
        // TODO: check translog trimming functionality once rollback is implemented in Lucene (ES trimming is done)
        Map<String, String> mappings =
            Collections.singletonMap(""type"", ""{ \""type\"": { \""properties\"": { \""f\"": { \""type\"": \""keyword\""} }}}"");
        try (ReplicationGroup shards = new ReplicationGroup(buildIndexMetaData(2, mappings))) {
            shards.startAll();
            int initialDocs = randomInt(10);

            for (int i = 0; i < initialDocs; i++) {
                final IndexRequest indexRequest = new IndexRequest(index.getName(), ""type"", ""initial_doc_"" + i)
                    .source(""{ \""f\"": \""normal\""}"", XContentType.JSON);
                shards.index(indexRequest);
            }

            boolean syncedGlobalCheckPoint = randomBoolean();
            if (syncedGlobalCheckPoint) {
                shards.syncGlobalCheckpoint();
            }

            final IndexShard oldPrimary = shards.getPrimary();
            final IndexShard newPrimary = shards.getReplicas().get(0);
            final IndexShard justReplica = shards.getReplicas().get(1);

            // simulate docs that were inflight when primary failed
            final int extraDocs = randomInt(5);
            logger.info(""--> indexing {} extra docs"", extraDocs);
            for (int i = 0; i < extraDocs; i++) {
                final IndexRequest indexRequest = new IndexRequest(index.getName(), ""type"", ""extra_doc_"" + i)
                    .source(""{ \""f\"": \""normal\""}"", XContentType.JSON);
                final BulkShardRequest bulkShardRequest = indexOnPrimary(indexRequest, oldPrimary);
                indexOnReplica(bulkShardRequest, shards, newPrimary);
            }

            final int extraDocsToBeTrimmed = randomIntBetween(0, 10);
            logger.info(""--> indexing {} extra docs to be trimmed"", extraDocsToBeTrimmed);
            for (int i = 0; i < extraDocsToBeTrimmed; i++) {
                final IndexRequest indexRequest = new IndexRequest(index.getName(), ""type"", ""extra_trimmed_"" + i)
                    .source(""{ \""f\"": \""trimmed\""}"", XContentType.JSON);
                final BulkShardRequest bulkShardRequest = indexOnPrimary(indexRequest, oldPrimary);
                // have to replicate to another replica != newPrimary one - the subject to trim
                indexOnReplica(bulkShardRequest, shards, justReplica);
            }

            logger.info(""--> resyncing replicas seqno_stats primary {} replica {}"", oldPrimary.seqNoStats(), newPrimary.seqNoStats());
            PrimaryReplicaSyncer.ResyncTask task = shards.promoteReplicaToPrimary(newPrimary).get();
            if (syncedGlobalCheckPoint) {
                assertEquals(extraDocs, task.getResyncedOperations());
            } else {
                assertThat(task.getResyncedOperations(), greaterThanOrEqualTo(extraDocs));
            }
            shards.assertAllEqual(initialDocs + extraDocs);
            for (IndexShard replica : shards.getReplicas()) {
                assertThat(replica.getMaxSeqNoOfUpdatesOrDeletes(),
                    greaterThanOrEqualTo(shards.getPrimary().getMaxSeqNoOfUpdatesOrDeletes()));
            }

            // check translog on replica is trimmed
            int translogOperations = 0;
            try(Translog.Snapshot snapshot = getTranslog(justReplica).newSnapshot()) {
                Translog.Operation next;
                while ((next = snapshot.next()) != null) {
                    translogOperations++;
                    assertThat(""unexpected op: "" + next, (int)next.seqNo(), lessThan(initialDocs + extraDocs));
                    assertThat(""unexpected primaryTerm: "" + next.primaryTerm(), next.primaryTerm(),
                        is(oldPrimary.getPendingPrimaryTerm()));
                    final Translog.Source source = next.getSource();
                    assertThat(source.source.utf8ToString(), is(""{ \""f\"": \""normal\""}""));
                }
            }
            assertThat(translogOperations, is(initialDocs + extraDocs));
        }
    }
",non-flaky,5
91452,strapdata_elassandra,RecoveryDuringReplicationTests.indexTranslogOperations,"    @TestLogging(
    public void testWaitForPendingSeqNo() throws Exception {
        IndexMetaData metaData = buildIndexMetaData(1);

        final int pendingDocs = randomIntBetween(1, 5);
        final BlockingEngineFactory primaryEngineFactory = new BlockingEngineFactory();

        try (ReplicationGroup shards = new ReplicationGroup(metaData) {
            @Override
            protected EngineFactory getEngineFactory(ShardRouting routing) {
                if (routing.primary()) {
                    return primaryEngineFactory;
                } else {
                    return new InternalEngineFactory();
                }
            }
        }) {
            shards.startAll();
            int docs = shards.indexDocs(randomIntBetween(1, 10));
            // simulate a background global checkpoint sync at which point we expect the global checkpoint to advance on the replicas
            shards.syncGlobalCheckpoint();
            IndexShard replica = shards.getReplicas().get(0);
            shards.removeReplica(replica);
            closeShards(replica);

            docs += pendingDocs;
            primaryEngineFactory.latchIndexers(pendingDocs);
            CountDownLatch pendingDocsDone = new CountDownLatch(pendingDocs);
            for (int i = 0; i < pendingDocs; i++) {
                final String id = ""pending_"" + i;
                threadPool.generic().submit(() -> {
                    try {
                        shards.index(new IndexRequest(index.getName(), ""type"", id).source(""{}"", XContentType.JSON));
                    } catch (Exception e) {
                        throw new AssertionError(e);
                    } finally {
                        pendingDocsDone.countDown();
                    }
                });
            }

            // wait for the pending ops to ""hang""
            primaryEngineFactory.awaitIndexersLatch();

            primaryEngineFactory.allowIndexing();
            // index some more
            docs += shards.indexDocs(randomInt(5));

            IndexShard newReplica = shards.addReplicaWithExistingPath(replica.shardPath(), replica.routingEntry().currentNodeId());

            CountDownLatch recoveryStart = new CountDownLatch(1);
            AtomicBoolean opsSent = new AtomicBoolean(false);
            final Future<Void> recoveryFuture = shards.asyncRecoverReplica(newReplica, (indexShard, node) -> {
                recoveryStart.countDown();
                return new RecoveryTarget(indexShard, node, recoveryListener, l -> {
                }) {
                    @Override
                    public long indexTranslogOperations(List<Translog.Operation> operations, int totalTranslogOps,
                                                        long maxSeenAutoIdTimestamp, long maxSeqNoOfUpdates) throws IOException {
                        opsSent.set(true);
                        return super.indexTranslogOperations(operations, totalTranslogOps, maxSeenAutoIdTimestamp, maxSeqNoOfUpdates);
                    }
",non-flaky,5
91453,strapdata_elassandra,RecoveryDuringReplicationTests.indexTranslogOperations,"    @TestLogging(
    public void testCheckpointsAndMarkingInSync() throws Exception {
        final IndexMetaData metaData = buildIndexMetaData(0);
        final BlockingEngineFactory replicaEngineFactory = new BlockingEngineFactory();
        try (
                ReplicationGroup shards = new ReplicationGroup(metaData) {
                    @Override
                    protected EngineFactory getEngineFactory(final ShardRouting routing) {
                        if (routing.primary()) {
                            return new InternalEngineFactory();
                        } else {
                            return replicaEngineFactory;
                        }
                    }
                };
                AutoCloseable ignored = replicaEngineFactory // make sure we release indexers before closing
        ) {
            shards.startPrimary();
            final int docs = shards.indexDocs(randomIntBetween(1, 10));
            logger.info(""indexed [{}] docs"", docs);
            final CountDownLatch pendingDocDone = new CountDownLatch(1);
            final CountDownLatch pendingDocActiveWithExtraDocIndexed = new CountDownLatch(1);
            final CountDownLatch phaseTwoStartLatch = new CountDownLatch(1);
            final IndexShard replica = shards.addReplica();
            final Future<Void> recoveryFuture = shards.asyncRecoverReplica(
                    replica,
                    (indexShard, node) -> new RecoveryTarget(indexShard, node, recoveryListener, l -> {}) {
                        @Override
                        public long indexTranslogOperations(final List<Translog.Operation> operations, final int totalTranslogOps,
                                                            final long maxAutoIdTimestamp, long maxSeqNoOfUpdates)
",non-flaky,5
91454,strapdata_elassandra,InternalEngineMergeIT.testMergesHappening,"    @TestLogging(""_root:DEBUG"")
    public void testMergesHappening() throws InterruptedException, IOException, ExecutionException {
        final int numOfShards = randomIntBetween(1, 5);
        // some settings to keep num segments low
        assertAcked(prepareCreate(""test"").setSettings(Settings.builder()
                .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, numOfShards)
                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
                .build()));
        long id = 0;
        final int rounds = scaledRandomIntBetween(50, 300);
        logger.info(""Starting rounds [{}] "", rounds);
        for (int i = 0; i < rounds; ++i) {
            final int numDocs = scaledRandomIntBetween(100, 1000);
            BulkRequestBuilder request = client().prepareBulk();
            for (int j = 0; j < numDocs; ++j) {
                request.add(Requests.indexRequest(""test"").type(""type1"").id(Long.toString(id++)).source(jsonBuilder().startObject().field(""l"", randomLong()).endObject()));
            }
            BulkResponse response = request.execute().actionGet();
            refresh();
            assertNoFailures(response);
            IndicesStatsResponse stats = client().admin().indices().prepareStats(""test"").setSegments(true).setMerge(true).get();
            logger.info(""index round [{}] - segments {}, total merges {}, current merge {}"", i, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
        }
        final long upperNumberSegments = 2 * numOfShards * 10;
        awaitBusy(() -> {
            IndicesStatsResponse stats = client().admin().indices().prepareStats().setSegments(true).setMerge(true).get();
            logger.info(""numshards {}, segments {}, total merges {}, current merge {}"", numOfShards, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
            long current = stats.getPrimaries().getMerge().getCurrent();
            long count = stats.getPrimaries().getSegments().getCount();
            return count < upperNumberSegments && current == 0;
        });
        IndicesStatsResponse stats = client().admin().indices().prepareStats().setSegments(true).setMerge(true).get();
        logger.info(""numshards {}, segments {}, total merges {}, current merge {}"", numOfShards, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
        long count = stats.getPrimaries().getSegments().getCount();
        assertThat(count, Matchers.lessThanOrEqualTo(upperNumberSegments));
    }
",non-flaky,5
91455,strapdata_elassandra,RecoveryWhileUnderLoadIT.testRecoverWhileUnderLoadAllocateReplicasTest,"@TestLogging(""_root:DEBUG,org.elasticsearch.index.shard:TRACE,org.elasticsearch.cluster.service:TRACE,org.elasticsearch.index.seqno:TRACE,org.elasticsearch.indices.recovery:TRACE"")
    public void testRecoverWhileUnderLoadAllocateReplicasTest() throws Exception {
        logger.info(""--> creating test index ..."");
        int numberOfShards = numberOfShards();
        assertAcked(prepareCreate(""test"", 1, Settings.builder().put(SETTING_NUMBER_OF_SHARDS, numberOfShards).put(SETTING_NUMBER_OF_REPLICAS, 1).put(IndexSettings.INDEX_TRANSLOG_DURABILITY_SETTING.getKey(), Translog.Durability.ASYNC)));

        final int totalNumDocs = scaledRandomIntBetween(200, 10000);
        int waitFor = totalNumDocs / 10;
        int extraDocs = waitFor;
        try (BackgroundIndexer indexer = new BackgroundIndexer(""test"", ""type"", client(), extraDocs)) {
            logger.info(""--> waiting for {} docs to be indexed ..."", waitFor);
            waitForDocs(waitFor, indexer);
            indexer.assertNoFailures();
            logger.info(""--> {} docs indexed"", waitFor);

            extraDocs = totalNumDocs / 10;
            waitFor += extraDocs;
            indexer.continueIndexing(extraDocs);
            logger.info(""--> flushing the index ...."");
            // now flush, just to make sure we have some data in the index, not just translog
            client().admin().indices().prepareFlush().execute().actionGet();

            logger.info(""--> waiting for {} docs to be indexed ..."", waitFor);
            waitForDocs(waitFor, indexer);
            indexer.assertNoFailures();
            logger.info(""--> {} docs indexed"", waitFor);

            extraDocs = totalNumDocs - waitFor;
            indexer.continueIndexing(extraDocs);

            logger.info(""--> allow 2 nodes for index [test] ..."");
            // now start another node, while we index
            allowNodes(""test"", 2);

            logger.info(""--> waiting for GREEN health status ..."");
            // make sure the cluster state is green, and all has been recovered
            assertNoTimeout(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setTimeout(""5m"").setWaitForGreenStatus());

            logger.info(""--> waiting for {} docs to be indexed ..."", totalNumDocs);
            waitForDocs(totalNumDocs, indexer);
            indexer.assertNoFailures();
            logger.info(""--> {} docs indexed"", totalNumDocs);

            logger.info(""--> marking and waiting for indexing threads to stop ..."");
            indexer.stop();
            logger.info(""--> indexing threads stopped"");

            logger.info(""--> refreshing the index"");
            refreshAndAssert();
            logger.info(""--> verifying indexed content"");
            iterateAssertCount(numberOfShards, 10, indexer.getIds());
        }
    }
",non-flaky,5
91456,strapdata_elassandra,RelocationIT.testSimpleRelocationNoIndexing,"@TestLogging(""_root:DEBUG,org.elasticsearch.indices.recovery:TRACE,org.elasticsearch.index.shard.service:TRACE"")
    public void testSimpleRelocationNoIndexing() {
        logger.info(""--> starting [node1] ..."");
        final String node_1 = internalCluster().startNode();

        logger.info(""--> creating test index ..."");
        prepareCreate(""test"", Settings.builder()
                .put(""index.number_of_shards"", 1)
                .put(""index.number_of_replicas"", 0)
        ).get();

        logger.info(""--> index 10 docs"");
        for (int i = 0; i < 10; i++) {
            client().prepareIndex(""test"", ""type"", Integer.toString(i)).setSource(""field"", ""value"" + i).execute().actionGet();
        }
        logger.info(""--> flush so we have an actual index"");
        client().admin().indices().prepareFlush().execute().actionGet();
        logger.info(""--> index more docs so we have something in the translog"");
        for (int i = 10; i < 20; i++) {
            client().prepareIndex(""test"", ""type"", Integer.toString(i)).setSource(""field"", ""value"" + i).execute().actionGet();
        }

        logger.info(""--> verifying count"");
        client().admin().indices().prepareRefresh().execute().actionGet();
        assertThat(client().prepareSearch(""test"").setSize(0).execute().actionGet().getHits().getTotalHits(), equalTo(20L));

        logger.info(""--> start another node"");
        final String node_2 = internalCluster().startNode();
        ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(""2"").execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        logger.info(""--> relocate the shard from node1 to node2"");
        client().admin().cluster().prepareReroute()
                .add(new MoveAllocationCommand(""test"", 0, node_1, node_2))
                .execute().actionGet();

        clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNoRelocatingShards(true).setTimeout(ACCEPTABLE_RELOCATION_TIME).execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        logger.info(""--> verifying count again..."");
        client().admin().indices().prepareRefresh().execute().actionGet();
        assertThat(client().prepareSearch(""test"").setSize(0).execute().actionGet().getHits().getTotalHits(), equalTo(20L));
    }
",non-flaky,5
91457,strapdata_elassandra,RelocationIT.testRelocationWhileIndexingRandom,"    @TestLogging(""org.elasticsearch.action.bulk:TRACE,org.elasticsearch.action.search:TRACE"")
    public void testRelocationWhileIndexingRandom() throws Exception {
        int numberOfRelocations = scaledRandomIntBetween(1, rarely() ? 10 : 4);
        int numberOfReplicas = randomBoolean() ? 0 : 1;
        int numberOfNodes = numberOfReplicas == 0 ? 2 : 3;

        logger.info(""testRelocationWhileIndexingRandom(numRelocations={}, numberOfReplicas={}, numberOfNodes={})"", numberOfRelocations, numberOfReplicas, numberOfNodes);

        String[] nodes = new String[numberOfNodes];
        logger.info(""--> starting [node1] ..."");
        nodes[0] = internalCluster().startNode();

        logger.info(""--> creating test index ..."");
        prepareCreate(""test"", Settings.builder()
            .put(""index.number_of_shards"", 1)
            .put(""index.number_of_replicas"", numberOfReplicas)
        ).get();


        for (int i = 2; i <= numberOfNodes; i++) {
            logger.info(""--> starting [node{}] ..."", i);
            nodes[i - 1] = internalCluster().startNode();
            if (i != numberOfNodes) {
                ClusterHealthResponse healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID)
                        .setWaitForNodes(Integer.toString(i)).setWaitForGreenStatus().execute().actionGet();
                assertThat(healthResponse.isTimedOut(), equalTo(false));
            }
        }

        int numDocs = scaledRandomIntBetween(200, 2500);
        try (BackgroundIndexer indexer = new BackgroundIndexer(""test"", ""type1"", client(), numDocs)) {
            logger.info(""--> waiting for {} docs to be indexed ..."", numDocs);
            waitForDocs(numDocs, indexer);
            logger.info(""--> {} docs indexed"", numDocs);

            logger.info(""--> starting relocations..."");
            int nodeShiftBased = numberOfReplicas; // if we have replicas shift those
            for (int i = 0; i < numberOfRelocations; i++) {
                int fromNode = (i % 2);
                int toNode = fromNode == 0 ? 1 : 0;
                fromNode += nodeShiftBased;
                toNode += nodeShiftBased;
                numDocs = scaledRandomIntBetween(200, 1000);
                logger.debug(""--> Allow indexer to index [{}] documents"", numDocs);
                indexer.continueIndexing(numDocs);
                logger.info(""--> START relocate the shard from {} to {}"", nodes[fromNode], nodes[toNode]);
                client().admin().cluster().prepareReroute()
                        .add(new MoveAllocationCommand(""test"", 0, nodes[fromNode], nodes[toNode]))
                        .get();
                if (rarely()) {
                    logger.debug(""--> flushing"");
                    client().admin().indices().prepareFlush().get();
                }
                ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNoRelocatingShards(true).setTimeout(ACCEPTABLE_RELOCATION_TIME).execute().actionGet();
                assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));
                indexer.pauseIndexing();
                logger.info(""--> DONE relocate the shard from {} to {}"", fromNode, toNode);
            }
            logger.info(""--> done relocations"");
            logger.info(""--> waiting for indexing threads to stop ..."");
            indexer.stop();
            logger.info(""--> indexing threads stopped"");

            logger.info(""--> refreshing the index"");
            client().admin().indices().prepareRefresh(""test"").execute().actionGet();
            logger.info(""--> searching the index"");
            boolean ranOnce = false;
            for (int i = 0; i < 10; i++) {
                    logger.info(""--> START search test round {}"", i + 1);
                    SearchHits hits = client().prepareSearch(""test"").setQuery(matchAllQuery()).setSize((int) indexer.totalIndexedDocs()).storedFields().execute().actionGet().getHits();
                    ranOnce = true;
                    if (hits.getTotalHits() != indexer.totalIndexedDocs()) {
                        int[] hitIds = new int[(int) indexer.totalIndexedDocs()];
                        for (int hit = 0; hit < indexer.totalIndexedDocs(); hit++) {
                            hitIds[hit] = hit + 1;
                        }
                        IntHashSet set = IntHashSet.from(hitIds);
                        for (SearchHit hit : hits.getHits()) {
                            int id = Integer.parseInt(hit.getId());
                            if (!set.remove(id)) {
                                logger.error(""Extra id [{}]"", id);
                            }
                        }
                        set.forEach((IntProcedure) value -> {
                            logger.error(""Missing id [{}]"", value);
                        });
                    }
                    assertThat(hits.getTotalHits(), equalTo(indexer.totalIndexedDocs()));
                    logger.info(""--> DONE search test round {}"", i + 1);

            }
            if (!ranOnce) {
                fail();
            }
        }
    }
",non-flaky,5
91458,strapdata_elassandra,RelocationIT.indexShardStateChanged,"    @TestLogging(""org.elasticsearch.action.bulk:TRACE,org.elasticsearch.action.search:TRACE"")
    public void testRelocationWhileRefreshing() throws Exception {
        int numberOfRelocations = scaledRandomIntBetween(1, rarely() ? 10 : 4);
        int numberOfReplicas = randomBoolean() ? 0 : 1;
        int numberOfNodes = numberOfReplicas == 0 ? 2 : 3;

        logger.info(""testRelocationWhileIndexingRandom(numRelocations={}, numberOfReplicas={}, numberOfNodes={})"", numberOfRelocations, numberOfReplicas, numberOfNodes);

        String[] nodes = new String[numberOfNodes];
        logger.info(""--> starting [node_0] ..."");
        nodes[0] = internalCluster().startNode();

        logger.info(""--> creating test index ..."");
        prepareCreate(
                ""test"",
                Settings.builder()
                        .put(""index.number_of_shards"", 1)
                        .put(""index.number_of_replicas"", numberOfReplicas)
                        .put(""index.refresh_interval"", -1) // we want to control refreshes
                        .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), ""100ms""))
                .get();

        for (int i = 1; i < numberOfNodes; i++) {
            logger.info(""--> starting [node_{}] ..."", i);
            nodes[i] = internalCluster().startNode();
            if (i != numberOfNodes - 1) {
                ClusterHealthResponse healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID)
                        .setWaitForNodes(Integer.toString(i + 1)).setWaitForGreenStatus().execute().actionGet();
                assertThat(healthResponse.isTimedOut(), equalTo(false));
            }
        }

        final Semaphore postRecoveryShards = new Semaphore(0);
        final IndexEventListener listener = new IndexEventListener() {
            @Override
            public void indexShardStateChanged(IndexShard indexShard, @Nullable IndexShardState previousState, IndexShardState currentState, @Nullable String reason) {
                if (currentState == IndexShardState.POST_RECOVERY) {
                    postRecoveryShards.release();
                }
            }
",non-flaky,5
91459,strapdata_elassandra,RelocationIT.testIndexAndRelocateConcurrently,"    @TestLogging(
    public void testIndexAndRelocateConcurrently() throws ExecutionException, InterruptedException {
        int halfNodes = randomIntBetween(1, 3);
        Settings[] nodeSettings = Stream.concat(
            Stream.generate(() -> Settings.builder().put(""node.attr.color"", ""blue"").build()).limit(halfNodes),
            Stream.generate(() -> Settings.builder().put(""node.attr.color"", ""red"").build()).limit(halfNodes)
            ).toArray(Settings[]::new);
        List<String> nodes = internalCluster().startNodes(nodeSettings);
        String[] blueNodes = nodes.subList(0, halfNodes).stream().toArray(String[]::new);
        String[] redNodes = nodes.subList(halfNodes, nodes.size()).stream().toArray(String[]::new);
        logger.info(""blue nodes: {}"", (Object)blueNodes);
        logger.info(""red nodes: {}"", (Object)redNodes);
        ensureStableCluster(halfNodes * 2);

        final Settings.Builder settings = Settings.builder()
                .put(""index.routing.allocation.exclude.color"", ""blue"")
                .put(indexSettings())
                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomInt(halfNodes - 1))
                .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), ""100ms"");
        assertAcked(prepareCreate(""test"", settings));
        assertAllShardsOnNodes(""test"", redNodes);
        int numDocs = randomIntBetween(100, 150);
        ArrayList<String> ids = new ArrayList<>();
        logger.info("" --> indexing [{}] docs"", numDocs);
        IndexRequestBuilder[] docs = new IndexRequestBuilder[numDocs];
        for (int i = 0; i < numDocs; i++) {
            String id = randomRealisticUnicodeOfLength(10) + String.valueOf(i);
            ids.add(id);
            docs[i] = client().prepareIndex(""test"", ""type1"", id).setSource(""field1"", English.intToEnglish(i));
        }
        indexRandom(true, docs);
        SearchResponse countResponse = client().prepareSearch(""test"").get();
        assertHitCount(countResponse, numDocs);

        logger.info("" --> moving index to new nodes"");
        Settings build = Settings.builder().put(""index.routing.allocation.exclude.color"", ""red"")
            .put(""index.routing.allocation.include.color"", ""blue"").build();
        client().admin().indices().prepareUpdateSettings(""test"").setSettings(build).execute().actionGet();

        // index while relocating
        logger.info("" --> indexing [{}] more docs"", numDocs);
        for (int i = 0; i < numDocs; i++) {
            String id = randomRealisticUnicodeOfLength(10) + String.valueOf(numDocs + i);
            ids.add(id);
            docs[i] = client().prepareIndex(""test"", ""type1"", id).setSource(""field1"", English.intToEnglish(numDocs + i));
        }
        indexRandom(true, docs);
        numDocs *= 2;

        logger.info("" --> waiting for relocation to complete"");
        ensureGreen(""test""); // move all shards to the new nodes (it waits on relocation)

        final int numIters = randomIntBetween(10, 20);
        for (int i = 0; i < numIters; i++) {
            logger.info("" --> checking iteration {}"", i);
            SearchResponse afterRelocation = client().prepareSearch().setSize(ids.size()).get();
            assertNoFailures(afterRelocation);
            assertSearchHits(afterRelocation, ids.toArray(new String[ids.size()]));
        }

    }
",non-flaky,5
91460,strapdata_elassandra,SnapshotDisruptionIT.clusterChanged,"@TestLogging(""org.elasticsearch.snapshot:TRACE"")
    public void testDisruptionOnSnapshotInitialization() throws Exception {
        final Settings settings = Settings.builder()
            .put(DEFAULT_SETTINGS)
            .put(DiscoverySettings.COMMIT_TIMEOUT_SETTING.getKey(), ""30s"") // wait till cluster state is committed
            .build();
        final String idxName = ""test"";
        configureCluster(settings, 4, null, 2);
        final List<String> allMasterEligibleNodes = internalCluster().startMasterOnlyNodes(3);
        final String dataNode = internalCluster().startDataOnlyNode();
        ensureStableCluster(4);

        createRandomIndex(idxName);

        logger.info(""-->  creating repository"");
        assertAcked(client().admin().cluster().preparePutRepository(""test-repo"")
            .setType(""fs"").setSettings(Settings.builder()
                .put(""location"", randomRepoPath())
                .put(""compress"", randomBoolean())
                .put(""chunk_size"", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));

        // Writing incompatible snapshot can cause this test to fail due to a race condition in repo initialization
        // by the current master and the former master. It is not causing any issues in real life scenario, but
        // might make this test to fail. We are going to complete initialization of the snapshot to prevent this failures.
        logger.info(""-->  initializing the repository"");
        assertEquals(SnapshotState.SUCCESS, client().admin().cluster().prepareCreateSnapshot(""test-repo"", ""test-snap-1"")
            .setWaitForCompletion(true).setIncludeGlobalState(true).setIndices().get().getSnapshotInfo().state());

        final String masterNode1 = internalCluster().getMasterName();
        Set<String> otherNodes = new HashSet<>();
        otherNodes.addAll(allMasterEligibleNodes);
        otherNodes.remove(masterNode1);
        otherNodes.add(dataNode);

        NetworkDisruption networkDisruption =
            new NetworkDisruption(new NetworkDisruption.TwoPartitions(Collections.singleton(masterNode1), otherNodes),
                new NetworkDisruption.NetworkUnresponsive());
        internalCluster().setDisruptionScheme(networkDisruption);

        ClusterService clusterService = internalCluster().clusterService(masterNode1);
        CountDownLatch disruptionStarted = new CountDownLatch(1);
        clusterService.addListener(new ClusterStateListener() {
            @Override
            public void clusterChanged(ClusterChangedEvent event) {
                SnapshotsInProgress snapshots = event.state().custom(SnapshotsInProgress.TYPE);
                if (snapshots != null && snapshots.entries().size() > 0) {
                    if (snapshots.entries().get(0).state() == SnapshotsInProgress.State.INIT) {
                        // The snapshot started, we can start disruption so the INIT state will arrive to another master node
                        logger.info(""--> starting disruption"");
                        networkDisruption.startDisrupting();
                        clusterService.removeListener(this);
                        disruptionStarted.countDown();
                    }
                }
            }
",non-flaky,5
91461,strapdata_elassandra,DiscoveryDisruptionIT.testIsolatedUnicastNodes,"@TestLogging(""_root:DEBUG,org.elasticsearch.cluster.service:TRACE"")
    public void testIsolatedUnicastNodes() throws Exception {
        List<String> nodes = startCluster(4, -1, new int[]{0});
        // Figure out what is the elected master node
        final String unicastTarget = nodes.get(0);

        Set<String> unicastTargetSide = new HashSet<>();
        unicastTargetSide.add(unicastTarget);

        Set<String> restOfClusterSide = new HashSet<>();
        restOfClusterSide.addAll(nodes);
        restOfClusterSide.remove(unicastTarget);

        // Forcefully clean temporal response lists on all nodes. Otherwise the node in the unicast host list
        // includes all the other nodes that have pinged it and the issue doesn't manifest
        ZenPing zenPing = ((TestZenDiscovery) internalCluster().getInstance(Discovery.class)).getZenPing();
        if (zenPing instanceof UnicastZenPing) {
            ((UnicastZenPing) zenPing).clearTemporalResponses();
        }

        // Simulate a network issue between the unicast target node and the rest of the cluster
        NetworkDisruption networkDisconnect = new NetworkDisruption(new TwoPartitions(unicastTargetSide, restOfClusterSide),
                new NetworkDisconnect());
        setDisruptionScheme(networkDisconnect);
        networkDisconnect.startDisrupting();
        // Wait until elected master has removed that the unlucky node...
        ensureStableCluster(3, nodes.get(1));

        // The isolate master node must report no master, so it starts with pinging
        assertNoMaster(unicastTarget);
        networkDisconnect.stopDisrupting();
        // Wait until the master node sees all 3 nodes again.
        ensureStableCluster(4);
    }
",non-flaky,5
91462,strapdata_elassandra,NodeJoinControllerTests.setUp,"@TestLogging(""org.elasticsearch.discovery.zen:TRACE,org.elasticsearch.cluster.service:TRACE"")
    public void setUp() throws Exception {
        super.setUp();
    }
",non-flaky,5
91463,strapdata_elassandra,ZenDiscoveryIT.testNoShardRelocationsOccurWhenElectedMasterNodeFails,"@TestLogging(""_root:DEBUG"")
    public void testNoShardRelocationsOccurWhenElectedMasterNodeFails() throws Exception {
        Settings defaultSettings = Settings.builder()
                .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), ""1s"")
                .put(FaultDetection.PING_RETRIES_SETTING.getKey(), ""1"")
                .build();

        Settings masterNodeSettings = Settings.builder()
                .put(Node.NODE_DATA_SETTING.getKey(), false)
                .put(defaultSettings)
                .build();
        internalCluster().startNodes(2, masterNodeSettings);
        Settings dateNodeSettings = Settings.builder()
                .put(Node.NODE_MASTER_SETTING.getKey(), false)
                .put(defaultSettings)
                .build();
        internalCluster().startNodes(2, dateNodeSettings);
        ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth()
                .setWaitForEvents(Priority.LANGUID)
                .setWaitForNodes(""4"")
                .setWaitForNoRelocatingShards(true)
                .get();
        assertThat(clusterHealthResponse.isTimedOut(), is(false));

        createIndex(""test"");
        ensureSearchable(""test"");
        RecoveryResponse r = client().admin().indices().prepareRecoveries(""test"").get();
        int numRecoveriesBeforeNewMaster = r.shardRecoveryStates().get(""test"").size();

        final String oldMaster = internalCluster().getMasterName();
        internalCluster().stopCurrentMasterNode();
        assertBusy(() -> {
            String current = internalCluster().getMasterName();
            assertThat(current, notNullValue());
            assertThat(current, not(equalTo(oldMaster)));
        });
        ensureSearchable(""test"");

        r = client().admin().indices().prepareRecoveries(""test"").get();
        int numRecoveriesAfterNewMaster = r.shardRecoveryStates().get(""test"").size();
        assertThat(numRecoveriesAfterNewMaster, equalTo(numRecoveriesBeforeNewMaster));
    }
",non-flaky,5
91464,strapdata_elassandra,PublishClusterStateActionTests.setAsMaster,"@TestLogging(""org.elasticsearch.discovery.zen.publish:TRACE"")
        public MockNode setAsMaster() {
            this.clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
                .masterNodeId(discoveryNode.getId())).build();
            return this;
        }
",non-flaky,5
91465,strapdata_elassandra,MasterDisruptionIT.testFailWithMinimumMasterNodesConfigured,"@TestLogging(""_root:DEBUG,org.elasticsearch.cluster.service:TRACE"")
    public void testFailWithMinimumMasterNodesConfigured() throws Exception {
        List<String> nodes = startCluster(3);

        // Figure out what is the elected master node
        final String masterNode = internalCluster().getMasterName();
        logger.info(""---> legit elected master node={}"", masterNode);

        // Pick a node that isn't the elected master.
        Set<String> nonMasters = new HashSet<>(nodes);
        nonMasters.remove(masterNode);
        final String unluckyNode = randomFrom(nonMasters.toArray(Strings.EMPTY_ARRAY));


        // Simulate a network issue between the unlucky node and elected master node in both directions.

        NetworkDisruption networkDisconnect = new NetworkDisruption(
                new NetworkDisruption.TwoPartitions(masterNode, unluckyNode),
                new NetworkDisruption.NetworkDisconnect());
        setDisruptionScheme(networkDisconnect);
        networkDisconnect.startDisrupting();

        // Wait until elected master has removed that the unlucky node...
        ensureStableCluster(2, masterNode);

        // The unlucky node must report *no* master node, since it can't connect to master and in fact it should
        // continuously ping until network failures have been resolved. However
        // It may a take a bit before the node detects it has been cut off from the elected master
        assertNoMaster(unluckyNode);

        networkDisconnect.stopDisrupting();

        // Wait until the master node sees all 3 nodes again.
        ensureStableCluster(3);

        // The elected master shouldn't have changed, since the unlucky node never could have elected himself as
        // master since m_m_n of 2 could never be satisfied.
        assertMaster(masterNode, nodes);
    }
",non-flaky,5
91466,strapdata_elassandra,MasterDisruptionIT.execute,"    @TestLogging(""_root:DEBUG,org.elasticsearch.cluster.service:TRACE,org.elasticsearch.test.disruption:TRACE"")
    public void testStaleMasterNotHijackingMajority() throws Exception {
        // 3 node cluster with unicast discovery and minimum_master_nodes set to 2:
        final List<String> nodes = startCluster(3, 2);

        // Save the current master node as old master node, because that node will get frozen
        final String oldMasterNode = internalCluster().getMasterName();
        for (String node : nodes) {
            ensureStableCluster(3, node);
        }
        assertMaster(oldMasterNode, nodes);

        // Simulating a painful gc by suspending all threads for a long time on the current elected master node.
        SingleNodeDisruption masterNodeDisruption = new LongGCDisruption(random(), oldMasterNode);

        // Save the majority side
        final List<String> majoritySide = new ArrayList<>(nodes);
        majoritySide.remove(oldMasterNode);

        // Keeps track of the previous and current master when a master node transition took place on each node on the majority side:
        final Map<String, List<Tuple<String, String>>> masters = Collections.synchronizedMap(new HashMap<String, List<Tuple<String,
                        String>>>());
        for (final String node : majoritySide) {
            masters.put(node, new ArrayList<Tuple<String, String>>());
            internalCluster().getInstance(ClusterService.class, node).addListener(event -> {
                DiscoveryNode previousMaster = event.previousState().nodes().getMasterNode();
                DiscoveryNode currentMaster = event.state().nodes().getMasterNode();
                if (!Objects.equals(previousMaster, currentMaster)) {
                    logger.info(""node {} received new cluster state: {} \n and had previous cluster state: {}"", node, event.state(),
                            event.previousState());
                    String previousMasterNodeName = previousMaster != null ? previousMaster.getName() : null;
                    String currentMasterNodeName = currentMaster != null ? currentMaster.getName() : null;
                    masters.get(node).add(new Tuple<>(previousMasterNodeName, currentMasterNodeName));
                }
            });
        }

        final CountDownLatch oldMasterNodeSteppedDown = new CountDownLatch(1);
        internalCluster().getInstance(ClusterService.class, oldMasterNode).addListener(event -> {
            if (event.state().nodes().getMasterNodeId() == null) {
                oldMasterNodeSteppedDown.countDown();
            }
        });

        internalCluster().setDisruptionScheme(masterNodeDisruption);
        logger.info(""freezing node [{}]"", oldMasterNode);
        masterNodeDisruption.startDisrupting();

        // Wait for the majority side to get stable
        assertDifferentMaster(majoritySide.get(0), oldMasterNode);
        assertDifferentMaster(majoritySide.get(1), oldMasterNode);

        // the test is periodically tripping on the following assertion. To find out which threads are blocking the nodes from making
        // progress we print a stack dump
        boolean failed = true;
        try {
            assertDiscoveryCompleted(majoritySide);
            failed = false;
        } finally {
            if (failed) {
                logger.error(""discovery failed to complete, probably caused by a blocked thread: {}"",
                        new HotThreads().busiestThreads(Integer.MAX_VALUE).ignoreIdleThreads(false).detect());
            }
        }

        // The old master node is frozen, but here we submit a cluster state update task that doesn't get executed,
        // but will be queued and once the old master node un-freezes it gets executed.
        // The old master node will send this update + the cluster state where he is flagged as master to the other
        // nodes that follow the new master. These nodes should ignore this update.
        internalCluster().getInstance(ClusterService.class, oldMasterNode).submitStateUpdateTask(""sneaky-update"", new
                ClusterStateUpdateTask(Priority.IMMEDIATE) {
                    @Override
                    public ClusterState execute(ClusterState currentState) throws Exception {
                        return ClusterState.builder(currentState).build();
                    }
",non-flaky,5
91467,strapdata_elassandra,MasterDisruptionIT.testIsolateMasterAndVerifyClusterStateConsensus,"    @TestLogging(
    public void testIsolateMasterAndVerifyClusterStateConsensus() throws Exception {
        final List<String> nodes = startCluster(3);

        assertAcked(prepareCreate(""test"")
                .setSettings(Settings.builder()
                        .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1 + randomInt(2))
                        .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomInt(2))
                ));

        ensureGreen();
        String isolatedNode = internalCluster().getMasterName();
        TwoPartitions partitions = isolateNode(isolatedNode);
        NetworkDisruption networkDisruption = addRandomDisruptionType(partitions);
        networkDisruption.startDisrupting();

        String nonIsolatedNode = partitions.getMajoritySide().iterator().next();

        // make sure cluster reforms
        ensureStableCluster(2, nonIsolatedNode);

        // make sure isolated need picks up on things.
        assertNoMaster(isolatedNode, TimeValue.timeValueSeconds(40));

        // restore isolation
        networkDisruption.stopDisrupting();

        for (String node : nodes) {
            ensureStableCluster(3, new TimeValue(DISRUPTION_HEALING_OVERHEAD.millis() + networkDisruption.expectedTimeToHeal().millis()),
                    true, node);
        }

        logger.info(""issue a reroute"");
        // trigger a reroute now, instead of waiting for the background reroute of RerouteService
        assertAcked(client().admin().cluster().prepareReroute());
        // and wait for it to finish and for the cluster to stabilize
        ensureGreen(""test"");

        // verify all cluster states are the same
        // use assert busy to wait for cluster states to be applied (as publish_timeout has low value)
        assertBusy(() -> {
            ClusterState state = null;
            for (String node : nodes) {
                ClusterState nodeState = getNodeClusterState(node);
                if (state == null) {
                    state = nodeState;
                    continue;
                }
                // assert nodes are identical
                try {
                    assertEquals(""unequal versions"", state.version(), nodeState.version());
                    assertEquals(""unequal node count"", state.nodes().getSize(), nodeState.nodes().getSize());
                    assertEquals(""different masters "", state.nodes().getMasterNodeId(), nodeState.nodes().getMasterNodeId());
                    assertEquals(""different meta data version"", state.metaData().version(), nodeState.metaData().version());
                    assertEquals(""different routing"", state.routingTable().toString(), nodeState.routingTable().toString());
                } catch (AssertionError t) {
                    fail(""failed comparing cluster state: "" + t.getMessage() + ""\n"" +
                            ""--- cluster state of node ["" + nodes.get(0) + ""]: ---\n"" + state +
                            ""\n--- cluster state ["" + node + ""]: ---\n"" + nodeState);
                }

            }
        });
    }
",non-flaky,5
91468,strapdata_elassandra,MasterDisruptionIT.testMappingTimeout,"    @TestLogging(
    public void testMappingTimeout() throws Exception {
        startCluster(3);
        createIndex(""test"", Settings.builder()
            .put(""index.number_of_shards"", 1)
            .put(""index.number_of_replicas"", 1)
            .put(""index.routing.allocation.exclude._name"", internalCluster().getMasterName())
        .build());

        // create one field
        index(""test"", ""doc"", ""1"", ""{ \""f\"": 1 }"");

        ensureGreen();

        assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(
            Settings.builder().put(""indices.mapping.dynamic_timeout"", ""1ms"")));

        ServiceDisruptionScheme disruption = new BlockMasterServiceOnMaster(random());
        setDisruptionScheme(disruption);

        disruption.startDisrupting();

        BulkRequestBuilder bulk = client().prepareBulk();
        bulk.add(client().prepareIndex(""test"", ""doc"", ""2"").setSource(""{ \""f\"": 1 }"", XContentType.JSON));
        bulk.add(client().prepareIndex(""test"", ""doc"", ""3"").setSource(""{ \""g\"": 1 }"", XContentType.JSON));
        bulk.add(client().prepareIndex(""test"", ""doc"", ""4"").setSource(""{ \""f\"": 1 }"", XContentType.JSON));
        BulkResponse bulkResponse = bulk.get();
        assertTrue(bulkResponse.hasFailures());

        disruption.stopDisrupting();

        assertBusy(() -> {
            IndicesStatsResponse stats = client().admin().indices().prepareStats(""test"").clear().get();
            for (ShardStats shardStats : stats.getShards()) {
                assertThat(shardStats.getShardRouting().toString(),
                    shardStats.getSeqNoStats().getGlobalCheckpoint(), equalTo(shardStats.getSeqNoStats().getLocalCheckpoint()));
            }
        });

    }
",non-flaky,5
91469,strapdata_elassandra,ClusterDisruptionIT.testAckedIndexing,"    @TestLogging(""_root:DEBUG,org.elasticsearch.action.bulk:TRACE,org.elasticsearch.action.get:TRACE,"" +
    public void testAckedIndexing() throws Exception {

        final int seconds = !(TEST_NIGHTLY && rarely()) ? 1 : 5;
        final String timeout = seconds + ""s"";

        final List<String> nodes = startCluster(rarely() ? 5 : 3);

        assertAcked(prepareCreate(""test"")
            .setSettings(Settings.builder()
                .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1 + randomInt(2))
                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomInt(2))
            ));
        ensureGreen();

        ServiceDisruptionScheme disruptionScheme = addRandomDisruptionScheme();
        logger.info(""disruption scheme [{}] added"", disruptionScheme);

        final ConcurrentHashMap<String, String> ackedDocs = new ConcurrentHashMap<>(); // id -> node sent.

        final AtomicBoolean stop = new AtomicBoolean(false);
        List<Thread> indexers = new ArrayList<>(nodes.size());
        List<Semaphore> semaphores = new ArrayList<>(nodes.size());
        final AtomicInteger idGenerator = new AtomicInteger(0);
        final AtomicReference<CountDownLatch> countDownLatchRef = new AtomicReference<>();
        final List<Exception> exceptedExceptions = Collections.synchronizedList(new ArrayList<Exception>());

        logger.info(""starting indexers"");
        try {
            for (final String node : nodes) {
                final Semaphore semaphore = new Semaphore(0);
                semaphores.add(semaphore);
                final Client client = client(node);
                final String name = ""indexer_"" + indexers.size();
                final int numPrimaries = getNumShards(""test"").numPrimaries;
                Thread thread = new Thread(() -> {
                    while (!stop.get()) {
                        String id = null;
                        try {
                            if (!semaphore.tryAcquire(10, TimeUnit.SECONDS)) {
                                continue;
                            }
                            logger.info(""[{}] Acquired semaphore and it has {} permits left"", name, semaphore.availablePermits());
                            try {
                                id = Integer.toString(idGenerator.incrementAndGet());
                                int shard = Math.floorMod(Murmur3HashFunction.hash(id), numPrimaries);
                                logger.trace(""[{}] indexing id [{}] through node [{}] targeting shard [{}]"", name, id, node, shard);
                                IndexResponse response =
                                        client.prepareIndex(""test"", ""type"", id)
                                                .setSource(""{}"", XContentType.JSON)
                                                .setTimeout(timeout)
                                                .get(timeout);
                                assertEquals(DocWriteResponse.Result.CREATED, response.getResult());
                                ackedDocs.put(id, node);
                                logger.trace(""[{}] indexed id [{}] through node [{}], response [{}]"", name, id, node, response);
                            } catch (ElasticsearchException e) {
                                exceptedExceptions.add(e);
                                final String docId = id;
                                logger.trace(() -> new ParameterizedMessage(""[{}] failed id [{}] through node [{}]"", name, docId, node), e);
                            } finally {
                                countDownLatchRef.get().countDown();
                                logger.trace(""[{}] decreased counter : {}"", name, countDownLatchRef.get().getCount());
                            }
                        } catch (InterruptedException e) {
                            // fine - semaphore interrupt
                        } catch (AssertionError | Exception e) {
                            logger.info(() -> new ParameterizedMessage(""unexpected exception in background thread of [{}]"", node), e);
                        }
                    }
                });

                thread.setName(name);
                thread.start();
                indexers.add(thread);
            }

            int docsPerIndexer = randomInt(3);
            logger.info(""indexing {} docs per indexer before partition"", docsPerIndexer);
            countDownLatchRef.set(new CountDownLatch(docsPerIndexer * indexers.size()));
            for (Semaphore semaphore : semaphores) {
                semaphore.release(docsPerIndexer);
            }
            assertTrue(countDownLatchRef.get().await(1, TimeUnit.MINUTES));

            for (int iter = 1 + randomInt(2); iter > 0; iter--) {
                logger.info(""starting disruptions & indexing (iteration [{}])"", iter);
                disruptionScheme.startDisrupting();

                docsPerIndexer = 1 + randomInt(5);
                logger.info(""indexing {} docs per indexer during partition"", docsPerIndexer);
                countDownLatchRef.set(new CountDownLatch(docsPerIndexer * indexers.size()));
                Collections.shuffle(semaphores, random());
                for (Semaphore semaphore : semaphores) {
                    assertThat(semaphore.availablePermits(), equalTo(0));
                    semaphore.release(docsPerIndexer);
                }
                logger.info(""waiting for indexing requests to complete"");
                assertTrue(countDownLatchRef.get().await(docsPerIndexer * seconds * 1000 + 2000, TimeUnit.MILLISECONDS));

                logger.info(""stopping disruption"");
                disruptionScheme.stopDisrupting();
                for (String node : internalCluster().getNodeNames()) {
                    ensureStableCluster(nodes.size(), TimeValue.timeValueMillis(disruptionScheme.expectedTimeToHeal().millis() +
                        DISRUPTION_HEALING_OVERHEAD.millis()), true, node);
                }
                // in case of a bridge partition, shard allocation can fail ""index.allocation.max_retries"" times if the master
                // is the super-connected node and recovery source and target are on opposite sides of the bridge
                if (disruptionScheme instanceof NetworkDisruption &&
                    ((NetworkDisruption) disruptionScheme).getDisruptedLinks() instanceof Bridge) {
                    assertAcked(client().admin().cluster().prepareReroute().setRetryFailed(true));
                }
                ensureGreen(""test"");

                logger.info(""validating successful docs"");
                assertBusy(() -> {
                    for (String node : nodes) {
                        try {
                            logger.debug(""validating through node [{}] ([{}] acked docs)"", node, ackedDocs.size());
                            for (String id : ackedDocs.keySet()) {
                                assertTrue(""doc ["" + id + ""] indexed via node ["" + ackedDocs.get(id) + ""] not found"",
                                    client(node).prepareGet(""test"", ""type"", id).setPreference(""_local"").get().isExists());
                            }
                        } catch (AssertionError | NoShardAvailableActionException e) {
                            throw new AssertionError(e.getMessage() + "" (checked via node ["" + node + ""]"", e);
                        }
                    }
                }, 30, TimeUnit.SECONDS);

                logger.info(""done validating (iteration [{}])"", iter);
            }
        } finally {
            if (exceptedExceptions.size() > 0) {
                StringBuilder sb = new StringBuilder();
                for (Exception e : exceptedExceptions) {
                    sb.append(""\n"").append(e.getMessage());
                }
                logger.debug(""Indexing exceptions during disruption: {}"", sb);
            }
            logger.info(""shutting down indexers"");
            stop.set(true);
            for (Thread indexer : indexers) {
                indexer.interrupt();
                indexer.join(60000);
            }
        }
    }
",non-flaky,5
91470,strapdata_elassandra,MasterServiceTests.execute,"    @TestLogging(""org.elasticsearch.cluster.service:TRACE"") // To ensure that we log cluster state events on TRACE level
    public void testClusterStateUpdateLogging() throws Exception {
        MockLogAppender mockAppender = new MockLogAppender();
        mockAppender.start();
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test1"",
                masterService.getClass().getCanonicalName(),
                Level.DEBUG,
                ""*processing [test1]: took [1s] no change in cluster state""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test2"",
                masterService.getClass().getCanonicalName(),
                Level.TRACE,
                ""*failed to execute cluster state update in [2s]*""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test3"",
                masterService.getClass().getCanonicalName(),
                Level.DEBUG,
                ""*processing [test3]: took [3s] done publishing updated cluster state (version: *, uuid: *)""));

        Logger clusterLogger = Loggers.getLogger(masterService.getClass().getPackage().getName());
        Loggers.addAppender(clusterLogger, mockAppender);
        try {
            final CountDownLatch latch = new CountDownLatch(4);
            masterService.currentTimeOverride = System.nanoTime();
            masterService.submitStateUpdateTask(""test1"", new ClusterStateUpdateTask() {
                @Override
                public ClusterState execute(ClusterState currentState) throws Exception {
                    masterService.currentTimeOverride += TimeValue.timeValueSeconds(1).nanos();
                    return currentState;
                }
",non-flaky,5
91471,strapdata_elassandra,MasterServiceTests.execute,"    @TestLogging(""org.elasticsearch.cluster.service:WARN"") // To ensure that we log cluster state events on WARN level
    public void testLongClusterStateUpdateLogging() throws Exception {
        MockLogAppender mockAppender = new MockLogAppender();
        mockAppender.start();
        mockAppender.addExpectation(
            new MockLogAppender.UnseenEventExpectation(
                ""test1 shouldn't see because setting is too low"",
                masterService.getClass().getCanonicalName(),
                Level.WARN,
                ""*cluster state update task [test1] took [*] above the warn threshold of *""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test2"",
                masterService.getClass().getCanonicalName(),
                Level.WARN,
                ""*cluster state update task [test2] took [32s] above the warn threshold of *""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test3"",
                masterService.getClass().getCanonicalName(),
                Level.WARN,
                ""*cluster state update task [test3] took [33s] above the warn threshold of *""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test4"",
                masterService.getClass().getCanonicalName(),
                Level.WARN,
                ""*cluster state update task [test4] took [34s] above the warn threshold of *""));

        Logger clusterLogger = Loggers.getLogger(masterService.getClass().getPackage().getName());
        Loggers.addAppender(clusterLogger, mockAppender);
        try {
            final CountDownLatch latch = new CountDownLatch(5);
            final CountDownLatch processedFirstTask = new CountDownLatch(1);
            masterService.currentTimeOverride = System.nanoTime();
            masterService.submitStateUpdateTask(""test1"", new ClusterStateUpdateTask() {
                @Override
                public ClusterState execute(ClusterState currentState) throws Exception {
                    masterService.currentTimeOverride += TimeValue.timeValueSeconds(1).nanos();
                    return currentState;
                }
",non-flaky,5
91472,strapdata_elassandra,ClusterApplierServiceTests.onSuccess,"    @TestLogging(""org.elasticsearch.cluster.service:TRACE"") // To ensure that we log cluster state events on TRACE level
    public void testClusterStateUpdateLogging() throws Exception {
        MockLogAppender mockAppender = new MockLogAppender();
        mockAppender.start();
        mockAppender.addExpectation(
                new MockLogAppender.SeenEventExpectation(
                        ""test1"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.DEBUG,
                        ""*processing [test1]: took [1s] no change in cluster state""));
        mockAppender.addExpectation(
                new MockLogAppender.SeenEventExpectation(
                        ""test2"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.TRACE,
                        ""*failed to execute cluster state applier in [2s]*""));

        Logger clusterLogger = Loggers.getLogger(""org.elasticsearch.cluster.service"");
        Loggers.addAppender(clusterLogger, mockAppender);
        try {
            final CountDownLatch latch = new CountDownLatch(3);
            clusterApplierService.currentTimeOverride = System.nanoTime();
            clusterApplierService.runOnApplierThread(""test1"",
                currentState -> clusterApplierService.currentTimeOverride += TimeValue.timeValueSeconds(1).nanos(),
                new ClusterApplyListener() {
                    @Override
                    public void onSuccess(String source) {
                        latch.countDown();
                    }
",non-flaky,5
91473,strapdata_elassandra,ClusterApplierServiceTests.onSuccess,"    @TestLogging(""org.elasticsearch.cluster.service:WARN"") // To ensure that we log cluster state events on WARN level
    public void testLongClusterStateUpdateLogging() throws Exception {
        MockLogAppender mockAppender = new MockLogAppender();
        mockAppender.start();
        mockAppender.addExpectation(
                new MockLogAppender.UnseenEventExpectation(
                        ""test1 shouldn't see because setting is too low"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.WARN,
                        ""*cluster state applier task [test1] took [*] above the warn threshold of *""));
        mockAppender.addExpectation(
                new MockLogAppender.SeenEventExpectation(
                        ""test2"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.WARN,
                        ""*cluster state applier task [test2] took [32s] above the warn threshold of *""));
        mockAppender.addExpectation(
                new MockLogAppender.SeenEventExpectation(
                        ""test4"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.WARN,
                        ""*cluster state applier task [test3] took [34s] above the warn threshold of *""));

        Logger clusterLogger = Loggers.getLogger(""org.elasticsearch.cluster.service"");
        Loggers.addAppender(clusterLogger, mockAppender);
        try {
            final CountDownLatch latch = new CountDownLatch(4);
            final CountDownLatch processedFirstTask = new CountDownLatch(1);
            clusterApplierService.currentTimeOverride = System.nanoTime();
            clusterApplierService.runOnApplierThread(""test1"",
                currentState -> clusterApplierService.currentTimeOverride += TimeValue.timeValueSeconds(1).nanos(),
                new ClusterApplyListener() {
                    @Override
                    public void onSuccess(String source) {
                        latch.countDown();
                        processedFirstTask.countDown();
                    }
",non-flaky,5
91474,strapdata_elassandra,ClusterServiceIT.execute,"    @TestLogging(""_root:debug,org.elasticsearch.action.admin.cluster.tasks:trace"")
    public void testPendingUpdateTask() throws Exception {
        String node_0 = internalCluster().startNode();
        internalCluster().startCoordinatingOnlyNode(Settings.EMPTY);

        final ClusterService clusterService = internalCluster().getInstance(ClusterService.class, node_0);
        final CountDownLatch block1 = new CountDownLatch(1);
        final CountDownLatch invoked1 = new CountDownLatch(1);
        clusterService.submitStateUpdateTask(""1"", new ClusterStateUpdateTask() {
            @Override
            public ClusterState execute(ClusterState currentState) {
                invoked1.countDown();
                try {
                    block1.await();
                } catch (InterruptedException e) {
                    fail();
                }
                return currentState;
            }
",non-flaky,5
91475,strapdata_elassandra,SpecificMasterNodesIT.testSimpleOnlyMasterNodeElection,"@TestLogging(""_root:DEBUG,org.elasticsearch.action.admin.cluster.state:TRACE"")
    public void testSimpleOnlyMasterNodeElection() throws IOException {
        logger.info(""--> start data node / non master node"");
        internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), true).put(Node.NODE_MASTER_SETTING.getKey(), false)
            .put(""discovery.initial_state_timeout"", ""1s""));
        try {
            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout(""100ms"").execute().actionGet().getState().nodes().getMasterNodeId(), nullValue());
            fail(""should not be able to find master"");
        } catch (MasterNotDiscoveredException e) {
            // all is well, no master elected
        }
        logger.info(""--> start master node"");
        final String masterNodeName = internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), false).put(Node.NODE_MASTER_SETTING.getKey(), true));
        assertThat(internalCluster().nonMasterClient().admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().getName(), equalTo(masterNodeName));
        assertThat(internalCluster().masterClient().admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().getName(), equalTo(masterNodeName));

        logger.info(""--> stop master node"");
        internalCluster().stopCurrentMasterNode();

        try {
            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout(""100ms"").execute().actionGet().getState().nodes().getMasterNodeId(), nullValue());
            fail(""should not be able to find master"");
        } catch (MasterNotDiscoveredException e) {
            // all is well, no master elected
        }

        logger.info(""--> start master node"");
        final String nextMasterEligibleNodeName = internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), false).put(Node.NODE_MASTER_SETTING.getKey(), true));
        assertThat(internalCluster().nonMasterClient().admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().getName(), equalTo(nextMasterEligibleNodeName));
        assertThat(internalCluster().masterClient().admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().getName(), equalTo(nextMasterEligibleNodeName));
    }
",non-flaky,5
91476,strapdata_elassandra,MinimumMasterNodesIT.testSimpleMinimumMasterNodes,"@TestLogging(""_root:DEBUG,org.elasticsearch.cluster.service:TRACE,org.elasticsearch.discovery.zen:TRACE"")
    public void testSimpleMinimumMasterNodes() throws Exception {

        Settings settings = Settings.builder()
                .put(""discovery.zen.minimum_master_nodes"", 2)
                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), ""200ms"")
                .put(""discovery.initial_state_timeout"", ""500ms"")
                .build();

        logger.info(""--> start first node"");
        internalCluster().startNode(settings);

        logger.info(""--> should be blocked, no master..."");
        ClusterState state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(true));
        assertThat(state.nodes().getSize(), equalTo(1)); // verify that we still see the local node in the cluster state

        logger.info(""--> start second node, cluster should be formed"");
        internalCluster().startNode(settings);

        ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(""2"").execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));
        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));

        state = client().admin().cluster().prepareState().execute().actionGet().getState();
        assertThat(state.nodes().getSize(), equalTo(2));
        assertThat(state.metaData().indices().containsKey(""test""), equalTo(false));

        createIndex(""test"");
        NumShards numShards = getNumShards(""test"");
        logger.info(""--> indexing some data"");
        for (int i = 0; i < 100; i++) {
            client().prepareIndex(""test"", ""type1"", Integer.toString(i)).setSource(""field"", ""value"").execute().actionGet();
        }
        // make sure that all shards recovered before trying to flush
        assertThat(client().admin().cluster().prepareHealth(""test"").setWaitForActiveShards(numShards.totalNumShards).execute().actionGet().getActiveShards(), equalTo(numShards.totalNumShards));
        // flush for simpler debugging
        flushAndRefresh();

        logger.info(""--> verify we the data back"");
        for (int i = 0; i < 10; i++) {
            assertThat(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet().getHits().getTotalHits(), equalTo(100L));
        }

        internalCluster().stopCurrentMasterNode();
        awaitBusy(() -> {
            ClusterState clusterState = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
            return clusterState.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID);
        });
        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(true));
        // verify that both nodes are still in the cluster state but there is no master
        assertThat(state.nodes().getSize(), equalTo(2));
        assertThat(state.nodes().getMasterNode(), equalTo(null));

        logger.info(""--> starting the previous master node again..."");
        internalCluster().startNode(settings);

        clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().setWaitForNodes(""2"").execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));
        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));

        state = client().admin().cluster().prepareState().execute().actionGet().getState();
        assertThat(state.nodes().getSize(), equalTo(2));
        assertThat(state.metaData().indices().containsKey(""test""), equalTo(true));

        ensureGreen();

        logger.info(""--> verify we the data back after cluster reform"");
        for (int i = 0; i < 10; i++) {
            assertHitCount(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet(), 100);
        }

        internalCluster().stopRandomNonMasterNode();
        assertBusy(() -> {
            ClusterState state1 = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
            assertThat(state1.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(true));
        });

        logger.info(""--> starting the previous master node again..."");
        internalCluster().startNode(settings);

        ensureGreen();
        clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(""2"").setWaitForGreenStatus().execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));
        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));

        state = client().admin().cluster().prepareState().execute().actionGet().getState();
        assertThat(state.nodes().getSize(), equalTo(2));
        assertThat(state.metaData().indices().containsKey(""test""), equalTo(true));

        logger.info(""Running Cluster Health"");
        ensureGreen();

        logger.info(""--> verify we the data back"");
        for (int i = 0; i < 10; i++) {
            assertHitCount(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet(), 100);
        }
    }
",non-flaky,5
91477,strapdata_elassandra,PrimaryAllocationIT.testPrimaryReplicaResyncFailed,"    @TestLogging(""_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE,"" +
    public void testPrimaryReplicaResyncFailed() throws Exception {
        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY);
        final int numberOfReplicas = between(2, 3);
        final String oldPrimary = internalCluster().startDataOnlyNode();
        assertAcked(
            prepareCreate(""test"", Settings.builder().put(indexSettings())
                .put(SETTING_NUMBER_OF_SHARDS, 1)
                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)));
        final ShardId shardId = new ShardId(clusterService().state().metaData().index(""test"").getIndex(), 0);
        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas));
        ensureGreen();
        assertAcked(
            client(master).admin().cluster().prepareUpdateSettings()
                .setTransientSettings(Settings.builder().put(""cluster.routing.allocation.enable"", ""none"")).get());
        logger.info(""--> Indexing with gap in seqno to ensure that some operations will be replayed in resync"");
        long numDocs = scaledRandomIntBetween(5, 50);
        for (int i = 0; i < numDocs; i++) {
            IndexResponse indexResult = index(""test"", ""doc"", Long.toString(i));
            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1));
        }
        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId);
        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard)); // Make gap in seqno.
        long moreDocs = scaledRandomIntBetween(1, 10);
        for (int i = 0; i < moreDocs; i++) {
            IndexResponse indexResult = index(""test"", ""doc"", Long.toString(numDocs + i));
            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1));
        }
        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes));
        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1);
        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect());
        internalCluster().setDisruptionScheme(partition);
        logger.info(""--> isolating some replicas during primary-replica resync"");
        partition.startDisrupting();
        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary));
        // Checks that we fails replicas in one side but not mark them as stale.
        assertBusy(() -> {
            ClusterState state = client(master).admin().cluster().prepareState().get().getState();
            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId);
            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName();
            assertThat(newPrimaryNode, not(equalTo(oldPrimary)));
            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2;
            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()));
            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {
                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition));
            }
            assertThat(state.metaData().index(""test"").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1));
        }, 1, TimeUnit.MINUTES);
        assertAcked(
            client(master).admin().cluster().prepareUpdateSettings()
                .setTransientSettings(Settings.builder().put(""cluster.routing.allocation.enable"", ""all"")).get());
        partition.stopDisrupting();
        partition.ensureHealthy(internalCluster());
        logger.info(""--> stop disrupting network and re-enable allocation"");
        assertBusy(() -> {
            ClusterState state = client(master).admin().cluster().prepareState().get().getState();
            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas));
            assertThat(state.metaData().index(""test"").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1));
            for (String node : replicaNodes) {
                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId);
                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs));
            }
        }, 30, TimeUnit.SECONDS);
        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex();
    }
",non-flaky,5
91478,strapdata_elassandra,PersistentTasksExecutorFullRestartIT.testFullClusterRestart,"    @TestLogging(""org.elasticsearch.persistent:TRACE,org.elasticsearch.cluster.service:DEBUG"")
    public void testFullClusterRestart() throws Exception {
        PersistentTasksService service = internalCluster().getInstance(PersistentTasksService.class);
        int numberOfTasks = randomIntBetween(1, 10);
        String[] taskIds = new String[numberOfTasks];
        List<PlainActionFuture<PersistentTask<TestParams>>> futures = new ArrayList<>(numberOfTasks);

        for (int i = 0; i < numberOfTasks; i++) {
            PlainActionFuture<PersistentTask<TestParams>> future = new PlainActionFuture<>();
            futures.add(future);
            taskIds[i] = UUIDs.base64UUID();
            service.sendStartRequest(taskIds[i], TestPersistentTasksExecutor.NAME, new TestParams(""Blah""), future);
        }

        for (int i = 0; i < numberOfTasks; i++) {
            assertThat(futures.get(i).get().getId(), equalTo(taskIds[i]));
        }

        PersistentTasksCustomMetaData tasksInProgress = internalCluster().clusterService().state().getMetaData()
                .custom(PersistentTasksCustomMetaData.TYPE);
        assertThat(tasksInProgress.tasks().size(), equalTo(numberOfTasks));

        // Make sure that at least one of the tasks is running
        assertBusy(() -> {
            // Wait for the task to start
            assertThat(client().admin().cluster().prepareListTasks().setActions(TestPersistentTasksExecutor.NAME + ""[c]"").get()
                    .getTasks().size(), greaterThan(0));
        });

        // Restart cluster
        internalCluster().fullRestart();
        ensureYellow();

        tasksInProgress = internalCluster().clusterService().state().getMetaData().custom(PersistentTasksCustomMetaData.TYPE);
        assertThat(tasksInProgress.tasks().size(), equalTo(numberOfTasks));
        // Check that cluster state is correct
        for (int i = 0; i < numberOfTasks; i++) {
            PersistentTask<?> task = tasksInProgress.getTask(taskIds[i]);
            assertNotNull(task);
        }

        logger.info(""Waiting for {} tasks to start"", numberOfTasks);
        assertBusy(() -> {
            // Wait for all tasks to start
            assertThat(client().admin().cluster().prepareListTasks().setActions(TestPersistentTasksExecutor.NAME + ""[c]"").get()
                            .getTasks().size(), equalTo(numberOfTasks));
        });

        logger.info(""Complete all tasks"");
        // Complete the running task and make sure it finishes properly
        assertThat(new TestPersistentTasksPlugin.TestTasksRequestBuilder(client()).setOperation(""finish"").get().getTasks().size(),
                equalTo(numberOfTasks));

        assertBusy(() -> {
            // Make sure the task is removed from the cluster state
            assertThat(((PersistentTasksCustomMetaData) internalCluster().clusterService().state().getMetaData()
                    .custom(PersistentTasksCustomMetaData.TYPE)).tasks(), empty());
        });

    }
",non-flaky,5
91479,strapdata_elassandra,SearchWhileCreatingIndexIT.testIndexCausesIndexCreation,"@TestLogging(""_root:DEBUG"")
    public void testIndexCausesIndexCreation() throws Exception {
        searchWhileCreatingIndex(false, 1); // 1 replica in our default...
    }
",non-flaky,5
91480,strapdata_elassandra,SharedClusterSnapshotRestoreIT.testReadonlyRepository,"    @TestLogging(""_root:DEBUG"")  // this fails every now and then: https://github.com/elastic/elasticsearch/issues/18121 but without
    public void testReadonlyRepository() throws Exception {
        Client client = client();
        logger.info(""-->  creating repository"");
        Path repositoryLocation = randomRepoPath();
        assertAcked(client.admin().cluster().preparePutRepository(""test-repo"")
                .setType(""fs"").setSettings(Settings.builder()
                        .put(""location"", repositoryLocation)
                        .put(""compress"", randomBoolean())
                        .put(""chunk_size"", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));

        createIndex(""test-idx"");
        ensureGreen();

        logger.info(""--> indexing some data"");
        for (int i = 0; i < 100; i++) {
            index(""test-idx"", ""_doc"", Integer.toString(i), ""foo"", ""bar"" + i);
        }
        refresh();

        logger.info(""--> snapshot"");
        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot(""test-repo"", ""test-snap"").setWaitForCompletion(true).setIndices(""test-idx"").get();
        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));

        assertThat(client.admin().cluster().prepareGetSnapshots(""test-repo"").setSnapshots(""test-snap"").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));

        logger.info(""--> delete index"");
        cluster().wipeIndices(""test-idx"");

        logger.info(""--> create read-only URL repository"");
        assertAcked(client.admin().cluster().preparePutRepository(""readonly-repo"")
                .setType(""fs"").setSettings(Settings.builder()
                        .put(""location"", repositoryLocation)
                        .put(""compress"", randomBoolean())
                        .put(""readonly"", true)
                        .put(""chunk_size"", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));
        logger.info(""--> restore index after deletion"");
        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot(""readonly-repo"", ""test-snap"").setWaitForCompletion(true).setIndices(""test-idx"").execute().actionGet();
        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));

        assertThat(client.prepareSearch(""test-idx"").setSize(0).get().getHits().getTotalHits(), equalTo(100L));

        logger.info(""--> list available shapshots"");
        GetSnapshotsResponse getSnapshotsResponse = client.admin().cluster().prepareGetSnapshots(""readonly-repo"").get();
        assertThat(getSnapshotsResponse.getSnapshots(), notNullValue());
        assertThat(getSnapshotsResponse.getSnapshots().size(), equalTo(1));

        logger.info(""--> try deleting snapshot"");
        assertThrows(client.admin().cluster().prepareDeleteSnapshot(""readonly-repo"", ""test-snap""), RepositoryException.class, ""cannot delete snapshot from a readonly repository"");

        logger.info(""--> try making another snapshot"");
        assertThrows(client.admin().cluster().prepareCreateSnapshot(""readonly-repo"", ""test-snap-2"").setWaitForCompletion(true).setIndices(""test-idx""), RepositoryException.class, ""cannot create snapshot in a readonly repository"");
    }
",non-flaky,5
91481,strapdata_elassandra,SharedClusterSnapshotRestoreIT.testAbortedSnapshotDuringInitDoesNotStart,"    @TestLogging(""org.elasticsearch.snapshots:TRACE"")
    public void testAbortedSnapshotDuringInitDoesNotStart() throws Exception {
        final Client client = client();

        // Blocks on initialization
        assertAcked(client.admin().cluster().preparePutRepository(""repository"")
            .setType(""mock"").setSettings(Settings.builder()
                .put(""location"", randomRepoPath())
                .put(""block_on_init"", true)
            ));

        createIndex(""test-idx"");
        final int nbDocs = scaledRandomIntBetween(100, 500);
        for (int i = 0; i < nbDocs; i++) {
            index(""test-idx"", ""_doc"", Integer.toString(i), ""foo"", ""bar"" + i);
        }
        flushAndRefresh(""test-idx"");
        assertThat(client.prepareSearch(""test-idx"").setSize(0).get().getHits().getTotalHits(), equalTo((long) nbDocs));

        // Create a snapshot
        client.admin().cluster().prepareCreateSnapshot(""repository"", ""snap"").execute();
        waitForBlock(internalCluster().getMasterName(), ""repository"", TimeValue.timeValueMinutes(1));
        boolean blocked = true;

        // Snapshot is initializing (and is blocked at this stage)
        SnapshotsStatusResponse snapshotsStatus = client.admin().cluster().prepareSnapshotStatus(""repository"").setSnapshots(""snap"").get();
        assertThat(snapshotsStatus.getSnapshots().iterator().next().getState(), equalTo(State.INIT));

        final List<State> states = new CopyOnWriteArrayList<>();
        final ClusterStateListener listener = event -> {
            SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE);
            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {
                if (""snap"".equals(entry.snapshot().getSnapshotId().getName())) {
                    states.add(entry.state());
                }
            }
        };

        try {
            // Record the upcoming states of the snapshot on all nodes
            internalCluster().getInstances(ClusterService.class).forEach(clusterService -> clusterService.addListener(listener));

            // Delete the snapshot while it is being initialized
            ActionFuture<AcknowledgedResponse> delete = client.admin().cluster().prepareDeleteSnapshot(""repository"", ""snap"").execute();

            // The deletion must set the snapshot in the ABORTED state
            assertBusy(() -> {
                SnapshotsStatusResponse status = client.admin().cluster().prepareSnapshotStatus(""repository"").setSnapshots(""snap"").get();
                assertThat(status.getSnapshots().iterator().next().getState(), equalTo(State.ABORTED));
            });

            // Now unblock the repository
            unblockNode(""repository"", internalCluster().getMasterName());
            blocked = false;

            assertAcked(delete.get());
            expectThrows(SnapshotMissingException.class, () ->
                client.admin().cluster().prepareGetSnapshots(""repository"").setSnapshots(""snap"").get());

            assertFalse(""Expecting snapshot state to be updated"", states.isEmpty());
            assertFalse(""Expecting snapshot to be aborted and not started at all"", states.contains(State.STARTED));
        } finally {
            internalCluster().getInstances(ClusterService.class).forEach(clusterService -> clusterService.removeListener(listener));
            if (blocked) {
                unblockNode(""repository"", internalCluster().getMasterName());
            }
        }
    }
",non-flaky,5
91482,strapdata_elassandra,QueueResizingEsThreadPoolExecutorTests.testAutoQueueSizingWithMax,"    @TestLogging(""org.elasticsearch.common.util.concurrent:DEBUG"")
    public void testAutoQueueSizingWithMax() throws Exception {
        ThreadContext context = new ThreadContext(Settings.EMPTY);
        ResizableBlockingQueue<Runnable> queue =
                new ResizableBlockingQueue<>(ConcurrentCollections.<Runnable>newBlockingQueue(),
                        5000);

        int threads = randomIntBetween(1, 5);
        int measureWindow = randomIntBetween(10, 100);
        int max = randomIntBetween(5010, 5024);
        logger.info(""--> auto-queue with a measurement window of {} tasks"", measureWindow);
        QueueResizingEsThreadPoolExecutor executor =
                new QueueResizingEsThreadPoolExecutor(
                        ""test-threadpool"", threads, threads, 1000,
                        TimeUnit.MILLISECONDS, queue, 10, max, fastWrapper(), measureWindow, TimeValue.timeValueMillis(1),
                        EsExecutors.daemonThreadFactory(""queuetest""), new EsAbortPolicy(), context);
        executor.prestartAllCoreThreads();
        logger.info(""--> executor: {}"", executor);

        // Execute a task multiple times that takes 1ms
        executeTask(executor, measureWindow * 3);

        // The queue capacity should increase, but no higher than the maximum
        assertBusy(() -> {
            assertThat(queue.capacity(), equalTo(max));
        });
        executor.shutdown();
        executor.awaitTermination(10, TimeUnit.SECONDS);
        context.close();
    }
",non-flaky,5
91483,strapdata_elassandra,IndexActionIT.testAutoGenerateIdNoDuplicates,"    @TestLogging(""_root:DEBUG,org.elasticsearch.index.shard.IndexShard:TRACE,org.elasticsearch.action.search:TRACE"")
    public void testAutoGenerateIdNoDuplicates() throws Exception {
        int numberOfIterations = scaledRandomIntBetween(10, 50);
        for (int i = 0; i < numberOfIterations; i++) {
            Exception firstError = null;
            createIndex(""test"");
            int numOfDocs = randomIntBetween(10, 100);
            logger.info(""indexing [{}] docs"", numOfDocs);
            List<IndexRequestBuilder> builders = new ArrayList<>(numOfDocs);
            for (int j = 0; j < numOfDocs; j++) {
                builders.add(client().prepareIndex(""test"", ""type"").setSource(""field"", ""value_"" + j));
            }
            indexRandom(true, builders);
            logger.info(""verifying indexed content"");
            int numOfChecks = randomIntBetween(8, 12);
            for (int j = 0; j < numOfChecks; j++) {
                try {
                    logger.debug(""running search with all types"");
                    SearchResponse response = client().prepareSearch(""test"").get();
                    if (response.getHits().getTotalHits() != numOfDocs) {
                        final String message = ""Count is "" + response.getHits().getTotalHits() + "" but "" + numOfDocs + "" was expected. ""
                            + ElasticsearchAssertions.formatShardStatus(response);
                        logger.error(""{}. search response: \n{}"", message, response);
                        fail(message);
                    }
                } catch (Exception e) {
                    logger.error(""search for all docs types failed"", e);
                    if (firstError == null) {
                        firstError = e;
                    }
                }
                try {
                    logger.debug(""running search with a specific type"");
                    SearchResponse response = client().prepareSearch(""test"").setTypes(""type"").get();
                    if (response.getHits().getTotalHits() != numOfDocs) {
                        final String message = ""Count is "" + response.getHits().getTotalHits() + "" but "" + numOfDocs + "" was expected. ""
                            + ElasticsearchAssertions.formatShardStatus(response);
                        logger.error(""{}. search response: \n{}"", message, response);
                        fail(message);
                    }
                } catch (Exception e) {
                    logger.error(""search for all docs of a specific type failed"", e);
                    if (firstError == null) {
                        firstError = e;
                    }
                }
            }
            if (firstError != null) {
                fail(firstError.getMessage());
            }
            internalCluster().wipeIndices(""test"");
        }
    }
",non-flaky,5
91484,strapdata_elassandra,IndexingMasterFailoverIT.run,"    @TestLogging(""_root:DEBUG"")
    public void testMasterFailoverDuringIndexingWithMappingChanges() throws Throwable {
        logger.info(""--> start 4 nodes, 3 master, 1 data"");

        final Settings sharedSettings = Settings.builder()
                .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), ""1s"") // for hitting simulated network failures quickly
                .put(FaultDetection.PING_RETRIES_SETTING.getKey(), ""1"") // for hitting simulated network failures quickly
                .put(""discovery.zen.join_timeout"", ""10s"")  // still long to induce failures but to long so test won't time out
                .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), ""1s"") // <-- for hitting simulated network failures quickly
                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2)
                .build();

        internalCluster().startMasterOnlyNodes(3, sharedSettings);

        String dataNode = internalCluster().startDataOnlyNode(sharedSettings);

        logger.info(""--> wait for all nodes to join the cluster"");
        ensureStableCluster(4);

        // We index data with mapping changes into cluster and have master failover at same time
        client().admin().indices().prepareCreate(""myindex"")
                .setSettings(Settings.builder().put(""index.number_of_shards"", 1).put(""index.number_of_replicas"", 0))
                .get();
        ensureGreen(""myindex"");

        final CyclicBarrier barrier = new CyclicBarrier(2);

        Thread indexingThread = new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    barrier.await();
                } catch (InterruptedException e) {
                    logger.warn(""Barrier interrupted"", e);
                    return;
                } catch (BrokenBarrierException e) {
                    logger.warn(""Broken barrier"", e);
                    return;
                }
                for (int i = 0; i < 10; i++) {
                    // index data with mapping changes
                    IndexResponse response = client(dataNode).prepareIndex(""myindex"", ""mytype"").setSource(""field_"" + i, ""val"").get();
                    assertEquals(DocWriteResponse.Result.CREATED, response.getResult());
                }
            }
",non-flaky,5
91485,strapdata_elassandra,IndicesShardStoreRequestIT.testEmpty,"@TestLogging(""_root:DEBUG,org.elasticsearch.action.admin.indices.shards:TRACE,org.elasticsearch.cluster.service:TRACE"")
    public void testEmpty() {
        ensureGreen();
        IndicesShardStoresResponse rsp = client().admin().indices().prepareShardStores().get();
        assertThat(rsp.getStoreStatuses().size(), equalTo(0));
    }
",non-flaky,5
91486,strapdata_elassandra,RemoteClusterConnectionTests.run,"    @TestLogging(""_root:DEBUG, org.elasticsearch.transport:TRACE"")
    public void testCloseWhileConcurrentlyConnecting() throws IOException, InterruptedException, BrokenBarrierException {
        List<DiscoveryNode> knownNodes = new CopyOnWriteArrayList<>();
        try (MockTransportService seedTransport = startTransport(""seed_node"", knownNodes, Version.CURRENT);
             MockTransportService seedTransport1 = startTransport(""seed_node_1"", knownNodes, Version.CURRENT);
             MockTransportService discoverableTransport = startTransport(""discoverable_node"", knownNodes, Version.CURRENT)) {
            DiscoveryNode seedNode = seedTransport.getLocalDiscoNode();
            DiscoveryNode seedNode1 = seedTransport1.getLocalDiscoNode();
            knownNodes.add(seedTransport.getLocalDiscoNode());
            knownNodes.add(discoverableTransport.getLocalDiscoNode());
            knownNodes.add(seedTransport1.getLocalDiscoNode());
            Collections.shuffle(knownNodes, random());
            List<Supplier<DiscoveryNode>> seedNodes = Arrays.asList(() -> seedNode1, () -> seedNode);
            Collections.shuffle(seedNodes, random());

            try (MockTransportService service = MockTransportService.createNewService(Settings.EMPTY, Version.CURRENT, threadPool, null)) {
                service.start();
                service.acceptIncomingRequests();
                try (RemoteClusterConnection connection = new RemoteClusterConnection(Settings.EMPTY, ""test-cluster"",
                    seedNodes, service, service.getConnectionManager(), Integer.MAX_VALUE, n -> true)) {
                    int numThreads = randomIntBetween(4, 10);
                    Thread[] threads = new Thread[numThreads];
                    CyclicBarrier barrier = new CyclicBarrier(numThreads + 1);
                    for (int i = 0; i < threads.length; i++) {
                        final int numConnectionAttempts = randomIntBetween(10, 100);
                        threads[i] = new Thread() {
                            @Override
                            public void run() {
                                try {
                                    barrier.await();
                                    CountDownLatch latch = new CountDownLatch(numConnectionAttempts);
                                    for (int i = 0; i < numConnectionAttempts; i++) {
                                        AtomicReference<Exception> executed = new AtomicReference<>();
                                        ActionListener<Void> listener = ActionListener.wrap(
                                            x -> {
                                                if (executed.compareAndSet(null, new RuntimeException())) {
                                                    latch.countDown();
                                                } else {
                                                    throw new AssertionError(""shit's been called twice"", executed.get());
                                                }
                                            },
                                            x -> {
                                                if (executed.compareAndSet(null, x)) {
                                                    latch.countDown();
                                                } else {
                                                    final String message = x.getMessage();
                                                    if ((executed.get().getClass() == x.getClass()
                                                        && ""operation was cancelled reason [connect handler is closed]"".equals(message)
                                                        && message.equals(executed.get().getMessage())) == false) {
                                                        // we do cancel the operation and that means that if timing allows it, the caller
                                                        // of a blocking call as well as the handler will get the exception from the
                                                        // ExecutionCancelledException concurrently. unless that is the case we fail
                                                        // if we get called more than once!
                                                        AssertionError assertionError = new AssertionError(""shit's been called twice"", x);
                                                        assertionError.addSuppressed(executed.get());
                                                        throw assertionError;
                                                    }
                                                }
                                                if (x instanceof RejectedExecutionException || x instanceof AlreadyClosedException
                                                    || x instanceof CancellableThreads.ExecutionCancelledException) {
                                                    // that's fine
                                                } else {
                                                    throw new AssertionError(x);
                                                }
                                            });
                                        try {
                                            connection.updateSeedNodes(null, seedNodes, listener);
                                        } catch (Exception e) {
                                            // it's ok if we're shutting down
                                            assertThat(e.getMessage(), containsString(""threadcontext is already closed""));
                                            latch.countDown();
                                        }
                                    }
                                    latch.await();
                                } catch (Exception ex) {
                                    throw new AssertionError(ex);
                                }
                            }
",non-flaky,5
91487,strapdata_elassandra,IndexPrimaryRelocationIT.run,"    @TestLogging(""_root:DEBUG,org.elasticsearch.action.bulk:TRACE,org.elasticsearch.index.shard:TRACE,org.elasticsearch.cluster.service:TRACE"")
    public void testPrimaryRelocationWhileIndexing() throws Exception {
        internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(2, 3));
        client().admin().indices().prepareCreate(""test"")
            .setSettings(Settings.builder().put(""index.number_of_shards"", 1).put(""index.number_of_replicas"", 0))
            .addMapping(""type"", ""field"", ""type=text"")
            .get();
        ensureGreen(""test"");
        AtomicInteger numAutoGenDocs = new AtomicInteger();
        final AtomicBoolean finished = new AtomicBoolean(false);
        Thread indexingThread = new Thread() {
            @Override
            public void run() {
                while (finished.get() == false) {
                    IndexResponse indexResponse = client().prepareIndex(""test"", ""type"", ""id"").setSource(""field"", ""value"").get();
                    assertEquals(DocWriteResponse.Result.CREATED, indexResponse.getResult());
                    DeleteResponse deleteResponse = client().prepareDelete(""test"", ""type"", ""id"").get();
                    assertEquals(DocWriteResponse.Result.DELETED, deleteResponse.getResult());
                    client().prepareIndex(""test"", ""type"").setSource(""auto"", true).get();
                    numAutoGenDocs.incrementAndGet();
                }
            }
",non-flaky,5
91488,strapdata_elassandra,IndexRecoveryIT.testRerouteRecovery,"    @TestLogging(
    public void testRerouteRecovery() throws Exception {
        logger.info(""--> start node A"");
        final String nodeA = internalCluster().startNode();

        logger.info(""--> create index on node: {}"", nodeA);
        ByteSizeValue shardSize = createAndPopulateIndex(INDEX_NAME, 1, SHARD_COUNT, REPLICA_COUNT).getShards()[0].getStats().getStore().size();

        logger.info(""--> start node B"");
        final String nodeB = internalCluster().startNode();

        ensureGreen();

        logger.info(""--> slowing down recoveries"");
        slowDownRecovery(shardSize);

        logger.info(""--> move shard from: {} to: {}"", nodeA, nodeB);
        client().admin().cluster().prepareReroute()
                .add(new MoveAllocationCommand(INDEX_NAME, 0, nodeA, nodeB))
                .execute().actionGet().getState();

        logger.info(""--> waiting for recovery to start both on source and target"");
        final Index index = resolveIndex(INDEX_NAME);
        assertBusy(() -> {
            IndicesService indicesService = internalCluster().getInstance(IndicesService.class, nodeA);
            assertThat(indicesService.indexServiceSafe(index).getShard(0).recoveryStats().currentAsSource(),
                    equalTo(1));
            indicesService = internalCluster().getInstance(IndicesService.class, nodeB);
            assertThat(indicesService.indexServiceSafe(index).getShard(0).recoveryStats().currentAsTarget(),
                    equalTo(1));
        });

        logger.info(""--> request recoveries"");
        RecoveryResponse response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();

        List<RecoveryState> recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);
        List<RecoveryState> nodeARecoveryStates = findRecoveriesForTargetNode(nodeA, recoveryStates);
        assertThat(nodeARecoveryStates.size(), equalTo(1));
        List<RecoveryState> nodeBRecoveryStates = findRecoveriesForTargetNode(nodeB, recoveryStates);
        assertThat(nodeBRecoveryStates.size(), equalTo(1));

        assertRecoveryState(nodeARecoveryStates.get(0), 0, RecoverySource.EmptyStoreRecoverySource.INSTANCE, true, Stage.DONE, null, nodeA);
        validateIndexRecoveryState(nodeARecoveryStates.get(0).getIndex());

        assertOnGoingRecoveryState(nodeBRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, nodeA, nodeB);
        validateIndexRecoveryState(nodeBRecoveryStates.get(0).getIndex());

        logger.info(""--> request node recovery stats"");
        NodesStatsResponse statsResponse = client().admin().cluster().prepareNodesStats().clear().setIndices(new CommonStatsFlags(CommonStatsFlags.Flag.Recovery)).get();
        long nodeAThrottling = Long.MAX_VALUE;
        long nodeBThrottling = Long.MAX_VALUE;
        for (NodeStats nodeStats : statsResponse.getNodes()) {
            final RecoveryStats recoveryStats = nodeStats.getIndices().getRecoveryStats();
            if (nodeStats.getNode().getName().equals(nodeA)) {
                assertThat(""node A should have ongoing recovery as source"", recoveryStats.currentAsSource(), equalTo(1));
                assertThat(""node A should not have ongoing recovery as target"", recoveryStats.currentAsTarget(), equalTo(0));
                nodeAThrottling = recoveryStats.throttleTime().millis();
            }
            if (nodeStats.getNode().getName().equals(nodeB)) {
                assertThat(""node B should not have ongoing recovery as source"", recoveryStats.currentAsSource(), equalTo(0));
                assertThat(""node B should have ongoing recovery as target"", recoveryStats.currentAsTarget(), equalTo(1));
                nodeBThrottling = recoveryStats.throttleTime().millis();
            }
        }

        logger.info(""--> checking throttling increases"");
        final long finalNodeAThrottling = nodeAThrottling;
        final long finalNodeBThrottling = nodeBThrottling;
        assertBusy(() -> {
            NodesStatsResponse statsResponse1 = client().admin().cluster().prepareNodesStats().clear().setIndices(new CommonStatsFlags(CommonStatsFlags.Flag.Recovery)).get();
            assertThat(statsResponse1.getNodes(), hasSize(2));
            for (NodeStats nodeStats : statsResponse1.getNodes()) {
                final RecoveryStats recoveryStats = nodeStats.getIndices().getRecoveryStats();
                if (nodeStats.getNode().getName().equals(nodeA)) {
                    assertThat(""node A throttling should increase"", recoveryStats.throttleTime().millis(), greaterThan(finalNodeAThrottling));
                }
                if (nodeStats.getNode().getName().equals(nodeB)) {
                    assertThat(""node B throttling should increase"", recoveryStats.throttleTime().millis(), greaterThan(finalNodeBThrottling));
                }
            }
        });


        logger.info(""--> speeding up recoveries"");
        restoreRecoverySpeed();

        // wait for it to be finished
        ensureGreen();

        response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();

        recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);
        assertThat(recoveryStates.size(), equalTo(1));

        assertRecoveryState(recoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, Stage.DONE, nodeA, nodeB);
        validateIndexRecoveryState(recoveryStates.get(0).getIndex());
        Consumer<String> assertNodeHasThrottleTimeAndNoRecoveries = nodeName ->  {
            NodesStatsResponse nodesStatsResponse = client().admin().cluster().prepareNodesStats().setNodesIds(nodeName)
                .clear().setIndices(new CommonStatsFlags(CommonStatsFlags.Flag.Recovery)).get();
            assertThat(nodesStatsResponse.getNodes(), hasSize(1));
            NodeStats nodeStats = nodesStatsResponse.getNodes().get(0);
            final RecoveryStats recoveryStats = nodeStats.getIndices().getRecoveryStats();
            assertThat(recoveryStats.currentAsSource(), equalTo(0));
            assertThat(recoveryStats.currentAsTarget(), equalTo(0));
            assertThat(nodeName + "" throttling should be >0"", recoveryStats.throttleTime().millis(), greaterThan(0L));
        };
        // we have to use assertBusy as recovery counters are decremented only when the last reference to the RecoveryTarget
        // is decremented, which may happen after the recovery was done.
        assertBusy(() -> assertNodeHasThrottleTimeAndNoRecoveries.accept(nodeA));
        assertBusy(() -> assertNodeHasThrottleTimeAndNoRecoveries.accept(nodeB));

        logger.info(""--> bump replica count"");
        client().admin().indices().prepareUpdateSettings(INDEX_NAME)
                .setSettings(Settings.builder().put(""number_of_replicas"", 1)).execute().actionGet();
        ensureGreen();

        assertBusy(() -> assertNodeHasThrottleTimeAndNoRecoveries.accept(nodeA));
        assertBusy(() -> assertNodeHasThrottleTimeAndNoRecoveries.accept(nodeB));

        logger.info(""--> start node C"");
        String nodeC = internalCluster().startNode();
        assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes(""3"").get().isTimedOut());

        logger.info(""--> slowing down recoveries"");
        slowDownRecovery(shardSize);

        logger.info(""--> move replica shard from: {} to: {}"", nodeA, nodeC);
        client().admin().cluster().prepareReroute()
                .add(new MoveAllocationCommand(INDEX_NAME, 0, nodeA, nodeC))
                .execute().actionGet().getState();

        response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();
        recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);

        nodeARecoveryStates = findRecoveriesForTargetNode(nodeA, recoveryStates);
        assertThat(nodeARecoveryStates.size(), equalTo(1));
        nodeBRecoveryStates = findRecoveriesForTargetNode(nodeB, recoveryStates);
        assertThat(nodeBRecoveryStates.size(), equalTo(1));
        List<RecoveryState> nodeCRecoveryStates = findRecoveriesForTargetNode(nodeC, recoveryStates);
        assertThat(nodeCRecoveryStates.size(), equalTo(1));

        assertRecoveryState(nodeARecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, Stage.DONE, nodeB, nodeA);
        validateIndexRecoveryState(nodeARecoveryStates.get(0).getIndex());

        assertRecoveryState(nodeBRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, Stage.DONE, nodeA, nodeB);
        validateIndexRecoveryState(nodeBRecoveryStates.get(0).getIndex());

        // relocations of replicas are marked as REPLICA and the source node is the node holding the primary (B)
        assertOnGoingRecoveryState(nodeCRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, nodeB, nodeC);
        validateIndexRecoveryState(nodeCRecoveryStates.get(0).getIndex());

        if (randomBoolean()) {
            // shutdown node with relocation source of replica shard and check if recovery continues
            internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodeA));
            ensureStableCluster(2);

            response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();
            recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);

            nodeARecoveryStates = findRecoveriesForTargetNode(nodeA, recoveryStates);
            assertThat(nodeARecoveryStates.size(), equalTo(0));
            nodeBRecoveryStates = findRecoveriesForTargetNode(nodeB, recoveryStates);
            assertThat(nodeBRecoveryStates.size(), equalTo(1));
            nodeCRecoveryStates = findRecoveriesForTargetNode(nodeC, recoveryStates);
            assertThat(nodeCRecoveryStates.size(), equalTo(1));

            assertRecoveryState(nodeBRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, Stage.DONE, nodeA, nodeB);
            validateIndexRecoveryState(nodeBRecoveryStates.get(0).getIndex());

            assertOnGoingRecoveryState(nodeCRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, nodeB, nodeC);
            validateIndexRecoveryState(nodeCRecoveryStates.get(0).getIndex());
        }

        logger.info(""--> speeding up recoveries"");
        restoreRecoverySpeed();
        ensureGreen();

        response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();
        recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);

        nodeARecoveryStates = findRecoveriesForTargetNode(nodeA, recoveryStates);
        assertThat(nodeARecoveryStates.size(), equalTo(0));
        nodeBRecoveryStates = findRecoveriesForTargetNode(nodeB, recoveryStates);
        assertThat(nodeBRecoveryStates.size(), equalTo(1));
        nodeCRecoveryStates = findRecoveriesForTargetNode(nodeC, recoveryStates);
        assertThat(nodeCRecoveryStates.size(), equalTo(1));

        assertRecoveryState(nodeBRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, Stage.DONE, nodeA, nodeB);
        validateIndexRecoveryState(nodeBRecoveryStates.get(0).getIndex());

        // relocations of replicas are marked as REPLICA and the source node is the node holding the primary (B)
        assertRecoveryState(nodeCRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, Stage.DONE, nodeB, nodeC);
        validateIndexRecoveryState(nodeCRecoveryStates.get(0).getIndex());
    }
",non-flaky,5
91489,strapdata_elassandra,IndexRecoveryIT.sendRequest,"    @TestLogging(""_root:DEBUG,org.elasticsearch.indices.recovery:TRACE"")
    public void testDisconnectsDuringRecovery() throws Exception {
        boolean primaryRelocation = randomBoolean();
        final String indexName = ""test"";
        final Settings nodeSettings = Settings.builder()
            .put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.getKey(), TimeValue.timeValueMillis(randomIntBetween(0, 100)))
            .build();
        TimeValue disconnectAfterDelay = TimeValue.timeValueMillis(randomIntBetween(0, 100));
        // start a master node
        String masterNodeName = internalCluster().startMasterOnlyNode(nodeSettings);

        final String blueNodeName = internalCluster().startNode(Settings.builder().put(""node.attr.color"", ""blue"").put(nodeSettings).build());
        final String redNodeName = internalCluster().startNode(Settings.builder().put(""node.attr.color"", ""red"").put(nodeSettings).build());

        client().admin().indices().prepareCreate(indexName)
            .setSettings(
                Settings.builder()
                    .put(IndexMetaData.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + ""color"", ""blue"")
                    .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
                    .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
            ).get();

        List<IndexRequestBuilder> requests = new ArrayList<>();
        int numDocs = scaledRandomIntBetween(25, 250);
        for (int i = 0; i < numDocs; i++) {
            requests.add(client().prepareIndex(indexName, ""type"").setSource(""{}"", XContentType.JSON));
        }
        indexRandom(true, requests);
        ensureSearchable(indexName);
        assertHitCount(client().prepareSearch(indexName).get(), numDocs);

        MockTransportService masterTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, masterNodeName);
        MockTransportService blueMockTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, blueNodeName);
        MockTransportService redMockTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, redNodeName);

        redMockTransportService.addSendBehavior(blueMockTransportService, new StubbableTransport.SendRequestBehavior() {
            private final AtomicInteger count = new AtomicInteger();

            @Override
            public void sendRequest(Transport.Connection connection, long requestId, String action, TransportRequest request,
                                    TransportRequestOptions options) throws IOException {
                logger.info(""--> sending request {} on {}"", action, connection.getNode());
                if (PeerRecoverySourceService.Actions.START_RECOVERY.equals(action) && count.incrementAndGet() == 1) {
                    // ensures that it's considered as valid recovery attempt by source
                    try {
                        awaitBusy(() -> client(blueNodeName).admin().cluster().prepareState().setLocal(true).get()
                            .getState().getRoutingTable().index(""test"").shard(0).getAllInitializingShards().isEmpty() == false);
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }
                    connection.sendRequest(requestId, action, request, options);
                    try {
                        Thread.sleep(disconnectAfterDelay.millis());
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }
                    throw new ConnectTransportException(connection.getNode(), ""DISCONNECT: simulation disconnect after successfully sending "" + action + "" request"");
                } else {
                    connection.sendRequest(requestId, action, request, options);
                }
            }
",non-flaky,5
91490,strapdata_elassandra,RareClusterStateIT.execute,"@TestLogging(""_root:DEBUG"")
    public void testAssignmentWithJustAddedNodes() throws Exception {
        internalCluster().startNode();
        final String index = ""index"";
        prepareCreate(index).setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
            .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)).get();
        ensureGreen(index);

        // close to have some unassigned started shards shards..
        client().admin().indices().prepareClose(index).get();


        final String masterName = internalCluster().getMasterName();
        final ClusterService clusterService = internalCluster().clusterService(masterName);
        final AllocationService allocationService = internalCluster().getInstance(AllocationService.class, masterName);
        clusterService.submitStateUpdateTask(""test-inject-node-and-reroute"", new ClusterStateUpdateTask() {
            @Override
            public ClusterState execute(ClusterState currentState) throws Exception {
                // inject a node
                ClusterState.Builder builder = ClusterState.builder(currentState);
                builder.nodes(DiscoveryNodes.builder(currentState.nodes()).add(new DiscoveryNode(""_non_existent"",
                        buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)));

                // open index
                final IndexMetaData indexMetaData = IndexMetaData.builder(currentState.metaData().index(index)).state(IndexMetaData.State.OPEN).build();

                builder.metaData(MetaData.builder(currentState.metaData()).put(indexMetaData, true));
                builder.blocks(ClusterBlocks.builder().blocks(currentState.blocks()).removeIndexBlocks(index));
                ClusterState updatedState = builder.build();

                RoutingTable.Builder routingTable = RoutingTable.builder(updatedState.routingTable());
                routingTable.addAsRecovery(updatedState.metaData().index(index));
                updatedState = ClusterState.builder(updatedState).routingTable(routingTable.build()).build();

                return allocationService.reroute(updatedState, ""reroute"");

            }
",non-flaky,5
91491,strapdata_elassandra,UpdateByQueryBasicTests.testBasics,"@TestLogging(""org.elasticsearch.index.reindex:TRACE,org.elasticsearch.action.bulk:TRACE,org.elasticsearch.search.SearchService:TRACE"")
    public void testBasics() throws Exception {
        indexRandom(true, client().prepareIndex(""test"", ""test"", ""1"").setSource(""foo"", ""a""),
                client().prepareIndex(""test"", ""test"", ""2"").setSource(""foo"", ""a""),
                client().prepareIndex(""test"", ""test"", ""3"").setSource(""foo"", ""b""),
                client().prepareIndex(""test"", ""test"", ""4"").setSource(""foo"", ""c""));
        assertHitCount(client().prepareSearch(""test"").setTypes(""test"").setSize(0).get(), 4);
        assertEquals(1, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(1, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());

        // Reindex all the docs
        assertThat(updateByQuery().source(""test"").refresh(true).get(), matcher().updated(4));
        assertEquals(2, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());

        // Now none of them
        assertThat(updateByQuery().source(""test"").filter(termQuery(""foo"", ""no_match"")).refresh(true).get(), matcher().updated(0));
        assertEquals(2, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());

        // Now half of them
        assertThat(updateByQuery().source(""test"").filter(termQuery(""foo"", ""a"")).refresh(true).get(), matcher().updated(2));
        assertEquals(3, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(3, client().prepareGet(""test"", ""test"", ""2"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""3"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());

        // Limit with size
        UpdateByQueryRequestBuilder request = updateByQuery().source(""test"").size(3).refresh(true);
        request.source().addSort(""foo.keyword"", SortOrder.ASC);
        assertThat(request.get(), matcher().updated(3));
        // Only the first three documents are updated because of sort
        assertEquals(4, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(4, client().prepareGet(""test"", ""test"", ""2"").get().getVersion());
        assertEquals(3, client().prepareGet(""test"", ""test"", ""3"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());
    }
",non-flaky,5
91492,strapdata_elassandra,ReindexFailureTests.testFailuresCauseAbortDefault,"@TestLogging(""_root:DEBUG"")
    public void testFailuresCauseAbortDefault() throws Exception {
        /*
         * Create the destination index such that the copy will cause a mapping
         * conflict on every request.
         */
        indexRandom(true,
                client().prepareIndex(""dest"", ""test"", ""test"").setSource(""test"", 10) /* Its a string in the source! */);

        indexDocs(100);

        ReindexRequestBuilder copy = reindex().source(""source"").destination(""dest"");
        /*
         * Set the search size to something very small to cause there to be
         * multiple batches for this request so we can assert that we abort on
         * the first batch.
         */
        copy.source().setSize(1);

        BulkByScrollResponse response = copy.get();
        assertThat(response, matcher()
                .batches(1)
                .failures(both(greaterThan(0)).and(lessThanOrEqualTo(maximumNumberOfShards()))));
        for (Failure failure: response.getBulkFailures()) {
            assertThat(failure.getMessage(), containsString(""IllegalArgumentException[For input string: \""words words\""]""));
        }
    }
",non-flaky,5
91493,strapdata_elassandra,CancelTests.clearAllowedOperations,"@TestLogging(""org.elasticsearch.index.reindex:DEBUG,org.elasticsearch.action.bulk:DEBUG"")
    public void clearAllowedOperations() {
        ALLOWED_OPERATIONS.drainPermits();
    }
",non-flaky,5
23,fabiomaffioletti_jsondoc,JSONDocApiAuthBuilderTest.testApiAuthToken,"@Test
public void testApiAuthToken() {
    ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>>newHashSet(Controller.class), URI).iterator().next();
    Assert.assertEquals(""TOKEN"", apiDoc.getAuth().getType());
    Assert.assertEquals("""", apiDoc.getAuth().getScheme());
    Assert.assertEquals(""abc"", apiDoc.getAuth().getTesttokens().iterator().next());
    for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
        if (apiMethodDoc.getPath().contains(""/inherit"")) {
            Assert.assertEquals(""TOKEN"", apiMethodDoc.getAuth().getType());
            Assert.assertEquals("""", apiMethodDoc.getAuth().getScheme());
            Assert.assertEquals(""abc"", apiMethodDoc.getAuth().getTesttokens().iterator().next());
        }
        if (apiMethodDoc.getPath().contains(""/override"")) {
            Assert.assertEquals(""TOKEN"", apiMethodDoc.getAuth().getType());
            Assert.assertEquals(""Bearer"", apiMethodDoc.getAuth().getScheme());
            Assert.assertEquals(""xyz"", apiMethodDoc.getAuth().getTesttokens().iterator().next());
        }
    }
}",unordered collections,3
42971,fabiomaffioletti_jsondoc,Issue151Test.testIssue151,"	@Test
	public void testIssue151() {
		JSONDoc jsonDoc = jsondocScanner.getJSONDoc("""", """", Lists.newArrayList(""org.jsondoc.core.issues.issue151""), true, MethodDisplay.URI);
		Assert.assertEquals(2, jsonDoc.getObjects().keySet().size());
		Assert.assertEquals(1, jsonDoc.getObjects().get(""bargroup"").size());
		Assert.assertEquals(1, jsonDoc.getObjects().get(""foogroup"").size());
	}
",non-flaky,5
42972,fabiomaffioletti_jsondoc,ApiDocTest.testApiErrorsDoc,"	@Test
	public void testApiErrorsDoc() throws Exception {

		final ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>>newHashSet(Test3Controller.class),
				MethodDisplay.URI).iterator().next();

		final Set<ApiMethodDoc> methods = apiDoc.getMethods();
		final ApiMethodDoc apiMethodDoc = methods.iterator().next();
		final List<ApiErrorDoc> apiErrors = apiMethodDoc.getApierrors();

		Assert.assertEquals(1, methods.size());
		Assert.assertEquals(3, apiErrors.size());
		Assert.assertEquals(""1000"", apiErrors.get(0).getCode());
		Assert.assertEquals(""method-level annotation should be applied"",
				""A test error #1"", apiErrors.get(0).getDescription());
		Assert.assertEquals(""2000"", apiErrors.get(1).getCode());
		Assert.assertEquals(""400"", apiErrors.get(2).getCode());

	}
",non-flaky,5
42973,fabiomaffioletti_jsondoc,ApiDocTest.testApiDoc,"	@Test
	public void testApiDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TestController.class);
		ApiDoc apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-controller"", apiDoc.getName());
		Assert.assertEquals(""a-test-controller"", apiDoc.getDescription());
		Assert.assertEquals(""1.0"", apiDoc.getSupportedversions().getSince());
		Assert.assertEquals(""2.12"", apiDoc.getSupportedversions().getUntil());
		Assert.assertEquals(ApiAuthType.NONE.name(), apiDoc.getAuth().getType());
		Assert.assertEquals(DefaultJSONDocScanner.ANONYMOUS, apiDoc.getAuth().getRoles().get(0));

		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			
			if (apiMethodDoc.getPath().contains(""/name"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				Assert.assertEquals(""200 - OK"", apiMethodDoc.getResponsestatuscode());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""name"")) {
						Assert.assertEquals(""string"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/age"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""204"", apiMethodDoc.getResponsestatuscode());
				Assert.assertEquals(""integer"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""integer"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""age"")) {
						Assert.assertEquals(""integer"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/avg"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""long"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""long"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""avg"")) {
						Assert.assertEquals(""long"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/map"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""map[string, integer]"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""map[string, integer]"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""map"")) {
						Assert.assertEquals(""map[string, integer]"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/parametrizedList"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""list of string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""list of string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""parametrizedList"")) {
						Assert.assertEquals(""list of string"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
				
			}

			if (apiMethodDoc.getPath().contains(""/wildcardParametrizedList"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""list of wildcard"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""list of wildcard"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""wildcardParametrizedList"")) {
						Assert.assertEquals(""list of wildcard"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/LongArray"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""array of long"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""array of long"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""LongArray"")) {
						Assert.assertEquals(""array of long"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/longArray"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""array of long"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""array of long"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""longArray"")) {
						Assert.assertEquals(""array of long"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}
			
			if (apiMethodDoc.getPath().contains(""/version"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""1.0"", apiMethodDoc.getSupportedversions().getSince());
				Assert.assertEquals(""2.12"", apiMethodDoc.getSupportedversions().getUntil());
			}
			
			if (apiMethodDoc.getPath().contains(""/child"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""child"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			
			if (apiMethodDoc.getPath().contains(""/pizza"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""customPizzaObject"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			
			if (apiMethodDoc.getPath().contains(""/multiple-request-methods"")) {
				Assert.assertEquals(2, apiMethodDoc.getVerb().size());
				Iterator<ApiVerb> iterator = apiMethodDoc.getVerb().iterator();
				Assert.assertEquals(ApiVerb.GET, iterator.next());
				Assert.assertEquals(ApiVerb.POST, iterator.next());
			}
			
		}

		classes.clear();
		classes.add(TestControllerWithBasicAuth.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-controller-with-basic-auth"", apiDoc.getName());
		Assert.assertEquals(ApiAuthType.BASIC_AUTH.name(), apiDoc.getAuth().getType());
		Assert.assertEquals(""ROLE_USER"", apiDoc.getAuth().getRoles().get(0));
		Assert.assertEquals(""ROLE_ADMIN"", apiDoc.getAuth().getRoles().get(1));
		Assert.assertTrue(apiDoc.getAuth().getTestusers().size() > 0);
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/basicAuth"")) {
				Assert.assertEquals(ApiAuthType.BASIC_AUTH.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(""ROLE_USER"", apiMethodDoc.getAuth().getRoles().get(0));
				Assert.assertTrue(apiMethodDoc.getAuth().getTestusers().size() > 0);
			}
			
			if (apiMethodDoc.getPath().contains(""/noAuth"")) {
				Assert.assertEquals(ApiAuthType.NONE.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(DefaultJSONDocScanner.ANONYMOUS, apiMethodDoc.getAuth().getRoles().get(0));
			}
			
			if (apiMethodDoc.getPath().contains(""/undefinedAuthWithAuthOnClass"")) {
				Assert.assertEquals(ApiAuthType.BASIC_AUTH.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(""ROLE_USER"", apiMethodDoc.getAuth().getRoles().get(0));
				Assert.assertEquals(""ROLE_ADMIN"", apiMethodDoc.getAuth().getRoles().get(1));
			}
			
		}
		
		classes.clear();
		classes.add(TestControllerWithNoAuthAnnotation.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-controller-with-no-auth-annotation"", apiDoc.getName());
		Assert.assertNull(apiDoc.getAuth());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/basicAuth"")) {
				Assert.assertEquals(ApiAuthType.BASIC_AUTH.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(""ROLE_USER"", apiMethodDoc.getAuth().getRoles().get(0));
				Assert.assertTrue(apiMethodDoc.getAuth().getTestusers().size() > 0);
			}
			
			if (apiMethodDoc.getPath().contains(""/noAuth"")) {
				Assert.assertEquals(ApiAuthType.NONE.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(DefaultJSONDocScanner.ANONYMOUS, apiMethodDoc.getAuth().getRoles().get(0));
			}
			
			if (apiMethodDoc.getPath().contains(""/undefinedAuthWithoutAuthOnClass"")) {
				Assert.assertNull(apiMethodDoc.getAuth());
			}
			
		}
		
		classes.clear();
		classes.add(TestOldStyleServlets.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-old-style-servlets"", apiDoc.getName());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/oldStyle"")) {
				Assert.assertEquals(1, apiMethodDoc.getPathparameters().size());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleWithList"")) {
				Assert.assertEquals(1, apiMethodDoc.getPathparameters().size());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleWithMap"")) {
				Assert.assertEquals(1, apiMethodDoc.getPathparameters().size());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleMixed"")) {
				Assert.assertEquals(3, apiMethodDoc.getPathparameters().size());
				Assert.assertEquals(1, apiMethodDoc.getQueryparameters().size());
				Assert.assertEquals(""qTest"", apiMethodDoc.getQueryparameters().iterator().next().getDefaultvalue());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleResponseObject"")) {
				Assert.assertEquals(""list"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleBodyObject"")) {
				Assert.assertEquals(""list"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
			}
		}
		
		classes.clear();
		classes.add(TestErrorsAndWarningsAndHints.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-errors-warnings-hints"", apiDoc.getName());
		ApiMethodDoc apiMethodDoc = apiDoc.getMethods().iterator().next();
		Assert.assertEquals(1, apiMethodDoc.getJsondocerrors().size());
		Assert.assertEquals(1, apiMethodDoc.getJsondocwarnings().size());
		Assert.assertEquals(2, apiMethodDoc.getJsondochints().size());
		
		classes.clear();
		classes.add(TestErrorsAndWarningsAndHintsMethodSummary.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.SUMMARY).iterator().next();
		apiMethodDoc = apiDoc.getMethods().iterator().next();
		Assert.assertEquals(1, apiMethodDoc.getJsondocerrors().size());
		Assert.assertEquals(1, apiMethodDoc.getJsondocwarnings().size());
		Assert.assertEquals(3, apiMethodDoc.getJsondochints().size());
		
		classes.clear();
		classes.add(InterfaceController.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""interface-controller"", apiDoc.getName());
		apiMethodDoc = apiDoc.getMethods().iterator().next();
		Assert.assertNotNull(apiMethodDoc);
		Assert.assertEquals(""/interface"", apiMethodDoc.getPath().iterator().next());
		
		classes.clear();
		classes.add(TestDeclaredMethods.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-declared-methods"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		
		
		classes.clear();
		classes.add(TestMultipleParamsWithSameMethod.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(3, apiDoc.getMethods().size());
		
	}
",non-flaky,5
42974,fabiomaffioletti_jsondoc,ApiMethodDocTest.testNotEqual,"	@Test
	public void testNotEqual() {
		first = new ApiMethodDoc();
		first.setPath(Sets.newHashSet(""/first""));
		first.setVerb(Sets.newHashSet(ApiVerb.GET));
		second = new ApiMethodDoc();
		second.setPath(Sets.newHashSet(""/second""));
		second.setVerb(Sets.newHashSet(ApiVerb.GET));
		Assert.assertNotEquals(0, first.compareTo(second));
	}
",non-flaky,5
42975,fabiomaffioletti_jsondoc,ApiMethodDocTest.testEqual,"	@Test
	public void testEqual() {
		first = new ApiMethodDoc();
		first.setPath(Sets.newHashSet(""/test""));
		first.setVerb(Sets.newHashSet(ApiVerb.GET));
		second = new ApiMethodDoc();
		second.setPath(Sets.newHashSet(""/test""));
		second.setVerb(Sets.newHashSet(ApiVerb.GET));
		Assert.assertEquals(0, first.compareTo(second));
	}
",non-flaky,5
42976,fabiomaffioletti_jsondoc,ApiMethodDocTest.testNotEqualMultipleVerbs,"	@Test
	public void testNotEqualMultipleVerbs() {
		first = new ApiMethodDoc();
		first.setPath(Sets.newHashSet(""/first""));
		first.setVerb(Sets.newHashSet(ApiVerb.GET, ApiVerb.POST));
		second = new ApiMethodDoc();
		second.setPath(Sets.newHashSet(""/second""));
		second.setVerb(Sets.newHashSet(ApiVerb.GET, ApiVerb.POST));
		Assert.assertNotEquals(0, first.compareTo(second));
		
		second.setPath(Sets.newHashSet(""/first""));
		second.setVerb(Sets.newHashSet(ApiVerb.PUT, ApiVerb.POST));
		Assert.assertNotEquals(0, first.compareTo(second));
	}
",non-flaky,5
42977,fabiomaffioletti_jsondoc,ApiMethodDocTest.testEqualMultipleVerbs,"	@Test
	public void testEqualMultipleVerbs() {
		first = new ApiMethodDoc();
		first.setPath(Sets.newHashSet(""/test""));
		first.setVerb(Sets.newHashSet(ApiVerb.GET, ApiVerb.POST));
		second = new ApiMethodDoc();
		second.setPath(Sets.newHashSet(""/test""));
		second.setVerb(Sets.newHashSet(ApiVerb.GET, ApiVerb.POST));
		Assert.assertEquals(0, first.compareTo(second));
		
		second.setVerb(Sets.newHashSet(ApiVerb.POST, ApiVerb.GET));
		Assert.assertEquals(0, first.compareTo(second));
	}
",non-flaky,5
42978,fabiomaffioletti_jsondoc,ApiObjectDocTest.testUndefinedVisibilityAndStageDoc,"	@Test
	public void testUndefinedVisibilityAndStageDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(UndefinedVisibilityAndStage.class);
		ApiObjectDoc apiObjectDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next();
		Assert.assertEquals(""undefinedvisibilityandstage"", apiObjectDoc.getName());
		Assert.assertEquals(ApiVisibility.UNDEFINED, apiObjectDoc.getVisibility());
		Assert.assertEquals(ApiStage.UNDEFINED, apiObjectDoc.getStage());
	}
",non-flaky,5
42979,fabiomaffioletti_jsondoc,ApiObjectDocTest.testTemplateApiObjectDoc,"	@Test
	public void testTemplateApiObjectDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TemplateApiObject.class);
		ApiObjectDoc apiObjectDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next();
		Assert.assertEquals(""templateapiobject"", apiObjectDoc.getName());
		Iterator<ApiObjectFieldDoc> iterator = apiObjectDoc.getFields().iterator();
		Assert.assertEquals(""id"", iterator.next().getName());
		Assert.assertEquals(""name"", iterator.next().getName());
	}
",non-flaky,5
42980,fabiomaffioletti_jsondoc,ApiObjectDocTest.testNoNameApiObjectDoc,"	@Test
	public void testNoNameApiObjectDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(NoNameApiObject.class);
		ApiObjectDoc apiObjectDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next();
		Assert.assertEquals(""nonameapiobject"", apiObjectDoc.getName());
		Assert.assertEquals(""id"", apiObjectDoc.getFields().iterator().next().getName());
		Assert.assertEquals(1, apiObjectDoc.getJsondochints().size());
	}
",non-flaky,5
42981,fabiomaffioletti_jsondoc,ApiObjectDocTest.testEnumObjectDoc,"	@Test
	public void testEnumObjectDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TestEnum.class);
		ApiObjectDoc childDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next(); 
		Assert.assertEquals(""test-enum"", childDoc.getName());
		Assert.assertEquals(0, childDoc.getFields().size());
		Assert.assertEquals(TestEnum.TESTENUM1.name(), childDoc.getAllowedvalues()[0]);
		Assert.assertEquals(TestEnum.TESTENUM2.name(), childDoc.getAllowedvalues()[1]);
		Assert.assertEquals(TestEnum.TESTENUM3.name(), childDoc.getAllowedvalues()[2]);
	}
",non-flaky,5
42982,fabiomaffioletti_jsondoc,ApiObjectDocTest.testApiObjectDoc,"	@Test
	public void testApiObjectDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TestObject.class);
		ApiObjectDoc childDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next(); 
		Assert.assertEquals(""test-object"", childDoc.getName());
		Assert.assertEquals(14, childDoc.getFields().size());
		Assert.assertEquals(""1.0"", childDoc.getSupportedversions().getSince());
		Assert.assertEquals(""2.12"", childDoc.getSupportedversions().getUntil());
		Assert.assertEquals(ApiVisibility.PUBLIC, childDoc.getVisibility());
		Assert.assertEquals(ApiStage.PRE_ALPHA, childDoc.getStage());
		
		for (ApiObjectFieldDoc fieldDoc : childDoc.getFields()) {
			if(fieldDoc.getName().equals(""wildcardParametrized"")) {
				Assert.assertEquals(""list"", fieldDoc.getJsondocType().getType().get(0));
			}
			
			if(fieldDoc.getName().equals(""unparametrizedList"")) {
				Assert.assertEquals(""list"", fieldDoc.getJsondocType().getType().get(0));
			}
			
			if(fieldDoc.getName().equals(""parametrizedList"")) {
				Assert.assertEquals(""list of string"", fieldDoc.getJsondocType().getOneLineText());
			}
			
			if(fieldDoc.getName().equals(""name"")) {
				Assert.assertEquals(""string"", fieldDoc.getJsondocType().getType().get(0));
				Assert.assertEquals(""name"", fieldDoc.getName());
				Assert.assertEquals(""true"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""age"")) {
				Assert.assertEquals(""integer"", fieldDoc.getJsondocType().getType().get(0));
				Assert.assertEquals(""age"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""avg"")) {
				Assert.assertEquals(""long"", fieldDoc.getJsondocType().getType().get(0));
				Assert.assertEquals(""avg"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""map"")) {
				Assert.assertEquals(""map"", fieldDoc.getJsondocType().getType().get(0));
				Assert.assertEquals(""string"", fieldDoc.getJsondocType().getMapKey().getType().get(0));
				Assert.assertEquals(""integer"", fieldDoc.getJsondocType().getMapValue().getType().get(0));
			}
			
			if(fieldDoc.getName().equals(""LongArray"")) {
				Assert.assertEquals(""array of long"", fieldDoc.getJsondocType().getOneLineText());
				Assert.assertEquals(""LongArray"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}

			if(fieldDoc.getName().equals(""longArray"")) {
				Assert.assertEquals(""array of long"", fieldDoc.getJsondocType().getOneLineText());
				Assert.assertEquals(""longArray"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""fooBar"")) {
				Assert.assertEquals(""string"", fieldDoc.getJsondocType().getOneLineText());
				Assert.assertEquals(""foo_bar"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""version"")) {
				Assert.assertEquals(""string"", fieldDoc.getJsondocType().getOneLineText());
				Assert.assertEquals(""1.0"", fieldDoc.getSupportedversions().getSince());
				Assert.assertEquals(""2.12"", fieldDoc.getSupportedversions().getUntil());
			}
			
			if(fieldDoc.getName().equals(""test-enum"")) {
				Assert.assertEquals(""test-enum"", fieldDoc.getName());
				Assert.assertEquals(TestEnum.TESTENUM1.name(), fieldDoc.getAllowedvalues()[0]);
				Assert.assertEquals(TestEnum.TESTENUM2.name(), fieldDoc.getAllowedvalues()[1]);
				Assert.assertEquals(TestEnum.TESTENUM3.name(), fieldDoc.getAllowedvalues()[2]);
			}
			
			if(fieldDoc.getName().equals(""test-enum-with-allowed-values"")) {
				Assert.assertEquals(""A"", fieldDoc.getAllowedvalues()[0]);
				Assert.assertEquals(""B"", fieldDoc.getAllowedvalues()[1]);
				Assert.assertEquals(""C"", fieldDoc.getAllowedvalues()[2]);
			}

			if(fieldDoc.getName().equals(""orderedProperty"")) {
				Assert.assertEquals(""orderedProperty"", fieldDoc.getName());
				Assert.assertEquals(1, fieldDoc.getOrder().intValue());
			} else {
				Assert.assertEquals(Integer.MAX_VALUE, fieldDoc.getOrder().intValue());
			}

		}
	}
",non-flaky,5
42983,fabiomaffioletti_jsondoc,ApiHeadersDocTest.testApiHeadersOnClass,"	@Test
	public void testApiHeadersOnClass() {
		final ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>>newHashSet(ApiHeadersController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""ApiHeadersController"", apiDoc.getName());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if(apiMethodDoc.getPath().contains(""/api-headers-controller-method-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getHeaders().size());
			}
			if(apiMethodDoc.getPath().contains(""/api-headers-controller-method-two"")) {
				Assert.assertEquals(3, apiMethodDoc.getHeaders().size());
			}
		}
	}
",non-flaky,5
42984,fabiomaffioletti_jsondoc,ApiFlowDocTest.testApiDoc,"	@Test
	public void testApiDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TestFlow.class);
		
		List<ApiMethodDoc> apiMethodDocs = new ArrayList<ApiMethodDoc>();
		ApiMethodDoc apiMethodDoc = new ApiMethodDoc();
		apiMethodDoc.setId(""F1"");
		apiMethodDocs.add(apiMethodDoc);
		
		Set<ApiFlowDoc> apiFlowDocs = jsondocScanner.getApiFlowDocs(classes, apiMethodDocs);
		for (ApiFlowDoc apiFlowDoc : apiFlowDocs) {
			if(apiFlowDoc.getName().equals(""flow"")) {
				Assert.assertEquals(""A test flow"", apiFlowDoc.getDescription());
				Assert.assertEquals(3, apiFlowDoc.getSteps().size());
				Assert.assertEquals(""F1"", apiFlowDoc.getSteps().get(0).getApimethodid());
				Assert.assertEquals(""F2"", apiFlowDoc.getSteps().get(1).getApimethodid());
				Assert.assertEquals(""Flows A"", apiFlowDoc.getGroup());
				Assert.assertNotNull(apiFlowDoc.getSteps().get(0).getApimethoddoc());
				Assert.assertEquals(""F1"", apiFlowDoc.getSteps().get(0).getApimethoddoc().getId());
			}
			
			if(apiFlowDoc.getName().equals(""flow2"")) {
				Assert.assertEquals(""A test flow 2"", apiFlowDoc.getDescription());
				Assert.assertEquals(3, apiFlowDoc.getSteps().size());
				Assert.assertEquals(""F4"", apiFlowDoc.getSteps().get(0).getApimethodid());
				Assert.assertEquals(""F5"", apiFlowDoc.getSteps().get(1).getApimethodid());
				Assert.assertEquals(""Flows B"", apiFlowDoc.getGroup());
			}
		}
	}
",non-flaky,5
42985,fabiomaffioletti_jsondoc,JSONDocApiObjectBuilderTest.testApiObjectDocWithHibernateValidator,"	@Test
	public void testApiObjectDocWithHibernateValidator() {
		Set<ApiObjectDoc> apiObjectDocs = jsondocScanner.getApiObjectDocs(Sets.<Class<?>>newHashSet(HibernateValidatorPojo.class));
		Iterator<ApiObjectDoc> iterator = apiObjectDocs.iterator();
		ApiObjectDoc next = iterator.next();
		Set<ApiObjectFieldDoc> fields = next.getFields();
		for (ApiObjectFieldDoc apiObjectFieldDoc : fields) {
			if(apiObjectFieldDoc.getName().equals(""id"")) {
				Iterator<String> formats = apiObjectFieldDoc.getFormat().iterator();
				Assert.assertEquals(""a not empty id"", formats.next());
				Assert.assertEquals(""length must be between 2 and 2147483647"", formats.next());
				Assert.assertEquals(""must be less than or equal to 9"", formats.next());
			}
		}
	}
",non-flaky,5
42986,fabiomaffioletti_jsondoc,JSONDocApiGlobalBuilderTest.testApiGlobalDoc,"	@Test
	public void testApiGlobalDoc() {
		ApiGlobalDoc apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(Global.class), Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet());
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(1, apiGlobalDoc.getSections().size());
		ApiGlobalSectionDoc sectionDoc = apiGlobalDoc.getSections().iterator().next();
		Assert.assertEquals(""title"", sectionDoc.getTitle());
		Assert.assertEquals(3, sectionDoc.getParagraphs().size());
		
		apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet(Changelog.class), Sets.<Class<?>>newHashSet());
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(1, apiGlobalDoc.getChangelogset().getChangelogs().size());

		apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(MultipleGlobalSections.class), Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet());
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(3, apiGlobalDoc.getSections().size());
		
		ApiGlobalSectionDoc[] apiGlobalSectionDocs = apiGlobalDoc.getSections().toArray(new ApiGlobalSectionDoc[apiGlobalDoc.getSections().size()]);
		Assert.assertEquals(""section1"", apiGlobalSectionDocs[0].getTitle());
		Assert.assertEquals(""abc"", apiGlobalSectionDocs[1].getTitle());
		Assert.assertEquals(""198xyz"", apiGlobalSectionDocs[2].getTitle());
		
		apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet(Migration.class));
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(1, apiGlobalDoc.getMigrationset().getMigrations().size());
		
		apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(AllTogether.class), Sets.<Class<?>>newHashSet(AllTogether.class), Sets.<Class<?>>newHashSet(AllTogether.class));
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(1, apiGlobalDoc.getSections().size());
		Assert.assertEquals(1, apiGlobalDoc.getMigrationset().getMigrations().size());
		Assert.assertEquals(1, apiGlobalDoc.getChangelogset().getChangelogs().size());
	}
",non-flaky,5
42987,fabiomaffioletti_jsondoc,JSONDocApiVisibilityBuilderTest.testApiVisibility,"	@Test
	public void testApiVisibility() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(Controller.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(ApiVisibility.PUBLIC, apiDoc.getVisibility());
		Assert.assertEquals(ApiStage.BETA, apiDoc.getStage());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if(apiMethodDoc.getPath().contains(""/inherit"")) {
				Assert.assertEquals(ApiVisibility.PUBLIC, apiMethodDoc.getVisibility());
				Assert.assertEquals(ApiStage.BETA, apiMethodDoc.getStage());
			}
			if(apiMethodDoc.getPath().contains(""/override"")) {
				Assert.assertEquals(ApiVisibility.PRIVATE, apiMethodDoc.getVisibility());
				Assert.assertEquals(ApiStage.GA, apiMethodDoc.getStage());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(Controller2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(ApiVisibility.UNDEFINED, apiDoc.getVisibility());
		Assert.assertEquals(ApiStage.UNDEFINED, apiDoc.getStage());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if(apiMethodDoc.getPath().contains(""/only-method"")) {
				Assert.assertEquals(ApiVisibility.PRIVATE, apiMethodDoc.getVisibility());
				Assert.assertEquals(ApiStage.DEPRECATED, apiMethodDoc.getStage());
			}
		}
		
	}
",non-flaky,5
42988,fabiomaffioletti_jsondoc,JSONDocApiMethodPathBuilderTest.apply,"	@Test
	public void testPathWithMethodDisplayURI() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(Controller.class), MethodDisplay.URI).iterator().next();

		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				return 
						input.getPath().contains(""/path1"") && 
						input.getPath().contains(""/path2"") && 
						input.getDisplayedMethodString().contains(""/path1"") &&
						input.getDisplayedMethodString().contains(""/path2"");
			}
",non-flaky,5
42989,fabiomaffioletti_jsondoc,JSONDocApiMethodPathBuilderTest.apply,"	@Test
	public void testPathWithMethodDisplayMethod() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(Controller.class), MethodDisplay.METHOD).iterator().next();
		
		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				return 
						input.getPath().contains(""/path1"") && 
						input.getPath().contains(""/path2"") && 
						input.getDisplayedMethodString().contains(""path"") &&
						!input.getDisplayedMethodString().contains(""/path1"");
			}
",non-flaky,5
42990,fabiomaffioletti_jsondoc,JSONDocTypeBuilderTest.testReflex,"	@Test
	public void testReflex() throws NoSuchMethodException, SecurityException, ClassNotFoundException, JsonGenerationException, JsonMappingException, IOException {
		mapper.setSerializationInclusion(Include.NON_NULL);
		JSONDocType jsonDocType = new JSONDocType();
		
		Method method = JSONDocTypeBuilderTest.class.getMethod(""getString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""integer"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getInt"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""int"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getLong"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""long"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getlong"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""long"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListSetString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list of set of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getStringArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getIntegerArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of integer"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListOfStringArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of list of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getSetOfStringArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of set of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getList"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListOfWildcard"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list of wildcard"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListOfWildcardArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of list of wildcard"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of list"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getSetArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of set"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMap"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getHashMap"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""hashmap"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapStringInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[string, integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapListOfStringInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[list of string, integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapStringSetOfInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[string, set of integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapListOfStringSetOfInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[list of string, set of integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapListOfSetOfStringSetOfInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[list of set of string, set of integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapWildcardInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[wildcard, integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapWildcardWildcard"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[wildcard, wildcard]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapListOfWildcardWildcard"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[list of wildcard, wildcard]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapMapInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[map, integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapMapStringLongInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[map[string, long], integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getResponseEntityString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""responseentity of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getResponseEntityListOfString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""responseentity of list of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getParentPojoList"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list of my_parent"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getSpecializedWGenericsPojo"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""fooPojo of T"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");		
	}
",non-flaky,5
42991,fabiomaffioletti_jsondoc,JSONDocEnumTemplateBuilderTest.testTemplate,"	@Test
	public void testTemplate() throws IOException, IllegalArgumentException, IllegalAccessException, InstantiationException {
		ObjectMapper mapper = new ObjectMapper();
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(MyEnum.class);
		
		Map<String, Object> template = JSONDocTemplateBuilder.build(MyEnum.class, classes);
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42992,fabiomaffioletti_jsondoc,JSONDocTemplateBuilderWithOrderTest.thatTemplateIsMappedToStringCorrectly,"	@Test
	public void thatTemplateIsMappedToStringCorrectly() throws Exception {
		final ObjectMapper mapper = new ObjectMapper();
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(Unordered.class, Ordered.class);

		Map<String, Object> unorderedTemplate = JSONDocTemplateBuilder.build(Unordered.class, classes);
		Assert.assertEquals(""{\""aField\"":\""\"",\""xField\"":\""\""}"", mapper.writeValueAsString(unorderedTemplate));

		Map<String, Object> orderedTemplate = JSONDocTemplateBuilder.build(Ordered.class, classes);
		Assert.assertEquals(""{\""xField\"":\""\"",\""aField\"":\""\"",\""bField\"":\""\""}"", mapper.writeValueAsString(orderedTemplate));
	}
",non-flaky,5
42993,fabiomaffioletti_jsondoc,DefaultJSONDocScannerTest.getJSONDoc,"    @Test
    public void getJSONDoc() throws IOException {
    	JSONDocScanner jsondocScanner = new DefaultJSONDocScanner();
        JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.core.util""), true, MethodDisplay.URI);
        assertEquals(1, jsondoc.getApis().size());

        int countApis = 0;
        for (String string : jsondoc.getApis().keySet()) {
            countApis += jsondoc.getApis().get(string).size();
        }
        assertEquals(4, countApis);

        assertEquals(3, jsondoc.getObjects().size());
        
        int countFlows = 0;
        for (String string : jsondoc.getFlows().keySet()) {
        	countFlows += jsondoc.getFlows().get(string).size();
        }
        assertEquals(2, countFlows);

        int countObjects = 0;
        for (String string : jsondoc.getObjects().keySet()) {
            countObjects += jsondoc.getObjects().get(string).size();
        }
        assertEquals(10, countObjects);

        Set<ApiVerb> apiVerbs = getAllTestedApiVerbs(jsondoc);
        assertEquals(ApiVerb.values().length, apiVerbs.size());

        log.debug(objectMapper.writeValueAsString(jsondoc));
    }
",non-flaky,5
42994,fabiomaffioletti_jsondoc,StackOverflowTemplateBuilderTest.testTemplate,"	@Test
	public void testTemplate() throws JsonGenerationException, JsonMappingException, IOException, IllegalArgumentException, IllegalAccessException, InstantiationException {
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(StackOverflowTemplateSelf.class, StackOverflowTemplateObjectOne.class, StackOverflowTemplateObjectTwo.class);
		
		StackOverflowTemplateSelf objectSelf = new StackOverflowTemplateSelf();
		Map<String, Object> template = JSONDocTemplateBuilder.build(objectSelf.getClass(), classes);
		System.out.println(mapper.writeValueAsString(template));
		
		StackOverflowTemplateObjectOne objectOne = new StackOverflowTemplateObjectOne();
		template = JSONDocTemplateBuilder.build(objectOne.getClass(), classes);
		System.out.println(mapper.writeValueAsString(template));
		
		StackOverflowTemplateObjectTwo objectTwo = new StackOverflowTemplateObjectTwo();
		template = JSONDocTemplateBuilder.build(objectTwo.getClass(), classes);
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42995,fabiomaffioletti_jsondoc,StackOverflowTemplateBuilderTest.typeOneTwo,"	@Test
	public void typeOneTwo() throws JsonGenerationException, JsonMappingException, IOException {
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(NotAnnotatedStackOverflowObjectOne.class, NotAnnotatedStackOverflowObjectTwo.class);
		
		NotAnnotatedStackOverflowObjectOne typeOne = new NotAnnotatedStackOverflowObjectOne();
		Map<String, Object> template = JSONDocTemplateBuilder.build(typeOne.getClass(), classes);
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42996,fabiomaffioletti_jsondoc,JSONDocTemplateBuilderTest.testTemplate,"	@Test
	public void testTemplate() throws IOException, IllegalArgumentException, IllegalAccessException, InstantiationException {
		ObjectMapper mapper = new ObjectMapper();
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(TemplateObject.class);
		
		Map<String, Object> template = JSONDocTemplateBuilder.build(TemplateObject.class, classes);

		Assert.assertEquals(0, template.get(""my_id""));
		Assert.assertEquals(0, template.get(""idint""));
		Assert.assertEquals(0, template.get(""idlong""));
		Assert.assertEquals("""", template.get(""name""));
		Assert.assertEquals("""", template.get(""gender""));
		Assert.assertEquals(true, template.get(""bool""));
		Assert.assertEquals(new ArrayList(), template.get(""intarrarr""));
		Assert.assertEquals(new JSONDocTemplate(), template.get(""sub_obj""));
		Assert.assertEquals(new ArrayList(), template.get(""untypedlist""));
		Assert.assertEquals(new ArrayList(), template.get(""subsubobjarr""));
		Assert.assertEquals(new ArrayList(), template.get(""stringlist""));
		Assert.assertEquals(new ArrayList(), template.get(""stringarrarr""));
		Assert.assertEquals(new ArrayList(), template.get(""integerarr""));
		Assert.assertEquals(new ArrayList(), template.get(""stringarr""));
		Assert.assertEquals(new ArrayList(), template.get(""intarr""));
		Assert.assertEquals(new ArrayList(), template.get(""subobjlist""));
		Assert.assertEquals(new ArrayList(), template.get(""wildcardlist""));
		Assert.assertEquals(new ArrayList(), template.get(""longlist""));
		Assert.assertEquals("""", template.get(""namechar""));
		Assert.assertEquals(new HashMap(), template.get(""map""));
		Assert.assertEquals(new HashMap(), template.get(""mapstringinteger""));
		Assert.assertEquals(new HashMap(), template.get(""mapsubobjinteger""));
		Assert.assertEquals(new HashMap(), template.get(""mapintegersubobj""));
		Assert.assertEquals(new HashMap(), template.get(""mapintegerlistsubsubobj""));
		
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42997,fabiomaffioletti_jsondoc,JSONDocTemplateBuilderTest.testTemplateWithConstant,"	@Test
	public void testTemplateWithConstant() throws Exception {
        final ObjectMapper mapper = new ObjectMapper();
        final Set<Class<?>> classes = Sets.<Class<?>>newHashSet(ClassWithConstant.class);

        final Map<String, Object> template = JSONDocTemplateBuilder.build(ClassWithConstant.class, classes);
        Assert.assertEquals("""", template.get(""identifier""));
        Assert.assertEquals(null, template.get(THIS_IS_A_CONSTANT));

        final String serializedTemplate =
            ""{"" +
                ""\""identifier\"":\""\"""" +
            ""}"";

        assertThat(mapper.writeValueAsString(template), is(serializedTemplate));
	}
",non-flaky,5
42998,fabiomaffioletti_jsondoc,JSONDocNoAnnotationTemplateBuilderTest.testTemplate,"	@Test
	public void testTemplate() throws IOException, IllegalArgumentException, IllegalAccessException, InstantiationException {
		ObjectMapper mapper = new ObjectMapper();
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(NoAnnotationPojo.class);
		
		Map<String, Object> template = JSONDocTemplateBuilder.build(NoAnnotationPojo.class, classes);
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42999,fabiomaffioletti_jsondoc,InterfaceApiObjectTest.testInvisible,"	@Test
	public void testInvisible() {
		JSONDoc jsonDoc = jsondocScanner.getJSONDoc(""version"", ""basePath"", Lists.newArrayList(""org.jsondoc.springmvc.issues.invisible""), true, MethodDisplay.URI);
		Assert.assertEquals(1, jsonDoc.getObjects().keySet().size());
		for (String string : jsonDoc.getObjects().keySet()) {
			Assert.assertEquals(2, jsonDoc.getObjects().get(string).size());
		}
		for (ApiDoc apiDoc : jsonDoc.getApis().get("""")) {
			for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
				Assert.assertEquals(""Resource Interface"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
		}
		
	}
",non-flaky,5
43000,fabiomaffioletti_jsondoc,Issue151Test.testIssue151,"	@Test
	public void testIssue151() {
		JSONDoc jsonDoc = jsondocScanner.getJSONDoc(""version"", ""basePath"", Lists.newArrayList(""org.jsondoc.springmvc.issues.issue151""), true, MethodDisplay.URI);
		Assert.assertEquals(2, jsonDoc.getObjects().keySet().size());
		Assert.assertEquals(1, jsonDoc.getObjects().get(""bargroup"").size());
		Assert.assertEquals(1, jsonDoc.getObjects().get(""foogroup"").size());
	}
",non-flaky,5
43001,fabiomaffioletti_jsondoc,SpringResponseStatusBuilderTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/status-one"")) {
				Assert.assertEquals(""201 - Created"", apiMethodDoc.getResponsestatuscode());
			}
			if (apiMethodDoc.getPath().contains(""/status-two"")) {
				Assert.assertEquals(""200 - OK"", apiMethodDoc.getResponsestatuscode());
			}
		}
	}
",non-flaky,5
43002,fabiomaffioletti_jsondoc,PlainSpringJSONDocScannerTest.testMergeApiDoc,"	@Test
	public void testMergeApiDoc() {
		Set<Class<?>> controllers = new LinkedHashSet<Class<?>>();
		controllers.add(SpringController.class);
		Set<ApiDoc> apiDocs = jsondocScanner.getApiDocs(controllers, MethodDisplay.URI);

		ApiDoc apiDoc = apiDocs.iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getDescription());
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertNotNull(apiDoc.getGroup());

		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			Assert.assertEquals(MethodDisplay.URI, apiMethodDoc.getDisplayMethodAs());
			Assert.assertNull(apiMethodDoc.getAuth());
			Assert.assertNull(apiMethodDoc.getSupportedversions());
			Assert.assertTrue(apiMethodDoc.getApierrors().isEmpty());
			Assert.assertNull(apiMethodDoc.getId());
			Assert.assertEquals("""", apiMethodDoc.getSummary());
			Assert.assertEquals("""", apiMethodDoc.getDescription());
			
			if (apiMethodDoc.getPath().contains(""/api/string/{name}"")) {
				Assert.assertEquals(2, apiMethodDoc.getHeaders().size());
				Set<ApiHeaderDoc> headers = apiMethodDoc.getHeaders();
				Iterator<ApiHeaderDoc> headersIterator = headers.iterator();
				ApiHeaderDoc headerTest = headersIterator.next();
				Assert.assertEquals(""header"", headerTest.getName());
				Assert.assertEquals(""test"", headerTest.getAllowedvalues()[0]);
				ApiHeaderDoc headerTwo = headersIterator.next();
				Assert.assertEquals(""header-two"", headerTwo.getName());
				Assert.assertEquals(""header-test"", headerTwo.getAllowedvalues()[0]);

				Assert.assertEquals(""string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""POST"", apiMethodDoc.getVerb().iterator().next().name());
				Assert.assertEquals(""application/json"", apiMethodDoc.getProduces().iterator().next());
				Assert.assertEquals(""application/json"", apiMethodDoc.getConsumes().iterator().next());
				Assert.assertEquals(""201 - Created"", apiMethodDoc.getResponsestatuscode());

				Set<ApiParamDoc> queryparameters = apiMethodDoc.getQueryparameters();
				Assert.assertEquals(4, queryparameters.size());
				Iterator<ApiParamDoc> qpIterator = queryparameters.iterator();
				ApiParamDoc apiParamDoc = qpIterator.next();
				Assert.assertEquals(""delete"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertEquals(null, apiParamDoc.getDefaultvalue());
				Assert.assertEquals(0, apiParamDoc.getAllowedvalues().length);
				apiParamDoc = qpIterator.next();
				Assert.assertEquals(""id"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertTrue(apiParamDoc.getDefaultvalue().isEmpty());
				apiParamDoc = qpIterator.next();
				Assert.assertEquals("""", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertEquals("""", apiParamDoc.getDefaultvalue());

				apiParamDoc = qpIterator.next();
				Assert.assertEquals(""user"", apiParamDoc.getName());
				Assert.assertEquals(""false"", apiParamDoc.getRequired());
				Assert.assertEquals(""admin"", apiParamDoc.getDefaultvalue());

				Set<ApiParamDoc> pathparameters = apiMethodDoc.getPathparameters();
				Iterator<ApiParamDoc> ppIterator = pathparameters.iterator();
				apiParamDoc = ppIterator.next();
				apiParamDoc = apiMethodDoc.getPathparameters().iterator().next();
				Assert.assertEquals(""test"", apiParamDoc.getName());
			}
		}

	}
",non-flaky,5
43003,fabiomaffioletti_jsondoc,Spring3JSONDocGenericsObjectScannerTest.getJSONDoc,"	@Test
	public void getJSONDoc() throws IOException {
		JSONDocScanner jsondocScanner = new Spring3JSONDocScanner();
		JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.springmvc.issues.issue174""), true, MethodDisplay.URI);

		Map<String, Set<ApiObjectDoc>> objects = jsondoc.getObjects();
		for (Set<ApiObjectDoc> values : objects.values()) {
			for (ApiObjectDoc apiObjectDoc : values) {
				System.out.println(apiObjectDoc.getName());
			}
		}
	}
",non-flaky,5
43004,fabiomaffioletti_jsondoc,SpringObjectBuilderTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiObjectDoc buildObject = SpringObjectBuilder.buildObject(MyObject.class);
		Assert.assertEquals(""MyObject"", buildObject.getName());
		Assert.assertEquals(3, buildObject.getFields().size());
	}
",non-flaky,5
43005,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());

		boolean slashPath = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				return input.getPath().contains(""/path"");
			}
",non-flaky,5
43006,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath2() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController2"", apiDoc.getName());

		boolean none = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			
			@Override
			public boolean apply(ApiMethodDoc input) {
				System.out.println(input.getPath());
				return input.getPath().contains(""/"");
			}
",non-flaky,5
43007,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath3() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController3.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController3"", apiDoc.getName());

		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				boolean allRight =
								input.getPath().contains(""/path1/path3"") && 
								input.getPath().contains(""/path1/path4"") && 
								input.getPath().contains(""/path2/path3"") && 
								input.getPath().contains(""/path2/path4"");     
				return allRight;
			}
",non-flaky,5
43008,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath4() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController4.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController4"", apiDoc.getName());

		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				boolean allRight =
								input.getPath().contains(""/path""); 
				return allRight;
			}
",non-flaky,5
43009,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath5() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController5.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController5"", apiDoc.getName());
		
		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				boolean allRight = input.getPath().contains(""/path"") && input.getPath().contains(""/path2"");
				return allRight;
			}
",non-flaky,5
43010,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath6() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController6.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController6"", apiDoc.getName());

		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				return input.getPath().contains(""/api/widget/frame"");
			}
",non-flaky,5
43011,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPathWithMethodDisplayMethod() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController5.class), MethodDisplay.METHOD).iterator().next();
		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				boolean allRight = input.getPath().contains(""/path"") && input.getPath().contains(""/path2"") && input.getDisplayedMethodString().contains(""none"");
				return allRight;
			}
",non-flaky,5
43012,fabiomaffioletti_jsondoc,SpringConsumesBuilderTest.testApiVerb,"    @Test
    public void testApiVerb() {
	ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
	Assert.assertEquals(""SpringController"", apiDoc.getName());
	Assert.assertEquals(3, apiDoc.getMethods().size());
	for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
	    if (apiMethodDoc.getPath().contains(""/consumes-one"")) {
		Assert.assertEquals(1, apiMethodDoc.getConsumes().size());
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, apiMethodDoc.getConsumes().iterator().next());
	    }
	    if (apiMethodDoc.getPath().contains(""/consumes-two"")) {
		Assert.assertEquals(2, apiMethodDoc.getConsumes().size());
		Iterator<String> iterator = apiMethodDoc.getConsumes().iterator();
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, iterator.next());
		Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, iterator.next());
	    }
	    if (apiMethodDoc.getPath().contains(""/consumes-three"")) {
		Assert.assertEquals(1, apiMethodDoc.getConsumes().size());
		String consumes = apiMethodDoc.getConsumes().iterator().next();
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, consumes);
	    }
	}

	apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
	Assert.assertEquals(""SpringController2"", apiDoc.getName());
	Assert.assertEquals(3, apiDoc.getMethods().size());
	for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
	    if (apiMethodDoc.getPath().contains(""/consumes-one"")) {
		Assert.assertEquals(1, apiMethodDoc.getConsumes().size());
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, apiMethodDoc.getConsumes().iterator().next());
	    }
	    if (apiMethodDoc.getPath().contains(""/consumes-two"")) {
		Assert.assertEquals(2, apiMethodDoc.getConsumes().size());
		Iterator<String> iterator = apiMethodDoc.getConsumes().iterator();
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, iterator.next());
		Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, iterator.next());
	    }
	    if (apiMethodDoc.getPath().contains(""/consumes-three"")) {
		Assert.assertEquals(1, apiMethodDoc.getConsumes().size());
		Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, apiMethodDoc.getConsumes().iterator().next());
	    }
	}
    }
",non-flaky,5
43013,fabiomaffioletti_jsondoc,SpringQueryParamBuilderTest.testQueryParam,"	@Test
	public void testQueryParam() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one"")) {
				Assert.assertEquals(1, apiMethodDoc.getQueryparameters().size());
			}
			if (apiMethodDoc.getPath().contains(""/param-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
			}
			if (apiMethodDoc.getPath().contains(""/param-three"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				Assert.assertEquals(""param"", param.getName());
				Assert.assertEquals(""value"", param.getAllowedvalues()[0]);
				ApiParamDoc param2 = iterator.next();
				Assert.assertEquals(""param2"", param2.getName());
				Assert.assertEquals(""value2"", param2.getAllowedvalues()[0]);
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController2"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
			}
			if (apiMethodDoc.getPath().contains(""/param-two"")) {
				Assert.assertEquals(3, apiMethodDoc.getQueryparameters().size());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController3.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController3"", apiDoc.getName());
		Assert.assertEquals(4, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				ApiParamDoc queryParam = iterator.next();
				Assert.assertEquals(""name"", queryParam.getName());
				Assert.assertEquals(""true"", queryParam.getRequired());
				Assert.assertEquals(""string"", queryParam.getJsondocType().getOneLineText());
				Assert.assertEquals("""", queryParam.getDefaultvalue());
			}
			if (apiMethodDoc.getPath().contains(""/param-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				ApiParamDoc queryParam = iterator.next();
				Assert.assertEquals(""name"", queryParam.getName());
				Assert.assertEquals(""false"", queryParam.getRequired());
				Assert.assertEquals(""string"", queryParam.getJsondocType().getOneLineText());
				Assert.assertEquals(""test"", queryParam.getDefaultvalue());
			}
			if (apiMethodDoc.getPath().contains(""/param-three"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				ApiParamDoc queryParam = iterator.next();
				Assert.assertEquals("""", queryParam.getName());
				Assert.assertEquals(""true"", queryParam.getRequired());
				Assert.assertEquals(""string"", queryParam.getJsondocType().getOneLineText());
				Assert.assertEquals("""", queryParam.getDefaultvalue());
			}
			if (apiMethodDoc.getPath().contains(""/param-four"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				ApiParamDoc queryParam = iterator.next();
				Assert.assertEquals(""value"", queryParam.getName());
				Assert.assertEquals(""false"", queryParam.getRequired());
				Assert.assertEquals(""string"", queryParam.getJsondocType().getOneLineText());
				Assert.assertEquals("""", queryParam.getDefaultvalue());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController4.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController4"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/"")) {
				Assert.assertEquals(1, apiMethodDoc.getQueryparameters().size());
				ApiParamDoc param = apiMethodDoc.getQueryparameters().iterator().next();
				Assert.assertEquals(""name"", param.getName());
			}
			if (apiMethodDoc.getPath().contains(""/two"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController5.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController5"", apiDoc.getName());
		Assert.assertEquals(1, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/"")) {
				Assert.assertEquals(1, apiMethodDoc.getQueryparameters().size());
				ApiParamDoc param = apiMethodDoc.getQueryparameters().iterator().next();
				Assert.assertEquals(""modelAttributePojo"", param.getName());
				Assert.assertEquals(""modelattributepojo"", param.getJsondocType().getOneLineText());
			}
		}
		
	}
",non-flaky,5
43014,fabiomaffioletti_jsondoc,SpringRequestMappingDerivativesTest.apply,"    @Test
    public void testGetMapping() {
        ApiDoc
            apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(RequestMappingController.class), MethodDisplay.URI).iterator().next();
        Assert.assertEquals(""RequestMappingController"", apiDoc.getName());

        boolean getMethodPresent = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
            @Override
            public boolean apply(ApiMethodDoc input) {
                return input.getMethod().equals(""get"");
            }
",non-flaky,5
43015,fabiomaffioletti_jsondoc,SpringApiHeadersDocTest.testApiHeadersOnClass,"	@Test
	public void testApiHeadersOnClass() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringApiHeadersController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringApiHeadersController"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/spring-api-headers-controller-method-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getHeaders().size());
			}
			if (apiMethodDoc.getPath().contains(""/spring-api-headers-controller-method-two"")) {
				Assert.assertEquals(3, apiMethodDoc.getHeaders().size());
			}
			if (apiMethodDoc.getPath().contains(""/spring-api-headers-controller-method-three"")) {
				Assert.assertEquals(4, apiMethodDoc.getHeaders().size());
				Iterator<ApiHeaderDoc> headers = apiMethodDoc.getHeaders().iterator();
				ApiHeaderDoc h1 = headers.next();
				ApiHeaderDoc h2 = headers.next();
				ApiHeaderDoc h4 = headers.next();
				Assert.assertEquals(""h4"", h4.getName());
				ApiHeaderDoc h5 = headers.next();
				Assert.assertEquals(""h5"", h5.getName());
			}
		}
	}
",non-flaky,5
43016,fabiomaffioletti_jsondoc,SpringPathVariableBuilderTest.testPathVariable,"	@Test
	public void testPathVariable() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one/{id}/{string}"")) {
				Assert.assertEquals(2, apiMethodDoc.getPathparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getPathparameters().iterator();
				ApiParamDoc id = iterator.next();
				Assert.assertEquals("""", id.getName());
				Assert.assertEquals(""long"", id.getJsondocType().getOneLineText());
				ApiParamDoc name = iterator.next();
				Assert.assertEquals(""name"", name.getName());
				Assert.assertEquals(""string"", name.getJsondocType().getOneLineText());
			}
			
			if (apiMethodDoc.getPath().contains(""/param-one/{id}/{string}/{test}"")) {
				Assert.assertEquals(3, apiMethodDoc.getPathparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getPathparameters().iterator();
				ApiParamDoc id = iterator.next();
				Assert.assertEquals(""id"", id.getName());
				Assert.assertEquals(""long"", id.getJsondocType().getOneLineText());
				ApiParamDoc name = iterator.next();
				Assert.assertEquals(""name"", name.getName());
				Assert.assertEquals(""string"", name.getJsondocType().getOneLineText());
				ApiParamDoc test = iterator.next();
				Assert.assertEquals("""", test.getName());
				Assert.assertEquals(""long"", test.getJsondocType().getOneLineText());
			}
		}
		
	}
",non-flaky,5
43017,fabiomaffioletti_jsondoc,SpringPathVariableBuilderTest.testPathVariableWithJSONDoc,"	@Test
	public void testPathVariableWithJSONDoc() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController2"", apiDoc.getName());
		Assert.assertEquals(1, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one/{id}/{string}"")) {
				Assert.assertEquals(2, apiMethodDoc.getPathparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getPathparameters().iterator();
				ApiParamDoc id = iterator.next();
				Assert.assertEquals("""", id.getName());
				Assert.assertEquals(""long"", id.getJsondocType().getOneLineText());
				Assert.assertEquals(""description for id"", id.getDescription());
				ApiParamDoc name = iterator.next();
				Assert.assertEquals(""name"", name.getName());
				Assert.assertEquals(""string"", name.getJsondocType().getOneLineText());
			}
		}
		
	}
",non-flaky,5
43018,fabiomaffioletti_jsondoc,SpringRequestBodyBuilderTest.testBodyOne,"	@Test
	public void testBodyOne() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/body-one"")) {
				Assert.assertNotNull(apiMethodDoc.getBodyobject());
				Assert.assertEquals(""string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
			}
			if (apiMethodDoc.getPath().contains(""/body-two"")) {
				Assert.assertNotNull(apiMethodDoc.getBodyobject());
				Assert.assertEquals(""body"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
			}
		}
	}
",non-flaky,5
43019,fabiomaffioletti_jsondoc,SpringResponseBuilderTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/response-one"")) {
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			if (apiMethodDoc.getPath().contains(""/response-two"")) {
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			if (apiMethodDoc.getPath().contains(""/response-three"")) {
				Assert.assertEquals(""map[string, integer]"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
		}
	}
",non-flaky,5
43020,fabiomaffioletti_jsondoc,SpringApiVerbDocTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringApiVerbController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringApiVerbController"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/api-verb/spring-api-verb-controller-method-one"")) {
				Assert.assertEquals(1, apiMethodDoc.getVerb().size());
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
			}
			if (apiMethodDoc.getPath().contains(""/api-verb/spring-api-verb-controller-method-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getVerb().size());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringApiVerbController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringApiVerbController2"", apiDoc.getName());
		Assert.assertEquals(1, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/api-verb-2/spring-api-verb-controller-method-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getVerb().size());
			}
		}
		
	}
",non-flaky,5
43021,fabiomaffioletti_jsondoc,SpringProducesBuilderTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/produces-one"")) {
				Assert.assertEquals(1, apiMethodDoc.getProduces().size());
				Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, apiMethodDoc.getProduces().iterator().next());
			}
			if (apiMethodDoc.getPath().contains(""/produces-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getProduces().size());
				Iterator<String> iterator = apiMethodDoc.getProduces().iterator();
				Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, iterator.next());
				Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, iterator.next());
			}
			if (apiMethodDoc.getPath().contains(""/produces-three"")) {
				Assert.assertEquals(1, apiMethodDoc.getProduces().size());
				String produces = apiMethodDoc.getProduces().iterator().next();
				Assert.assertEquals(""application/json"", produces);
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController2"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/produces-one"")) {
				Assert.assertEquals(1, apiMethodDoc.getProduces().size());
				Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, apiMethodDoc.getProduces().iterator().next());
			}
			if (apiMethodDoc.getPath().contains(""/produces-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getProduces().size());
				Iterator<String> iterator = apiMethodDoc.getProduces().iterator();
				Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, iterator.next());
				Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, iterator.next());
			}
			if (apiMethodDoc.getPath().contains(""/produces-three"")) {
				Assert.assertEquals(1, apiMethodDoc.getProduces().size());
				Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, apiMethodDoc.getProduces().iterator().next());
			}
		}
	}
",non-flaky,5
43022,fabiomaffioletti_jsondoc,JSONDocSpringJSONDocScannerTest.testMergeApiDoc,"	@Test
	public void testMergeApiDoc() {
		Set<Class<?>> controllers = new LinkedHashSet<Class<?>>();
		controllers.add(SpringController.class);
		Set<ApiDoc> apiDocs = jsondocScanner.getApiDocs(controllers, MethodDisplay.URI);
		
		ApiDoc apiDoc = apiDocs.iterator().next();
		Assert.assertEquals(""A spring controller"", apiDoc.getDescription());
		Assert.assertEquals(""Spring controller"", apiDoc.getName());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if(apiMethodDoc.getPath().contains(""/api/string/{name}"")) {
				Assert.assertNotNull(apiMethodDoc.getAuth());
				Assert.assertNotNull(apiMethodDoc.getSupportedversions());
				Assert.assertFalse(apiMethodDoc.getApierrors().isEmpty());
				
				Assert.assertEquals(""string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""/api/string/{name}"", apiMethodDoc.getPath().iterator().next());
				Assert.assertEquals(""POST"", apiMethodDoc.getVerb().iterator().next().name());
				Assert.assertEquals(""application/json"", apiMethodDoc.getProduces().iterator().next());
				Assert.assertEquals(""application/json"", apiMethodDoc.getConsumes().iterator().next());
				Assert.assertEquals(""201 - Created"", apiMethodDoc.getResponsestatuscode());
				
				Set<ApiHeaderDoc> headers = apiMethodDoc.getHeaders();
				ApiHeaderDoc header = headers.iterator().next();
				Assert.assertEquals(""header"", header.getName());
				Assert.assertEquals(""test"", header.getAllowedvalues()[0]);
				
				Set<ApiParamDoc> queryparameters = apiMethodDoc.getQueryparameters();
				Assert.assertEquals(3, queryparameters.size());
				Iterator<ApiParamDoc> qpIterator = queryparameters.iterator();
				ApiParamDoc apiParamDoc = qpIterator.next();
				Assert.assertEquals(""delete"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertEquals(null, apiParamDoc.getDefaultvalue());
				Assert.assertEquals(0, apiParamDoc.getAllowedvalues().length);
				apiParamDoc = qpIterator.next();
				Assert.assertEquals(""id"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertTrue(apiParamDoc.getDefaultvalue().isEmpty());
				apiParamDoc = qpIterator.next();
				Assert.assertEquals(""myquery"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertEquals("""", apiParamDoc.getDefaultvalue());
				
				Set<ApiParamDoc> pathparameters = apiMethodDoc.getPathparameters();
				Iterator<ApiParamDoc> ppIterator = pathparameters.iterator();
				apiParamDoc = ppIterator.next();
				apiParamDoc = apiMethodDoc.getPathparameters().iterator().next();
				Assert.assertEquals(""test"", apiParamDoc.getName());
			}
		}
		
	}
",non-flaky,5
43023,fabiomaffioletti_jsondoc,Spring3JSONDocObjectScannerTest.getJSONDoc,"    @Test
    public void getJSONDoc() throws IOException {
        JSONDocScanner jsondocScanner = new Spring3JSONDocScanner();
        JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.springmvc.controller""), true, MethodDisplay.URI);

        Map<String, Set<ApiObjectDoc>> objects = jsondoc.getObjects();
        for (Set<ApiObjectDoc> values : objects.values()) {
            for (ApiObjectDoc apiObjectDoc : values) {
                System.out.println(apiObjectDoc.getName());
            }
        }

    }
",non-flaky,5
43024,fabiomaffioletti_jsondoc,Spring3JSONDocObjectScannerTest.findsNestedObject,"    @Test
    public void findsNestedObject() throws Exception {
        JSONDocScanner jsondocScanner = new Spring3JSONDocScanner();
        JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.springmvc.controller""), true, MethodDisplay.URI);

        Map<String, Set<ApiObjectDoc>> objects = jsondoc.getObjects();
        for (Set<ApiObjectDoc> values : objects.values()) {
            assertContainsDoc(values, ""NestedObject1"");
        }
    }
",non-flaky,5
43025,fabiomaffioletti_jsondoc,Spring3JSONDocObjectScannerTest.findsDeeplyNestedObjects,"    @Test
    public void findsDeeplyNestedObjects() throws Exception {
        JSONDocScanner jsondocScanner = new Spring3JSONDocScanner();
        JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.springmvc.controller""), true, MethodDisplay.URI);

        Map<String, Set<ApiObjectDoc>> objects = jsondoc.getObjects();
        for (Set<ApiObjectDoc> values : objects.values()) {
            assertContainsDoc(values, ""NestedObject2"");
            assertContainsDoc(values, ""NestedObject3"");
        }
    }
",non-flaky,5
130,Kong_unirest-java,DefectTest.nullAndObjectValuesInMap,"@Test
void nullAndObjectValuesInMap() {
    Map<String, Object> queryParams = new HashMap<>();
    queryParams.put(""foo"", null);
    queryParams.put(""baz"", ""qux"");
    Unirest.get(GET).queryString(queryParams).asObject(RequestCapture.class).getBody().assertParam(""foo"", """").assertParam(""baz"", ""qux"").assertQueryString(""foo&baz=qux"");
}",unordered collections,3
98337,Kong_unirest-java,MockClientInterceptorIssueTest.setup,"    @BeforeEach
    public void setup() {
        this.unirestInstance = Unirest.spawnInstance();
        this.unirestInstance.config().interceptor(interceptor);
    }
",non-flaky,5
98338,Kong_unirest-java,ExpectedResponseTest.writeValue,"    @Test
        public String writeValue(Object value) {
            return ""derp"";
        }
",non-flaky,5
98339,Kong_unirest-java,AssertTest.expectAnyPath,"    @Test
    public void expectAnyPath(){
        client.expect(HttpMethod.GET)
                .thenReturn(""woh"");

        Unirest.get(path).asEmpty();

        client.verifyAll();
    }
",non-flaky,5
98340,Kong_unirest-java,ApacheBehaviorTest.setTimeoutsAndCustomClient,"    @Test
    public void setTimeoutsAndCustomClient() {
        try {
            Unirest.config().connectTimeout(1000).socketTimeout(2000);
        } catch (Exception e) {
            fail();
        }

        try {
            Unirest.config().asyncClient(HttpAsyncClientBuilder.create().build());
        } catch (Exception e) {
            fail();
        }

        try {
            Unirest.config().asyncClient(HttpAsyncClientBuilder.create().build());
            Unirest.config().connectTimeout(1000).socketTimeout(2000);
            fail();
        } catch (Exception e) {
            // Ok
        }

        try {
            Unirest.config().httpClient(HttpClientBuilder.create().build());
            Unirest.config().connectTimeout(1000).socketTimeout(2000);
            fail();
        } catch (Exception e) {
            // Ok
        }
    }
",non-flaky,5
98341,Kong_unirest-java,ApacheInterceptorTest.process,"    @Test
        public void process(org.apache.http.HttpRequest httpRequest, org.apache.http.protocol.HttpContext httpContext) throws HttpException, IOException {
            httpRequest.addHeader(""x-custom"", ""foo"");
        }
",non-flaky,5
98342,Kong_unirest-java,ApacheClientTest.setUp,"    @BeforeEach
    public void setUp() {
        super.setUp();
    }
",non-flaky,5
98343,Kong_unirest-java,ApacheClientTest.tearDown,"    @AfterEach
    public void tearDown() {
        super.tearDown();
        requestConfigUsed = false;
    }
",non-flaky,5
98344,Kong_unirest-java,CacheManagerTest.getClient,"    @Test
        public Object getClient() {
            return null;
        }
",non-flaky,5
98345,Kong_unirest-java,UriFormatterTest.testMangler_encoding,"    @Test
    public void testMangler_encoding() {
        assertLinkSurvives(""http://localhost/test%2Fthis"");
    }
",non-flaky,5
98346,Kong_unirest-java,UriFormatterTest.testMangler_fragment,"    @Test
    public void testMangler_fragment() {
        assertLinkSurvives(""http://localhost/test?a=b#fragment"");
    }
",non-flaky,5
98347,Kong_unirest-java,UriFormatterTest.basicBoringUri,"    @Test
    public void basicBoringUri() {
        assertLinkSurvives(""http://localhost/test?a=b"");
    }
",non-flaky,5
98348,Kong_unirest-java,UriFormatterTest.semicolonsAsParam,"    @Test
    public void semicolonsAsParam() {
        assertLinkSurvives(""http://localhost/test?a=b;foo=bar"");
    }
",non-flaky,5
98349,Kong_unirest-java,UriFormatterTest.utf8Chars,"    @Test
    public void utf8Chars(){
        assertLinkSurvives(""http://localhost/test?foo=ããã«ã¡ã¯"");
    }
",non-flaky,5
98350,Kong_unirest-java,ClientFactoryTest.before,"    @AfterEach
    public void before(){
        Unirest.shutDown(true);
    }
",non-flaky,5
98351,Kong_unirest-java,JSONArrayTest.toString,"    @Test
        public String toString(){
            return ""Hello World"";
        }
",non-flaky,5
98352,Kong_unirest-java,JsonObjectMapperTest.getDate,"    @Test
        public Date getDate() {
            return date;
        }
",non-flaky,5
98353,Kong_unirest-java,ConsumerTest.tearDown,"    @AfterEach
    public void tearDown() {
        super.tearDown();
        asyncDone = false;
        status = 0;
        File file = test.toFile();
        if(file.exists()){
            file.delete();
        }
    }
",non-flaky,5
98354,Kong_unirest-java,AsObjectTest.writeValue,"    @Test
        public String writeValue(Object value) {
            writeWasCalled = true;
            return new Gson().toJson(value);
        }
",non-flaky,5
98355,Kong_unirest-java,ShutDownHooksTest.setUp,"    @Override @BeforeEach
    public void setUp() {
        super.setUp();
        clearUnirestHooks();
    }
",non-flaky,5
98356,Kong_unirest-java,AsGenericTypeTest.getSomeTees,"    @Test
        public T getSomeTees() {
            return someTees;
        }
",non-flaky,5
98357,Kong_unirest-java,CallbackFutureTest.completed,"    @Test @Disabled
                        public void completed(HttpResponse<JsonNode> response) {
                            throw new UnirestException(""Failure!"");
                        }
",non-flaky,5
98358,Kong_unirest-java,CallbackFutureTest.completed,"    @Test @Disabled
                    public void completed(HttpResponse<JsonNode> response) {
                        throw new UnirestException(""Failure!"");
                    }
",non-flaky,5
98359,Kong_unirest-java,DownloadProgressTest.setUp,"    @BeforeEach
    public void setUp() {
        super.setUp();
        this.monitor = new TestMonitor();
    }
",non-flaky,5
98360,Kong_unirest-java,PostRequestHandlersTest.tearDown,"    @AfterEach
    public void tearDown() {
        super.tearDown();
        captured = null;
    }
",non-flaky,5
98361,Kong_unirest-java,PostRequestHandlersTest.accept,"    @Test
        public void accept(HttpResponse<?> httpResponse) {

            this.httpResponse = httpResponse;
        }
",non-flaky,5
98362,Kong_unirest-java,AsFileTest.tearDown,"    @Override @AfterEach
    public void tearDown() {
        try {
            Files.delete(test);
        } catch (Exception ignored) { }
    }
",non-flaky,5
98363,Kong_unirest-java,CachingAlternativeTest.invalidate,"    @Test
        public void invalidate() {
            regular.invalidateAll();
            async.invalidateAll();
        }
",non-flaky,5
98364,Kong_unirest-java,ProxyTest.tearDown,"    @AfterEach
    public void tearDown() {
        super.tearDown();
        Unirest.shutDown(true);
        JankyProxy.shutdown();
    }
",non-flaky,5
98365,Kong_unirest-java,InterceptorTest.setUp,"    @BeforeEach
    public void setUp() {
        super.setUp();
        interceptor = new UniInterceptor(""x-custom"", ""foo"");
    }
",non-flaky,5
98366,Kong_unirest-java,InterceptorTest.onRequest,"    @Test
            public void onRequest(HttpRequest<?> request, Config config) {
                request.getBody().ifPresent(b ->
                        b.multiParts().forEach(part ->
                                values.add(part.toString())));
            }
",non-flaky,5
98367,Kong_unirest-java,CustomObjectMapperTest.setUp,"    @BeforeEach
    public void setUp() {
        super.setUp();
        customOm = Mockito.spy(JsonObjectMapper.class);
    }
",non-flaky,5
98368,Kong_unirest-java,UploadProgressTest.setUp,"    @Override @BeforeEach
    public void setUp() {
        super.setUp();
        this.monitor = new TestMonitor();
    }
",non-flaky,5
156,opensource4you_astraea,MetricsTest.testBytes,"@Test
void testBytes() throws InterruptedException {
    final CountDownLatch countDownLatch = new CountDownLatch(1);
    final Metrics metrics = new Metrics();
    final LongAdder longAdder = new LongAdder();
    final long input = 100;
    final int loopCount = 10000;
    Thread adder = new Thread(() -> {
        try {
            countDownLatch.await();
        } catch (InterruptedException ignore) {
        }
        for (int i = 0; i < loopCount; ++i) {
            metrics.addBytes(input);
        }
    });
    Thread getter = new Thread(() -> {
        try {
            countDownLatch.await();
        } catch (InterruptedException ignore) {
        }
        for (int i = 0; i < loopCount; ++i) {
            longAdder.add(metrics.bytesThenReset());
        }
    });
    adder.start();
    getter.start();
    countDownLatch.countDown();
    adder.join();
    longAdder.add(metrics.bytesThenReset());
    Assertions.assertEquals(loopCount * input, longAdder.sum());
}",concurrency,1
110830,opensource4you_astraea,ArgumentUtilTest.testParse,"  @Test
  public void testParse() {
    var param =
        ArgumentUtil.parseArgument(new FakeParameter(), new String[] {""--require"", ""require""});
    Assertions.assertEquals(""require"", param.require);
  }
",non-flaky,5
110831,opensource4you_astraea,ArgumentUtilTest.testRequired,"  @Test
  public void testRequired() {
    Assertions.assertThrows(
        ParameterException.class,
        () -> ArgumentUtil.parseArgument(new FakeParameter(), new String[] {}));
  }
",non-flaky,5
110832,opensource4you_astraea,ArgumentUtilTest.testLongPositive,"  @Test
  public void testLongPositive() {
    var param =
        ArgumentUtil.parseArgument(
            new FakeParameter(), new String[] {""--require"", ""require"", ""--longPositive"", ""1000""});

    Assertions.assertEquals(1000, param.longPositive);
    Assertions.assertThrows(
        ParameterException.class,
        () ->
            ArgumentUtil.parseArgument(
                new FakeParameter(), new String[] {""--require"", ""require"", ""--longPositive"", ""0""}));
  }
",non-flaky,5
110833,opensource4you_astraea,ArgumentUtilTest.testNotNegative,"  @Test
  public void testNotNegative() {
    FakeParameter param =
        ArgumentUtil.parseArgument(
            new FakeParameter(),
            new String[] {""--require"", ""require"", ""--longNotNegative"", ""1000""});

    Assertions.assertEquals(1000, param.longNotNegative);
    Assertions.assertThrows(
        ParameterException.class,
        () ->
            ArgumentUtil.parseArgument(
                new FakeParameter(),
                new String[] {""--require"", ""require"", ""--longNotNegative"", ""-1""}));
  }
",non-flaky,5
110834,opensource4you_astraea,ArgumentUtilTest.testDurationConvert,"  @Test
  public void testDurationConvert() {
    FakeParameter param =
        ArgumentUtil.parseArgument(
            new FakeParameter(),
            new String[] {""--require"", ""require"", ""--durationConvert"", ""1000""});

    Assertions.assertEquals(Duration.ofSeconds(1000), param.durationConvert);
  }
",non-flaky,5
110835,opensource4you_astraea,ArgumentUtilTest.testSetConverter,"  @Test
  public void testSetConverter() {
    FakeParameter param =
        ArgumentUtil.parseArgument(
            new FakeParameter(),
            new String[] {""--require"", ""require"", ""--setConverter"", ""1"", ""1"", ""2""});

    Assertions.assertEquals(Set.of(""1"", ""2""), param.setConverter);
  }
",non-flaky,5
